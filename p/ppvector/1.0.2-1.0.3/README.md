# Comparing `tmp/ppvector-1.0.2-py3-none-any.whl.zip` & `tmp/ppvector-1.0.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,35 +1,35 @@
-Zip file size: 63389 bytes, number of entries: 33
--rw-rw-rw-  2.0 fat      134 b- defN 24-Apr-27 06:49 ppvector/__init__.py
--rw-rw-rw-  2.0 fat    17739 b- defN 24-Jan-10 12:55 ppvector/predict.py
--rw-rw-rw-  2.0 fat    34807 b- defN 24-Apr-21 12:12 ppvector/trainer.py
+Zip file size: 64337 bytes, number of entries: 33
+-rw-rw-rw-  2.0 fat      134 b- defN 24-May-03 03:43 ppvector/__init__.py
+-rw-rw-rw-  2.0 fat    17483 b- defN 24-May-02 09:44 ppvector/predict.py
+-rw-rw-rw-  2.0 fat    36494 b- defN 24-May-02 12:43 ppvector/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/data_utils/__init__.py
 -rw-rw-rw-  2.0 fat    22221 b- defN 24-Apr-27 05:31 ppvector/data_utils/audio.py
--rw-rw-rw-  2.0 fat      909 b- defN 23-Feb-15 13:35 ppvector/data_utils/collate_fn.py
--rw-rw-rw-  2.0 fat     3771 b- defN 23-Aug-29 13:07 ppvector/data_utils/featurizer.py
--rw-rw-rw-  2.0 fat     5892 b- defN 23-Sep-13 12:01 ppvector/data_utils/reader.py
+-rw-rw-rw-  2.0 fat      947 b- defN 24-May-02 09:35 ppvector/data_utils/collate_fn.py
+-rw-rw-rw-  2.0 fat     3919 b- defN 24-May-02 09:36 ppvector/data_utils/featurizer.py
+-rw-rw-rw-  2.0 fat     7419 b- defN 24-May-02 09:47 ppvector/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     1581 b- defN 23-Aug-29 13:07 ppvector/data_utils/spec_aug.py
 -rw-rw-rw-  2.0 fat     4713 b- defN 24-Jan-06 11:55 ppvector/data_utils/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-16 05:03 ppvector/metric/__init__.py
 -rw-rw-rw-  2.0 fat     1208 b- defN 23-Aug-29 13:07 ppvector/metric/metrics.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-05 11:08 ppvector/models/__init__.py
 -rw-rw-rw-  2.0 fat    12745 b- defN 23-Aug-29 13:07 ppvector/models/campplus.py
--rw-rw-rw-  2.0 fat    11077 b- defN 23-Aug-29 13:07 ppvector/models/ecapa_tdnn.py
+-rw-rw-rw-  2.0 fat    10987 b- defN 24-May-02 09:33 ppvector/models/ecapa_tdnn.py
 -rw-rw-rw-  2.0 fat    10135 b- defN 23-Aug-29 13:07 ppvector/models/eres2net.py
 -rw-rw-rw-  2.0 fat     3731 b- defN 24-Jan-10 12:55 ppvector/models/fc.py
 -rw-rw-rw-  2.0 fat     9521 b- defN 24-Jan-10 12:55 ppvector/models/loss.py
 -rw-rw-rw-  2.0 fat     5279 b- defN 23-Aug-29 13:07 ppvector/models/pooling.py
 -rw-rw-rw-  2.0 fat     6843 b- defN 23-Aug-29 13:07 ppvector/models/res2net.py
 -rw-rw-rw-  2.0 fat     5662 b- defN 23-Aug-29 13:07 ppvector/models/resnet_se.py
 -rw-rw-rw-  2.0 fat     3459 b- defN 23-Aug-29 13:07 ppvector/models/tdnn.py
 -rw-rw-rw-  2.0 fat     5049 b- defN 23-Aug-05 13:45 ppvector/models/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2839 b- defN 22-Nov-04 14:44 ppvector/utils/logger.py
 -rw-rw-rw-  2.0 fat     1385 b- defN 24-Jan-15 12:44 ppvector/utils/record.py
 -rw-rw-rw-  2.0 fat     3399 b- defN 24-Jan-17 14:13 ppvector/utils/scheduler.py
 -rw-rw-rw-  2.0 fat     2790 b- defN 23-Mar-16 11:21 ppvector/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    33186 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        9 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2709 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/RECORD
-33 files, 224443 bytes uncompressed, 59073 bytes compressed:  73.7%
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-May-03 03:43 ppvector-1.0.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    34237 b- defN 24-May-03 03:43 ppvector-1.0.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-03 03:43 ppvector-1.0.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 24-May-03 03:43 ppvector-1.0.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2709 b- defN 24-May-03 03:43 ppvector-1.0.3.dist-info/RECORD
+33 files, 228548 bytes uncompressed, 60021 bytes compressed:  73.7%
```

## zipnote {}

```diff
@@ -78,23 +78,23 @@
 
 Filename: ppvector/utils/scheduler.py
 Comment: 
 
 Filename: ppvector/utils/utils.py
 Comment: 
 
-Filename: ppvector-1.0.2.dist-info/LICENSE
+Filename: ppvector-1.0.3.dist-info/LICENSE
 Comment: 
 
-Filename: ppvector-1.0.2.dist-info/METADATA
+Filename: ppvector-1.0.3.dist-info/METADATA
 Comment: 
 
-Filename: ppvector-1.0.2.dist-info/WHEEL
+Filename: ppvector-1.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: ppvector-1.0.2.dist-info/top_level.txt
+Filename: ppvector-1.0.3.dist-info/top_level.txt
 Comment: 
 
-Filename: ppvector-1.0.2.dist-info/RECORD
+Filename: ppvector-1.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ppvector/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "1.0.2"
+__version__ = "1.0.3"
 # 项目支持的模型
 SUPPORT_MODEL = ['ERes2Net', 'CAMPPlus', 'EcapaTdnn', 'Res2Net', 'ResNetSE', 'TDNN']
```

## ppvector/predict.py

```diff
@@ -222,21 +222,17 @@
         :param audio_data: 需要识别的数据，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整并带格式的字节文件
         :param sample_rate: 如果传入的事numpy数据，需要指定采样率
         :return: 声纹特征向量
         """
         # 加载音频文件，并进行预处理
         input_data = self._load_audio(audio_data=audio_data, sample_rate=sample_rate)
         input_data = paddle.to_tensor(input_data.samples, dtype=paddle.float32).unsqueeze(0)
-        input_len_ratio = paddle.to_tensor([1], dtype=paddle.float32)
-        audio_feature, _ = self._audio_featurizer(input_data, input_len_ratio)
+        audio_feature = self._audio_featurizer(input_data)
         # 执行预测
-        if self.configs.use_model == 'EcapaTdnn':
-            feature = self.predictor([audio_feature, input_len_ratio]).numpy()[0]
-        else:
-            feature = self.predictor(audio_feature).numpy()[0]
+        feature = self.predictor(audio_feature).numpy()[0]
         return feature
 
     def predict_batch(self, audios_data, sample_rate=16000):
         """预测一批音频的特征
 
         :param audios_data: 需要预测音频的路径
         :param sample_rate: 如果传入的事numpy数据，需要指定采样率
@@ -248,25 +244,25 @@
             input_data = self._load_audio(audio_data=audio_data, sample_rate=sample_rate)
             audios_data1.append(input_data.samples)
         # 找出音频长度最长的
         batch = sorted(audios_data1, key=lambda a: a.shape[0], reverse=True)
         max_audio_length = batch[0].shape[0]
         batch_size = len(batch)
         # 以最大的长度创建0张量
-        inputs = np.zeros((batch_size, max_audio_length), dtype='float32')
+        inputs = np.zeros((batch_size, max_audio_length), dtype=np.float32)
         input_lens_ratio = []
         for x in range(batch_size):
             tensor = audios_data1[x]
             seq_length = tensor.shape[0]
             # 将数据插入都0张量中，实现了padding
             inputs[x, :seq_length] = tensor[:]
             input_lens_ratio.append(seq_length / max_audio_length)
-        audios_data = paddle.to_tensor(inputs, dtype=paddle.float32)
+        inputs = paddle.to_tensor(inputs, dtype=paddle.float32)
         input_lens_ratio = paddle.to_tensor(input_lens_ratio, dtype=paddle.float32)
-        audio_feature, _ = self._audio_featurizer(audios_data, input_lens_ratio)
+        audio_feature = self._audio_featurizer(inputs, input_lens_ratio)
         # 执行预测
         if self.configs.use_model == 'EcapaTdnn':
             features = self.predictor([audio_feature, input_lens_ratio]).numpy()
         else:
             features = self.predictor(audio_feature).numpy()
         return features
```

## ppvector/trainer.py

```diff
@@ -17,15 +17,15 @@
 from sklearn.metrics.pairwise import cosine_similarity
 from tqdm import tqdm
 from visualdl import LogWriter
 
 from ppvector import SUPPORT_MODEL, __version__
 from ppvector.data_utils.collate_fn import collate_fn
 from ppvector.data_utils.featurizer import AudioFeaturizer
-from ppvector.data_utils.reader import CustomDataset
+from ppvector.data_utils.reader import PPVectorDataset
 from ppvector.data_utils.spec_aug import SpecAug
 from ppvector.metric.metrics import compute_fnr_fpr, compute_eer, compute_dcf
 from ppvector.models.campplus import CAMPPlus
 from ppvector.models.ecapa_tdnn import EcapaTdnn
 from ppvector.models.eres2net import ERes2Net
 from ppvector.models.fc import SpeakerIdentification
 from ppvector.models.loss import AAMLoss, AMLoss, ARMLoss, CELoss, SubCenterLoss, SphereFace2
@@ -58,82 +58,117 @@
             with open(configs, 'r', encoding='utf-8') as f:
                 configs = yaml.load(f.read(), Loader=yaml.FullLoader)
             print_arguments(configs=configs)
         self.configs = dict_to_object(configs)
         assert self.configs.use_model in SUPPORT_MODEL, f'没有该模型：{self.configs.use_model}'
         self.model = None
         self.backbone = None
+        self.audio_featurizer = None
+        self.train_dataset = None
+        self.train_loader = None
+        self.enroll_dataset = None
         self.enroll_loader = None
+        self.trials_dataset = None
         self.trials_loader = None
         self.margin_scheduler = None
         self.amp_scaler = None
-        # 获取特征器
-        self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
-                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
         self.spec_aug = SpecAug(**self.configs.dataset_conf.get('spec_aug_args', {}))
         if platform.system().lower() == 'windows':
             self.configs.dataset_conf.dataLoader.num_workers = 0
             logger.warning('Windows系统不支持多线程读取数据，已自动关闭！')
         self.max_step, self.train_step = None, None
         self.train_loss, self.train_acc = None, None
         self.train_eta_sec = None
         self.eval_eer, self.eval_min_dcf, self.eval_threshold = None, None, None
         self.test_log_step, self.train_log_step = 0, 0
         self.stop_train, self.stop_eval = False, False
 
     def __setup_dataloader(self, is_train=False):
+        # 获取特征器
+        self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
+                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
         if is_train:
-            self.train_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.train_list,
-                                               do_vad=self.configs.dataset_conf.do_vad,
-                                               max_duration=self.configs.dataset_conf.max_duration,
-                                               min_duration=self.configs.dataset_conf.min_duration,
-                                               sample_rate=self.configs.dataset_conf.sample_rate,
-                                               aug_conf=self.configs.dataset_conf.aug_conf,
-                                               num_speakers=self.configs.model_conf.classifier.num_speakers,
-                                               use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
-                                               target_dB=self.configs.dataset_conf.target_dB,
-                                               mode='train')
+            self.train_dataset = PPVectorDataset(data_list_path=self.configs.dataset_conf.train_list,
+                                                 audio_featurizer=self.audio_featurizer,
+                                                 do_vad=self.configs.dataset_conf.do_vad,
+                                                 max_duration=self.configs.dataset_conf.max_duration,
+                                                 min_duration=self.configs.dataset_conf.min_duration,
+                                                 sample_rate=self.configs.dataset_conf.sample_rate,
+                                                 aug_conf=self.configs.dataset_conf.aug_conf,
+                                                 num_speakers=self.configs.model_conf.classifier.num_speakers,
+                                                 use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                                 target_dB=self.configs.dataset_conf.target_dB,
+                                                 mode='train')
             # 设置支持多卡训练
             train_sampler = None
             if paddle.distributed.get_world_size() > 1:
                 # 设置支持多卡训练
                 train_sampler = DistributedBatchSampler(dataset=self.train_dataset,
                                                         batch_size=self.configs.dataset_conf.dataLoader.batch_size,
                                                         shuffle=True)
             self.train_loader = DataLoader(dataset=self.train_dataset,
                                            collate_fn=collate_fn,
                                            shuffle=(train_sampler is None),
                                            batch_sampler=train_sampler,
                                            **self.configs.dataset_conf.dataLoader)
         # 获取评估的注册数据和检验数据
-        self.enroll_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.enroll_list,
-                                            do_vad=self.configs.dataset_conf.do_vad,
-                                            max_duration=self.configs.dataset_conf.eval_conf.max_duration,
-                                            min_duration=self.configs.dataset_conf.min_duration,
-                                            sample_rate=self.configs.dataset_conf.sample_rate,
-                                            use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
-                                            target_dB=self.configs.dataset_conf.target_dB,
-                                            mode='eval')
+        self.enroll_dataset = PPVectorDataset(data_list_path=self.configs.dataset_conf.enroll_list,
+                                              audio_featurizer=self.audio_featurizer,
+                                              do_vad=self.configs.dataset_conf.do_vad,
+                                              max_duration=self.configs.dataset_conf.eval_conf.max_duration,
+                                              min_duration=self.configs.dataset_conf.min_duration,
+                                              sample_rate=self.configs.dataset_conf.sample_rate,
+                                              use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                              target_dB=self.configs.dataset_conf.target_dB,
+                                              mode='eval')
         self.enroll_loader = DataLoader(dataset=self.enroll_dataset,
                                         collate_fn=collate_fn,
                                         batch_size=self.configs.dataset_conf.eval_conf.batch_size,
                                         num_workers=self.configs.dataset_conf.dataLoader.num_workers)
-        self.trials_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.trials_list,
-                                            do_vad=self.configs.dataset_conf.do_vad,
-                                            max_duration=self.configs.dataset_conf.eval_conf.max_duration,
-                                            min_duration=self.configs.dataset_conf.min_duration,
-                                            sample_rate=self.configs.dataset_conf.sample_rate,
-                                            use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
-                                            target_dB=self.configs.dataset_conf.target_dB,
-                                            mode='eval')
+        self.trials_dataset = PPVectorDataset(data_list_path=self.configs.dataset_conf.trials_list,
+                                              audio_featurizer=self.audio_featurizer,
+                                              do_vad=self.configs.dataset_conf.do_vad,
+                                              max_duration=self.configs.dataset_conf.eval_conf.max_duration,
+                                              min_duration=self.configs.dataset_conf.min_duration,
+                                              sample_rate=self.configs.dataset_conf.sample_rate,
+                                              use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                              target_dB=self.configs.dataset_conf.target_dB,
+                                              mode='eval')
         self.trials_loader = DataLoader(dataset=self.trials_dataset,
                                         collate_fn=collate_fn,
                                         batch_size=self.configs.dataset_conf.eval_conf.batch_size,
                                         num_workers=self.configs.dataset_conf.dataLoader.num_workers)
 
+    # 提取特征保存文件
+    def extract_features(self, save_dir='dataset/features'):
+        self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
+                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
+        for i, data_list in enumerate([self.configs.dataset_conf.train_list,
+                                       self.configs.dataset_conf.enroll_list,
+                                       self.configs.dataset_conf.trials_list]):
+            # 获取测试数据
+            test_dataset = PPVectorDataset(data_list_path=data_list,
+                                           audio_featurizer=self.audio_featurizer,
+                                           do_vad=self.configs.dataset_conf.do_vad,
+                                           sample_rate=self.configs.dataset_conf.sample_rate,
+                                           use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                           target_dB=self.configs.dataset_conf.target_dB,
+                                           mode='extract_feature')
+            save_data_list = data_list.replace('.txt', '_features.txt')
+            with open(save_data_list, 'w', encoding='utf-8') as f:
+                for i in tqdm(range(len(test_dataset))):
+                    feature, label = test_dataset[i]
+                    feature = feature.numpy()
+                    label = int(label)
+                    save_path = os.path.join(save_dir, str(label), f'{int(time.time() * 1000)}.npy').replace('\\', '/')
+                    os.makedirs(os.path.dirname(save_path), exist_ok=True)
+                    np.save(save_path, feature)
+                    f.write(f'{save_path}\t{label}\n')
+            logger.info(f'{data_list}列表中的数据已提取特征完成，新列表为：{save_data_list}')
+
     def __setup_model(self, input_size, is_train=False):
         # 获取模型
         if self.configs.use_model == 'ERes2Net':
             self.backbone = ERes2Net(input_size=input_size, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'CAMPPlus':
             self.backbone = CAMPPlus(input_size=input_size, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'EcapaTdnn':
@@ -221,15 +256,15 @@
                                                            learning_rate=self.scheduler,
                                                            weight_decay=self.configs.optimizer_conf.weight_decay)
             else:
                 raise Exception(f'不支持优化方法：{optimizer}')
         else:
             # 不训练模型不包含分类器
             self.model = nn.Sequential(self.backbone)
-        summary(self.model, (1, 98, self.audio_featurizer.feature_dim))
+        summary(self.model, (1, 98, input_size))
 
     def __load_pretrained(self, pretrained_model):
         # 加载预训练模型
         if pretrained_model is not None:
             if os.path.isdir(pretrained_model):
                 pretrained_model = os.path.join(pretrained_model, 'model.pdparams')
             assert os.path.exists(pretrained_model), f"{pretrained_model} 模型不存在！"
@@ -320,25 +355,21 @@
                 shutil.rmtree(old_model_path)
         logger.info('已保存模型：{}'.format(model_path))
 
     def __train_epoch(self, epoch_id, save_model_path, local_rank, writer):
         train_times, accuracies, loss_sum = [], [], []
         start = time.time()
         use_loss = self.configs.loss_conf.get('use_loss', 'AAMLoss')
-        for batch_id, (audio, label, input_lens_ratio) in enumerate(self.train_loader()):
+        for batch_id, (features, label, input_lens) in enumerate(self.train_loader()):
             if self.stop_train: break
-            features, _ = self.audio_featurizer(audio, input_lens_ratio)
             if self.configs.dataset_conf.use_spec_aug:
                 features = self.spec_aug(features)
             # 执行模型计算，是否开启自动混合精度
             with paddle.amp.auto_cast(enable=self.configs.train_conf.enable_amp, level='O1'):
-                if self.configs.use_model == 'EcapaTdnn':
-                    output = self.model([features, input_lens_ratio])
-                else:
-                    output = self.model(features)
+                output = self.model(features)
             # 计算损失值
             los = self.loss(output, label)
             # 是否开启自动混合精度
             if self.configs.train_conf.enable_amp:
                 # loss缩放，乘以系数loss_scaling
                 scaled = self.amp_scaler.scale(los)
                 scaled.backward()
@@ -502,35 +533,27 @@
             eval_model = self.model._layers if len(self.model._layers) == 1 else self.model._layers[0]
         else:
             eval_model = self.model if len(self.model) == 1 else self.model[0]
 
         # 获取注册的声纹特征和标签
         enroll_features, enroll_labels = None, None
         with paddle.no_grad():
-            for batch_id, (audio, label, input_lens_ratio) in enumerate(tqdm(self.enroll_loader, desc="注册音频声纹特征")):
+            for batch_id, (audio_features, label, input_lens) in enumerate(tqdm(self.enroll_loader, desc="注册音频声纹特征")):
                 if self.stop_eval: break
-                audio_features, _ = self.audio_featurizer(audio, input_lens_ratio)
-                if self.configs.use_model == 'EcapaTdnn':
-                    feature = eval_model([audio_features, input_lens_ratio]).numpy()
-                else:
-                    feature = eval_model(audio_features).numpy()
+                feature = eval_model(audio_features).numpy()
                 label = label.numpy()
                 # 存放特征
                 enroll_features = np.concatenate((enroll_features, feature)) if enroll_features is not None else feature
                 enroll_labels = np.concatenate((enroll_labels, label)) if enroll_labels is not None else label
         # 获取检验的声纹特征和标签
         trials_features, trials_labels = None, None
         with paddle.no_grad():
-            for batch_id, (audio, label, input_lens_ratio) in enumerate(tqdm(self.trials_loader, desc="验证音频声纹特征")):
+            for batch_id, (audio_features, label, input_lens) in enumerate(tqdm(self.trials_loader, desc="验证音频声纹特征")):
                 if self.stop_eval: break
-                audio_features, _ = self.audio_featurizer(audio, input_lens_ratio)
-                if self.configs.use_model == 'EcapaTdnn':
-                    feature = eval_model([audio_features, input_lens_ratio]).numpy()
-                else:
-                    feature = eval_model(audio_features).numpy()
+                feature = eval_model(audio_features).numpy()
                 label = label.numpy()
                 # 存放特征
                 trials_features = np.concatenate((trials_features, feature)) if trials_features is not None else feature
                 trials_labels = np.concatenate((trials_labels, label)) if trials_labels is not None else label
         self.model.train()
         enroll_labels = enroll_labels.astype(np.int32)
         trials_labels = trials_labels.astype(np.int32)
```

## ppvector/data_utils/collate_fn.py

```diff
@@ -1,24 +1,23 @@
-import numpy as np
+import paddle
 
 
 # 对一个batch的数据处理
 def collate_fn(batch):
     # 找出音频长度最长的
-    batch = sorted(batch, key=lambda sample: sample[0].shape[0], reverse=True)
-    max_audio_length = batch[0][0].shape[0]
-    batch_size = len(batch)
+    batch_sorted = sorted(batch, key=lambda sample: sample[0].shape[0], reverse=True)
+    freq_size = batch_sorted[0][0].shape[1]
+    max_freq_length = batch_sorted[0][0].shape[0]
+    batch_size = len(batch_sorted)
     # 以最大的长度创建0张量
-    inputs = np.zeros((batch_size, max_audio_length), dtype='float32')
-    input_lens_ratio = []
-    labels = []
+    features = paddle.zeros((batch_size, max_freq_length, freq_size), dtype=paddle.float32)
+    input_lens, labels = [], []
     for x in range(batch_size):
-        sample = batch[x]
-        tensor = sample[0]
-        labels.append(sample[1])
+        tensor, label = batch[x]
         seq_length = tensor.shape[0]
         # 将数据插入都0张量中，实现了padding
-        inputs[x, :seq_length] = tensor[:]
-        input_lens_ratio.append(seq_length/max_audio_length)
-    input_lens_ratio = np.array(input_lens_ratio, dtype='float32')
-    labels = np.array(labels, dtype='int64')
-    return inputs, labels, input_lens_ratio
+        features[x, :seq_length, :] = tensor[:, :]
+        labels.append(label)
+        input_lens.append(seq_length)
+    labels = paddle.to_tensor(labels, dtype=paddle.int64)
+    input_lens = paddle.to_tensor(input_lens, dtype=paddle.int64)
+    return features, labels, input_lens
```

## ppvector/data_utils/featurizer.py

```diff
@@ -26,39 +26,42 @@
         elif feature_method == 'MFCC':
             self.feat_fun = MFCC(**method_args)
         elif feature_method == 'Fbank':
             self.feat_fun = KaldiFbank(**method_args)
         else:
             raise Exception(f'预处理方法 {self._feature_method} 不存在!')
 
-    def forward(self, waveforms, input_lens_ratio):
+    def forward(self, waveforms, input_lens_ratio=None):
         """从AudioSegment中提取音频特征
 
         :param waveforms: Audio segment to extract features from.
         :type waveforms: AudioSegment
         :param input_lens_ratio: input length ratio
         :type input_lens_ratio: tensor
         :return: Spectrogram audio feature in 2darray.
         :rtype: ndarray
         """
+        if len(waveforms.shape) == 1:
+            waveforms = waveforms.unsqueeze(0)
         feature = self.feat_fun(waveforms)
         feature = feature.transpose([0, 2, 1])
         # 归一化
         feature = feature - feature.mean(1, keepdim=True)
-        # 对掩码比例进行扩展
-        input_lens = (input_lens_ratio * feature.shape[1]).astype(paddle.int32)
-        mask_lens = input_lens.unsqueeze(1)
-        # 生成掩码张量
-        idxs = paddle.arange(feature.shape[1])
-        idxs = idxs.tile([feature.shape[0], 1])
-        mask = idxs < mask_lens
-        mask = mask.unsqueeze(-1)
-        # 对特征进行掩码操作
-        feature_masked = paddle.where(mask, feature, paddle.zeros_like(feature))
-        return feature_masked, input_lens
+        if input_lens_ratio is not None:
+            # 对掩码比例进行扩展
+            input_lens = (input_lens_ratio * feature.shape[1]).astype(paddle.int32)
+            mask_lens = input_lens.unsqueeze(1)
+            # 生成掩码张量
+            idxs = paddle.arange(feature.shape[1])
+            idxs = idxs.tile([feature.shape[0], 1])
+            mask = idxs < mask_lens
+            mask = mask.unsqueeze(-1)
+            # 对特征进行掩码操作
+            feature = paddle.where(mask, feature, paddle.zeros_like(feature))
+        return feature
 
     @property
     def feature_dim(self):
         """返回特征大小
 
         :return: 特征大小
         :rtype: int
```

## ppvector/data_utils/reader.py

```diff
@@ -1,90 +1,118 @@
 import os
 import random
 
 import numpy as np
+import paddle
 from paddle.io import Dataset
 
 from ppvector.data_utils.audio import AudioSegment
+from ppvector.data_utils.featurizer import AudioFeaturizer
 from ppvector.utils.logger import setup_logger
 
 logger = setup_logger(__name__)
 
 
 # 音频数据加载器
-class CustomDataset(Dataset):
+class PPVectorDataset(Dataset):
     def __init__(self,
                  data_list_path,
+                 audio_featurizer: AudioFeaturizer,
                  do_vad=True,
                  max_duration=3,
                  min_duration=0.5,
                  mode='train',
                  sample_rate=16000,
                  aug_conf={},
                  num_speakers=1000,
                  use_dB_normalization=True,
                  target_dB=-20):
         """音频数据加载器
 
         Args:
             data_list_path: 包含音频路径和标签的数据列表文件的路径
+            audio_featurizer: 声纹特征提取器
             do_vad: 是否对音频进行语音活动检测（VAD）来裁剪静音部分
             max_duration: 最长的音频长度，大于这个长度会裁剪掉
             min_duration: 过滤最短的音频长度
             aug_conf: 用于指定音频增强的配置
             mode: 数据集模式。在训练模式下，数据集可能会进行一些数据增强的预处理
             sample_rate: 采样率
             num_speakers: 总说话人数量
             use_dB_normalization: 是否对音频进行音量归一化
             target_dB: 音量归一化的大小
         """
-        super(CustomDataset, self).__init__()
+        super(PPVectorDataset, self).__init__()
+        assert mode in ['train', 'eval', 'create_data', 'extract_feature']
         self.do_vad = do_vad
         self.max_duration = max_duration
         self.min_duration = min_duration
         self.mode = mode
         self._target_sample_rate = sample_rate
         self._use_dB_normalization = use_dB_normalization
         self._target_dB = target_dB
         self.aug_conf = aug_conf
         self.num_speakers = num_speakers
         self.noises_path = None
+        # 获取特征器
+        self.audio_featurizer = audio_featurizer
+        # 获取特征裁剪的大小
+        self.max_feature_len = self.get_crop_feature_len()
         # 获取数据列表
         with open(data_list_path, 'r') as f:
             self.lines = f.readlines()
 
     def __getitem__(self, idx):
         # 分割音频路径和标签
-        audio_path, spk_id = self.lines[idx].strip().split('\t')
+        data_path, spk_id = self.lines[idx].strip().split('\t')
         spk_id = int(spk_id)
-        # 读取音频
-        audio_segment = AudioSegment.from_file(audio_path)
-        # 裁剪静音
-        if self.do_vad:
-            audio_segment.vad()
-        # 数据太短不利于训练
-        if self.mode == 'train':
-            if audio_segment.duration < self.min_duration:
-                return self.__getitem__(idx + 1 if idx < len(self.lines) - 1 else 0)
-        # 重采样
-        if audio_segment.sample_rate != self._target_sample_rate:
-            audio_segment.resample(self._target_sample_rate)
-        # 音频增强
-        if self.mode == 'train':
-            audio_segment, spk_id = self.augment_audio(audio_segment, spk_id, **self.aug_conf)
-        # decibel normalization
-        if self._use_dB_normalization:
-            audio_segment.normalize(target_db=self._target_dB)
-        # 裁剪需要的数据
-        audio_segment.crop(duration=self.max_duration, mode=self.mode)
-        return np.array(audio_segment.samples, dtype=np.float32), np.array(int(spk_id), dtype=np.int64)
+        # 如果后缀名为.npy的文件，那么直接读取
+        if data_path.endswith('.npy'):
+            feature = np.load(data_path)
+            if feature.shape[0] > self.max_feature_len:
+                crop_start = random.randint(0, feature.shape[0] - self.max_feature_len) if self.mode == 'eval' else 0
+                feature = feature[crop_start:crop_start + self.max_feature_len, :]
+            feature = paddle.to_tensor(feature, dtype=paddle.float32)
+        else:
+            # 读取音频
+            audio_segment = AudioSegment.from_file(data_path)
+            # 裁剪静音
+            if self.do_vad:
+                audio_segment.vad()
+            # 数据太短不利于训练
+            if self.mode == 'train':
+                if audio_segment.duration < self.min_duration:
+                    return self.__getitem__(idx + 1 if idx < len(self.lines) - 1 else 0)
+            # 重采样
+            if audio_segment.sample_rate != self._target_sample_rate:
+                audio_segment.resample(self._target_sample_rate)
+            # 音频增强
+            if self.mode == 'train':
+                audio_segment, spk_id = self.augment_audio(audio_segment, spk_id, **self.aug_conf)
+            # decibel normalization
+            if self._use_dB_normalization:
+                audio_segment.normalize(target_db=self._target_dB)
+            # 裁剪需要的数据
+            if self.mode != 'extract_feature' and audio_segment.duration > self.max_duration:
+                audio_segment.crop(duration=self.max_duration, mode=self.mode)
+            samples = paddle.to_tensor(audio_segment.samples, dtype=paddle.float32)
+            feature = self.audio_featurizer(samples)
+            feature = feature.squeeze(0)
+        spk_id = paddle.to_tensor(int(spk_id), dtype=paddle.int64)
+        return feature, spk_id
 
     def __len__(self):
         return len(self.lines)
 
+    def get_crop_feature_len(self):
+        samples = paddle.randn((1, self.max_duration * self._target_sample_rate))
+        feature = self.audio_featurizer(samples).squeeze(0)
+        freq_len = feature.shape[0]
+        return freq_len
+
     # 音频增强
     def augment_audio(self,
                       audio_segment,
                       spk_id,
                       speed_perturb=False,
                       speed_perturb_3_class=False,
                       volume_perturb=False,
```

## ppvector/models/ecapa_tdnn.py

```diff
@@ -238,29 +238,25 @@
             # Final linear transformation
             self.fc = Conv1d(in_channels=cat_channels * 2,
                              out_channels=self.embd_dim,
                              kernel_size=1)
         else:
             raise Exception(f'没有{pooling_type}池化层！')
 
-    def forward(self, x):
+    def forward(self, x, lengths=None):
         """
         Compute embeddings.
 
         Args:
             x (paddle.Tensor): Input data with shape (N, time, freq).
             lengths (paddle.Tensor, optional): Length proportions of batch length with shape (N). Defaults to None.
 
         Returns:
             paddle.Tensor: Output embeddings with shape (N, self.emb_size, 1)
         """
-        if isinstance(x, list):
-            x, lengths = x
-        else:
-            lengths = None
         x = x.transpose([0, 2, 1])
         xl = []
         for layer in self.blocks:
             try:
                 x = layer(x, lengths=lengths)
             except TypeError:
                 x = layer(x)
```

## Comparing `ppvector-1.0.2.dist-info/LICENSE` & `ppvector-1.0.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ppvector-1.0.2.dist-info/METADATA` & `ppvector-1.0.3.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ppvector
-Version: 1.0.2
+Version: 1.0.3
 Summary: Voice Print Recognition toolkit on PaddlePaddle
 Home-page: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle
 Download-URL: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
 Author: yeyupiaoling
 License: Apache License 2.0
 Keywords: Voice,paddle
 Classifier: Intended Audience :: Developers
@@ -164,29 +164,42 @@
 dataset/CN-Celeb2_flac/data/id11999/recitation-05-003.flac      2795
 dataset/CN-Celeb2_flac/data/id11999/recitation-04-017.flac      2795
 dataset/CN-Celeb2_flac/data/id11999/recitation-10-016.flac      2795
 dataset/CN-Celeb2_flac/data/id11999/recitation-09-001.flac      2795
 dataset/CN-Celeb2_flac/data/id11999/recitation-05-010.flac      2795
 ```
 
-# 修改预处理方法
+# 修改预处理方法（可选）
 
 配置文件中默认使用的是Fbank预处理方法，如果要使用其他预处理方法，可以修改配置文件中的安装下面方式修改，具体的值可以根据自己情况修改。如果不清楚如何设置参数，可以直接删除该部分，直接使用默认值。
 
 ```yaml
 # 数据预处理参数
 preprocess_conf:
   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC、Fbank
   feature_method: 'Fbank'
   # 设置API参数，更参数查看对应API，不清楚的可以直接删除该部分，直接使用默认值
   method_args:
     sr: 16000
     n_mels: 80
 ```
 
+# 提取特征（可选）
+
+在训练过程中，首先是要读取音频数据，然后提取特征，最后再进行训练。其中读取音频数据、提取特征也是比较消耗时间的，所以我们可以选择提前提取好取特征，训练模型的是就可以直接加载提取好的特征，这样训练速度会更快。这个提取特征是可选择，如果没有提取好的特征，训练模型的时候就会从读取音频数据，然后提取特征开始。提取特征步骤如下：
+
+1. 执行`extract_features.py`，提取特征，特征会保存在`dataset/features`目录下，并生成新的数据列表`train_list_features.txt`、`enroll_list_features.txt`和`trials_list_features.txt`。
+
+```shell
+python extract_features.py --configs=configs/cam++.yml --save_dir=dataset/features
+```
+
+2. 修改配置文件，将`dataset_conf.train_list`、`dataset_conf.enroll_list`和`dataset_conf.trials_list`修改为`train_list_features.txt`、`enroll_list_features.txt`和`trials_list_features.txt`。
+
+
 # 训练模型
 使用`train.py`训练模型，本项目支持多个音频预处理方式，通过`configs/ecapa_tdnn.yml`配置文件的参数`preprocess_conf.feature_method`可以指定，`MelSpectrogram`为梅尔频谱，`Spectrogram`为语谱图，`MFCC`梅尔频谱倒谱系数。通过参数`augment_conf_path`可以指定数据增强方式。训练过程中，会使用VisualDL保存训练日志，通过启动VisualDL可以随时查看训练结果，启动命令`visualdl --logdir=log --host 0.0.0.0`
 ```shell
 # 单卡训练
 CUDA_VISIBLE_DEVICES=0 python train.py
 # 多卡训练
 python -m paddle.distributed.launch --gpus '0,1' train.py
```

## Comparing `ppvector-1.0.2.dist-info/RECORD` & `ppvector-1.0.3.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-ppvector/__init__.py,sha256=nuwAt02ANJKWpNLCcQvX8UfLYxrRTI3OhFIZ_x5pb5g,134
-ppvector/predict.py,sha256=ZVnd7B5uC0gh0-2oGcr1EtkNcRmGYdxagVV0tAtSzuQ,17739
-ppvector/trainer.py,sha256=R_plMxX9HDT9C0xj1Sm1fbzBqXC5LQLP_O75sGo5_XM,34807
+ppvector/__init__.py,sha256=1Nqnj-dBG8NZOTjUbPp9NWTy0ddKc5o7d-vkgz53aw0,134
+ppvector/predict.py,sha256=yELfuPb6_bWbIsjbYdy_mYbVh9LoWNR5yzZBsIEHSio,17483
+ppvector/trainer.py,sha256=UWpENmFackuipoxsjGTb46UbhHSSb1cKInhi4CXFDB0,36494
 ppvector/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/data_utils/audio.py,sha256=If9HGHdfZeNjQgERNl8TTY8MFbkkWF0NKXq_RA5uTjs,22221
-ppvector/data_utils/collate_fn.py,sha256=tb5yu7VH69x1nGKxJUBoZ5UNsVgYxZ_zbth2YN52zPw,909
-ppvector/data_utils/featurizer.py,sha256=9j7nEQbe1f5udaxZwFIG6D_erTW3gMKuIZB2cd3-hMI,3771
-ppvector/data_utils/reader.py,sha256=3OJvJI_psix9duILZ7t0IzG3xK4SOXhIg0giRiH99v0,5892
+ppvector/data_utils/collate_fn.py,sha256=uzm-U3avFXzjJXbM6u5CfUJUr4WlIYcaz7WDmgetRys,947
+ppvector/data_utils/featurizer.py,sha256=wWgZoR0ZdTZ0Ux0byI4qT2Go86njSxFskQFz8xtipf0,3919
+ppvector/data_utils/reader.py,sha256=oCfYgQVVA1YHfWu_HjV4PDd64RNIAEhhbrOenNrVCbs,7419
 ppvector/data_utils/spec_aug.py,sha256=Qw32xZTw1f8Fk3Kw9aQt_5IKiH8k1giXbCX3XUzU1xw,1581
 ppvector/data_utils/utils.py,sha256=wripWTTPcEOgskVli8eTyHROs8wok8CItFd8uYpIBcU,4713
 ppvector/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/metric/metrics.py,sha256=BuQcf0E_6JUwbdFYKvzpb3rCAssizJjk_JvrQmLlfkA,1208
 ppvector/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/models/campplus.py,sha256=Xfq1O8zdf9sDjMsoQ-1E2rpScoMlaJjfErBxCyBytgM,12745
-ppvector/models/ecapa_tdnn.py,sha256=x6d4Hd4e7KKlUDZ66egQw4Zpu4V39krPgvGW1QL-Za0,11077
+ppvector/models/ecapa_tdnn.py,sha256=QNfiMvCjlvFC2msxsYdzQZOUt-n4X4KwZ5wIHA3euO4,10987
 ppvector/models/eres2net.py,sha256=5y39Ol5FOWZvKW803CGa5ZFux7GfkzfFzDsS3KwsAfc,10135
 ppvector/models/fc.py,sha256=dtMca714xkcaP_399V-1pzfBGUeR1cXpkQ3bhJj5jx4,3731
 ppvector/models/loss.py,sha256=6WSKIpRiUFktzkClZdvTae2GYxZKzS2LmtINgWX6unE,9521
 ppvector/models/pooling.py,sha256=WVJILqRl9YyghxsTUJR9fTwRA3XCr69Kns_08FPvCGA,5279
 ppvector/models/res2net.py,sha256=f2-cv1VQi6KkF8NETt2-BsLvVIlvjM2whUiBaCCeZtQ,6843
 ppvector/models/resnet_se.py,sha256=dmet6cJZvE3DGmfnlpT95exy-9cP2eeWiK-6AmtCH_Q,5662
 ppvector/models/tdnn.py,sha256=t_thrn6_UxV9xn4Hn6I3-l-V9_YxP8vH5qm7uvRtNiI,3459
 ppvector/models/utils.py,sha256=WFaGb66sSLalS-2yr9VIOnICrLjR8rGmiggGaWnZV_Y,5049
 ppvector/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/utils/logger.py,sha256=-ssorx8FlHA_wrd2Eq6f4HkOqaOG2YseBGvYAo8NXN8,2839
 ppvector/utils/record.py,sha256=S2sGoLPJrdRsrG7_ojNt4kwL05VNrOxnmnMOwNOZ9-0,1385
 ppvector/utils/scheduler.py,sha256=IZ1kU6S86hfnuZLfkepflStogDyYvRPEDMP4P_mwGvs,3399
 ppvector/utils/utils.py,sha256=zzBjiCYwNwxYGMfZyo_wubYp5MTmN3YRuFMKXZ65S7c,2790
-ppvector-1.0.2.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-ppvector-1.0.2.dist-info/METADATA,sha256=gmN0gr0-WDhrPOT-w97MBJzlwx4EY5hoIUqzPWGSkFQ,33186
-ppvector-1.0.2.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-ppvector-1.0.2.dist-info/top_level.txt,sha256=NywKjkr9phu2LhphLKQRNvdJUG8iJGaZQbe_HC0PhcQ,9
-ppvector-1.0.2.dist-info/RECORD,,
+ppvector-1.0.3.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+ppvector-1.0.3.dist-info/METADATA,sha256=T0dM1Hfh19MRd1N52CUZDJsO1ntDFWaelz66FK3VU9Q,34237
+ppvector-1.0.3.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+ppvector-1.0.3.dist-info/top_level.txt,sha256=NywKjkr9phu2LhphLKQRNvdJUG8iJGaZQbe_HC0PhcQ,9
+ppvector-1.0.3.dist-info/RECORD,,
```


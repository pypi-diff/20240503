# Comparing `tmp/ax-platform-0.3.7.tar.gz` & `tmp/ax-platform-0.4.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "ax-platform-0.3.7.tar", last modified: Fri Mar  1 16:28:04 2024, max compression
+gzip compressed data, was "ax-platform-0.4.0.tar", last modified: Thu May  2 22:09:20 2024, max compression
```

## Comparing `ax-platform-0.3.7.tar` & `ax-platform-0.4.0.tar`

### file list

```diff
@@ -1,763 +1,805 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.635032 ax-platform-0.3.7/
--rw-r--r--   0 runner    (1001) docker     (127)      407 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.flake8
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.519031 ax-platform-0.3.7/.github/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.531031 ax-platform-0.3.7/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (127)     1731 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/build-and-test.yml
--rw-r--r--   0 runner    (1001) docker     (127)     2550 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/cron.yml
--rw-r--r--   0 runner    (1001) docker     (127)      674 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/cron_pinned.yml
--rw-r--r--   0 runner    (1001) docker     (127)     1962 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/deploy.yml
--rw-r--r--   0 runner    (1001) docker     (127)     2263 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/reusable_test.yml
--rw-r--r--   0 runner    (1001) docker     (127)     1390 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/reusable_tutorials.yml
--rw-r--r--   0 runner    (1001) docker     (127)      382 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.github/workflows/tutorials.yml
--rw-r--r--   0 runner    (1001) docker     (127)     2092 2024-03-01 16:24:58.000000 ax-platform-0.3.7/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)      106 2024-03-01 16:24:58.000000 ax-platform-0.3.7/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-03-01 16:24:58.000000 ax-platform-0.3.7/CODE_OF_CONDUCT.md
--rw-r--r--   0 runner    (1001) docker     (127)     5428 2024-03-01 16:24:58.000000 ax-platform-0.3.7/CONTRIBUTING.md
--rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-03-01 16:24:58.000000 ax-platform-0.3.7/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)     8526 2024-03-01 16:28:04.635032 ax-platform-0.3.7/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     7877 2024-03-01 16:24:58.000000 ax-platform-0.3.7/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.531031 ax-platform-0.3.7/ax/
--rw-r--r--   0 runner    (1001) docker     (127)     1657 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.531031 ax-platform-0.3.7/ax/analysis/
--rw-r--r--   0 runner    (1001) docker     (127)      900 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/analysis/base_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)      688 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/analysis/base_plotly_visualization.py
--rw-r--r--   0 runner    (1001) docker     (127)     3024 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/analysis/parallel_coordinates_plot.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.531031 ax-platform-0.3.7/ax/analysis/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     3058 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/analysis/tests/test_parallel_coordinates_plot.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5270 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     1620 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/benchmark_method.py
--rw-r--r--   0 runner    (1001) docker     (127)    13771 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/benchmark_problem.py
--rw-r--r--   0 runner    (1001) docker     (127)     4169 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/benchmark_result.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/methods/
--rw-r--r--   0 runner    (1001) docker     (127)      179 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/methods/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2861 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/methods/modular_botorch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/problems/
--rw-r--r--   0 runner    (1001) docker     (127)      179 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2032 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/hd_embedding.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/problems/hpo/
--rw-r--r--   0 runner    (1001) docker     (127)      179 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/hpo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7815 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/hpo/pytorch_cnn.py
--rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/hpo/torchvision.py
--rw-r--r--   0 runner    (1001) docker     (127)     8977 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)    11239 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/surrogate.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/problems/synthetic/
--rw-r--r--   0 runner    (1001) docker     (127)      179 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/synthetic/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/problems/synthetic/discretized/
--rw-r--r--   0 runner    (1001) docker     (127)      179 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/synthetic/discretized/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/synthetic/discretized/mixed_integer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/problems/synthetic/hss/
--rw-r--r--   0 runner    (1001) docker     (127)      179 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/synthetic/hss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2451 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/problems/synthetic/hss/jenatton.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.535031 ax-platform-0.3.7/ax/benchmark/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11627 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     1595 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_benchmark_method.py
--rw-r--r--   0 runner    (1001) docker     (127)     8545 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_benchmark_problem.py
--rw-r--r--   0 runner    (1001) docker     (127)     2503 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_methods.py
--rw-r--r--   0 runner    (1001) docker     (127)     4757 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_mixed_integer_problems.py
--rw-r--r--   0 runner    (1001) docker     (127)      869 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_problem_storage.py
--rw-r--r--   0 runner    (1001) docker     (127)     1487 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_problems.py
--rw-r--r--   0 runner    (1001) docker     (127)     1975 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_surrogate_problems.py
--rw-r--r--   0 runner    (1001) docker     (127)     2668 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/benchmark/tests/test_surrogate_runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.539031 ax-platform-0.3.7/ax/core/
--rw-r--r--   0 runner    (1001) docker     (127)     1883 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4629 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/arm.py
--rw-r--r--   0 runner    (1001) docker     (127)    32502 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/base_trial.py
--rw-r--r--   0 runner    (1001) docker     (127)    29808 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/batch_trial.py
--rw-r--r--   0 runner    (1001) docker     (127)    21859 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/data.py
--rw-r--r--   0 runner    (1001) docker     (127)    73098 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/experiment.py
--rw-r--r--   0 runner    (1001) docker     (127)     7140 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/formatting_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4742 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/generation_strategy_interface.py
--rw-r--r--   0 runner    (1001) docker     (127)    16777 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/generator_run.py
--rw-r--r--   0 runner    (1001) docker     (127)    16762 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/map_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/map_metric.py
--rw-r--r--   0 runner    (1001) docker     (127)    25732 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/metric.py
--rw-r--r--   0 runner    (1001) docker     (127)    10835 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/multi_type_experiment.py
--rw-r--r--   0 runner    (1001) docker     (127)     8918 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/objective.py
--rw-r--r--   0 runner    (1001) docker     (127)    20424 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/observation.py
--rw-r--r--   0 runner    (1001) docker     (127)    19340 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/optimization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)    12430 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/outcome_constraint.py
--rw-r--r--   0 runner    (1001) docker     (127)    28652 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/parameter.py
--rw-r--r--   0 runner    (1001) docker     (127)    10253 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/parameter_constraint.py
--rw-r--r--   0 runner    (1001) docker     (127)     5723 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/parameter_distribution.py
--rw-r--r--   0 runner    (1001) docker     (127)     2270 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/risk_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/runner.py
--rw-r--r--   0 runner    (1001) docker     (127)    47213 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/search_space.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.543031 ax-platform-0.3.7/ax/core/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2440 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)    32387 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_batch_trial.py
--rw-r--r--   0 runner    (1001) docker     (127)    12839 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_data.py
--rw-r--r--   0 runner    (1001) docker     (127)    58843 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_experiment.py
--rw-r--r--   0 runner    (1001) docker     (127)     4320 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_formatting_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     7416 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_generator_run.py
--rw-r--r--   0 runner    (1001) docker     (127)    11964 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_map_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_map_metric.py
--rw-r--r--   0 runner    (1001) docker     (127)     4289 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_metric.py
--rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_multi_type_experiment.py
--rw-r--r--   0 runner    (1001) docker     (127)     5701 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_objective.py
--rw-r--r--   0 runner    (1001) docker     (127)    31543 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_observation.py
--rw-r--r--   0 runner    (1001) docker     (127)    22235 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_optimization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     9907 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_outcome_constraint.py
--rw-r--r--   0 runner    (1001) docker     (127)    24206 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_parameter.py
--rw-r--r--   0 runner    (1001) docker     (127)     8364 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_parameter_constraint.py
--rw-r--r--   0 runner    (1001) docker     (127)     2888 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_parameter_distribution.py
--rw-r--r--   0 runner    (1001) docker     (127)      885 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_risk_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)     2203 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_runner.py
--rw-r--r--   0 runner    (1001) docker     (127)    48752 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_search_space.py
--rw-r--r--   0 runner    (1001) docker     (127)    14216 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_trial.py
--rw-r--r--   0 runner    (1001) docker     (127)     3160 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_types.py
--rw-r--r--   0 runner    (1001) docker     (127)    24097 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    13362 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/trial.py
--rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/types.py
--rw-r--r--   0 runner    (1001) docker     (127)    15598 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/core/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.543031 ax-platform-0.3.7/ax/early_stopping/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/early_stopping/strategies/
--rw-r--r--   0 runner    (1001) docker     (127)      961 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/strategies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    25514 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/strategies/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/strategies/logical.py
--rw-r--r--   0 runner    (1001) docker     (127)    10720 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/strategies/percentile.py
--rw-r--r--   0 runner    (1001) docker     (127)     8576 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/strategies/threshold.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/early_stopping/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    29630 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/tests/test_strategies.py
--rw-r--r--   0 runner    (1001) docker     (127)     7507 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/early_stopping/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/exceptions/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      899 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     4940 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/data_provider.py
--rw-r--r--   0 runner    (1001) docker     (127)     2377 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/generation_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)      480 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1082 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/exceptions/storage.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/global_stopping/
--rw-r--r--   0 runner    (1001) docker     (127)      272 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/global_stopping/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/global_stopping/strategies/
--rw-r--r--   0 runner    (1001) docker     (127)      456 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/global_stopping/strategies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3250 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/global_stopping/strategies/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    14002 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/global_stopping/strategies/improvement.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/global_stopping/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/global_stopping/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14137 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/global_stopping/tests/tests_strategies.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/health_check/
--rw-r--r--   0 runner    (1001) docker     (127)     4035 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/health_check/search_space.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.547031 ax-platform-0.3.7/ax/health_check/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     1325 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/health_check/tests/test_search_space.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.551031 ax-platform-0.3.7/ax/metrics/
--rw-r--r--   0 runner    (1001) docker     (127)      511 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1877 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/botorch_test_problem.py
--rw-r--r--   0 runner    (1001) docker     (127)      840 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/branin.py
--rw-r--r--   0 runner    (1001) docker     (127)     5751 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/branin_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/chemistry.py
--rw-r--r--   0 runner    (1001) docker     (127)    31150 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/chemistry_data.zip
--rw-r--r--   0 runner    (1001) docker     (127)    21690 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/curve.py
--rw-r--r--   0 runner    (1001) docker     (127)     3897 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/dict_lookup.py
--rw-r--r--   0 runner    (1001) docker     (127)     4602 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/factorial.py
--rw-r--r--   0 runner    (1001) docker     (127)      694 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/hartmann6.py
--rw-r--r--   0 runner    (1001) docker     (127)     2176 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/jenatton.py
--rw-r--r--   0 runner    (1001) docker     (127)      401 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/l2norm.py
--rw-r--r--   0 runner    (1001) docker     (127)     5196 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/noisy_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3968 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/noisy_function_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/sklearn.py
--rw-r--r--   0 runner    (1001) docker     (127)     5502 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tensorboard.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.551031 ax-platform-0.3.7/ax/metrics/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3855 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/test_chemistry.py
--rw-r--r--   0 runner    (1001) docker     (127)      641 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/test_curve.py
--rw-r--r--   0 runner    (1001) docker     (127)     2620 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/test_dict_lookup.py
--rw-r--r--   0 runner    (1001) docker     (127)     2618 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/test_noisy_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     4368 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/test_sklearn.py
--rw-r--r--   0 runner    (1001) docker     (127)     7270 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/tests/test_tensorboard.py
--rw-r--r--   0 runner    (1001) docker     (127)     2679 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/metrics/torchx.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.555031 ax-platform-0.3.7/ax/modelbridge/
--rw-r--r--   0 runner    (1001) docker     (127)      772 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    48567 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    24781 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/cross_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)     9832 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/discrete.py
--rw-r--r--   0 runner    (1001) docker     (127)    28293 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/dispatch_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    23888 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/factory.py
--rw-r--r--   0 runner    (1001) docker     (127)    35276 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/generation_node.py
--rw-r--r--   0 runner    (1001) docker     (127)    35969 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/generation_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    17314 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/map_torch.py
--rw-r--r--   0 runner    (1001) docker     (127)    13893 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/model_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)    55099 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/modelbridge_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4569 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/pairwise.py
--rw-r--r--   0 runner    (1001) docker     (127)     6211 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/prediction_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3889 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/random.py
--rw-r--r--   0 runner    (1001) docker     (127)    23523 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.555031 ax-platform-0.3.7/ax/modelbridge/strategies/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/strategies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4582 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/strategies/alebo.py
--rw-r--r--   0 runner    (1001) docker     (127)    10010 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/strategies/rembo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.559032 ax-platform-0.3.7/ax/modelbridge/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5393 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_aepsych_criterion.py
--rw-r--r--   0 runner    (1001) docker     (127)     2807 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_alebo_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    32067 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_base_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    16908 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_cross_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)    11824 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_discrete_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    34271 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_dispatch_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    22587 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_factory.py
--rw-r--r--   0 runner    (1001) docker     (127)    14174 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_generation_node.py
--rw-r--r--   0 runner    (1001) docker     (127)    55391 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_generation_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)     5705 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_map_torch_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)     3528 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_model_fit_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     6477 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_model_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)    13482 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_modelbridge_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8579 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_pairwise_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)     6246 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_prediction_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6756 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_random_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    22845 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     4356 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_rembo_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)     4592 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_robust_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    35452 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_torch_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    33203 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_torch_moo_modelbridge.py
--rw-r--r--   0 runner    (1001) docker     (127)     3194 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_transform_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    16629 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_transition_criterion.py
--rw-r--r--   0 runner    (1001) docker     (127)    12570 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/torch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.563032 ax-platform-0.3.7/ax/modelbridge/transforms/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9810 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2105 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/cap_parameter.py
--rw-r--r--   0 runner    (1001) docker     (127)     5117 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/cast.py
--rw-r--r--   0 runner    (1001) docker     (127)      463 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/centered_unit_x.py
--rw-r--r--   0 runner    (1001) docker     (127)     9232 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/choice_encode.py
--rw-r--r--   0 runner    (1001) docker     (127)     6011 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/convert_metric_names.py
--rw-r--r--   0 runner    (1001) docker     (127)     4869 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/derelativize.py
--rw-r--r--   0 runner    (1001) docker     (127)     3290 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/int_range_to_choice.py
--rw-r--r--   0 runner    (1001) docker     (127)     8355 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/int_to_float.py
--rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/inverse_gaussian_cdf_y.py
--rw-r--r--   0 runner    (1001) docker     (127)     5265 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/ivw.py
--rw-r--r--   0 runner    (1001) docker     (127)     3225 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/log.py
--rw-r--r--   0 runner    (1001) docker     (127)     7792 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/log_y.py
--rw-r--r--   0 runner    (1001) docker     (127)     2954 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/logit.py
--rw-r--r--   0 runner    (1001) docker     (127)     3457 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/map_unit_x.py
--rw-r--r--   0 runner    (1001) docker     (127)     5362 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/merge_repeated_measurements.py
--rw-r--r--   0 runner    (1001) docker     (127)     5856 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/metrics_as_task.py
--rw-r--r--   0 runner    (1001) docker     (127)     8653 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/one_hot.py
--rw-r--r--   0 runner    (1001) docker     (127)     4109 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/percentile_y.py
--rw-r--r--   0 runner    (1001) docker     (127)     9098 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/power_transform_y.py
--rw-r--r--   0 runner    (1001) docker     (127)    10138 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/relativize.py
--rw-r--r--   0 runner    (1001) docker     (127)     3617 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/remove_fixed.py
--rw-r--r--   0 runner    (1001) docker     (127)     2461 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/rounding.py
--rw-r--r--   0 runner    (1001) docker     (127)     3903 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/search_space_to_choice.py
--rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/search_space_to_float.py
--rw-r--r--   0 runner    (1001) docker     (127)     7200 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/standardize_y.py
--rw-r--r--   0 runner    (1001) docker     (127)     9369 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/stratified_standardize_y.py
--rw-r--r--   0 runner    (1001) docker     (127)     4165 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/task_encode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.567031 ax-platform-0.3.7/ax/modelbridge/transforms/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_base_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     2340 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_cap_parameter_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     8039 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_cast_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)      604 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_centered_unit_x_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    10988 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_choice_encode_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     4062 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_convert_metric_names_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    11406 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_derelativize_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     7126 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_int_range_to_choice_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    14341 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_int_to_float_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     2601 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_inverse_gaussian_cdf_y_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     4390 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_ivw_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     4013 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_log_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    10894 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_log_y_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     5591 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_logit_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     5082 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_map_unit_x_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     6339 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_merge_repeated_measurements_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     5164 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_metrics_as_task_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    10335 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_one_hot_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     4966 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_percentile_y_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    15137 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_power_y_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    22283 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_relativize_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     4306 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_remove_fixed_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     1081 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_rounding_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     5361 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_search_space_to_choice_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     2195 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_search_space_to_float_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     6047 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_standardize_y_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    13799 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_stratified_standardize_y_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     5839 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_task_encode_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     7834 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_trial_as_task_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    10999 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_unit_x_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_winsorize_legacy_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)    29264 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_winsorize_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     7275 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/trial_as_task.py
--rw-r--r--   0 runner    (1001) docker     (127)    10930 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/unit_x.py
--rw-r--r--   0 runner    (1001) docker     (127)     5908 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    21993 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transforms/winsorize.py
--rw-r--r--   0 runner    (1001) docker     (127)    15437 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/modelbridge/transition_criterion.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.571032 ax-platform-0.3.7/ax/models/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.571032 ax-platform-0.3.7/ax/models/discrete/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/discrete/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1753 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/discrete/eb_thompson.py
--rw-r--r--   0 runner    (1001) docker     (127)     2980 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/discrete/full_factorial.py
--rw-r--r--   0 runner    (1001) docker     (127)    11208 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/discrete/thompson.py
--rw-r--r--   0 runner    (1001) docker     (127)     5655 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/discrete_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    24371 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/model_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.571032 ax-platform-0.3.7/ax/models/random/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/random/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3692 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/random/alebo_initializer.py
--rw-r--r--   0 runner    (1001) docker     (127)    11695 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/random/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2567 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/random/rembo_initializer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5243 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/random/sobol.py
--rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/random/uniform.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.575032 ax-platform-0.3.7/ax/models/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14119 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_alebo.py
--rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_alebo_initializer.py
--rw-r--r--   0 runner    (1001) docker     (127)      773 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    22632 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_botorch_defaults.py
--rw-r--r--   0 runner    (1001) docker     (127)    17199 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_botorch_kg.py
--rw-r--r--   0 runner    (1001) docker     (127)    11830 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_botorch_mes.py
--rw-r--r--   0 runner    (1001) docker     (127)    32359 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_botorch_model.py
--rw-r--r--   0 runner    (1001) docker     (127)    21396 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_botorch_moo_defaults.py
--rw-r--r--   0 runner    (1001) docker     (127)    36993 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_botorch_moo_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     4057 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_cbo_lcea.py
--rw-r--r--   0 runner    (1001) docker     (127)     2611 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_cbo_lcem.py
--rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_cbo_sac.py
--rw-r--r--   0 runner    (1001) docker     (127)     1764 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_discrete.py
--rw-r--r--   0 runner    (1001) docker     (127)     7756 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_eb_thompson.py
--rw-r--r--   0 runner    (1001) docker     (127)     3065 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_full_factorial.py
--rw-r--r--   0 runner    (1001) docker     (127)    50002 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_fully_bayesian.py
--rw-r--r--   0 runner    (1001) docker     (127)     7167 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4421 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_posterior_mean.py
--rw-r--r--   0 runner    (1001) docker     (127)     3464 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_random.py
--rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_randomforest.py
--rw-r--r--   0 runner    (1001) docker     (127)     4278 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_rembo.py
--rw-r--r--   0 runner    (1001) docker     (127)     1186 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_rembo_initializer.py
--rw-r--r--   0 runner    (1001) docker     (127)    13172 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_sobol.py
--rw-r--r--   0 runner    (1001) docker     (127)    13228 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_thompson.py
--rw-r--r--   0 runner    (1001) docker     (127)     2323 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_torch.py
--rw-r--r--   0 runner    (1001) docker     (127)    10121 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_torch_model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9912 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_torch_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6729 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/tests/test_uniform.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.579032 ax-platform-0.3.7/ax/models/torch/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    36771 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/alebo.py
--rw-r--r--   0 runner    (1001) docker     (127)    23194 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch.py
--rw-r--r--   0 runner    (1001) docker     (127)    35613 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_defaults.py
--rw-r--r--   0 runner    (1001) docker     (127)    15864 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_kg.py
--rw-r--r--   0 runner    (1001) docker     (127)    10445 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_mes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.579032 ax-platform-0.3.7/ax/models/torch/botorch_modular/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26043 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/acquisition.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.579032 ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4951 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/covar_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     6970 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/input_transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)     2546 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/outcome_transform.py
--rw-r--r--   0 runner    (1001) docker     (127)     5966 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/kernels.py
--rw-r--r--   0 runner    (1001) docker     (127)      448 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/list_surrogate.py
--rw-r--r--   0 runner    (1001) docker     (127)    27442 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     3826 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/multi_fidelity.py
--rw-r--r--   0 runner    (1001) docker     (127)     5668 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/optimizer_argparse.py
--rw-r--r--   0 runner    (1001) docker     (127)    17115 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/sebo.py
--rw-r--r--   0 runner    (1001) docker     (127)    35225 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/surrogate.py
--rw-r--r--   0 runner    (1001) docker     (127)    17867 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_modular/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    16270 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_moo.py
--rw-r--r--   0 runner    (1001) docker     (127)    33031 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/botorch_moo_defaults.py
--rw-r--r--   0 runner    (1001) docker     (127)     7475 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/cbo_lcea.py
--rw-r--r--   0 runner    (1001) docker     (127)     3521 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/cbo_lcem.py
--rw-r--r--   0 runner    (1001) docker     (127)     4366 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/cbo_sac.py
--rw-r--r--   0 runner    (1001) docker     (127)      510 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/frontier_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    24402 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/fully_bayesian.py
--rw-r--r--   0 runner    (1001) docker     (127)     6854 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/fully_bayesian_model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3531 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/posterior_mean.py
--rw-r--r--   0 runner    (1001) docker     (127)     4595 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/randomforest.py
--rw-r--r--   0 runner    (1001) docker     (127)     9595 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/rembo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.583032 ax-platform-0.3.7/ax/models/torch/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    30233 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_acquisition.py
--rw-r--r--   0 runner    (1001) docker     (127)     5239 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_covar_modules_argparse.py
--rw-r--r--   0 runner    (1001) docker     (127)     6646 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_input_transform_argparse.py
--rw-r--r--   0 runner    (1001) docker     (127)     6683 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_kernels.py
--rw-r--r--   0 runner    (1001) docker     (127)    40711 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     7462 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_multi_fidelity.py
--rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_optimizer_argparse.py
--rw-r--r--   0 runner    (1001) docker     (127)     2262 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_outcome_transform_argparse.py
--rw-r--r--   0 runner    (1001) docker     (127)    14720 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_sebo.py
--rw-r--r--   0 runner    (1001) docker     (127)    49878 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_surrogate.py
--rw-r--r--   0 runner    (1001) docker     (127)    24893 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    27477 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    10682 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/torch_base.py
--rw-r--r--   0 runner    (1001) docker     (127)      720 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/types.py
--rw-r--r--   0 runner    (1001) docker     (127)     1452 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/models/winsorization_config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.583032 ax-platform-0.3.7/ax/plot/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2414 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/bandit_rollout.py
--rw-r--r--   0 runner    (1001) docker     (127)     2444 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2718 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/color.py
--rw-r--r--   0 runner    (1001) docker     (127)    32766 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/contour.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.583032 ax-platform-0.3.7/ax/plot/css/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/css/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      992 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/css/base.css
--rw-r--r--   0 runner    (1001) docker     (127)    24877 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/diagnostic.py
--rw-r--r--   0 runner    (1001) docker     (127)    11680 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/feature_importances.py
--rw-r--r--   0 runner    (1001) docker     (127)    33378 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/helper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.587032 ax-platform-0.3.7/ax/plot/js/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.587032 ax-platform-0.3.7/ax/plot/js/common/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      344 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/common/css.js
--rw-r--r--   0 runner    (1001) docker     (127)     5344 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/common/helpers.js
--rw-r--r--   0 runner    (1001) docker     (127)      369 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/common/plotly_offline.js
--rw-r--r--   0 runner    (1001) docker     (127)      385 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/common/plotly_online.js
--rw-r--r--   0 runner    (1001) docker     (127)      342 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/common/plotly_requires.js
--rw-r--r--   0 runner    (1001) docker     (127)      269 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/js/generic_plotly.js
--rw-r--r--   0 runner    (1001) docker     (127)     2402 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/marginal_effects.py
--rw-r--r--   0 runner    (1001) docker     (127)     2834 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/parallel_coordinates.py
--rw-r--r--   0 runner    (1001) docker     (127)    38724 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/pareto_frontier.py
--rw-r--r--   0 runner    (1001) docker     (127)    27750 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/pareto_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4362 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/render.py
--rw-r--r--   0 runner    (1001) docker     (127)    61750 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/scatter.py
--rw-r--r--   0 runner    (1001) docker     (127)    20015 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/slice.py
--rw-r--r--   0 runner    (1001) docker     (127)     5646 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/table_view.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.587032 ax-platform-0.3.7/ax/plot/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.587032 ax-platform-0.3.7/ax/plot/tests/long_running/
--rw-r--r--   0 runner    (1001) docker     (127)     2436 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/long_running/test_pareto_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2169 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_contours.py
--rw-r--r--   0 runner    (1001) docker     (127)     2133 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_diagnostic.py
--rw-r--r--   0 runner    (1001) docker     (127)     4912 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_feature_importances.py
--rw-r--r--   0 runner    (1001) docker     (127)     2099 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_fitted_scatter.py
--rw-r--r--   0 runner    (1001) docker     (127)     1291 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_helper.py
--rw-r--r--   0 runner    (1001) docker     (127)      823 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_parallel_coordinates.py
--rw-r--r--   0 runner    (1001) docker     (127)    16529 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_pareto_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1561 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_slices.py
--rw-r--r--   0 runner    (1001) docker     (127)     5147 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_tile_fitted.py
--rw-r--r--   0 runner    (1001) docker     (127)     4371 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/tests/test_traces.py
--rw-r--r--   0 runner    (1001) docker     (127)    33805 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/plot/trace.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.587032 ax-platform-0.3.7/ax/runners/
--rw-r--r--   0 runner    (1001) docker     (127)      394 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/botorch_test_problem.py
--rw-r--r--   0 runner    (1001) docker     (127)     3326 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/simulated_backend.py
--rw-r--r--   0 runner    (1001) docker     (127)     2357 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/single_running_trial_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1318 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/synthetic.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.587032 ax-platform-0.3.7/ax/runners/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4117 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/tests/test_botorch_test_problem.py
--rw-r--r--   0 runner    (1001) docker     (127)     1745 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/tests/test_single_running_trial_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     6264 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/tests/test_torchx.py
--rw-r--r--   0 runner    (1001) docker     (127)     6993 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/runners/torchx.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.591032 ax-platform-0.3.7/ax/service/
--rw-r--r--   0 runner    (1001) docker     (127)      311 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    81566 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/ax_client.py
--rw-r--r--   0 runner    (1001) docker     (127)     8287 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/interactive_loop.py
--rw-r--r--   0 runner    (1001) docker     (127)    11879 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/managed_loop.py
--rw-r--r--   0 runner    (1001) docker     (127)    91677 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/scheduler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.591032 ax-platform-0.3.7/ax/service/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    89406 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/scheduler_test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)   123590 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_ax_client.py
--rw-r--r--   0 runner    (1001) docker     (127)     7220 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_best_point.py
--rw-r--r--   0 runner    (1001) docker     (127)    22933 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_best_point_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1668 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)     5405 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_global_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)    13180 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_instantiation_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4917 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_interactive_loop.py
--rw-r--r--   0 runner    (1001) docker     (127)    19704 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_managed_loop.py
--rw-r--r--   0 runner    (1001) docker     (127)    49936 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_report_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      304 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (127)    14904 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/tests/test_with_db_settings_base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.595032 ax-platform-0.3.7/ax/service/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    35725 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/best_point.py
--rw-r--r--   0 runner    (1001) docker     (127)    27277 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/best_point_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1916 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)    37119 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/instantiation.py
--rw-r--r--   0 runner    (1001) docker     (127)    58366 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/report_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     7633 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/scheduler_options.py
--rw-r--r--   0 runner    (1001) docker     (127)    22819 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/service/utils/with_db_settings_base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.595032 ax-platform-0.3.7/ax/storage/
--rw-r--r--   0 runner    (1001) docker     (127)      313 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9720 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/botorch_modular_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.595032 ax-platform-0.3.7/ax/storage/json_store/
--rw-r--r--   0 runner    (1001) docker     (127)      378 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    44514 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/decoder.py
--rw-r--r--   0 runner    (1001) docker     (127)    12016 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/decoders.py
--rw-r--r--   0 runner    (1001) docker     (127)     6736 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/encoder.py
--rw-r--r--   0 runner    (1001) docker     (127)    29064 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/encoders.py
--rw-r--r--   0 runner    (1001) docker     (127)     1217 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/load.py
--rw-r--r--   0 runner    (1001) docker     (127)    16753 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1743 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/save.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.595032 ax-platform-0.3.7/ax/storage/json_store/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26123 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/json_store/tests/test_json_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4754 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/metric_registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     9327 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/registry_bundle.py
--rw-r--r--   0 runner    (1001) docker     (127)     4256 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/runner_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.599032 ax-platform-0.3.7/ax/storage/sqa_store/
--rw-r--r--   0 runner    (1001) docker     (127)      543 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8930 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/db.py
--rw-r--r--   0 runner    (1001) docker     (127)    51126 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/decoder.py
--rw-r--r--   0 runner    (1001) docker     (127)     2274 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/delete.py
--rw-r--r--   0 runner    (1001) docker     (127)    45012 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/encoder.py
--rw-r--r--   0 runner    (1001) docker     (127)     3359 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/json.py
--rw-r--r--   0 runner    (1001) docker     (127)    21879 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/load.py
--rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/reduced_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    20516 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/save.py
--rw-r--r--   0 runner    (1001) docker     (127)    26044 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/sqa_classes.py
--rw-r--r--   0 runner    (1001) docker     (127)     4480 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/sqa_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/sqa_enum.py
--rw-r--r--   0 runner    (1001) docker     (127)      841 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/structs.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.599032 ax-platform-0.3.7/ax/storage/sqa_store/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    80417 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/tests/test_sqa_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4307 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6577 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/tests/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      964 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/timestamp.py
--rw-r--r--   0 runner    (1001) docker     (127)     6394 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4008 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/sqa_store/validation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.599032 ax-platform-0.3.7/ax/storage/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     2095 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/tests/test_botorch_modular_registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1706 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/tests/test_registry_bundle.py
--rw-r--r--   0 runner    (1001) docker     (127)     3143 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/transform_registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1719 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/storage/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.599032 ax-platform-0.3.7/ax/telemetry/
--rw-r--r--   0 runner    (1001) docker     (127)     4729 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/ax_client.py
--rw-r--r--   0 runner    (1001) docker     (127)     3230 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/common.py
--rw-r--r--   0 runner    (1001) docker     (127)    10913 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/experiment.py
--rw-r--r--   0 runner    (1001) docker     (127)     2354 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/generation_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    23398 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/optimization.py
--rw-r--r--   0 runner    (1001) docker     (127)     8759 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/scheduler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.599032 ax-platform-0.3.7/ax/telemetry/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     5769 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/tests/test_ax_client.py
--rw-r--r--   0 runner    (1001) docker     (127)     2464 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/tests/test_experiment.py
--rw-r--r--   0 runner    (1001) docker     (127)     1015 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/tests/test_generation_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    11025 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/tests/test_optimization.py
--rw-r--r--   0 runner    (1001) docker     (127)    11163 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/telemetry/tests/test_scheduler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.599032 ax-platform-0.3.7/ax/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.603032 ax-platform-0.3.7/ax/utils/common/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1586 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2924 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     2829 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/decorator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1631 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/docutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9098 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/equality.py
--rw-r--r--   0 runner    (1001) docker     (127)    10612 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/executils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5268 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/kwargs.py
--rw-r--r--   0 runner    (1001) docker     (127)     7741 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/logger.py
--rw-r--r--   0 runner    (1001) docker     (127)     1110 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/mock.py
--rw-r--r--   0 runner    (1001) docker     (127)     6748 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/result.py
--rw-r--r--   0 runner    (1001) docker     (127)     5419 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/serialization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.603032 ax-platform-0.3.7/ax/utils/common/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1187 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_docutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2670 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_equality.py
--rw-r--r--   0 runner    (1001) docker     (127)    10218 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_executils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5761 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_kwargutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2890 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_logger.py
--rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_result.py
--rw-r--r--   0 runner    (1001) docker     (127)      824 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_serialization.py
--rw-r--r--   0 runner    (1001) docker     (127)     3393 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_testutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2070 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/tests/test_typeutils.py
--rw-r--r--   0 runner    (1001) docker     (127)    19238 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/testutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1405 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/timeutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5315 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/typeutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/common/typeutils_torch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.603032 ax-platform-0.3.7/ax/utils/flake8_plugins/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/flake8_plugins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3279 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/flake8_plugins/docstring_checker.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.603032 ax-platform-0.3.7/ax/utils/measurement/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/measurement/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11348 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/measurement/synthetic_functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.603032 ax-platform-0.3.7/ax/utils/measurement/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/measurement/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6182 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/measurement/tests/test_synthetic_functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.603032 ax-platform-0.3.7/ax/utils/notebook/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/notebook/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1603 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/notebook/plotting.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/report/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4236 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/render.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/report/resources/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/resources/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/resources/base_template.html
--rw-r--r--   0 runner    (1001) docker     (127)     3150 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/resources/report.css
--rw-r--r--   0 runner    (1001) docker     (127)      215 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/resources/simple_template.html
--rw-r--r--   0 runner    (1001) docker     (127)      657 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/resources/sufficient_statistic.html
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/report/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1100 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/report/tests/test_render.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/sensitivity/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/sensitivity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4271 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/sensitivity/derivative_gp.py
--rw-r--r--   0 runner    (1001) docker     (127)    16135 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/sensitivity/derivative_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)    37921 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/sensitivity/sobol_measures.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/sensitivity/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/sensitivity/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15692 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/sensitivity/tests/test_sensitivity.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/stats/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/stats/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7787 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/stats/model_fit_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)    16637 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/stats/statstools.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.607032 ax-platform-0.3.7/ax/utils/stats/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/stats/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      916 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/stats/tests/test_model_fit_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     6983 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/stats/tests/test_statstools.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.611032 ax-platform-0.3.7/ax/utils/testing/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4277 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/backend_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (127)    18606 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/backend_simulator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5729 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/benchmark_stubs.py
--rw-r--r--   0 runner    (1001) docker     (127)    73231 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/core_stubs.py
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/manifest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.611032 ax-platform-0.3.7/ax/utils/testing/metrics/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1649 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/metrics/backend_simulator_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     2432 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/metrics/branin_backend_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     5535 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/mock.py
--rw-r--r--   0 runner    (1001) docker     (127)    21537 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/modeling_stubs.py
--rw-r--r--   0 runner    (1001) docker     (127)      792 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/test_init_files.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.611032 ax-platform-0.3.7/ax/utils/testing/tests/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5402 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/tests/test_backend_simulator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2012 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1539 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/torch_stubs.py
--rw-r--r--   0 runner    (1001) docker     (127)     1090 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/testing/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.611032 ax-platform-0.3.7/ax/utils/tutorials/
--rw-r--r--   0 runner    (1001) docker     (127)      202 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/tutorials/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9247 2024-03-01 16:24:58.000000 ax-platform-0.3.7/ax/utils/tutorials/cnn_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      411 2024-03-01 16:28:04.000000 ax-platform-0.3.7/ax/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.611032 ax-platform-0.3.7/ax_platform.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)     8526 2024-03-01 16:28:04.000000 ax-platform-0.3.7/ax_platform.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    22942 2024-03-01 16:28:04.000000 ax-platform-0.3.7/ax_platform.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-01 16:28:04.000000 ax-platform-0.3.7/ax_platform.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-03-01 16:28:04.000000 ax-platform-0.3.7/ax_platform.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)        3 2024-03-01 16:28:04.000000 ax-platform-0.3.7/ax_platform.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.611032 ax-platform-0.3.7/docs/
--rw-r--r--   0 runner    (1001) docker     (127)      145 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/algo-overview.md
--rw-r--r--   0 runner    (1001) docker     (127)     6387 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/api.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.615032 ax-platform-0.3.7/docs/assets/
--rw-r--r--   0 runner    (1001) docker     (127)    81587 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/bandit_allocation.png
--rw-r--r--   0 runner    (1001) docker     (127)   345484 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/bo_1d_opt.gif
--rw-r--r--   0 runner    (1001) docker     (127)   156260 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/contour.js
--rw-r--r--   0 runner    (1001) docker     (127)    14875 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/cv.js
--rw-r--r--   0 runner    (1001) docker     (127)   131612 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/example_shrinkage.png
--rw-r--r--   0 runner    (1001) docker     (127)    13598 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/fitted.js
--rw-r--r--   0 runner    (1001) docker     (127)    65905 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/gp_opt.png
--rw-r--r--   0 runner    (1001) docker     (127)   349223 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/gp_posterior.png
--rw-r--r--   0 runner    (1001) docker     (127)   121838 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/mab_animate.gif
--rw-r--r--   0 runner    (1001) docker     (127)   132372 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/mab_probs.png
--rw-r--r--   0 runner    (1001) docker     (127)   325134 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/mab_regret.png
--rw-r--r--   0 runner    (1001) docker     (127)    26226 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/assets/slice.js
--rw-r--r--   0 runner    (1001) docker     (127)     8863 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/banditopt.md
--rw-r--r--   0 runner    (1001) docker     (127)    10181 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/bayesopt.md
--rw-r--r--   0 runner    (1001) docker     (127)     9871 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/core.md
--rw-r--r--   0 runner    (1001) docker     (127)     2484 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/data.md
--rw-r--r--   0 runner    (1001) docker     (127)     6876 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/glossary.md
--rw-r--r--   0 runner    (1001) docker     (127)     3369 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/installation.md
--rw-r--r--   0 runner    (1001) docker     (127)    22509 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/models.md
--rw-r--r--   0 runner    (1001) docker     (127)     7454 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/storage.md
--rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/trial-evaluation.md
--rw-r--r--   0 runner    (1001) docker     (127)     2218 2024-03-01 16:24:58.000000 ax-platform-0.3.7/docs/why-ax.md
--rw-r--r--   0 runner    (1001) docker     (127)      158 2024-03-01 16:24:58.000000 ax-platform-0.3.7/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (127)      319 2024-03-01 16:24:58.000000 ax-platform-0.3.7/pytest.ini
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.619032 ax-platform-0.3.7/scripts/
--rwxr-xr-x   0 runner    (1001) docker     (127)      632 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/build_ax.sh
--rwxr-xr-x   0 runner    (1001) docker     (127)      535 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/docker_install.sh
--rw-r--r--   0 runner    (1001) docker     (127)      342 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/import_ax.py
--rw-r--r--   0 runner    (1001) docker     (127)     2263 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/insert_api_refs.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3855 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/make_docs.sh
--rw-r--r--   0 runner    (1001) docker     (127)     9551 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/make_tutorials.py
--rw-r--r--   0 runner    (1001) docker     (127)     3195 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/parse_sphinx.py
--rw-r--r--   0 runner    (1001) docker     (127)     1444 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/patch_site_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     8153 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/publish_site.sh
--rw-r--r--   0 runner    (1001) docker     (127)     2477 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/update_versions_html.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4119 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/validate_sphinx.py
--rw-r--r--   0 runner    (1001) docker     (127)     2981 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/versions.js
--rw-r--r--   0 runner    (1001) docker     (127)      846 2024-03-01 16:24:58.000000 ax-platform-0.3.7/scripts/wheels_build.ps1
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-03-01 16:28:04.635032 ax-platform-0.3.7/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     3849 2024-03-01 16:24:58.000000 ax-platform-0.3.7/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.619032 ax-platform-0.3.7/sphinx/
--rw-r--r--   0 runner    (1001) docker     (127)     7466 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)     7442 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/make.bat
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.619032 ax-platform-0.3.7/sphinx/source/
--rw-r--r--   0 runner    (1001) docker     (127)      151 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/ax.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2860 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/benchmark.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8425 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/conf.py
--rw-r--r--   0 runner    (1001) docker     (127)     3608 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/core.rst
--rw-r--r--   0 runner    (1001) docker     (127)      959 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/early_stopping.rst
--rw-r--r--   0 runner    (1001) docker     (127)      928 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/exceptions.rst
--rw-r--r--   0 runner    (1001) docker     (127)      526 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/global_stopping.rst
--rw-r--r--   0 runner    (1001) docker     (127)      538 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1941 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/metrics.rst
--rw-r--r--   0 runner    (1001) docker     (127)     9235 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/modelbridge.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8772 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/models.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2252 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/plot.rst
--rw-r--r--   0 runner    (1001) docker     (127)      841 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/runners.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1601 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/service.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4954 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/storage.rst
--rw-r--r--   0 runner    (1001) docker     (127)      520 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/telemetry.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6001 2024-03-01 16:24:58.000000 ax-platform-0.3.7/sphinx/source/utils.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.627032 ax-platform-0.3.7/tutorials/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.627032 ax-platform-0.3.7/tutorials/early_stopping/
--rw-r--r--   0 runner    (1001) docker     (127)    24254 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/early_stopping/early_stopping.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     5450 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/early_stopping/mnist_train_nas.py
--rw-r--r--   0 runner    (1001) docker     (127)    22794 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/factorial.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    19338 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/generation_strategy.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    23875 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/gpei_hartmann_developer.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     7217 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/gpei_hartmann_loop.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    17911 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/gpei_hartmann_service.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    18017 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/gss.ipynb
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/tutorials/human_in_the_loop/
--rw-r--r--   0 runner    (1001) docker     (127)    16283 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/human_in_the_loop/hitl_data.json
--rw-r--r--   0 runner    (1001) docker     (127)   634579 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/human_in_the_loop/hitl_exp.json
--rw-r--r--   0 runner    (1001) docker     (127)    19081 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/human_in_the_loop/human_in_the_loop.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    36237 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/modular_botax.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    23173 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/multi_task.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)   462777 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/multiobjective_optimization.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    11514 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/raytune_pytorch_cnn.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    12518 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/saasbo.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    22937 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/saasbo_nehvi.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)   287298 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/scheduler.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    20192 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/sebo.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)  4095331 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/submitit.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    31120 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/tune_cnn_service.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    13031 2024-03-01 16:24:58.000000 ax-platform-0.3.7/tutorials/visualizations.ipynb
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/core/
--rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/core/Footer.js
--rw-r--r--   0 runner    (1001) docker     (127)     4320 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/core/Tutorial.js
--rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/core/TutorialSidebar.js
--rw-r--r--   0 runner    (1001) docker     (127)      375 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/package.json
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.531031 ax-platform-0.3.7/website/pages/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/pages/en/
--rw-r--r--   0 runner    (1001) docker     (127)     6646 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/pages/en/index.js
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/pages/tutorials/
--rw-r--r--   0 runner    (1001) docker     (127)     7118 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/pages/tutorials/index.js
--rw-r--r--   0 runner    (1001) docker     (127)      233 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/sidebars.json
--rw-r--r--   0 runner    (1001) docker     (127)     2740 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/siteConfig.js
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/static/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/.nojekyll
--rw-r--r--   0 runner    (1001) docker     (127)        7 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/CNAME
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/static/css/
--rw-r--r--   0 runner    (1001) docker     (127)    10913 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/css/base_sphinx.css
--rw-r--r--   0 runner    (1001) docker     (127)     4960 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/css/custom.css
--rw-r--r--   0 runner    (1001) docker     (127)    15413 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/css/custom_sphinx.css
--rw-r--r--   0 runner    (1001) docker     (127)     4315 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/css/pygments.css
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/static/img/
--rw-r--r--   0 runner    (1001) docker     (127)     1981 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/ax.svg
--rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/ax_lockup.svg
--rw-r--r--   0 runner    (1001) docker     (127)     2244 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/ax_lockup_white.svg
--rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/ax_logo_lockup.svg
--rw-r--r--   0 runner    (1001) docker     (127)      369 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/ax_wireframe.svg
--rw-r--r--   0 runner    (1001) docker     (127)      785 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/database-solid.svg
--rw-r--r--   0 runner    (1001) docker     (127)     1360 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/dice-solid.svg
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.631032 ax-platform-0.3.7/website/static/img/favicon/
--rw-r--r--   0 runner    (1001) docker     (127)     9662 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/favicon/favicon.ico
--rw-r--r--   0 runner    (1001) docker     (127)     2731 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/favicon.png
--rw-r--r--   0 runner    (1001) docker     (127)     4370 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/oss_logo.png
--rw-r--r--   0 runner    (1001) docker     (127)      910 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/img/th-large-solid.svg
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.635032 ax-platform-0.3.7/website/static/js/
--rw-r--r--   0 runner    (1001) docker     (127)      716 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/js/mathjax.js
--rw-r--r--   0 runner    (1001) docker     (127)     1491 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/static/js/plotUtils.js
--rw-r--r--   0 runner    (1001) docker     (127)     1920 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/tutorials.json
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.635032 ax-platform-0.3.7/website/versioned_docs/
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/versioned_docs/.gitkeep
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-01 16:28:04.635032 ax-platform-0.3.7/website/versioned_sidebars/
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-01 16:24:58.000000 ax-platform-0.3.7/website/versioned_sidebars/.gitkeep
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.737141 ax-platform-0.4.0/
+-rw-r--r--   0 runner    (1001) docker     (127)      407 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.flake8
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.609140 ax-platform-0.4.0/.github/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.621140 ax-platform-0.4.0/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     1753 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/build-and-test.yml
+-rw-r--r--   0 runner    (1001) docker     (127)     2594 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/cron.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      716 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/cron_pinned.yml
+-rw-r--r--   0 runner    (1001) docker     (127)     2006 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/deploy.yml
+-rw-r--r--   0 runner    (1001) docker     (127)     2324 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/reusable_test.yml
+-rw-r--r--   0 runner    (1001) docker     (127)     1391 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/reusable_tutorials.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      382 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.github/workflows/tutorials.yml
+-rw-r--r--   0 runner    (1001) docker     (127)     2092 2024-05-02 22:04:28.000000 ax-platform-0.4.0/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)      106 2024-05-02 22:04:28.000000 ax-platform-0.4.0/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-05-02 22:04:28.000000 ax-platform-0.4.0/CODE_OF_CONDUCT.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5428 2024-05-02 22:04:28.000000 ax-platform-0.4.0/CONTRIBUTING.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-05-02 22:04:28.000000 ax-platform-0.4.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)     8508 2024-05-02 22:09:20.737141 ax-platform-0.4.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     7878 2024-05-02 22:04:28.000000 ax-platform-0.4.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.621140 ax-platform-0.4.0/ax/
+-rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.621140 ax-platform-0.4.0/ax/analysis/
+-rw-r--r--   0 runner    (1001) docker     (127)      915 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/base_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)      703 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/base_plotly_visualization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8272 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/cross_validation_plot.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.621140 ax-platform-0.4.0/ax/analysis/helpers/
+-rw-r--r--   0 runner    (1001) docker     (127)      478 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/color_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)      523 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5962 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/cross_validation_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3096 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/layout_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3059 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/plot_data_df_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2667 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/plot_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9286 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/scatter_helpers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.621140 ax-platform-0.4.0/ax/analysis/helpers/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     3255 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/tests/test_cross_validation_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4481 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/helpers/tests/test_cv_consistency_checks.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3050 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/parallel_coordinates_plot.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4623 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/predicted_outcomes_dot_plot.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/analysis/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1555 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/tests/test_cross_validation_plot.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3073 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/tests/test_parallel_coordinates_plot.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1804 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/analysis/tests/test_predicted_outcomes_dot_plot.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13441 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4765 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/benchmark_method.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15538 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/benchmark_problem.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4184 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/benchmark_result.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/methods/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/methods/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5452 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/methods/modular_botorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/methods/sobol.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3055 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/metrics/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4547 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/metrics/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3463 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/metrics/jenatton.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3967 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/metrics/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/problems/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/hd_embedding.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/problems/hpo/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/hpo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7514 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/hpo/pytorch_cnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4022 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/hpo/torchvision.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9720 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9048 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/surrogate.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/problems/synthetic/
+-rw-r--r--   0 runner    (1001) docker     (127)      179 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/synthetic/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.625140 ax-platform-0.4.0/ax/benchmark/problems/synthetic/discretized/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/synthetic/discretized/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6109 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/synthetic/discretized/mixed_integer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/problems/synthetic/hss/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/synthetic/hss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2581 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/problems/synthetic/hss/jenatton.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/runners/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/runners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/runners/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8678 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/runners/botorch_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6682 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/runners/surrogate.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/tests/methods/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/methods/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5560 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/methods/test_methods.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/tests/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/metrics/test_benchmark_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4175 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/metrics/test_jennaton.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/tests/problems/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/problems/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4812 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/problems/test_mixed_integer_problems.py
+-rw-r--r--   0 runner    (1001) docker     (127)      884 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/problems/test_problem_storage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/problems/test_problems.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2340 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/problems/test_surrogate_problems.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.629140 ax-platform-0.4.0/ax/benchmark/tests/runners/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/runners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7537 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/runners/test_botorch_test_problem.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3782 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/runners/test_surrogate_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21314 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/test_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2451 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/test_benchmark_method.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8236 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/benchmark/tests/test_benchmark_problem.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.633140 ax-platform-0.4.0/ax/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1898 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4639 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34100 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/base_trial.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29353 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/batch_trial.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21850 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    73436 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/experiment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7165 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/formatting_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4814 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/generation_strategy_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16840 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/generator_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16817 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/map_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1171 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/map_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25899 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10898 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/multi_type_experiment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8414 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/objective.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21424 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/observation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19355 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/optimization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12445 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/outcome_constraint.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28211 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/parameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10225 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/parameter_constraint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5738 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/parameter_distribution.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2285 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/risk_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6027 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    48894 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/search_space.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/core/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2412 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33087 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_batch_trial.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13068 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    60247 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_experiment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4335 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_formatting_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2204 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_generation_strategy_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7455 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_generator_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12003 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_map_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1852 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_map_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4326 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5371 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_multi_type_experiment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5375 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_objective.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35733 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_observation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22297 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_optimization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9994 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_outcome_constraint.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24348 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_parameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8451 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_parameter_constraint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2903 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_parameter_distribution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      900 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_risk_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2242 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54672 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_search_space.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16189 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_trial.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3199 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_types.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27593 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13852 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/trial.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7248 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/types.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15733 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/core/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/early_stopping/
+-rw-r--r--   0 runner    (1001) docker     (127)      286 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/early_stopping/strategies/
+-rw-r--r--   0 runner    (1001) docker     (127)      976 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/strategies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24793 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/strategies/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2442 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/strategies/logical.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10368 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/strategies/percentile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8224 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/strategies/threshold.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/early_stopping/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29298 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/tests/test_strategies.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7522 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/early_stopping/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/exceptions/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      914 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5126 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1665 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/data_provider.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2540 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/generation_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      495 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1097 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/exceptions/storage.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/global_stopping/
+-rw-r--r--   0 runner    (1001) docker     (127)      287 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/global_stopping/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/global_stopping/strategies/
+-rw-r--r--   0 runner    (1001) docker     (127)      471 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/global_stopping/strategies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3265 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/global_stopping/strategies/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14906 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/global_stopping/strategies/improvement.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/global_stopping/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/global_stopping/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14314 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/global_stopping/tests/test_strategies.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/health_check/
+-rw-r--r--   0 runner    (1001) docker     (127)     4035 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/health_check/search_space.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.641140 ax-platform-0.4.0/ax/health_check/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1340 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/health_check/tests/test_search_space.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.645140 ax-platform-0.4.0/ax/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      526 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1892 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/botorch_test_problem.py
+-rw-r--r--   0 runner    (1001) docker     (127)      855 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/branin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5766 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/branin_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4707 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/chemistry.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31150 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/chemistry_data.zip
+-rw-r--r--   0 runner    (1001) docker     (127)    21705 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/curve.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3912 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/dict_lookup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4617 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/factorial.py
+-rw-r--r--   0 runner    (1001) docker     (127)      709 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/hartmann6.py
+-rw-r--r--   0 runner    (1001) docker     (127)      416 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/l2norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5211 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/noisy_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3983 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/noisy_function_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6723 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/sklearn.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12391 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tensorboard.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.645140 ax-platform-0.4.0/ax/metrics/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3870 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/test_chemistry.py
+-rw-r--r--   0 runner    (1001) docker     (127)      656 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/test_curve.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2659 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/test_dict_lookup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2633 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/test_noisy_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4383 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/test_sklearn.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11819 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/tests/test_tensorboard.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2694 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/metrics/torchx.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.649140 ax-platform-0.4.0/ax/modelbridge/
+-rw-r--r--   0 runner    (1001) docker     (127)      741 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    48632 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25943 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/cross_validation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9847 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/discrete.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28630 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/dispatch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9188 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/external_generation_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21487 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34373 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/generation_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36865 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/generation_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17329 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/map_torch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13908 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/model_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)    53835 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/modelbridge_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4584 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/pairwise.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6226 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/prediction_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3904 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/random.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23043 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.649140 ax-platform-0.4.0/ax/modelbridge/strategies/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/strategies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/strategies/alebo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10025 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/strategies/rembo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.653140 ax-platform-0.4.0/ax/modelbridge/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5514 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_aepsych_criterion.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2822 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_alebo_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32290 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_base_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16990 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_cross_validation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11863 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_discrete_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34658 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_dispatch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3071 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_external_generation_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20200 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12020 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_generation_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    57432 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_generation_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9408 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_hierarchical_search_space.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5720 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_map_torch_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6056 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_model_fit_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6517 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_model_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13497 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_modelbridge_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8618 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_pairwise_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6261 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_prediction_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6719 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_random_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22819 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4316 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_rembo_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4607 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_robust_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37007 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_torch_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31766 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_torch_moo_modelbridge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3209 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_transform_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16697 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_transition_criterion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12671 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44760 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/torch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.661140 ax-platform-0.4.0/ax/modelbridge/transforms/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9825 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2120 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/cap_parameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5985 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/cast.py
+-rw-r--r--   0 runner    (1001) docker     (127)      478 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/centered_unit_x.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9613 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/choice_encode.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6026 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/convert_metric_names.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1553 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/deprecated_transform_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4884 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/derelativize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3347 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/int_range_to_choice.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8366 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/int_to_float.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2530 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/inverse_gaussian_cdf_y.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5280 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/ivw.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3240 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/log.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7807 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/log_y.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2970 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/logit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3472 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/map_unit_x.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5549 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/merge_repeated_measurements.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5871 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/metrics_as_task.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8668 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/one_hot.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4124 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/percentile_y.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9113 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/power_transform_y.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10153 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/relativize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3632 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/remove_fixed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2476 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/rounding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3918 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/search_space_to_choice.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1603 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/search_space_to_float.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7215 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/standardize_y.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9384 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/stratified_standardize_y.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4196 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/task_encode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.665140 ax-platform-0.4.0/ax/modelbridge/transforms/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     2253 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_base_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2379 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_cap_parameter_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9344 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_cast_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)      619 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_centered_unit_x_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11818 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_choice_encode_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_convert_metric_names_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2465 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_deprecated_transform_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11476 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_derelativize_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7165 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_int_range_to_choice_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14291 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_int_to_float_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2640 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_inverse_gaussian_cdf_y_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4405 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_ivw_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4052 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_log_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10888 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_log_y_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5630 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_logit_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5121 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_map_unit_x_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7170 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_merge_repeated_measurements_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_metrics_as_task_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10374 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_one_hot_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5005 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_percentile_y_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14356 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_power_y_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22298 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_relativize_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4345 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_remove_fixed_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_rounding_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5376 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_search_space_to_choice_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2234 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_search_space_to_float_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6086 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_standardize_y_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13838 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_stratified_standardize_y_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5878 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_task_encode_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7873 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_trial_as_task_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11038 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_unit_x_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8201 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_winsorize_legacy_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29050 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_winsorize_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7290 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/trial_as_task.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10945 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/unit_x.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5923 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21926 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transforms/winsorize.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23901 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/modelbridge/transition_criterion.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.665140 ax-platform-0.4.0/ax/models/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2627 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.665140 ax-platform-0.4.0/ax/models/discrete/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/discrete/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1768 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/discrete/eb_thompson.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2995 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/discrete/full_factorial.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11221 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/discrete/thompson.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5670 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/discrete_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24386 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/model_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.665140 ax-platform-0.4.0/ax/models/random/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/random/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3707 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/random/alebo_initializer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11710 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/random/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2582 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/random/rembo_initializer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5258 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/random/sobol.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1553 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/random/uniform.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.673140 ax-platform-0.4.0/ax/models/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14084 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_alebo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_alebo_initializer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      788 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22647 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_botorch_defaults.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17238 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_botorch_kg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11869 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_botorch_mes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32374 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_botorch_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21433 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_botorch_moo_defaults.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37484 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_botorch_moo_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4072 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_cbo_lcea.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_cbo_lcem.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3793 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_cbo_sac.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1736 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_discrete.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7771 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_eb_thompson.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3080 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_full_factorial.py
+-rw-r--r--   0 runner    (1001) docker     (127)    52628 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_fully_bayesian.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7139 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4436 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_posterior_mean.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3503 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_random.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1749 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_randomforest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4293 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_rembo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1201 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_rembo_initializer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13211 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_sobol.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13267 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_thompson.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2362 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_torch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12021 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_torch_model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10724 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_torch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/tests/test_uniform.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.673140 ax-platform-0.4.0/ax/models/torch/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36719 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/alebo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23070 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35804 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_defaults.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15879 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_kg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10460 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_mes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.677140 ax-platform-0.4.0/ax/models/torch/botorch_modular/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26913 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/acquisition.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.677140 ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4966 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/covar_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6985 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/input_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2561 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/outcome_transform.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5981 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/kernels.py
+-rw-r--r--   0 runner    (1001) docker     (127)      463 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/list_surrogate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27377 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/multi_fidelity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/optimizer_argparse.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17549 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/sebo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36373 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/surrogate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17964 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_modular/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16200 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_moo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33166 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/botorch_moo_defaults.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7477 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/cbo_lcea.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3398 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/cbo_lcem.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4381 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/cbo_sac.py
+-rw-r--r--   0 runner    (1001) docker     (127)      525 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/frontier_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24121 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/fully_bayesian.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6901 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/fully_bayesian_model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3546 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/posterior_mean.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4610 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/randomforest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9610 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/rembo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.677140 ax-platform-0.4.0/ax/models/torch/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32457 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_acquisition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5258 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_covar_modules_argparse.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6685 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_input_transform_argparse.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6706 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_kernels.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40661 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7501 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_multi_fidelity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5914 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_optimizer_argparse.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2301 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_outcome_transform_argparse.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14828 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_sebo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    50033 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_surrogate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24899 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27982 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10733 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/torch_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      760 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/types.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/models/winsorization_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.681140 ax-platform-0.4.0/ax/plot/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2429 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/bandit_rollout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2459 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2733 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/color.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33724 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/contour.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.681140 ax-platform-0.4.0/ax/plot/css/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/css/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      992 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/css/base.css
+-rw-r--r--   0 runner    (1001) docker     (127)    24802 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/diagnostic.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12419 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/feature_importances.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33281 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/helper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.681140 ax-platform-0.4.0/ax/plot/js/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.681140 ax-platform-0.4.0/ax/plot/js/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      344 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/common/css.js
+-rw-r--r--   0 runner    (1001) docker     (127)     5344 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/common/helpers.js
+-rw-r--r--   0 runner    (1001) docker     (127)      369 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/common/plotly_offline.js
+-rw-r--r--   0 runner    (1001) docker     (127)      385 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/common/plotly_online.js
+-rw-r--r--   0 runner    (1001) docker     (127)      342 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/common/plotly_requires.js
+-rw-r--r--   0 runner    (1001) docker     (127)      269 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/js/generic_plotly.js
+-rw-r--r--   0 runner    (1001) docker     (127)     2417 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/marginal_effects.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2849 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/parallel_coordinates.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38739 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/pareto_frontier.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29036 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/pareto_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4377 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/render.py
+-rw-r--r--   0 runner    (1001) docker     (127)    61765 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/scatter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20098 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/slice.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5661 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/table_view.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.685140 ax-platform-0.4.0/ax/plot/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.685140 ax-platform-0.4.0/ax/plot/tests/long_running/
+-rw-r--r--   0 runner    (1001) docker     (127)     2474 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/long_running/test_pareto_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3097 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_contours.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2172 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_diagnostic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4927 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_feature_importances.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2114 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_fitted_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1306 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      838 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_parallel_coordinates.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16776 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_pareto_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1576 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_slices.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_tile_fitted.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4410 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/tests/test_traces.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31686 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/plot/trace.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.685140 ax-platform-0.4.0/ax/runners/
+-rw-r--r--   0 runner    (1001) docker     (127)      409 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3341 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/simulated_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2372 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/single_running_trial_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1333 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/synthetic.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.685140 ax-platform-0.4.0/ax/runners/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1761 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/tests/test_single_running_trial_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6303 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/tests/test_torchx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7008 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/runners/torchx.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.685140 ax-platform-0.4.0/ax/service/
+-rw-r--r--   0 runner    (1001) docker     (127)      326 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    82405 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/ax_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8302 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/interactive_loop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11970 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/managed_loop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    90998 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.689140 ax-platform-0.4.0/ax/service/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    94141 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/scheduler_test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)   125800 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_ax_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7246 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_best_point.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23012 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_best_point_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5420 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_global_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13799 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_instantiation_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_interactive_loop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19719 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_managed_loop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    50611 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_report_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      320 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14919 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/tests/test_with_db_settings_base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.689140 ax-platform-0.4.0/ax/service/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35788 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/best_point.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27338 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/best_point_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1982 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37597 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/instantiation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    60756 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/report_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7648 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/scheduler_options.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22834 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/service/utils/with_db_settings_base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.693140 ax-platform-0.4.0/ax/storage/
+-rw-r--r--   0 runner    (1001) docker     (127)      328 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9897 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/botorch_modular_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.693140 ax-platform-0.4.0/ax/storage/json_store/
+-rw-r--r--   0 runner    (1001) docker     (127)      393 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38366 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12071 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/decoders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6761 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29299 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/encoders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1232 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/load.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17102 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1758 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/save.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.693140 ax-platform-0.4.0/ax/storage/json_store/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27766 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/json_store/tests/test_json_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4745 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/metric_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9342 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/registry_bundle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4247 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/runner_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.697140 ax-platform-0.4.0/ax/storage/sqa_store/
+-rw-r--r--   0 runner    (1001) docker     (127)      558 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8945 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/db.py
+-rw-r--r--   0 runner    (1001) docker     (127)    51914 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/delete.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45403 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3374 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/json.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22286 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/load.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1652 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/reduced_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21279 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/save.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26059 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/sqa_classes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4495 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/sqa_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2371 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/sqa_enum.py
+-rw-r--r--   0 runner    (1001) docker     (127)      856 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/structs.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.697140 ax-platform-0.4.0/ax/storage/sqa_store/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    81504 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/tests/test_sqa_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6592 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/tests/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      979 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/timestamp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6409 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4024 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/sqa_store/validation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.697140 ax-platform-0.4.0/ax/storage/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     2110 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/tests/test_botorch_modular_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1686 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/tests/test_registry_bundle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3746 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/transform_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/storage/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.697140 ax-platform-0.4.0/ax/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (127)     4744 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/ax_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3245 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11028 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/experiment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2377 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/generation_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23465 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/optimization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8774 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.697140 ax-platform-0.4.0/ax/telemetry/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     6576 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/tests/test_ax_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2590 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/tests/test_experiment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/tests/test_generation_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11040 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/tests/test_optimization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11178 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/telemetry/tests/test_scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.701140 ax-platform-0.4.0/ax/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.701140 ax-platform-0.4.0/ax/utils/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2103 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2899 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2844 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/decorator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1631 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/docutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9123 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/equality.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10627 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/executils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5293 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/kwargs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7742 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/logger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1110 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/mock.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1464 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/random.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6763 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/result.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5434 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/serialization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/common/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_docutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2685 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_equality.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10233 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_executils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5776 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_kwargutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2929 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_logger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2897 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_random.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2290 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_result.py
+-rw-r--r--   0 runner    (1001) docker     (127)      839 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_serialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3408 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_testutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2128 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/tests/test_typeutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19494 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/testutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1420 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/timeutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3983 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/typeutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1693 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/typeutils_nonnative.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1203 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/common/typeutils_torch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/flake8_plugins/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/flake8_plugins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3279 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/flake8_plugins/docstring_checker.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/measurement/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/measurement/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11013 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/measurement/synthetic_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/measurement/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/measurement/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6197 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/measurement/tests/test_synthetic_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/notebook/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/notebook/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1618 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/notebook/plotting.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/report/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/render.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/report/resources/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/resources/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/resources/base_template.html
+-rw-r--r--   0 runner    (1001) docker     (127)     3150 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/resources/report.css
+-rw-r--r--   0 runner    (1001) docker     (127)      215 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/resources/simple_template.html
+-rw-r--r--   0 runner    (1001) docker     (127)      657 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/resources/sufficient_statistic.html
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/report/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1115 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/report/tests/test_render.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.705140 ax-platform-0.4.0/ax/utils/sensitivity/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/sensitivity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/sensitivity/derivative_gp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18260 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/sensitivity/derivative_measures.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40256 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/sensitivity/sobol_measures.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/sensitivity/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/sensitivity/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20203 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/sensitivity/tests/test_sensitivity.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/stats/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/stats/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10248 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/stats/model_fit_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16652 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/stats/statstools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/stats/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/stats/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3151 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/stats/tests/test_model_fit_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6998 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/stats/tests/test_statstools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4292 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/backend_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18621 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/backend_simulator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6632 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/benchmark_stubs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    75265 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/core_stubs.py
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/manifest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/testing/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1664 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/metrics/backend_simulator_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2447 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/metrics/branin_backend_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5550 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/mock.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21654 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/modeling_stubs.py
+-rw-r--r--   0 runner    (1001) docker     (127)      807 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/test_init_files.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/testing/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5417 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/tests/test_backend_simulator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1554 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/torch_stubs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/testing/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.709140 ax-platform-0.4.0/ax/utils/tutorials/
+-rw-r--r--   0 runner    (1001) docker     (127)      202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/tutorials/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9262 2024-05-02 22:04:28.000000 ax-platform-0.4.0/ax/utils/tutorials/cnn_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      411 2024-05-02 22:09:20.000000 ax-platform-0.4.0/ax/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.713140 ax-platform-0.4.0/ax_platform.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     8508 2024-05-02 22:09:20.000000 ax-platform-0.4.0/ax_platform.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    24491 2024-05-02 22:09:20.000000 ax-platform-0.4.0/ax_platform.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-02 22:09:20.000000 ax-platform-0.4.0/ax_platform.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-05-02 22:09:20.000000 ax-platform-0.4.0/ax_platform.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        3 2024-05-02 22:09:20.000000 ax-platform-0.4.0/ax_platform.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.713140 ax-platform-0.4.0/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)      145 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/algo-overview.md
+-rw-r--r--   0 runner    (1001) docker     (127)     6624 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/api.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.717141 ax-platform-0.4.0/docs/assets/
+-rw-r--r--   0 runner    (1001) docker     (127)    81587 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/bandit_allocation.png
+-rw-r--r--   0 runner    (1001) docker     (127)   345484 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/bo_1d_opt.gif
+-rw-r--r--   0 runner    (1001) docker     (127)   156260 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/contour.js
+-rw-r--r--   0 runner    (1001) docker     (127)    14875 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/cv.js
+-rw-r--r--   0 runner    (1001) docker     (127)   131612 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/example_shrinkage.png
+-rw-r--r--   0 runner    (1001) docker     (127)    13598 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/fitted.js
+-rw-r--r--   0 runner    (1001) docker     (127)    65905 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/gp_opt.png
+-rw-r--r--   0 runner    (1001) docker     (127)   349223 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/gp_posterior.png
+-rw-r--r--   0 runner    (1001) docker     (127)   121838 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/mab_animate.gif
+-rw-r--r--   0 runner    (1001) docker     (127)   132372 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/mab_probs.png
+-rw-r--r--   0 runner    (1001) docker     (127)   325134 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/mab_regret.png
+-rw-r--r--   0 runner    (1001) docker     (127)    26226 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/assets/slice.js
+-rw-r--r--   0 runner    (1001) docker     (127)     8863 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/banditopt.md
+-rw-r--r--   0 runner    (1001) docker     (127)    10181 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/bayesopt.md
+-rw-r--r--   0 runner    (1001) docker     (127)     9871 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/core.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2484 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/data.md
+-rw-r--r--   0 runner    (1001) docker     (127)     6876 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/glossary.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3370 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/installation.md
+-rw-r--r--   0 runner    (1001) docker     (127)    22509 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/models.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7454 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/storage.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/trial-evaluation.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2218 2024-05-02 22:04:28.000000 ax-platform-0.4.0/docs/why-ax.md
+-rw-r--r--   0 runner    (1001) docker     (127)      158 2024-05-02 22:04:28.000000 ax-platform-0.4.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)      319 2024-05-02 22:04:28.000000 ax-platform-0.4.0/pytest.ini
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.721140 ax-platform-0.4.0/scripts/
+-rw-r--r--   0 runner    (1001) docker     (127)      342 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/import_ax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2257 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/insert_api_refs.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3855 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/make_docs.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     9551 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/make_tutorials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3195 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/parse_sphinx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1444 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/patch_site_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8153 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/publish_site.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     2477 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/update_versions_html.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4119 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/validate_sphinx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2981 2024-05-02 22:04:28.000000 ax-platform-0.4.0/scripts/versions.js
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-02 22:09:20.737141 ax-platform-0.4.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     3793 2024-05-02 22:04:28.000000 ax-platform-0.4.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.721140 ax-platform-0.4.0/sphinx/
+-rw-r--r--   0 runner    (1001) docker     (127)     7466 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)     7442 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/make.bat
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.721140 ax-platform-0.4.0/sphinx/source/
+-rw-r--r--   0 runner    (1001) docker     (127)      151 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/ax.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3461 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/benchmark.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     8470 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3352 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/core.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      941 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/early_stopping.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      844 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/exceptions.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      511 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/global_stopping.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      504 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1926 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/metrics.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     9511 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/modelbridge.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     8391 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/models.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2021 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/plot.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      840 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/runners.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1557 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/service.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/storage.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      520 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/telemetry.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5269 2024-05-02 22:04:28.000000 ax-platform-0.4.0/sphinx/source/utils.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.729141 ax-platform-0.4.0/tutorials/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/tutorials/early_stopping/
+-rw-r--r--   0 runner    (1001) docker     (127)    24254 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/early_stopping/early_stopping.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     5450 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/early_stopping/mnist_train_nas.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17920 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/external_generation_node.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    22810 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/factorial.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    19338 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/generation_strategy.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    23875 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/gpei_hartmann_developer.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     7217 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/gpei_hartmann_loop.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    17911 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/gpei_hartmann_service.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    18017 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/gss.ipynb
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/tutorials/human_in_the_loop/
+-rw-r--r--   0 runner    (1001) docker     (127)    16283 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/human_in_the_loop/hitl_data.json
+-rw-r--r--   0 runner    (1001) docker     (127)   634579 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/human_in_the_loop/hitl_exp.json
+-rw-r--r--   0 runner    (1001) docker     (127)    21078 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/human_in_the_loop/human_in_the_loop.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    36202 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/modular_botax.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    23173 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/multi_task.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)   462777 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/multiobjective_optimization.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    11514 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/raytune_pytorch_cnn.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    12518 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/saasbo.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    22937 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/saasbo_nehvi.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)   287298 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/scheduler.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    20192 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/sebo.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)  4095331 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/submitit.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    31120 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/tune_cnn_service.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    13031 2024-05-02 22:04:28.000000 ax-platform-0.4.0/tutorials/visualizations.ipynb
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/website/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/website/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/core/Footer.js
+-rw-r--r--   0 runner    (1001) docker     (127)     4320 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/core/Tutorial.js
+-rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/core/TutorialSidebar.js
+-rw-r--r--   0 runner    (1001) docker     (127)      375 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/package.json
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.617140 ax-platform-0.4.0/website/pages/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/website/pages/en/
+-rw-r--r--   0 runner    (1001) docker     (127)     6646 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/pages/en/index.js
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/website/pages/tutorials/
+-rw-r--r--   0 runner    (1001) docker     (127)     7553 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/pages/tutorials/index.js
+-rw-r--r--   0 runner    (1001) docker     (127)      233 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/sidebars.json
+-rw-r--r--   0 runner    (1001) docker     (127)     2740 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/siteConfig.js
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/website/static/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/.nojekyll
+-rw-r--r--   0 runner    (1001) docker     (127)        7 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/CNAME
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.733141 ax-platform-0.4.0/website/static/css/
+-rw-r--r--   0 runner    (1001) docker     (127)    10913 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/css/base_sphinx.css
+-rw-r--r--   0 runner    (1001) docker     (127)     4960 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/css/custom.css
+-rw-r--r--   0 runner    (1001) docker     (127)    15413 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/css/custom_sphinx.css
+-rw-r--r--   0 runner    (1001) docker     (127)     4315 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/css/pygments.css
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.737141 ax-platform-0.4.0/website/static/img/
+-rw-r--r--   0 runner    (1001) docker     (127)     1981 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/ax.svg
+-rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/ax_lockup.svg
+-rw-r--r--   0 runner    (1001) docker     (127)     2244 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/ax_lockup_white.svg
+-rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/ax_logo_lockup.svg
+-rw-r--r--   0 runner    (1001) docker     (127)      369 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/ax_wireframe.svg
+-rw-r--r--   0 runner    (1001) docker     (127)      785 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/database-solid.svg
+-rw-r--r--   0 runner    (1001) docker     (127)     1360 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/dice-solid.svg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.737141 ax-platform-0.4.0/website/static/img/favicon/
+-rw-r--r--   0 runner    (1001) docker     (127)     9662 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/favicon/favicon.ico
+-rw-r--r--   0 runner    (1001) docker     (127)     2731 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/favicon.png
+-rw-r--r--   0 runner    (1001) docker     (127)     4370 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/oss_logo.png
+-rw-r--r--   0 runner    (1001) docker     (127)      910 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/img/th-large-solid.svg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.737141 ax-platform-0.4.0/website/static/js/
+-rw-r--r--   0 runner    (1001) docker     (127)      716 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/js/mathjax.js
+-rw-r--r--   0 runner    (1001) docker     (127)     1491 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/static/js/plotUtils.js
+-rw-r--r--   0 runner    (1001) docker     (127)     2074 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/tutorials.json
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.737141 ax-platform-0.4.0/website/versioned_docs/
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/versioned_docs/.gitkeep
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-02 22:09:20.737141 ax-platform-0.4.0/website/versioned_sidebars/
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-02 22:04:28.000000 ax-platform-0.4.0/website/versioned_sidebars/.gitkeep
```

### Comparing `ax-platform-0.3.7/.github/workflows/build-and-test.yml` & `ax-platform-0.4.0/.github/workflows/build-and-test.yml`

 * *Files 7% similar despite different names*

```diff
@@ -9,31 +9,32 @@
 
 jobs:
   tests-and-coverage:
     name: Tests with latest BoTorch
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: false
+    secrets: inherit
 
   lint:
 
     runs-on: ubuntu-latest
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
     - name: Install dependencies
       # Pin ufmt deps so they match intermal pyfmt.
       run: |
-        pip install black==22.3.0
-        pip install usort==1.0.2
-        pip install libcst==0.3.19
+        pip install black==24.2.0
+        pip install usort==1.0.8
+        pip install libcst==1.1.0
         pip install ufmt
         pip install flake8
     - name: ufmt
       run: |
         ufmt diff .
     - name: Flake8
       # run even if previous step (ufmt) failed
@@ -42,19 +43,19 @@
         flake8
 
   docs:
 
     runs-on: ubuntu-latest
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
     - name: Install dependencies
       env:
         ALLOW_BOTORCH_LATEST: true
         ALLOW_LATEST_GPYTORCH_LINOP: true
       run: |
         # use latest Botorch
         pip install git+https://github.com/cornellius-gp/gpytorch.git
```

### Comparing `ax-platform-0.3.7/.github/workflows/cron.yml` & `ax-platform-0.4.0/.github/workflows/cron.yml`

 * *Files 7% similar despite different names*

```diff
@@ -15,39 +15,41 @@
 
   tests-and-coverage-minimal:
     name: Tests with latest BoTorch & minimal dependencies
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: false
       minimal_dependencies: true
+    secrets: inherit
 
   tests-and-coverage-full:
     name: Tests with latest BoTorch & full dependencies
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: false
       minimal_dependencies: false
+    secrets: inherit
 
   build-tutorials:
     name: Build tutorials with latest BoTorch
     uses: ./.github/workflows/reusable_tutorials.yml
     with:
       smoke_test: false
       pinned_botorch: false
 
   publish-latest-website:
 
     runs-on: ubuntu-latest
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
     - name: Install dependencies
       env:
         ALLOW_BOTORCH_LATEST: true
         ALLOW_LATEST_GPYTORCH_LINOP: true
       run: |
         # use latest BoTorch
         pip install git+https://github.com/cornellius-gp/gpytorch.git
@@ -60,21 +62,21 @@
         bash scripts/publish_site.sh -d
 
   deploy-test-pypi:
 
     runs-on: ubuntu-latest
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Fetch all history for all tags and branches
       run: git fetch --prune --unshallow
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
     - name: Install dependencies
       env:
         ALLOW_BOTORCH_LATEST: true
         ALLOW_LATEST_GPYTORCH_LINOP: true
       run: |
         # use latest BoTorch
         pip install git+https://github.com/cornellius-gp/gpytorch.git
```

### Comparing `ax-platform-0.3.7/.github/workflows/cron_pinned.yml` & `ax-platform-0.4.0/.github/workflows/cron_pinned.yml`

 * *Files 22% similar despite different names*

```diff
@@ -8,21 +8,23 @@
 
   tests-and-coverage-minimal:
     name: Tests with pinned BoTorch & minimal dependencies
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: true
       minimal_dependencies: true
+    secrets: inherit
 
   tests-and-coverage-full:
     name: Tests with pinned BoTorch & full dependencies
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: true
       minimal_dependencies: false
+    secrets: inherit
 
   build-tutorials:
     name: Build tutorials with pinned BoTorch
     uses: ./.github/workflows/reusable_tutorials.yml
     with:
       smoke_test: false
       pinned_botorch: true
```

### Comparing `ax-platform-0.3.7/.github/workflows/deploy.yml` & `ax-platform-0.4.0/.github/workflows/deploy.yml`

 * *Files 11% similar despite different names*

```diff
@@ -9,32 +9,34 @@
 
 jobs:
   tests-and-coverage-latest:
     name: Tests with latest BoTorch
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: false
+    secrets: inherit
 
   tests-and-coverage-pinned:
     name: Tests with pinned BoTorch
     uses: ./.github/workflows/reusable_test.yml
     with:
       pinned_botorch: true
+    secrets: inherit
 
   publish-stable-website:
 
     needs: tests-and-coverage-pinned # only run if test step succeeds
     runs-on: ubuntu-latest
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
     - name: Install dependencies
       run: |
         # use stable Botorch
         pip install -e ".[tutorial]"
     - name: Publish latest website
       env:
         DOCUSAURUS_PUBLISH_TOKEN: ${{ secrets.DOCUSAURUS_PUBLISH_TOKEN }}
@@ -43,19 +45,19 @@
 
   deploy:
 
     needs: tests-and-coverage-pinned # only run if test step succeeds
     runs-on: ubuntu-latest
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
     - name: Install dependencies
       run: |
         # use stable Botorch
         pip install -e ".[dev,mysql,notebook]"
         pip install wheel
     - name: Fetch all history for all tags and branches
       run: git fetch --prune --unshallow
```

### Comparing `ax-platform-0.3.7/.github/workflows/reusable_test.yml` & `ax-platform-0.4.0/.github/workflows/reusable_test.yml`

 * *Files 10% similar despite different names*

```diff
@@ -21,21 +21,21 @@
         default: false
 
 jobs:
   tests-and-coverage:
     runs-on: ubuntu-latest
     strategy:
       matrix:
-        python-version: ["3.9", "3.11"]
+        python-version: ["3.10", "3.11"]
       fail-fast: false
 
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python ${{ matrix.python-version }}
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
         python-version: ${{ matrix.python-version }}
 
     - if: ${{ inputs.pinned_botorch }}
       name: Install dependencies with pinned BoTorch (minimal dependencies ${{ inputs.minimal_dependencies }})
       run: |
         # The brackets returns '.[unittest_minimal]' if using minimal dependencies and '.[unittest]'
@@ -57,12 +57,14 @@
       run: |
         python scripts/import_ax.py
     - if: ${{ !inputs.minimal_dependencies }}
       # Only run with full dependencies. Minimal does not include pytest.
       name: Tests and coverage
       run: |
         pytest -ra --cov=ax
-    - if: ${{ !inputs.minimal_dependencies }}
-      # Using same condition as above since we need the coverage report for upload.
+    - if: ${{ !inputs.minimal_dependencies && matrix.python-version == 3.10 }}
+      # Only upload codecov once per workflow.
       name: Upload coverage
-      run: |
-        bash <(curl -s https://codecov.io/bash)
+      uses: codecov/codecov-action@v4
+      with:
+        fail_ci_if_error: true
+        token: ${{ secrets.CODECOV_TOKEN }}
```

### Comparing `ax-platform-0.3.7/.github/workflows/reusable_tutorials.yml` & `ax-platform-0.4.0/.github/workflows/reusable_tutorials.yml`

 * *Files 2% similar despite different names*

```diff
@@ -20,19 +20,19 @@
 
 jobs:
 
   build-tutorials:
     name: Tutorials
     runs-on: ubuntu-latest
     steps:
-    - uses: actions/checkout@v3
+    - uses: actions/checkout@v4
     - name: Set up Python
-      uses: actions/setup-python@v4
+      uses: actions/setup-python@v5
       with:
-        python-version: "3.9"
+        python-version: "3.10"
 
     - if: ${{ inputs.pinned_botorch }}
       name: Install dependencies with pinned BoTorch
       run: |
         pip install -e ".[tutorial]"
 
     - if: ${{ !inputs.pinned_botorch }}
```

### Comparing `ax-platform-0.3.7/.gitignore` & `ax-platform-0.4.0/.gitignore`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/CODE_OF_CONDUCT.md` & `ax-platform-0.4.0/CODE_OF_CONDUCT.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/CONTRIBUTING.md` & `ax-platform-0.4.0/CONTRIBUTING.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/LICENSE` & `ax-platform-0.4.0/LICENSE`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/PKG-INFO` & `ax-platform-0.4.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 Metadata-Version: 2.1
 Name: ax-platform
-Version: 0.3.7
+Version: 0.4.0
 Summary: Adaptive Experimentation
 Home-page: https://github.com/facebook/Ax
 Author: Facebook, Inc.
 License: MIT
 Keywords: Experimentation,Optimization
-Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Programming Language :: Python :: 3
-Requires-Python: >=3.9
+Requires-Python: >=3.10
 Description-Content-Type: text/markdown
 Provides-Extra: dev
 Provides-Extra: mysql
 Provides-Extra: notebook
 Provides-Extra: unittest
 Provides-Extra: unittest_minimal
 Provides-Extra: tutorial
@@ -82,15 +81,15 @@
 
 # best_parameters contains {'x1': 1.02, 'x2': 2.97}; the global min is (1, 3)
 ```
 
 ## Installation
 
 ### Requirements
-You need Python 3.9 or later to run Ax.
+You need Python 3.10 or later to run Ax.
 
 The required Python dependencies are:
 
 * [botorch](https://www.botorch.org)
 * jinja2
 * pandas
 * scipy
@@ -200,9 +199,7 @@
 [`--depth`](https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt)
 argument to `git clone`. If you require the entire commit history you may remove this
 argument.
 
 ## License
 
 Ax is licensed under the [MIT license](./LICENSE).
-
-
```

### Comparing `ax-platform-0.3.7/README.md` & `ax-platform-0.4.0/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -59,15 +59,15 @@
 
 # best_parameters contains {'x1': 1.02, 'x2': 2.97}; the global min is (1, 3)
 ```
 
 ## Installation
 
 ### Requirements
-You need Python 3.9 or later to run Ax.
+You need Python 3.10 or later to run Ax.
 
 The required Python dependencies are:
 
 * [botorch](https://www.botorch.org)
 * jinja2
 * pandas
 * scipy
```

### Comparing `ax-platform-0.3.7/ax/__init__.py` & `ax-platform-0.4.0/ax/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core import (
     Arm,
     BatchTrial,
     ChoiceParameter,
     ComparisonOp,
     Data,
     Experiment,
```

### Comparing `ax-platform-0.3.7/ax/analysis/base_analysis.py` & `ax-platform-0.4.0/ax/analysis/base_analysis.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from abc import ABC, abstractmethod
 
 import pandas as pd
 from ax.core.experiment import Experiment
 
 
 class BaseAnalysis(ABC):
```

### Comparing `ax-platform-0.3.7/ax/analysis/base_plotly_visualization.py` & `ax-platform-0.4.0/ax/analysis/base_plotly_visualization.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from abc import abstractmethod
 
 import plotly.graph_objects as go
 
 from ax.analysis.base_analysis import BaseAnalysis
```

### Comparing `ax-platform-0.3.7/ax/analysis/parallel_coordinates_plot.py` & `ax-platform-0.4.0/ax/analysis/parallel_coordinates_plot.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional, Tuple
 
 import pandas as pd
 
+from ax.analysis.base_plotly_visualization import BasePlotlyVisualization
+
 from ax.core.arm import Arm
 from ax.core.base_trial import BaseTrial
 
 from ax.core.batch_trial import BatchTrial
 from ax.core.experiment import Experiment
 from ax.core.trial import Trial
 
 from plotly import express as px, graph_objs as go
 
-from .base_plotly_visualization import BasePlotlyVisualization
-
 
 class ParallelCoordinatesPlot(BasePlotlyVisualization):
     def __init__(
         self,
         experiment: Experiment,
         objective_name: Optional[str] = None,
     ) -> None:
```

### Comparing `ax-platform-0.3.7/ax/analysis/tests/test_parallel_coordinates_plot.py` & `ax-platform-0.4.0/ax/analysis/tests/test_parallel_coordinates_plot.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from ax.analysis.parallel_coordinates_plot import ParallelCoordinatesPlot
 from ax.core.batch_trial import BatchTrial
 
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_data_batch, get_branin_experiment
 from plotly import graph_objs as go
```

### Comparing `ax-platform-0.3.7/ax/benchmark/benchmark_problem.py` & `ax-platform-0.4.0/ax/benchmark/benchmark_problem.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,72 +1,79 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+# NOTE: Do not add `from __future__ import annotations` to this file. Adding
+# `annotations` postpones evaluation of types and will break FBLearner's usage of
+# `BenchmarkProblem` as return type annotation, used for serialization and rendering
+# in the UI.
 
 import abc
-from typing import Any, Dict, List, Optional, Protocol, runtime_checkable, Type
+from typing import Any, Dict, List, Optional, Protocol, runtime_checkable, Type, Union
 
-from ax.core.metric import Metric
+from ax.benchmark.metrics.base import BenchmarkMetricBase
 
+from ax.benchmark.metrics.benchmark import BenchmarkMetric
+from ax.benchmark.runners.botorch_test import BotorchTestProblemRunner
 from ax.core.objective import MultiObjective, Objective
 from ax.core.optimization_config import (
     MultiObjectiveOptimizationConfig,
     ObjectiveThreshold,
     OptimizationConfig,
 )
 from ax.core.outcome_constraint import OutcomeConstraint
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.runner import Runner
 from ax.core.search_space import SearchSpace
 from ax.core.types import ComparisonOp
-from ax.metrics.botorch_test_problem import BotorchTestProblemMetric
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
 from ax.utils.common.base import Base
 from ax.utils.common.typeutils import checked_cast
 from botorch.test_functions.base import BaseTestProblem, ConstrainedBaseTestProblem
 from botorch.test_functions.multi_objective import MultiObjectiveTestProblem
 from botorch.test_functions.synthetic import SyntheticTestFunction
 
-# NOTE: Do not add `from __future__ import annotatations` to this file. Adding
-# `annotations` postpones evaluation of types and will break FBLearner's usage of
-# `BenchmarkProblem` as return type annotation, used for serialization and rendering
-# in the UI.
-
 
 def _get_name(
-    test_problem: BaseTestProblem, infer_noise: bool, dim: Optional[int] = None
+    test_problem: BaseTestProblem,
+    observe_noise_sd: bool,
+    dim: Optional[int] = None,
 ) -> str:
     """
     Get a string name describing the problem, in a format such as
     "hartmann_fixed_noise_6d" or "jenatton" (where the latter would
     not have fixed noise and have the default dimensionality).
     """
     base_name = f"{test_problem.__class__.__name__}"
-    fixed_noise = "" if infer_noise else "_fixed_noise"
+    observed_noise = "_observed_noise" if observe_noise_sd else ""
     dim_str = "" if dim is None else f"_{dim}d"
-    return f"{base_name}{fixed_noise}{dim_str}"
+    return f"{base_name}{observed_noise}{dim_str}"
 
 
 @runtime_checkable
-class BenchmarkProblemBase(Protocol):
+class BenchmarkProblemProtocol(Protocol):
     """
     Specifies the interface any benchmark problem must adhere to.
 
-    Subclasses include BenchmarkProblem, SurrogateBenchmarkProblem, and
-    MOOSurrogateBenchmarkProblem.
+    Classes implementing this interface include BenchmarkProblem,
+    SurrogateBenchmarkProblem, and MOOSurrogateBenchmarkProblem.
     """
 
     name: str
     search_space: SearchSpace
     optimization_config: OptimizationConfig
     num_trials: int
-    infer_noise: bool
-    tracking_metrics: List[Metric]
+    tracking_metrics: List[BenchmarkMetricBase]
+    is_noiseless: bool  # If True, evaluations are deterministic
+    observe_noise_stds: Union[
+        bool, Dict[str, bool]
+    ]  # Whether we observe the observation noise level
+    has_ground_truth: bool  # if True, evals (w/o synthetic noise) are determinstic
 
     @abc.abstractproperty
     def runner(self) -> Runner:
         pass  # pragma: no cover
 
 
 @runtime_checkable
@@ -82,152 +89,155 @@
     def __init__(
         self,
         name: str,
         search_space: SearchSpace,
         optimization_config: OptimizationConfig,
         runner: Runner,
         num_trials: int,
-        infer_noise: bool,
-        tracking_metrics: Optional[List[Metric]] = None,
+        is_noiseless: bool = False,
+        observe_noise_sd: bool = False,
+        has_ground_truth: bool = False,
+        tracking_metrics: Optional[List[BenchmarkMetricBase]] = None,
     ) -> None:
         self.name = name
         self.search_space = search_space
         self.optimization_config = optimization_config
         self._runner = runner
         self.num_trials = num_trials
-        self.infer_noise = infer_noise
-        self.tracking_metrics: List[Metric] = (
-            [] if tracking_metrics is None else tracking_metrics
-        )
+        self.is_noiseless = is_noiseless
+        self.observe_noise_sd = observe_noise_sd
+        self.has_ground_truth = has_ground_truth
+        self.tracking_metrics: List[BenchmarkMetricBase] = tracking_metrics or []
 
     @property
     def runner(self) -> Runner:
         return self._runner
 
+    @property
+    def observe_noise_stds(self) -> Union[bool, Dict[str, bool]]:
+        # TODO: Handle cases where some outcomes have noise levels observed
+        # and others do not.
+        return self.observe_noise_sd
+
     @classmethod
     def from_botorch(
         cls,
         test_problem_class: Type[BaseTestProblem],
         test_problem_kwargs: Dict[str, Any],
+        lower_is_better: bool,
         num_trials: int,
-        infer_noise: bool = True,
+        observe_noise_sd: bool = False,
     ) -> "BenchmarkProblem":
         """
         Create a BenchmarkProblem from a BoTorch BaseTestProblem using
         specialized Metrics and Runners. The test problem's result will be
         computed on the Runner and retrieved by the Metric.
 
         Args:
-            test_problem_class: The BoTorch test problem class which will be
-                used to define the `search_space`, `optimization_config`, and
-                `runner`.
+            test_problem_class: The BoTorch test problem class which will be used
+                to define the `search_space`, `optimization_config`, and `runner`.
             test_problem_kwargs: Keyword arguments used to instantiate the
                 `test_problem_class`.
-            num_trials: Simply the `num_trials` of the `BenchmarkProblem`
-                created.
-            infer_noise: Whether noise will be inferred. This is separate from
-                whether synthetic noise is added to the problem, which is
-                controlled by the `noise_std` of the test problem.
+            num_trials: Simply the `num_trials` of the `BenchmarkProblem` created.
+            observe_noise_sd: Whether the standard deviation of the observation noise is
+                observed or not (in which case it must be inferred by the model).
+                This is separate from whether synthetic noise is added to the
+                problem, which is controlled by the `noise_std` of the test problem.
         """
 
         # pyre-fixme [45]: Invalid class instantiation
         test_problem = test_problem_class(**test_problem_kwargs)
         is_constrained = isinstance(test_problem, ConstrainedBaseTestProblem)
 
         search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     name=f"x{i}",
                     parameter_type=ParameterType.FLOAT,
-                    lower=test_problem._bounds[i][0],
-                    upper=test_problem._bounds[i][1],
+                    lower=lower,
+                    upper=upper,
                 )
-                for i in range(test_problem.dim)
+                for i, (lower, upper) in enumerate(test_problem._bounds)
             ]
         )
 
         dim = test_problem_kwargs.get("dim", None)
-        name = _get_name(test_problem, infer_noise, dim)
-
-        if infer_noise:
-            noise_sd = None
-        elif isinstance(test_problem.noise_std, list):
-            # Convention is to have the first outcome be the objective,
-            # and the remaining ones the constraints.
-            noise_sd = test_problem.noise_std[0]
-        else:
-            noise_sd = checked_cast(float, test_problem.noise_std or 0.0)
+        name = _get_name(
+            test_problem=test_problem, observe_noise_sd=observe_noise_sd, dim=dim
+        )
 
         # TODO: Support constrained MOO problems.
 
         objective = Objective(
-            metric=BotorchTestProblemMetric(
+            metric=BenchmarkMetric(
                 name=name,
-                noise_sd=noise_sd,
-                index=0,
+                lower_is_better=lower_is_better,
+                observe_noise_sd=observe_noise_sd,
+                outcome_index=0,
             ),
             minimize=True,
         )
 
+        outcome_names = [name]
+        outcome_constraints = []
+
+        # NOTE: Currently we don't support the case where only some of the
+        # outcomes have noise levels observed.
+
         if is_constrained:
-            n_con = test_problem.num_constraints
-            if infer_noise:
-                constraint_noise_sds = [None] * n_con
-            elif test_problem.constraint_noise_std is None:
-                constraint_noise_sds = [0.0] * n_con
-            elif isinstance(test_problem.constraint_noise_std, list):
-                constraint_noise_sds = test_problem.constraint_noise_std[:n_con]
-            else:
-                constraint_noise_sds = [test_problem.constraint_noise_std] * n_con
-
-            outcome_constraints = [
-                OutcomeConstraint(
-                    metric=BotorchTestProblemMetric(
-                        name=f"constraint_slack_{i}",
-                        noise_sd=constraint_noise_sds[i],
-                        index=i,
-                    ),
-                    op=ComparisonOp.GEQ,
-                    bound=0.0,
-                    relative=False,
+            for i in range(test_problem.num_constraints):
+                outcome_name = f"constraint_slack_{i}"
+                outcome_constraints.append(
+                    OutcomeConstraint(
+                        metric=BenchmarkMetric(
+                            name=outcome_name,
+                            lower_is_better=False,  # positive slack = feasible
+                            observe_noise_sd=observe_noise_sd,
+                            outcome_index=i,
+                        ),
+                        op=ComparisonOp.GEQ,
+                        bound=0.0,
+                        relative=False,
+                    )
                 )
-                for i in range(n_con)
-            ]
-
-        else:
-            outcome_constraints = []
+                outcome_names.append(outcome_name)
 
         optimization_config = OptimizationConfig(
             objective=objective,
             outcome_constraints=outcome_constraints,
         )
 
         return cls(
             name=name,
             search_space=search_space,
             optimization_config=optimization_config,
             runner=BotorchTestProblemRunner(
                 test_problem_class=test_problem_class,
                 test_problem_kwargs=test_problem_kwargs,
+                outcome_names=outcome_names,
             ),
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            observe_noise_sd=observe_noise_sd,
+            is_noiseless=test_problem.noise_std in (None, 0.0),
+            has_ground_truth=True,  # all synthetic problems have ground truth
         )
 
     def __repr__(self) -> str:
         """
         Return a string representation that includes only the attributes that
         print nicely and contain information likely to be useful.
         """
         return (
             f"{self.__class__.__name__}("
             f"name={self.name}, "
             f"optimization_config={self.optimization_config}, "
             f"num_trials={self.num_trials}, "
-            f"infer_noise={self.infer_noise}, "
+            f"is_noiseless={self.is_noiseless}, "
+            f"observe_noise_sd={self.observe_noise_sd}, "
+            f"has_ground_truth={self.has_ground_truth}, "
             f"tracking_metrics={self.tracking_metrics})"
         )
 
 
 class SingleObjectiveBenchmarkProblem(BenchmarkProblem):
     """The most basic BenchmarkProblem, with a single objective and a known optimal
     value.
@@ -238,61 +248,71 @@
         optimal_value: float,
         *,
         name: str,
         search_space: SearchSpace,
         optimization_config: OptimizationConfig,
         runner: Runner,
         num_trials: int,
-        infer_noise: bool,
-        tracking_metrics: Optional[List[Metric]] = None,
+        is_noiseless: bool = False,
+        observe_noise_sd: bool = False,
+        has_ground_truth: bool = False,
+        tracking_metrics: Optional[List[BenchmarkMetricBase]] = None,
     ) -> None:
         super().__init__(
             name=name,
             search_space=search_space,
             optimization_config=optimization_config,
             runner=runner,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            is_noiseless=is_noiseless,
+            observe_noise_sd=observe_noise_sd,
+            has_ground_truth=has_ground_truth,
             tracking_metrics=tracking_metrics,
         )
         self.optimal_value = optimal_value
 
     @classmethod
     def from_botorch_synthetic(
         cls,
         test_problem_class: Type[SyntheticTestFunction],
         test_problem_kwargs: Dict[str, Any],
+        lower_is_better: bool,
         num_trials: int,
-        infer_noise: bool = True,
+        observe_noise_sd: bool = False,
     ) -> "SingleObjectiveBenchmarkProblem":
         """Create a BenchmarkProblem from a BoTorch BaseTestProblem using specialized
         Metrics and Runners. The test problem's result will be computed on the Runner
         and retrieved by the Metric.
         """
 
         # pyre-fixme [45]: Invalid class instantiation
         test_problem = test_problem_class(**test_problem_kwargs)
 
         problem = BenchmarkProblem.from_botorch(
             test_problem_class=test_problem_class,
             test_problem_kwargs=test_problem_kwargs,
+            lower_is_better=lower_is_better,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            observe_noise_sd=observe_noise_sd,
         )
 
         dim = test_problem_kwargs.get("dim", None)
-        name = _get_name(test_problem, infer_noise, dim)
+        name = _get_name(
+            test_problem=test_problem, observe_noise_sd=observe_noise_sd, dim=dim
+        )
 
         return cls(
             name=name,
             search_space=problem.search_space,
             optimization_config=problem.optimization_config,
             runner=problem.runner,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            is_noiseless=problem.is_noiseless,
+            observe_noise_sd=problem.observe_noise_sd,
+            has_ground_truth=problem.has_ground_truth,
             optimal_value=test_problem.optimal_value,
         )
 
 
 class MultiObjectiveBenchmarkProblem(BenchmarkProblem):
     """A BenchmarkProblem support multiple objectives. Rather than knowing each
     objective's optimal value we track a known maximum hypervolume computed from a
@@ -305,99 +325,106 @@
         reference_point: List[float],
         *,
         name: str,
         search_space: SearchSpace,
         optimization_config: OptimizationConfig,
         runner: Runner,
         num_trials: int,
-        infer_noise: bool,
-        tracking_metrics: Optional[List[Metric]] = None,
+        is_noiseless: bool = False,
+        observe_noise_sd: bool = False,
+        has_ground_truth: bool = False,
+        tracking_metrics: Optional[List[BenchmarkMetricBase]] = None,
     ) -> None:
         self.maximum_hypervolume = maximum_hypervolume
         self.reference_point = reference_point
         super().__init__(
             name=name,
             search_space=search_space,
             optimization_config=optimization_config,
             runner=runner,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            is_noiseless=is_noiseless,
+            observe_noise_sd=observe_noise_sd,
+            has_ground_truth=has_ground_truth,
             tracking_metrics=tracking_metrics,
         )
 
     @property
     def optimal_value(self) -> float:
         return self.maximum_hypervolume
 
     @classmethod
     def from_botorch_multi_objective(
         cls,
         test_problem_class: Type[MultiObjectiveTestProblem],
         test_problem_kwargs: Dict[str, Any],
+        # TODO: Figure out whether we should use `lower_is_better` here.
         num_trials: int,
-        infer_noise: bool = True,
+        observe_noise_sd: bool = False,
     ) -> "MultiObjectiveBenchmarkProblem":
         """Create a BenchmarkProblem from a BoTorch BaseTestProblem using specialized
         Metrics and Runners. The test problem's result will be computed on the Runner
         once per trial and each Metric will retrieve its own result by index.
         """
 
         # pyre-fixme [45]: Invalid class instantiation
         test_problem = test_problem_class(**test_problem_kwargs)
 
         problem = BenchmarkProblem.from_botorch(
             test_problem_class=test_problem_class,
             test_problem_kwargs=test_problem_kwargs,
+            lower_is_better=True,  # Seems like we always assume minimization for MOO?
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            observe_noise_sd=observe_noise_sd,
         )
 
         dim = test_problem_kwargs.get("dim", None)
-        name = _get_name(test_problem, infer_noise, dim)
+        name = _get_name(
+            test_problem=test_problem, observe_noise_sd=observe_noise_sd, dim=dim
+        )
 
         n_obj = test_problem.num_objectives
-        if infer_noise:
+        if not observe_noise_sd:
             noise_sds = [None] * n_obj
         elif isinstance(test_problem.noise_std, list):
             noise_sds = test_problem.noise_std
         else:
             noise_sds = [checked_cast(float, test_problem.noise_std or 0.0)] * n_obj
 
         metrics = [
-            BotorchTestProblemMetric(
+            BenchmarkMetric(
                 name=f"{name}_{i}",
-                noise_sd=noise_sd,
-                index=i,
+                lower_is_better=True,
+                observe_noise_sd=observe_noise_sd,
+                outcome_index=i,
             )
             for i, noise_sd in enumerate(noise_sds)
         ]
         optimization_config = MultiObjectiveOptimizationConfig(
             objective=MultiObjective(
                 objectives=[
-                    Objective(
-                        metric=metric,
-                        minimize=True,
-                    )
-                    for metric in metrics
+                    Objective(metric=metric, minimize=True) for metric in metrics
                 ]
             ),
             objective_thresholds=[
                 ObjectiveThreshold(
-                    metric=metrics[i],
+                    metric=metric,
                     bound=test_problem.ref_point[i].item(),
                     relative=False,
                     op=ComparisonOp.LEQ,
                 )
-                for i in range(test_problem.num_objectives)
+                for i, metric in enumerate(metrics)
             ],
         )
 
         return cls(
             name=name,
             search_space=problem.search_space,
             optimization_config=optimization_config,
             runner=problem.runner,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            is_noiseless=problem.is_noiseless,
+            observe_noise_sd=observe_noise_sd,
+            has_ground_truth=problem.has_ground_truth,
             maximum_hypervolume=test_problem.max_hv,
             reference_point=test_problem._ref_point,
         )
```

### Comparing `ax-platform-0.3.7/ax/benchmark/benchmark_result.py` & `ax-platform-0.4.0/ax/benchmark/benchmark_result.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+# NOTE: Do not add `from __future__ import annotatations` to this file. Adding
+# `annotations` postpones evaluation of types and will break FBLearner's usage of
+# `BenchmarkResult` as return type annotation, used for serialization and rendering
+# in the UI.
+
 from dataclasses import dataclass
 from typing import Dict, Iterable, List, Optional
 
 import numpy as np
 from ax.core.experiment import Experiment
 from ax.utils.common.base import Base
 from numpy import nanmean, nanquantile, ndarray
 from pandas import DataFrame
 from scipy.stats import sem
 
-# NOTE: Do not add `from __future__ import annotatations` to this file. Adding
-# `annotations` postpones evaluation of types and will break FBLearner's usage of
-# `BenchmarkResult` as return type annotation, used for serialization and rendering
-# in the UI.
-
 PERCENTILES = [0.25, 0.5, 0.75]
 
 
 @dataclass(eq=False)
 class BenchmarkResult(Base):
     """The result of a single optimization loop from one
     (BenchmarkProblem, BenchmarkMethod) pair.
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/hd_embedding.py` & `ax-platform-0.4.0/ax/benchmark/problems/hd_embedding.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import copy
 from typing import TypeVar
 
 from ax.benchmark.benchmark_problem import BenchmarkProblem
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/hpo/pytorch_cnn.py` & `ax-platform-0.4.0/ax/benchmark/problems/hpo/pytorch_cnn.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Iterable, Set
 
 import pandas as pd
 import torch
 from ax.benchmark.benchmark_problem import SingleObjectiveBenchmarkProblem
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.data import Data
@@ -15,15 +17,15 @@
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.runner import Runner
 from ax.core.search_space import SearchSpace
 from ax.utils.common.base import Base
 from ax.utils.common.equality import equality_typechecker
 from ax.utils.common.result import Err, Ok
-from torch import nn, optim
+from torch import nn, optim, Tensor
 from torch.nn import functional as F
 from torch.utils.data import DataLoader, Dataset
 
 
 class PyTorchCNNBenchmarkProblem(SingleObjectiveBenchmarkProblem):
     @equality_typechecker
     def __eq__(self, other: Base) -> bool:
@@ -37,17 +39,16 @@
     @classmethod
     def from_datasets(
         cls,
         name: str,
         num_trials: int,
         train_set: Dataset,
         test_set: Dataset,
-        infer_noise: bool = True,
     ) -> "PyTorchCNNBenchmarkProblem":
-        optimal_value = 1
+        optimal_value = 1.0
 
         search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     name="lr", parameter_type=ParameterType.FLOAT, lower=1e-6, upper=0.4
                 ),
                 RangeParameter(
@@ -74,49 +75,50 @@
                     lower=0,
                     upper=1,
                 ),
             ]
         )
         optimization_config = OptimizationConfig(
             objective=Objective(
-                metric=PyTorchCNNMetric(infer_noise=infer_noise),
+                metric=PyTorchCNNMetric(),
                 minimize=False,
             )
         )
 
         runner = PyTorchCNNRunner(name=name, train_set=train_set, test_set=test_set)
 
         return cls(
             name=f"HPO_PyTorchCNN_{name}",
             optimal_value=optimal_value,
             search_space=search_space,
             optimization_config=optimization_config,
             runner=runner,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            is_noiseless=False,
+            observe_noise_sd=False,
+            has_ground_truth=False,
         )
 
 
 class PyTorchCNNMetric(Metric):
-    def __init__(self, infer_noise: bool = True) -> None:
+    def __init__(self) -> None:
         super().__init__(name="accuracy")
-        self.infer_noise = infer_noise
 
     def fetch_trial_data(self, trial: BaseTrial, **kwargs: Any) -> MetricFetchResult:
         try:
             accuracy = [
                 trial.run_metadata["accuracy"][name]
                 for name, arm in trial.arms_by_name.items()
             ]
             df = pd.DataFrame(
                 {
-                    "arm_name": [name for name, _ in trial.arms_by_name.items()],
+                    "arm_name": list(trial.arms_by_name.keys()),
                     "metric_name": self.name,
                     "mean": accuracy,
-                    "sem": None if self.infer_noise else 0,
+                    "sem": None,
                     "trial_index": trial.index,
                 }
             )
 
             return Ok(value=Data(df=df))
 
         except Exception as e:
@@ -127,36 +129,28 @@
                 )
             )
 
 
 class PyTorchCNNRunner(Runner):
     def __init__(self, name: str, train_set: Dataset, test_set: Dataset) -> None:
         self.name = name
-
-        # pyre-fixme[4]: Attribute must be annotated.
-        self.train_loader = DataLoader(train_set)
-        # pyre-fixme[4]: Attribute must be annotated.
-        self.test_loader = DataLoader(test_set)
-
+        self.train_loader: DataLoader = DataLoader(train_set)
+        self.test_loader: DataLoader = DataLoader(test_set)
         self.results: Dict[int, float] = {}
         self.statuses: Dict[int, TrialStatus] = {}
-
         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
     class CNN(nn.Module):
-        # pyre-fixme[3]: Return type must be annotated.
-        def __init__(self):
+        def __init__(self) -> None:
             super().__init__()
             self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1)
             self.fc1 = nn.Linear(8 * 8 * 20, 64)
             self.fc2 = nn.Linear(64, 10)
 
-        # pyre-fixme[3]: Return type must be annotated.
-        # pyre-fixme[2]: Parameter must be annotated.
-        def forward(self, x):
+        def forward(self, x: Tensor) -> Tensor:
             x = F.relu(self.conv1(x))
             x = F.max_pool2d(x, 3, 3)
             x = x.view(-1, 8 * 8 * 20)
             x = F.relu(self.fc1(x))
             x = self.fc2(x)
             return F.log_softmax(x, dim=-1)
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/hpo/torchvision.py` & `ax-platform-0.4.0/ax/benchmark/problems/hpo/torchvision.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 from typing import Any, Dict, Optional
 
 from ax.benchmark.problems.hpo.pytorch_cnn import (
     PyTorchCNNBenchmarkProblem,
     PyTorchCNNRunner,
 )
@@ -41,19 +43,18 @@
 
 class PyTorchCNNTorchvisionBenchmarkProblem(PyTorchCNNBenchmarkProblem):
     @classmethod
     def from_dataset_name(
         cls,
         name: str,
         num_trials: int,
-        infer_noise: bool = True,
     ) -> "PyTorchCNNTorchvisionBenchmarkProblem":
         if name not in _REGISTRY:
             raise UserInputError(
-                f"Unrecognized torchvision dataset {name}. Please ensure it is listed"
+                f"Unrecognized torchvision dataset {name}. Please ensure it is listed "
                 "in PyTorchCNNTorchvisionBenchmarkProblem registry."
             )
         dataset_fn = _REGISTRY[name]
 
         train_set = dataset_fn(
             root="./data",
             train=True,
@@ -69,27 +70,28 @@
         )
 
         problem = cls.from_datasets(
             name=name,
             num_trials=num_trials,
             train_set=train_set,
             test_set=test_set,
-            infer_noise=infer_noise,
         )
         runner = PyTorchCNNTorchvisionRunner(
             name=name, train_set=train_set, test_set=test_set
         )
 
         return cls(
             name=f"HPO_PyTorchCNN_Torchvision::{name}",
             search_space=problem.search_space,
             optimization_config=problem.optimization_config,
             runner=runner,
             num_trials=num_trials,
-            infer_noise=infer_noise,
+            is_noiseless=False,
+            observe_noise_sd=False,
+            has_ground_truth=False,
             optimal_value=problem.optimal_value,
         )
 
 
 class PyTorchCNNTorchvisionRunner(PyTorchCNNRunner):
     """
     A subclass to aid in serialization. This allows us to save only the name of the
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/registry.py` & `ax-platform-0.4.0/ax/benchmark/problems/registry.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import copy
 from dataclasses import dataclass
 from typing import Any, Callable, Dict
 
 from ax.benchmark.benchmark_problem import (
     BenchmarkProblem,
     MultiObjectiveBenchmarkProblem,
@@ -27,207 +29,228 @@
 
 BENCHMARK_PROBLEM_REGISTRY = {
     "ackley4": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Ackley,
             "test_problem_kwargs": {"dim": 4},
+            "lower_is_better": True,
             "num_trials": 40,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "branin": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Branin,
             "test_problem_kwargs": {},
+            "lower_is_better": True,
             "num_trials": 30,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "branin_currin": BenchmarkProblemRegistryEntry(
         factory_fn=MultiObjectiveBenchmarkProblem.from_botorch_multi_objective,
         factory_kwargs={
             "test_problem_class": BraninCurrin,
             "test_problem_kwargs": {},
             "num_trials": 30,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "branin_currin30": BenchmarkProblemRegistryEntry(
         factory_fn=lambda n, num_trials: embed_higher_dimension(
             problem=MultiObjectiveBenchmarkProblem.from_botorch_multi_objective(
                 test_problem_class=BraninCurrin,
                 test_problem_kwargs={},
                 num_trials=num_trials,
-                infer_noise=True,
+                observe_noise_sd=False,
             ),
             total_dimensionality=n,
         ),
         factory_kwargs={"n": 30, "num_trials": 30},
     ),
     "griewank4": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Griewank,
             "test_problem_kwargs": {"dim": 4},
+            "lower_is_better": True,
             "num_trials": 40,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "hartmann3": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Hartmann,
             "test_problem_kwargs": {"dim": 3},
+            "lower_is_better": True,
             "num_trials": 30,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "hartmann6": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Hartmann,
             "test_problem_kwargs": {"dim": 6},
+            "lower_is_better": True,
             "num_trials": 35,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "hartmann30": BenchmarkProblemRegistryEntry(
         factory_fn=lambda n, num_trials: embed_higher_dimension(
             problem=SingleObjectiveBenchmarkProblem.from_botorch_synthetic(
                 test_problem_class=synthetic.Hartmann,
                 test_problem_kwargs={"dim": 6},
+                lower_is_better=True,
                 num_trials=num_trials,
-                infer_noise=True,
+                observe_noise_sd=False,
             ),
             total_dimensionality=n,
         ),
         factory_kwargs={"n": 30, "num_trials": 25},
     ),
     "hpo_pytorch_cnn_MNIST": BenchmarkProblemRegistryEntry(
         factory_fn=PyTorchCNNTorchvisionBenchmarkProblem.from_dataset_name,
-        factory_kwargs={"name": "MNIST", "num_trials": 20, "infer_noise": True},
+        factory_kwargs={
+            "name": "MNIST",
+            "num_trials": 20,
+        },
     ),
     "hpo_pytorch_cnn_FashionMNIST": BenchmarkProblemRegistryEntry(
         factory_fn=PyTorchCNNTorchvisionBenchmarkProblem.from_dataset_name,
-        factory_kwargs={"name": "FashionMNIST", "num_trials": 50, "infer_noise": True},
+        factory_kwargs={
+            "name": "FashionMNIST",
+            "num_trials": 50,
+        },
     ),
     "jenatton": BenchmarkProblemRegistryEntry(
         factory_fn=get_jenatton_benchmark_problem,
-        factory_kwargs={"num_trials": 50, "infer_noise": True},
+        factory_kwargs={"num_trials": 50, "observe_noise_sd": False},
     ),
     "levy4": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Levy,
             "test_problem_kwargs": {"dim": 4},
+            "lower_is_better": True,
             "num_trials": 40,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "powell4": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Powell,
             "test_problem_kwargs": {"dim": 4},
+            "lower_is_better": True,
             "num_trials": 40,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "rosenbrock4": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Rosenbrock,
             "test_problem_kwargs": {"dim": 4},
+            "lower_is_better": True,
             "num_trials": 40,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "six_hump_camel": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.SixHumpCamel,
             "test_problem_kwargs": {},
+            "lower_is_better": True,
             "num_trials": 30,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
     "three_hump_camel": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.ThreeHumpCamel,
             "test_problem_kwargs": {},
+            "lower_is_better": True,
             "num_trials": 30,
-            "infer_noise": True,
+            "observe_noise_sd": False,
         },
     ),
-    # Problems without inferred noise
-    "branin_fixed_noise": BenchmarkProblemRegistryEntry(
+    # Problems where we observe the noise level
+    "branin_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Branin,
             "test_problem_kwargs": {},
+            "lower_is_better": True,
             "num_trials": 20,
-            "infer_noise": False,
+            "observe_noise_sd": True,
         },
     ),
-    "branin_currin_fixed_noise": BenchmarkProblemRegistryEntry(
+    "branin_currin_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=MultiObjectiveBenchmarkProblem.from_botorch_multi_objective,
         factory_kwargs={
             "test_problem_class": BraninCurrin,
             "test_problem_kwargs": {},
             "num_trials": 30,
-            "infer_noise": False,
+            "observe_noise_sd": True,
         },
     ),
-    "branin_currin30_fixed_noise": BenchmarkProblemRegistryEntry(
+    "branin_currin30_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=lambda n, num_trials: embed_higher_dimension(
             problem=MultiObjectiveBenchmarkProblem.from_botorch_multi_objective(
                 test_problem_class=BraninCurrin,
                 test_problem_kwargs={},
                 num_trials=num_trials,
-                infer_noise=False,
+                observe_noise_sd=True,
             ),
             total_dimensionality=n,
         ),
         factory_kwargs={"n": 30, "num_trials": 30},
     ),
-    "hartmann6_fixed_noise": BenchmarkProblemRegistryEntry(
+    "hartmann6_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.Hartmann,
             "test_problem_kwargs": {"dim": 6},
+            "lower_is_better": True,
             "num_trials": 50,
-            "infer_noise": False,
+            "observe_noise_sd": True,
         },
     ),
-    "hartmann30_fixed_noise": BenchmarkProblemRegistryEntry(
+    "hartmann30_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=lambda n, num_trials: embed_higher_dimension(
             problem=SingleObjectiveBenchmarkProblem.from_botorch_synthetic(
                 test_problem_class=synthetic.Hartmann,
                 test_problem_kwargs={"dim": 6},
+                lower_is_better=True,
                 num_trials=num_trials,
-                infer_noise=False,
+                observe_noise_sd=True,
             ),
             total_dimensionality=n,
         ),
         factory_kwargs={"n": 30, "num_trials": 25},
     ),
-    "jenatton_fixed_noise": BenchmarkProblemRegistryEntry(
+    "jenatton_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=get_jenatton_benchmark_problem,
-        factory_kwargs={"num_trials": 25, "infer_noise": False},
+        factory_kwargs={"num_trials": 25, "observe_noise_sd": True},
     ),
-    "constrained_gramacy_fixed_noise": BenchmarkProblemRegistryEntry(
+    "constrained_gramacy_observed_noise": BenchmarkProblemRegistryEntry(
         factory_fn=SingleObjectiveBenchmarkProblem.from_botorch_synthetic,
         factory_kwargs={
             "test_problem_class": synthetic.ConstrainedGramacy,
             "test_problem_kwargs": {},
+            "lower_is_better": True,
             "num_trials": 50,
-            "infer_noise": False,
+            "observe_noise_sd": True,
         },
     ),
 }
 
 
 def get_problem(problem_name: str, **additional_kwargs: Any) -> BenchmarkProblem:
     entry = BENCHMARK_PROBLEM_REGISTRY[problem_name]
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/surrogate.py` & `ax-platform-0.4.0/ax/core/multi_type_experiment.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,321 +1,272 @@
+#!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Tuple
+# pyre-strict
 
-import pandas as pd
-import torch
-from ax.benchmark.benchmark_problem import BenchmarkProblemBase
-from ax.core.base_trial import BaseTrial, TrialStatus
+import logging
+from typing import Any, Dict, List, Optional, Set
+
+from ax.core.arm import Arm
+from ax.core.base_trial import BaseTrial
 from ax.core.data import Data
-from ax.core.metric import Metric, MetricFetchE, MetricFetchResult
-from ax.core.observation import ObservationFeatures
-from ax.core.optimization_config import (
-    MultiObjectiveOptimizationConfig,
-    OptimizationConfig,
-)
-from ax.core.parameter import RangeParameter
+from ax.core.experiment import DataType, Experiment
+from ax.core.metric import Metric, MetricFetchResult
+from ax.core.optimization_config import OptimizationConfig
 from ax.core.runner import Runner
 from ax.core.search_space import SearchSpace
-from ax.core.types import TParameterization
-from ax.modelbridge.transforms.int_to_float import IntToFloat
-from ax.modelbridge.transforms.log import Log
-from ax.models.torch.botorch_modular.surrogate import Surrogate
-
-from ax.utils.common.base import Base
-from ax.utils.common.equality import equality_typechecker
-from ax.utils.common.result import Err, Ok
-from ax.utils.common.serialization import TClassDecoderRegistry, TDecoderRegistry
-from ax.utils.common.typeutils import not_none
-from botorch.utils.datasets import SupervisedDataset
-
+from ax.utils.common.docutils import copy_doc
+from ax.utils.common.logger import get_logger
 
-class SurrogateBenchmarkProblemBase(Base, BenchmarkProblemBase):
-    """
-    Base class for SOOSurrogateBenchmarkProblem and MOOSurrogateBenchmarkProblem.
-
-    Allows for lazy creation of objects needed to construct a `runner`,
-    including a surrogate and datasets.
-    """
 
-    def __init__(
-        self,
-        *,
-        name: str,
-        search_space: SearchSpace,
-        optimization_config: OptimizationConfig,
-        num_trials: int,
-        infer_noise: bool,
-        metric_names: List[str],
-        get_surrogate_and_datasets: Optional[
-            Callable[[], Tuple[Surrogate, List[SupervisedDataset]]]
-        ] = None,
-        tracking_metrics: Optional[List[Metric]] = None,
-        _runner: Optional[Runner] = None,
-    ) -> None:
-        if get_surrogate_and_datasets is None and _runner is None:
-            raise ValueError(
-                "Either `get_surrogate_and_datasets` or `_runner` required."
-            )
-        self.name = name
-        self.search_space = search_space
-        self.optimization_config = optimization_config
-        self.num_trials = num_trials
-        self.infer_noise = infer_noise
-        self.metric_names = metric_names
-        self.get_surrogate_and_datasets = get_surrogate_and_datasets
-        self.tracking_metrics: List[Metric] = (
-            [] if tracking_metrics is None else tracking_metrics
-        )
-        self._runner = _runner
+logger: logging.Logger = get_logger(__name__)
 
-    @equality_typechecker
-    def __eq__(self, other: Base) -> bool:
-        if type(other) is not type(self):
-            return False
-
-        # Checking the whole datasets' equality here would be too expensive to be
-        # worth it; just check names instead
-        return self.name == other.name
-
-    def set_runner(self) -> None:
-        surrogate, datasets = not_none(self.get_surrogate_and_datasets)()
-        self._runner = SurrogateRunner(
-            name=self.name,
-            surrogate=surrogate,
-            datasets=datasets,
-            search_space=self.search_space,
-            metric_names=self.metric_names,
-        )
 
-    @property
-    def runner(self) -> Runner:
-        if self._runner is None:
-            self.set_runner()
-        return not_none(self._runner)
+class MultiTypeExperiment(Experiment):
+    """Class for experiment with multiple trial types.
 
-    def __repr__(self) -> str:
-        """
-        Return a string representation that includes only the attributes that
-        print nicely and contain information likely to be useful.
-        """
-        return (
-            f"{self.__class__.__name__}("
-            f"name={self.name}, "
-            f"optimization_config={self.optimization_config}, "
-            f"num_trials={self.num_trials}, "
-            f"infer_noise={self.infer_noise}, "
-            f"tracking_metrics={self.tracking_metrics})"
-        )
+    A canonical use case for this is tuning a large production system
+    with limited evaluation budget and a simulator which approximates
+    evaluations on the main system. Trial deployment and data fetching
+    is separate for the two systems, but the final data is combined and
+    fed into multi-task models.
 
+    See the Multi-Task Modeling tutorial for more details.
 
-class SOOSurrogateBenchmarkProblem(SurrogateBenchmarkProblemBase):
-    """
-    Has the same attributes/properties as a `SingleObjectiveBenchmarkProblem`,
-    but allows for constructing from a surrogate.
+    Attributes:
+        name: Name of the experiment.
+        description: Description of the experiment.
     """
 
     def __init__(
         self,
-        *,
         name: str,
         search_space: SearchSpace,
-        optimization_config: OptimizationConfig,
-        num_trials: int,
-        infer_noise: bool,
-        optimal_value: float,
-        metric_names: List[str],
-        get_surrogate_and_datasets: Optional[
-            Callable[[], Tuple[Surrogate, List[SupervisedDataset]]]
-        ] = None,
-        tracking_metrics: Optional[List[Metric]] = None,
-        _runner: Optional[Runner] = None,
+        default_trial_type: str,
+        default_runner: Runner,
+        optimization_config: Optional[OptimizationConfig] = None,
+        status_quo: Optional[Arm] = None,
+        description: Optional[str] = None,
+        is_test: bool = False,
+        experiment_type: Optional[str] = None,
+        properties: Optional[Dict[str, Any]] = None,
+        default_data_type: Optional[DataType] = None,
     ) -> None:
-        super().__init__(
-            name=name,
-            search_space=search_space,
-            optimization_config=optimization_config,
-            num_trials=num_trials,
-            infer_noise=infer_noise,
-            metric_names=metric_names,
-            get_surrogate_and_datasets=get_surrogate_and_datasets,
-            tracking_metrics=tracking_metrics,
-            _runner=_runner,
-        )
-        self.optimization_config = optimization_config
-        self.optimal_value = optimal_value
+        """Inits Experiment.
 
+        Args:
+            name: Name of the experiment.
+            search_space: Search space of the experiment.
+            default_trial_type: Default type for trials on this experiment.
+            default_runner: Default runner for trials of the default type.
+            optimization_config: Optimization config of the experiment.
+            tracking_metrics: Additional tracking metrics not used for optimization.
+            runner: Default runner used for trials on this experiment.
+            status_quo: Arm representing existing "control" arm.
+            description: Description of the experiment.
+            is_test: Convenience metadata tracker for the user to mark test experiments.
+            experiment_type: The class of experiments this one belongs to.
+            properties: Dictionary of this experiment's properties.
+            default_data_type: Enum representing the data type this experiment uses.
+        """
 
-class MOOSurrogateBenchmarkProblem(SurrogateBenchmarkProblemBase):
-    """
-    Has the same attributes/properties as a `MultiObjectiveBenchmarkProblem`,
-    but its runner is not constructed until needed, to allow for deferring
-    constructing the surrogate.
-
-    Simple aspects of the problem problem such as its search space
-    are defined immediately, while the surrogate is only defined when [TODO]
-    in order to avoid expensive operations like downloading files and fitting
-    a model.
-    """
+        self._default_trial_type = default_trial_type
 
-    optimization_config: MultiObjectiveOptimizationConfig
+        # Map from trial type to default runner of that type
+        self._trial_type_to_runner: Dict[str, Runner] = {
+            default_trial_type: default_runner
+        }
 
-    def __init__(
-        self,
-        *,
-        name: str,
-        search_space: SearchSpace,
-        optimization_config: MultiObjectiveOptimizationConfig,
-        num_trials: int,
-        infer_noise: bool,
-        maximum_hypervolume: float,
-        reference_point: List[float],
-        metric_names: List[str],
-        get_surrogate_and_datasets: Optional[
-            Callable[[], Tuple[Surrogate, List[SupervisedDataset]]]
-        ] = None,
-        tracking_metrics: Optional[List[Metric]] = None,
-        _runner: Optional[Runner] = None,
-    ) -> None:
-        super().__init__(
+        # Specifies which trial type each metric belongs to
+        self._metric_to_trial_type: Dict[str, str] = {}
+
+        # Maps certain metric names to a canonical name. Useful for ancillary trial
+        # types' metrics, to specify which primary metrics they correspond to
+        # (e.g. 'comment_prediction' => 'comment')
+        self._metric_to_canonical_name: Dict[str, str] = {}
+
+        # call super.__init__() after defining fields above, because we need
+        # them to be populated before optimization config is set
+        super(MultiTypeExperiment, self).__init__(
             name=name,
             search_space=search_space,
             optimization_config=optimization_config,
-            num_trials=num_trials,
-            infer_noise=infer_noise,
-            metric_names=metric_names,
-            get_surrogate_and_datasets=get_surrogate_and_datasets,
-            tracking_metrics=tracking_metrics,
-            _runner=_runner,
+            status_quo=status_quo,
+            description=description,
+            is_test=is_test,
+            experiment_type=experiment_type,
+            properties=properties,
+            default_data_type=default_data_type,
         )
-        self.reference_point = reference_point
-        self.maximum_hypervolume = maximum_hypervolume
 
-    @property
-    def optimal_value(self) -> float:
-        return self.maximum_hypervolume
+    def add_trial_type(self, trial_type: str, runner: Runner) -> "MultiTypeExperiment":
+        """Add a new trial_type to be supported by this experiment.
 
+        Args:
+            trial_type: The new trial_type to be added.
+            runner: The default runner for trials of this type.
+        """
+        if self.supports_trial_type(trial_type):
+            raise ValueError(f"Experiment already contains trial_type `{trial_type}`")
 
-class SurrogateMetric(Metric):
-    def __init__(
-        self, name: str, lower_is_better: bool, infer_noise: bool = True
-    ) -> None:
-        super().__init__(name=name, lower_is_better=lower_is_better)
-        self.infer_noise = infer_noise
+        self._trial_type_to_runner[trial_type] = runner
+        return self
 
-    # pyre-fixme[2]: Parameter must be annotated.
-    def fetch_trial_data(self, trial: BaseTrial, **kwargs) -> MetricFetchResult:
-        try:
-            prediction = [
-                trial.run_metadata[self.name][name]
-                for name, arm in trial.arms_by_name.items()
-            ]
-            df = pd.DataFrame(
-                {
-                    "arm_name": [name for name, _ in trial.arms_by_name.items()],
-                    "metric_name": self.name,
-                    "mean": prediction,
-                    "sem": None if self.infer_noise else 0,
-                    "trial_index": trial.index,
-                }
-            )
+    def update_runner(self, trial_type: str, runner: Runner) -> "MultiTypeExperiment":
+        """Update the default runner for an existing trial_type.
 
-            return Ok(value=Data(df=df))
+        Args:
+            trial_type: The new trial_type to be added.
+            runner: The new runner for trials of this type.
+        """
+        if not self.supports_trial_type(trial_type):
+            raise ValueError(f"Experiment does not contain trial_type `{trial_type}`")
 
-        except Exception as e:
-            return Err(
-                MetricFetchE(
-                    message=f"Failed to predict for trial {trial}", exception=e
-                )
+        self._trial_type_to_runner[trial_type] = runner
+        return self
+
+    # pyre-fixme[14]: `add_tracking_metric` overrides method defined in `Experiment`
+    #  inconsistently.
+    def add_tracking_metric(
+        self, metric: Metric, trial_type: str, canonical_name: Optional[str] = None
+    ) -> "MultiTypeExperiment":
+        """Add a new metric to the experiment.
+
+        Args:
+            metric: The metric to add.
+            trial_type: The trial type for which this metric is used.
+            canonical_name: The default metric for which this metric is a proxy.
+        """
+        if not self.supports_trial_type(trial_type):
+            raise ValueError(f"`{trial_type}` is not a supported trial type.")
+
+        super(MultiTypeExperiment, self).add_tracking_metric(metric)
+        self._metric_to_trial_type[metric.name] = trial_type
+        if canonical_name is not None:
+            self._metric_to_canonical_name[metric.name] = canonical_name
+        return self
+
+    # pyre-fixme[14]: `update_tracking_metric` overrides method defined in
+    #  `Experiment` inconsistently.
+    def update_tracking_metric(
+        self, metric: Metric, trial_type: str, canonical_name: Optional[str] = None
+    ) -> "MultiTypeExperiment":
+        """Update an existing metric on the experiment.
+
+        Args:
+            metric: The metric to add.
+            trial_type: The trial type for which this metric is used.
+            canonical_name: The default metric for which this metric is a proxy.
+        """
+        oc = self.optimization_config
+        oc_metrics = oc.metrics if oc else []
+        if metric.name in oc_metrics and trial_type != self._default_trial_type:
+            raise ValueError(
+                f"Metric `{metric.name}` must remain a `{self._default_trial_type}` "
+                "metric because it is part of the optimization_config."
             )
+        elif not self.supports_trial_type(trial_type):
+            raise ValueError(f"`{trial_type}` is not a supported trial type.")
 
+        super(MultiTypeExperiment, self).update_tracking_metric(metric)
+        self._metric_to_trial_type[metric.name] = trial_type
+        if canonical_name is not None:
+            self._metric_to_canonical_name[metric.name] = canonical_name
+        return self
+
+    @copy_doc(Experiment.remove_tracking_metric)
+    def remove_tracking_metric(self, metric_name: str) -> "MultiTypeExperiment":
+        if metric_name not in self._tracking_metrics:
+            raise ValueError(f"Metric `{metric_name}` doesn't exist on experiment.")
+
+        # Required fields
+        del self._tracking_metrics[metric_name]
+        del self._metric_to_trial_type[metric_name]
+
+        # Optional
+        if metric_name in self._metric_to_canonical_name:
+            del self._metric_to_canonical_name[metric_name]
+        return self
 
-class SurrogateRunner(Runner):
-    def __init__(
+    @copy_doc(Experiment.fetch_data)
+    def fetch_data(
         self,
-        name: str,
-        surrogate: Surrogate,
-        datasets: List[SupervisedDataset],
-        search_space: SearchSpace,
-        metric_names: List[str],
-    ) -> None:
-        self.name = name
-        self.surrogate = surrogate
-        self.metric_names = metric_names
-        self.datasets = datasets
-        self.search_space = search_space
-
-        self.results: Dict[int, float] = {}
-        self.statuses: Dict[int, TrialStatus] = {}
-
-        # If there are log scale parameters, these need to be transformed.
-        if any(
-            isinstance(p, RangeParameter) and p.log_scale
-            for p in search_space.parameters.values()
-        ):
-            int_to_float_tf = IntToFloat(search_space=search_space)
-            log_tf = Log(
-                search_space=int_to_float_tf.transform_search_space(
-                    search_space.clone()
+        metrics: Optional[List[Metric]] = None,
+        combine_with_last_data: bool = False,
+        overwrite_existing_data: bool = False,
+        **kwargs: Any,
+    ) -> Data:
+        return self.default_data_constructor.from_multiple_data(
+            [
+                (
+                    trial.fetch_data(**kwargs, metrics=metrics)
+                    if trial.status.expecting_data
+                    else Data()
                 )
-            )
-            self.transforms: Optional[Tuple[IntToFloat, Log]] = (
-                int_to_float_tf,
-                log_tf,
-            )
-        else:
-            self.transforms = None
+                for trial in self.trials.values()
+            ]
+        )
 
-    def _get_transformed_parameters(
-        self, parameters: TParameterization
-    ) -> TParameterization:
-        if self.transforms is None:
-            return parameters
-
-        obs_ft = ObservationFeatures(parameters=parameters)
-        for t in not_none(self.transforms):
-            obs_ft = t.transform_observation_features([obs_ft])[0]
-        return obs_ft.parameters
-
-    def run(self, trial: BaseTrial) -> Dict[str, Any]:
-        self.statuses[trial.index] = TrialStatus.COMPLETED
-        preds = {  # Cache predictions for each arm
-            arm.name: self.surrogate.predict(
-                X=torch.tensor(
-                    [*self._get_transformed_parameters(arm.parameters).values()]
-                ).reshape([1, len(arm.parameters)])
-            )[0].squeeze(0)
-            for arm in trial.arms
-        }
+    @copy_doc(Experiment._fetch_trial_data)
+    def _fetch_trial_data(
+        self, trial_index: int, metrics: Optional[List[Metric]] = None, **kwargs: Any
+    ) -> Dict[str, MetricFetchResult]:
+        trial = self.trials[trial_index]
+        metrics = [
+            metric
+            for metric in (metrics or self.metrics.values())
+            if self.metric_to_trial_type[metric.name] == trial.trial_type
+        ]
+        # Invoke parent's fetch method using only metrics for this trial_type
+        return super()._fetch_trial_data(trial.index, metrics=metrics, **kwargs)
+
+    @property
+    def default_trials(self) -> Set[int]:
+        """Return the indicies for trials of the default type."""
         return {
-            metric_name: {arm_name: float(pred[i]) for arm_name, pred in preds.items()}
-            for i, metric_name in enumerate(self.metric_names)
+            idx
+            for idx, trial in self.trials.items()
+            if trial.trial_type == self.default_trial_type
         }
 
-    def poll_trial_status(
-        self, trials: Iterable[BaseTrial]
-    ) -> Dict[TrialStatus, Set[int]]:
-        return {TrialStatus.COMPLETED: {t.index for t in trials}}
-
-    @classmethod
-    # pyre-fixme[2]: Parameter annotation cannot be `Any`.
-    def serialize_init_args(cls, obj: Any) -> Dict[str, Any]:
-        """Serialize the properties needed to initialize the runner.
-        Used for storage.
-
-        WARNING: Because of issues with consistently saving and loading BoTorch and
-        GPyTorch modules the SurrogateRunner cannot be serialized at this time. At load
-        time the runner will be replaced with a SyntheticRunner.
+    @property
+    def metric_to_trial_type(self) -> Dict[str, str]:
+        """Map metrics to trial types.
+
+        Adds in default trial type for OC metrics to custom defined trial types..
         """
-        return {}
+        opt_config_types = {
+            metric_name: self.default_trial_type
+            # pyre-fixme[16]: `Optional` has no attribute `metrics`.
+            for metric_name in self.optimization_config.metrics.keys()
+        }
+        return {**opt_config_types, **self._metric_to_trial_type}
 
-    @classmethod
-    def deserialize_init_args(
-        cls,
-        args: Dict[str, Any],
-        decoder_registry: Optional[TDecoderRegistry] = None,
-        class_decoder_registry: Optional[TClassDecoderRegistry] = None,
-    ) -> Dict[str, Any]:
-        return {}
+    # -- Overridden functions from Base Experiment Class --
+    @property
+    def default_trial_type(self) -> Optional[str]:
+        """Default trial type assigned to trials in this experiment."""
+        return self._default_trial_type
+
+    def runner_for_trial(self, trial: BaseTrial) -> Optional[Runner]:
+        """The default runner to use for a given trial.
+
+        Looks up the appropriate runner for this trial type in the trial_type_to_runner.
+        """
+        if trial.trial_type is None or not self.supports_trial_type(trial.trial_type):
+            raise ValueError(f"Batch type `{trial.trial_type}` is not supported.")
+        return self._trial_type_to_runner[trial.trial_type]
+
+    def supports_trial_type(self, trial_type: Optional[str]) -> bool:
+        """Whether this experiment allows trials of the given type.
+
+        Only trial types defined in the trial_type_to_runner are allowed.
+        """
+        return trial_type in self._trial_type_to_runner.keys()
+
+    def reset_runners(self, runner: Runner) -> None:
+        raise NotImplementedError(
+            "MultiTypeExperiment does not support resetting all runners."
+        )
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/synthetic/discretized/mixed_integer.py` & `ax-platform-0.4.0/ax/benchmark/problems/synthetic/discretized/mixed_integer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Mixed integer extensions of some common synthetic test functions.
 These are adapted from [Daulton2022bopr]_.
 
 References
 
 .. [Daulton2022bopr]
@@ -15,99 +17,106 @@
     Reparameterization. Advances in Neural Information Processing Systems
     35, 2022.
 """
 
 from typing import Any, Dict, List, Optional, Tuple, Type
 
 from ax.benchmark.benchmark_problem import BenchmarkProblem
+from ax.benchmark.metrics.benchmark import BenchmarkMetric
+from ax.benchmark.runners.botorch_test import BotorchTestProblemRunner
 from ax.core.objective import Objective
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
-from ax.metrics.botorch_test_problem import BotorchTestProblemMetric
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
 from botorch.test_functions.synthetic import (
     Ackley,
     Hartmann,
     Rosenbrock,
     SyntheticTestFunction,
 )
 
 
 def _get_problem_from_common_inputs(
     bounds: List[Tuple[float, float]],
     dim_int: int,
     metric_name: str,
-    infer_noise: bool,
+    lower_is_better: bool,
+    observe_noise_sd: bool,
     test_problem_class: Type[SyntheticTestFunction],
     benchmark_name: str,
     num_trials: int,
     test_problem_bounds: Optional[List[Tuple[float, float]]] = None,
 ) -> BenchmarkProblem:
     """This is a helper that deduplicates common bits of the below problems.
 
     Args:
         bounds: The parameter bounds.
         dim_int: The number of integer dimensions. First `dim_int` parameters
             are assumed to be integers.
         metric_name: The name of the metric.
-        infer_noise: Whether to infer noise or assume noise-free objective.
+        lower_is_better: If true, the goal is to minimize the metric.
+        observe_noise_sd: Whether to report the standard deviation of the
+            observation noise.
         test_problem_class: The BoTorch test problem class.
         benchmark_name: The name of the benchmark problem.
         num_trials: The number of trials.
         test_problem_bounds: Optional bounds to evaluate the base test problem on.
             These are passed in as `bounds` while initializing the test problem.
 
     Returns:
         A mixed-integer BenchmarkProblem constructed from the given inputs.
     """
     dim = len(bounds)
     search_space = SearchSpace(
         parameters=[
             RangeParameter(
                 name=f"x{i + 1}",
-                parameter_type=ParameterType.INT
-                if i < dim_int
-                else ParameterType.FLOAT,
+                parameter_type=(
+                    ParameterType.INT if i < dim_int else ParameterType.FLOAT
+                ),
                 lower=bounds[i][0],
                 upper=bounds[i][1],
             )
             for i in range(dim)
         ]
     )
     optimization_config = OptimizationConfig(
         objective=Objective(
-            metric=BotorchTestProblemMetric(
+            metric=BenchmarkMetric(
                 name=metric_name,
-                noise_sd=None if infer_noise else 0.0,
+                lower_is_better=lower_is_better,
+                observe_noise_sd=observe_noise_sd,
             ),
             minimize=True,
         )
     )
     test_problem_kwargs: Dict[str, Any] = {"dim": dim}
     if test_problem_bounds is not None:
         test_problem_kwargs["bounds"] = test_problem_bounds
     runner = BotorchTestProblemRunner(
         test_problem_class=test_problem_class,
         test_problem_kwargs=test_problem_kwargs,
+        outcome_names=[metric_name],
         modified_bounds=bounds,
     )
     return BenchmarkProblem(
-        name=benchmark_name,
+        name=benchmark_name + ("_observed_noise" if observe_noise_sd else ""),
         search_space=search_space,
         optimization_config=optimization_config,
         runner=runner,
         num_trials=num_trials,
-        infer_noise=infer_noise,
+        is_noiseless=True,
+        observe_noise_sd=observe_noise_sd,
+        has_ground_truth=True,
     )
 
 
 def get_discrete_hartmann(
     num_trials: int = 50,
-    infer_noise: bool = True,
+    observe_noise_sd: bool = False,
     bounds: Optional[List[Tuple[float, float]]] = None,
 ) -> BenchmarkProblem:
     """6D Hartmann problem where first 4 dimensions are discretized."""
     dim_int = 4
     if bounds is None:
         bounds = [
             (0, 3),
@@ -117,24 +126,25 @@
             (0.0, 1.0),
             (0.0, 1.0),
         ]
     return _get_problem_from_common_inputs(
         bounds=bounds,
         dim_int=dim_int,
         metric_name="Hartmann",
-        infer_noise=infer_noise,
+        lower_is_better=True,
+        observe_noise_sd=observe_noise_sd,
         test_problem_class=Hartmann,
         benchmark_name="Discrete Hartmann",
         num_trials=num_trials,
     )
 
 
 def get_discrete_ackley(
     num_trials: int = 50,
-    infer_noise: bool = True,
+    observe_noise_sd: bool = False,
     bounds: Optional[List[Tuple[float, float]]] = None,
 ) -> BenchmarkProblem:
     """13D Ackley problem where first 10 dimensions are discretized.
 
     This also restricts Ackley evaluation bounds to [0, 1].
     """
     dim = 13
@@ -145,36 +155,38 @@
             *[(0, 4)] * 5,
             *[(0.0, 1.0)] * 3,
         ]
     return _get_problem_from_common_inputs(
         bounds=bounds,
         dim_int=dim_int,
         metric_name="Ackley",
-        infer_noise=infer_noise,
+        lower_is_better=True,
+        observe_noise_sd=observe_noise_sd,
         test_problem_class=Ackley,
         benchmark_name="Discrete Ackley",
         num_trials=num_trials,
         test_problem_bounds=[(0.0, 1.0)] * dim,
     )
 
 
 def get_discrete_rosenbrock(
     num_trials: int = 50,
-    infer_noise: bool = True,
+    observe_noise_sd: bool = False,
     bounds: Optional[List[Tuple[float, float]]] = None,
 ) -> BenchmarkProblem:
     """10D Rosenbrock problem where first 6 dimensions are discretized."""
     dim_int = 6
     if bounds is None:
         bounds = [
             *[(0, 3)] * 6,
             *[(0.0, 1.0)] * 4,
         ]
     return _get_problem_from_common_inputs(
         bounds=bounds,
         dim_int=dim_int,
         metric_name="Rosenbrock",
-        infer_noise=infer_noise,
+        lower_is_better=True,
+        observe_noise_sd=observe_noise_sd,
         test_problem_class=Rosenbrock,
         benchmark_name="Discrete Rosenbrock",
         num_trials=num_trials,
     )
```

### Comparing `ax-platform-0.3.7/ax/benchmark/problems/synthetic/hss/jenatton.py` & `ax-platform-0.4.0/ax/benchmark/problems/synthetic/hss/jenatton.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.benchmark.benchmark_problem import SingleObjectiveBenchmarkProblem
+from ax.benchmark.metrics.jenatton import JenattonMetric
 from ax.core.objective import Objective
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import HierarchicalSearchSpace
-from ax.metrics.jenatton import JenattonMetric
 from ax.runners.synthetic import SyntheticRunner
 
 
 def get_jenatton_benchmark_problem(
     num_trials: int = 50,
-    infer_noise: bool = True,
+    observe_noise_sd: bool = False,
 ) -> SingleObjectiveBenchmarkProblem:
     search_space = HierarchicalSearchSpace(
         parameters=[
             ChoiceParameter(
                 name="x1",
                 parameter_type=ParameterType.INT,
                 values=[0, 1],
@@ -52,22 +54,25 @@
                 name="r9", parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0
             ),
         ]
     )
 
     optimization_config = OptimizationConfig(
         objective=Objective(
-            metric=JenattonMetric(infer_noise=infer_noise), minimize=True
+            metric=JenattonMetric(observe_noise_sd=observe_noise_sd),
+            minimize=True,
         )
     )
 
-    name = "Jenatton" + ("" if infer_noise else "_fixed_noise")
+    name = "Jenatton" + ("_observed_noise" if observe_noise_sd else "")
 
     return SingleObjectiveBenchmarkProblem(
         name=name,
         search_space=search_space,
         optimization_config=optimization_config,
         runner=SyntheticRunner(),
         num_trials=num_trials,
-        infer_noise=infer_noise,
+        is_noiseless=True,
+        observe_noise_sd=observe_noise_sd,
+        has_ground_truth=True,
         optimal_value=0.1,
     )
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_benchmark.py` & `ax-platform-0.4.0/ax/telemetry/tests/test_scheduler.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,293 +1,280 @@
+#!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-import tempfile
+# pyre-strict
+
+from typing import cast, Dict
+from unittest import mock
 
 import numpy as np
-from ax.benchmark.benchmark import (
-    benchmark_multiple_problems_methods,
-    benchmark_one_method_problem,
-    benchmark_replication,
-)
-from ax.benchmark.benchmark_method import (
-    BenchmarkMethod,
-    get_sequential_optimization_scheduler_options,
-)
-from ax.benchmark.benchmark_problem import SingleObjectiveBenchmarkProblem
-from ax.benchmark.benchmark_result import BenchmarkResult
-from ax.benchmark.methods.modular_botorch import get_sobol_botorch_modular_acquisition
-from ax.benchmark.problems.registry import get_problem
-from ax.modelbridge.modelbridge_utils import extract_search_space_digest
-from ax.service.utils.scheduler_options import SchedulerOptions
-from ax.storage.json_store.load import load_experiment
-from ax.storage.json_store.save import save_experiment
-from ax.utils.common.testutils import TestCase
-from ax.utils.common.typeutils import not_none
-from ax.utils.testing.benchmark_stubs import (
-    get_moo_surrogate,
-    get_multi_objective_benchmark_problem,
-    get_single_objective_benchmark_problem,
-    get_sobol_benchmark_method,
-    get_soo_surrogate,
-)
-from ax.utils.testing.core_stubs import get_dataset, get_experiment
-from ax.utils.testing.mock import fast_botorch_optimize
-from botorch.acquisition.logei import qLogNoisyExpectedImprovement
-from botorch.acquisition.multi_objective.monte_carlo import (
-    qNoisyExpectedHypervolumeImprovement,
-)
-from botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP
-from botorch.models.gp_regression import FixedNoiseGP, SingleTaskGP
-from botorch.test_functions.synthetic import Branin
-
-
-class TestBenchmark(TestCase):
-    def test_storage(self) -> None:
-        problem = get_single_objective_benchmark_problem()
-        res = benchmark_replication(
-            problem=problem, method=get_sobol_benchmark_method(), seed=0
-        )
-        # Experiment is not in storage yet
-        self.assertTrue(res.experiment is not None)
-        self.assertEqual(res.experiment_storage_id, None)
-        experiment = res.experiment
-
-        # test saving to temporary file
-        with tempfile.NamedTemporaryFile(mode="w", delete=True, suffix=".json") as f:
-            save_experiment(not_none(res.experiment), f.name)
-            res.experiment_storage_id = f.name
-            res.experiment = None
-            self.assertIsNone(res.experiment)
-            self.assertEqual(res.experiment_storage_id, f.name)
-
-            # load it back
-            experiment = load_experiment(f.name)
-            self.assertEqual(experiment, experiment)
-
-    def test_benchmark_result_invalid_inputs(self) -> None:
-        """
-        Test that a BenchmarkResult cannot be specified with both an `experiment`
-        and an `experiment_storage_id`.
-        """
-        with self.assertRaisesRegex(ValueError, "Cannot specify both an `experiment` "):
-            BenchmarkResult(
-                name="name",
-                seed=0,
-                optimization_trace=np.array([]),
-                score_trace=np.array([]),
-                fit_time=0.0,
-                gen_time=0.0,
-                experiment=get_experiment(),
-                experiment_storage_id="experiment_storage_id",
-            )
 
-        with self.assertRaisesRegex(
-            ValueError, "Must provide an `experiment` or `experiment_storage_id`"
-        ):
-            BenchmarkResult(
-                name="name",
-                seed=0,
-                optimization_trace=np.array([]),
-                score_trace=np.array([]),
-                fit_time=0.0,
-                gen_time=0.0,
-            )
-
-    def test_replication_sobol_synthetic(self) -> None:
-        method = get_sobol_benchmark_method()
-        problem = get_single_objective_benchmark_problem()
-        res = benchmark_replication(problem=problem, method=method, seed=0)
+from ax.core.experiment import Experiment
+from ax.core.objective import Objective
+from ax.core.optimization_config import OptimizationConfig
+from ax.metrics.branin import BraninMetric
+from ax.modelbridge.cross_validation import compute_model_fit_metrics_from_modelbridge
+from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
+from ax.modelbridge.registry import Models
+from ax.runners.synthetic import SyntheticRunner
+from ax.service.scheduler import get_fitted_model_bridge, Scheduler, SchedulerOptions
+from ax.telemetry.experiment import ExperimentCompletedRecord, ExperimentCreatedRecord
+from ax.telemetry.generation_strategy import GenerationStrategyCreatedRecord
+from ax.telemetry.scheduler import SchedulerCompletedRecord, SchedulerCreatedRecord
+from ax.utils.common.constants import Keys
+from ax.utils.common.testutils import TestCase
+from ax.utils.testing.core_stubs import get_branin_experiment, get_branin_search_space
+from ax.utils.testing.modeling_stubs import get_generation_strategy
 
-        self.assertEqual(
-            min(problem.num_trials, not_none(method.scheduler_options.total_trials)),
-            len(not_none(res.experiment).trials),
-        )
 
-        self.assertTrue(np.isfinite(res.score_trace).all())
-        self.assertTrue(np.all(res.score_trace <= 100))
+NUM_SOBOL = 5
 
-    def test_replication_sobol_surrogate(self) -> None:
-        method = get_sobol_benchmark_method()
-
-        for name, problem in [
-            ("soo", get_soo_surrogate()),
-            ("moo", get_moo_surrogate()),
-        ]:
-
-            with self.subTest(name, problem=problem):
-                surrogate, datasets = not_none(problem.get_surrogate_and_datasets)()
-                datasets = [get_dataset()]
-                surrogate.fit(
-                    datasets,
-                    search_space_digest=extract_search_space_digest(
-                        problem.search_space,
-                        param_names=[*problem.search_space.parameters.keys()],
-                    ),
-                )
-                res = benchmark_replication(problem=problem, method=method, seed=0)
 
-                self.assertEqual(
-                    min(
-                        problem.num_trials,
-                        not_none(method.scheduler_options.total_trials),
-                    ),
-                    len(not_none(res.experiment).trials),
-                )
+class TestScheduler(TestCase):
+    def test_scheduler_created_record_from_scheduler(self) -> None:
+        scheduler = Scheduler(
+            experiment=get_branin_experiment(),
+            generation_strategy=get_generation_strategy(),
+            options=SchedulerOptions(
+                total_trials=0,
+                tolerated_trial_failure_rate=0.2,
+                init_seconds_between_polls=10,
+            ),
+        )
 
-                self.assertTrue(np.isfinite(res.score_trace).all())
-                self.assertTrue(np.all(res.score_trace <= 100))
+        record = SchedulerCreatedRecord.from_scheduler(scheduler=scheduler)
 
-    @fast_botorch_optimize
-    def test_replication_mbm(self) -> None:
-        for method, problem, expected_name in [
-            (
-                get_sobol_botorch_modular_acquisition(
-                    model_cls=SingleTaskGP,
-                    acquisition_cls=qLogNoisyExpectedImprovement,
-                    distribute_replications=True,
-                ),
-                get_problem("constrained_gramacy_fixed_noise", num_trials=6),
-                "MBM::SingleTaskGP_qLogNEI",
-            ),
-            (
-                get_sobol_botorch_modular_acquisition(
-                    model_cls=SingleTaskGP,
-                    acquisition_cls=qLogNoisyExpectedImprovement,
-                    scheduler_options=get_sequential_optimization_scheduler_options(),
-                    distribute_replications=False,
-                ),
-                get_single_objective_benchmark_problem(infer_noise=False, num_trials=6),
-                "MBM::SingleTaskGP_qLogNEI",
-            ),
-            (
-                get_sobol_botorch_modular_acquisition(
-                    model_cls=FixedNoiseGP,
-                    acquisition_cls=qLogNoisyExpectedImprovement,
-                    distribute_replications=False,
-                ),
-                get_single_objective_benchmark_problem(infer_noise=False, num_trials=6),
-                "MBM::FixedNoiseGP_qLogNEI",
+        expected = SchedulerCreatedRecord(
+            experiment_created_record=ExperimentCreatedRecord.from_experiment(
+                experiment=scheduler.experiment
             ),
-            (
-                get_sobol_botorch_modular_acquisition(
-                    model_cls=FixedNoiseGP,
-                    acquisition_cls=qNoisyExpectedHypervolumeImprovement,
-                    distribute_replications=False,
-                ),
-                get_multi_objective_benchmark_problem(infer_noise=False, num_trials=6),
-                "MBM::FixedNoiseGP_qNEHVI",
+            generation_strategy_created_record=(
+                GenerationStrategyCreatedRecord.from_generation_strategy(
+                    generation_strategy=scheduler.standard_generation_strategy
+                )
             ),
-            (
-                get_sobol_botorch_modular_acquisition(
-                    model_cls=SaasFullyBayesianSingleTaskGP,
-                    acquisition_cls=qLogNoisyExpectedImprovement,
-                    distribute_replications=False,
-                ),
-                get_multi_objective_benchmark_problem(num_trials=6),
-                "MBM::SAAS_qLogNEI",
+            scheduler_total_trials=0,
+            scheduler_max_pending_trials=10,
+            arms_per_trial=1,
+            early_stopping_strategy_cls=None,
+            global_stopping_strategy_cls=None,
+            transformed_dimensionality=2,
+        )
+        self.assertEqual(record, expected)
+
+        flat = record.flatten()
+        expected_dict = {
+            **ExperimentCreatedRecord.from_experiment(
+                experiment=scheduler.experiment
+            ).__dict__,
+            **GenerationStrategyCreatedRecord.from_generation_strategy(
+                generation_strategy=scheduler.standard_generation_strategy
+            ).__dict__,
+            "scheduler_total_trials": 0,
+            "scheduler_max_pending_trials": 10,
+            "arms_per_trial": 1,
+            "early_stopping_strategy_cls": None,
+            "global_stopping_strategy_cls": None,
+            "transformed_dimensionality": 2,
+        }
+        self.assertEqual(flat, expected_dict)
+
+    def test_scheduler_completed_record_from_scheduler(self) -> None:
+        scheduler = Scheduler(
+            experiment=get_branin_experiment(),
+            generation_strategy=get_generation_strategy(),
+            options=SchedulerOptions(
+                total_trials=0,
+                tolerated_trial_failure_rate=0.2,
+                init_seconds_between_polls=10,
             ),
-        ]:
-            with self.subTest(method=method, problem=problem):
-                res = benchmark_replication(problem=problem, method=method, seed=0)
-                self.assertEqual(
-                    problem.num_trials,
-                    len(not_none(res.experiment).trials),
-                )
-                self.assertTrue(np.all(res.score_trace <= 100))
-                self.assertEqual(method.name, method.generation_strategy.name)
-                self.assertEqual(method.name, expected_name)
-
-    def test_replication_moo_sobol(self) -> None:
-        problem = get_multi_objective_benchmark_problem()
-
-        res = benchmark_replication(
-            problem=problem, method=get_sobol_benchmark_method(), seed=0
-        )
-
-        self.assertEqual(
-            problem.num_trials,
-            len(not_none(res.experiment).trials),
-        )
-        self.assertEqual(
-            problem.num_trials * 2,
-            len(not_none(res.experiment).fetch_data().df),
         )
 
-        self.assertTrue(np.all(res.score_trace <= 100))
-
-    def test_benchmark_one_method_problem(self) -> None:
-        problem = get_single_objective_benchmark_problem()
-        agg = benchmark_one_method_problem(
-            problem=problem,
-            method=get_sobol_benchmark_method(),
-            seeds=(0, 1),
+        with mock.patch.object(
+            scheduler, "get_improvement_over_baseline", return_value=5.0
+        ):
+            record = SchedulerCompletedRecord.from_scheduler(scheduler=scheduler)
+        expected = SchedulerCompletedRecord(
+            experiment_completed_record=ExperimentCompletedRecord.from_experiment(
+                experiment=scheduler.experiment
+            ),
+            best_point_quality=float("nan"),
+            model_fit_quality=float("nan"),  # nan because no model has been fit
+            model_std_quality=float("nan"),
+            model_fit_generalization=float("nan"),
+            model_std_generalization=float("nan"),
+            improvement_over_baseline=5.0,
+            num_metric_fetch_e_encountered=0,
+            num_trials_bad_due_to_err=0,
+        )
+        self._compare_scheduler_completed_records(record, expected)
+
+        flat = record.flatten()
+        expected_dict = {
+            **ExperimentCompletedRecord.from_experiment(
+                experiment=scheduler.experiment
+            ).__dict__,
+            "best_point_quality": float("nan"),
+            "model_fit_quality": float("nan"),
+            "model_std_quality": float("nan"),
+            "model_fit_generalization": float("nan"),
+            "model_std_generalization": float("nan"),
+            "improvement_over_baseline": 5.0,
+            "num_metric_fetch_e_encountered": 0,
+            "num_trials_bad_due_to_err": 0,
+        }
+        self.assertDictsAlmostEqual(flat, expected_dict, consider_nans_equal=True)
+
+    def test_scheduler_raise_exceptions(self) -> None:
+        scheduler = Scheduler(
+            experiment=get_branin_experiment(),
+            generation_strategy=get_generation_strategy(),
+            options=SchedulerOptions(
+                total_trials=0,
+                tolerated_trial_failure_rate=0.2,
+                init_seconds_between_polls=10,
+            ),
         )
 
-        self.assertEqual(len(agg.results), 2)
-        self.assertTrue(
-            all(
-                len(not_none(result.experiment).trials) == problem.num_trials
-                for result in agg.results
+        with mock.patch.object(
+            scheduler,
+            "get_improvement_over_baseline",
+            side_effect=Exception("test_exception"),
+        ):
+            record = SchedulerCompletedRecord.from_scheduler(scheduler=scheduler)
+        flat = record.flatten()
+        self.assertTrue(np.isnan(flat["improvement_over_baseline"]))
+
+    def test_scheduler_model_fit_metrics_logging(self) -> None:
+        # set up for model fit metrics
+        branin_experiment = Experiment(
+            name="branin_test_experiment",
+            search_space=get_branin_search_space(),
+            runner=SyntheticRunner(),
+            optimization_config=OptimizationConfig(
+                objective=Objective(
+                    metric=BraninMetric(name="branin", param_names=["x1", "x2"]),
+                    minimize=True,
+                ),
             ),
-            "All experiments must have 4 trials",
+            is_test=True,
         )
-
-        for col in ["mean", "P25", "P50", "P75"]:
-            self.assertTrue((agg.score_trace[col] <= 100).all())
-
-    @fast_botorch_optimize
-    def test_benchmark_multiple_problems_methods(self) -> None:
-        aggs = benchmark_multiple_problems_methods(
-            problems=[get_single_objective_benchmark_problem(num_trials=6)],
-            methods=[
-                get_sobol_benchmark_method(),
-                get_sobol_botorch_modular_acquisition(
-                    model_cls=SingleTaskGP,
-                    acquisition_cls=qLogNoisyExpectedImprovement,
-                    distribute_replications=False,
+        branin_experiment._properties[Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF] = True
+        generation_strategy = GenerationStrategy(
+            steps=[
+                GenerationStep(
+                    model=Models.SOBOL, num_trials=NUM_SOBOL, max_parallelism=NUM_SOBOL
                 ),
-            ],
-            seeds=(0, 1),
+                GenerationStep(model=Models.GPEI, num_trials=-1),
+            ]
         )
 
-        self.assertEqual(len(aggs), 2)
-        for agg in aggs:
-            for col in ["mean", "P25", "P50", "P75"]:
-                self.assertTrue((agg.score_trace[col] <= 100).all())
-
-    def test_timeout(self) -> None:
-        problem = SingleObjectiveBenchmarkProblem.from_botorch_synthetic(
-            test_problem_class=Branin,
-            test_problem_kwargs={},
-            num_trials=1000,  # Unachievable num_trials
-        )
-
-        generation_strategy = get_sobol_botorch_modular_acquisition(
-            model_cls=SingleTaskGP,
-            acquisition_cls=qLogNoisyExpectedImprovement,
-            distribute_replications=False,
-        ).generation_strategy
-
-        method = BenchmarkMethod(
-            name=generation_strategy.name,
+        # starting proper tests
+        scheduler = Scheduler(
+            experiment=branin_experiment,
             generation_strategy=generation_strategy,
-            scheduler_options=SchedulerOptions(
-                max_pending_trials=1,
-                init_seconds_between_polls=0,
-                min_seconds_before_poll=0,
-                timeout_hours=0.001,  # Strict timeout of 3.6 seconds
-            ),
+            options=SchedulerOptions(),
         )
+        # Trying to attain a record without any trials yields an error in ModelFitRecord
+        # and a warning in SchedulerCompletedRecord.
 
-        # Each replication will have a different number of trials
-        result = benchmark_one_method_problem(
-            problem=problem, method=method, seeds=(0, 1, 2, 3)
-        )
+        scheduler.run_n_trials(max_trials=NUM_SOBOL + 1)
 
-        # Test the traces get composited correctly. The AggregatedResult's traces
-        # should be the length of the shortest trace in the BenchmarkResults
-        min_num_trials = min(len(res.optimization_trace) for res in result.results)
-        self.assertEqual(len(result.optimization_trace), min_num_trials)
-        self.assertEqual(len(result.score_trace), min_num_trials)
+        # end-to-end test with Scheduler
+        record = SchedulerCompletedRecord.from_scheduler(scheduler=scheduler)
+        model_bridge = get_fitted_model_bridge(scheduler)
+
+        fit_metrics = compute_model_fit_metrics_from_modelbridge(
+            model_bridge=model_bridge,
+            experiment=scheduler.experiment,
+            generalization=False,
+            untransform=False,
+        )
+        # checking fit metrics
+        r2 = fit_metrics.get("coefficient_of_determination")
+        self.assertIsInstance(r2, dict)
+        r2 = cast(Dict[str, float], r2)
+        self.assertTrue("branin" in r2)
+        r2_branin = r2["branin"]
+        self.assertIsInstance(r2_branin, float)
+
+        std = fit_metrics.get("std_of_the_standardized_error")
+        self.assertIsInstance(std, dict)
+        std = cast(Dict[str, float], std)
+        self.assertTrue("branin" in std)
+        std_branin = std["branin"]
+        self.assertIsInstance(std_branin, float)
+
+        model_std_quality = 1 / std_branin
+
+        # check generalization metrics
+        gen_metrics = compute_model_fit_metrics_from_modelbridge(
+            model_bridge=model_bridge,
+            experiment=scheduler.experiment,
+            generalization=True,
+            untransform=False,
+        )
+        r2_gen = gen_metrics.get("coefficient_of_determination")
+        r2_gen = cast(Dict[str, float], r2_gen)
+        r2_gen_branin = r2_gen["branin"]
+        gen_std = gen_metrics.get("std_of_the_standardized_error")
+        gen_std = cast(Dict[str, float], gen_std)
+        gen_std_branin = gen_std["branin"]
+        model_std_generalization = 1 / gen_std_branin
+
+        expected = SchedulerCompletedRecord(
+            experiment_completed_record=ExperimentCompletedRecord.from_experiment(
+                experiment=scheduler.experiment
+            ),
+            best_point_quality=float("nan"),
+            model_fit_quality=r2_branin,
+            model_std_quality=model_std_quality,
+            model_fit_generalization=r2_gen_branin,
+            model_std_generalization=model_std_generalization,
+            improvement_over_baseline=float("nan"),
+            num_metric_fetch_e_encountered=0,
+            num_trials_bad_due_to_err=0,
+        )
+        self._compare_scheduler_completed_records(record, expected)
+
+        flat = record.flatten()
+        expected_dict = {
+            **ExperimentCompletedRecord.from_experiment(
+                experiment=scheduler.experiment
+            ).__dict__,
+            "best_point_quality": float("nan"),
+            "model_fit_quality": r2_branin,
+            "model_std_quality": model_std_quality,
+            "model_fit_generalization": r2_gen_branin,
+            "model_std_generalization": model_std_generalization,
+            "improvement_over_baseline": float("nan"),
+            "num_metric_fetch_e_encountered": 0,
+            "num_trials_bad_due_to_err": 0,
+        }
+        self.assertDictsAlmostEqual(flat, expected_dict, consider_nans_equal=True)
+
+    def _compare_scheduler_completed_records(
+        self, record: SchedulerCompletedRecord, expected: SchedulerCompletedRecord
+    ) -> None:
+        self.assertEqual(
+            record.experiment_completed_record, expected.experiment_completed_record
+        )
+        numeric_fields = [
+            "best_point_quality",
+            "model_fit_quality",
+            "model_std_quality",
+            "model_fit_generalization",
+            "model_std_generalization",
+            "improvement_over_baseline",
+            "num_metric_fetch_e_encountered",
+            "num_trials_bad_due_to_err",
+        ]
+        for field in numeric_fields:
+            rec_field = getattr(record, field)
+            exp_field = getattr(expected, field)
+            if np.isnan(rec_field):
+                self.assertTrue(np.isnan(exp_field))
+            else:
+                self.assertAlmostEqual(rec_field, exp_field)
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_benchmark_problem.py` & `ax-platform-0.4.0/ax/benchmark/tests/test_benchmark_problem.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional, Union
 
 from ax.benchmark.benchmark_problem import (
     MultiObjectiveBenchmarkProblem,
     SingleObjectiveBenchmarkProblem,
 )
+from ax.benchmark.metrics.benchmark import BenchmarkMetric
+from ax.benchmark.runners.botorch_test import BotorchTestProblemRunner
 from ax.core.types import ComparisonOp
-from ax.metrics.botorch_test_problem import BotorchTestProblemMetric
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast
 from botorch.test_functions.multi_objective import BraninCurrin
 from botorch.test_functions.synthetic import (
     Ackley,
     ConstrainedGramacy,
     ConstrainedHartmann,
@@ -25,14 +27,15 @@
 
 class TestBenchmarkProblem(TestCase):
     def test_single_objective_from_botorch(self) -> None:
         for botorch_test_problem in [Ackley(), ConstrainedHartmann(dim=6)]:
             test_problem = SingleObjectiveBenchmarkProblem.from_botorch_synthetic(
                 test_problem_class=botorch_test_problem.__class__,
                 test_problem_kwargs={},
+                lower_is_better=True,
                 num_trials=1,
             )
 
             # Test search space
             self.assertEqual(
                 len(test_problem.search_space.parameters), botorch_test_problem.dim
             )
@@ -74,15 +77,17 @@
                 )
                 expected_repr = (
                     "SingleObjectiveBenchmarkProblem(name=Ackley, "
                     "optimization_config=OptimizationConfig(objective=Objective("
                     'metric_name="Ackley", '
                     "minimize=True), outcome_constraints=[]), "
                     "num_trials=1, "
-                    "infer_noise=True, "
+                    "is_noiseless=True, "
+                    "observe_noise_sd=False, "
+                    "has_ground_truth=True, "
                     "tracking_metrics=[])"
                 )
             else:
                 outcome_constraint = (
                     test_problem.optimization_config.outcome_constraints[0]
                 )
                 self.assertEqual(outcome_constraint.metric.name, "constraint_slack_0")
@@ -90,15 +95,19 @@
                 self.assertFalse(outcome_constraint.relative)
                 self.assertEqual(outcome_constraint.bound, 0.0)
                 expected_repr = (
                     "SingleObjectiveBenchmarkProblem(name=ConstrainedHartmann, "
                     "optimization_config=OptimizationConfig(objective=Objective("
                     'metric_name="ConstrainedHartmann", minimize=True), '
                     "outcome_constraints=[OutcomeConstraint(constraint_slack_0"
-                    " >= 0.0)]), num_trials=1, infer_noise=True, "
+                    " >= 0.0)]), "
+                    "num_trials=1, "
+                    "is_noiseless=True, "
+                    "observe_noise_sd=False, "
+                    "has_ground_truth=True, "
                     "tracking_metrics=[])"
                 )
 
             self.assertEqual(repr(test_problem), expected_repr)
 
     # pyre-fixme[56]: Invalid decoration. Pyre was not able to infer the type of
     # argument `hypothesis.strategies.booleans()` to decorator factory
@@ -106,68 +115,51 @@
     @given(
         st.booleans(),
         st.one_of(st.none(), st.just(0.1)),
         st.one_of(st.none(), st.just(0.2), st.just([0.3, 0.4])),
     )
     def test_constrained_from_botorch(
         self,
-        infer_noise: bool,
+        observe_noise_sd: bool,
         objective_noise_std: Optional[float],
         constraint_noise_std: Optional[Union[float, List[float]]],
     ) -> None:
         ax_problem = SingleObjectiveBenchmarkProblem.from_botorch_synthetic(
             test_problem_class=ConstrainedGramacy,
             test_problem_kwargs={
                 "noise_std": objective_noise_std,
                 "constraint_noise_std": constraint_noise_std,
             },
+            lower_is_better=True,
             num_trials=1,
-            infer_noise=infer_noise,
+            observe_noise_sd=observe_noise_sd,
         )
         runner = checked_cast(BotorchTestProblemRunner, ax_problem.runner)
         self.assertTrue(runner._is_constrained)
         botorch_problem = checked_cast(ConstrainedGramacy, runner.test_problem)
         self.assertEqual(botorch_problem.noise_std, objective_noise_std)
         self.assertEqual(botorch_problem.constraint_noise_std, constraint_noise_std)
         opt_config = ax_problem.optimization_config
         outcome_constraints = opt_config.outcome_constraints
         self.assertEqual(
             [constraint.metric.name for constraint in outcome_constraints],
             [f"constraint_slack_{i}" for i in range(botorch_problem.num_constraints)],
         )
-        if infer_noise:
-            expected_opt_noise_sd = None
-        elif objective_noise_std is None:
-            expected_opt_noise_sd = 0.0
-        else:
-            expected_opt_noise_sd = objective_noise_std
 
         self.assertEqual(
-            checked_cast(
-                BotorchTestProblemMetric, opt_config.objective.metric
-            ).noise_sd,
-            expected_opt_noise_sd,
+            checked_cast(BenchmarkMetric, opt_config.objective.metric).observe_noise_sd,
+            observe_noise_sd,
         )
 
-        if infer_noise:
-            expected_constraint_noise_sd = [None for _ in range(2)]
-        elif constraint_noise_std is None:
-            expected_constraint_noise_sd = [0.0 for _ in range(2)]
-        elif isinstance(constraint_noise_std, float):
-            expected_constraint_noise_sd = [constraint_noise_std for _ in range(2)]
-        else:
-            expected_constraint_noise_sd = constraint_noise_std
-
-        self.assertEqual(
-            [
-                checked_cast(BotorchTestProblemMetric, constraint.metric).noise_sd
-                for constraint in outcome_constraints
-            ],
-            expected_constraint_noise_sd,
-        )
+        # TODO: Support observing noise variance only for some outputs
+        for constraint in outcome_constraints:
+            self.assertEqual(
+                checked_cast(BenchmarkMetric, constraint.metric).observe_noise_sd,
+                observe_noise_sd,
+            )
 
     def test_moo_from_botorch(self) -> None:
         test_problem = BraninCurrin()
         branin_currin_problem = (
             MultiObjectiveBenchmarkProblem.from_botorch_multi_objective(
                 test_problem_class=test_problem.__class__,
                 test_problem_kwargs={},
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_mixed_integer_problems.py` & `ax-platform-0.4.0/ax/benchmark/tests/problems/test_mixed_integer_problems.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest.mock import MagicMock
 
 import torch
 
 from ax.benchmark.problems.synthetic.discretized.mixed_integer import (
     get_discrete_ackley,
     get_discrete_hartmann,
     get_discrete_rosenbrock,
 )
+from ax.benchmark.runners.botorch_test import BotorchTestProblemRunner
 from ax.core.arm import Arm
 from ax.core.parameter import ParameterType
 from ax.core.trial import Trial
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast, not_none
 
 
-class TestMixedIntegerProblems(TestCase):
+class MixedIntegerProblemsTest(TestCase):
     def test_problems(self) -> None:
         for name, constructor, dim, dim_int in (
             ("Hartmann", get_discrete_hartmann, 6, 4),
             ("Ackley", get_discrete_ackley, 13, 10),
             ("Rosenbrock", get_discrete_rosenbrock, 10, 6),
         ):
             problem = constructor()
@@ -55,17 +57,18 @@
                 expected_bounds,
             )
 
         # Test that they match correctly to the original problems.
         # Hartmann - evaluate at 0 - should correspond to 0.
         runner = checked_cast(BotorchTestProblemRunner, get_discrete_hartmann().runner)
         mock_call = MagicMock(return_value=torch.tensor(0.0))
-        runner.test_problem.forward = mock_call
+        runner.test_problem.evaluate_true = mock_call
         trial = Trial(experiment=MagicMock())
-        trial.add_arm(Arm(parameters={f"x{i+1}": 0.0 for i in range(6)}, name="--"))
+        arm = Arm(parameters={f"x{i + 1}": 0.0 for i in range(6)}, name="--")
+        trial.add_arm(arm)
         runner.run(trial)
         actual = mock_call.call_args[0][0]
         self.assertTrue(torch.allclose(actual, torch.zeros(6, dtype=actual.dtype)))
         # Evaluate at 3, 3, 19, 19, 1, 1 - corresponds to 1.
         arm = not_none(trial.arm)
         arm._parameters = {
             "x1": 3,
@@ -76,15 +79,15 @@
             "x6": 1.0,
         }
         runner.run(trial)
         actual = mock_call.call_args[0][0]
         self.assertTrue(torch.allclose(actual, torch.ones(6, dtype=actual.dtype)))
         # Ackley - evaluate at 0 - corresponds to 0.
         runner = checked_cast(BotorchTestProblemRunner, get_discrete_ackley().runner)
-        runner.test_problem.forward = mock_call
+        runner.test_problem.evaluate_true = mock_call
         arm._parameters = {f"x{i+1}": 0.0 for i in range(13)}
         runner.run(trial)
         actual = mock_call.call_args[0][0]
         self.assertTrue(torch.allclose(actual, torch.zeros(13, dtype=actual.dtype)))
         # Evaluate at 2 x 5, 4 x 5, 1.0 x 3 - corresponds to 1.
         arm._parameters = {
             **{f"x{i+1}": 2 for i in range(0, 5)},
@@ -94,15 +97,15 @@
         runner.run(trial)
         actual = mock_call.call_args[0][0]
         self.assertTrue(torch.allclose(actual, torch.ones(13, dtype=actual.dtype)))
         # Rosenbrock - evaluate at 0 - corresponds to -5.0.
         runner = checked_cast(
             BotorchTestProblemRunner, get_discrete_rosenbrock().runner
         )
-        runner.test_problem.forward = mock_call
+        runner.test_problem.evaluate_true = mock_call
         arm._parameters = {f"x{i+1}": 0.0 for i in range(10)}
         runner.run(trial)
         actual = mock_call.call_args[0][0]
         self.assertTrue(
             torch.allclose(actual, torch.full((10,), -5.0, dtype=actual.dtype))
         )
         # Evaluate at 3 x 6, 1.0 x 4 - corresponds to 10.0.
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_problem_storage.py` & `ax-platform-0.4.0/ax/benchmark/tests/problems/test_problem_storage.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.benchmark.problems.hpo.torchvision import PyTorchCNNTorchvisionBenchmarkProblem
 from ax.storage.json_store.decoder import object_from_json
 from ax.storage.json_store.encoder import object_to_json
 from ax.utils.common.testutils import TestCase
 
 
 class TestProblems(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_problems.py` & `ax-platform-0.4.0/ax/benchmark/tests/problems/test_problems.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.benchmark.problems.registry import BENCHMARK_PROBLEM_REGISTRY, get_problem
 from ax.utils.common.testutils import TestCase
 
 
 class TestProblems(TestCase):
     def test_load_problems(self) -> None:
 
@@ -19,16 +21,16 @@
 
     def test_name(self) -> None:
         expected_names = [
             ("branin", "Branin"),
             ("hartmann3", "Hartmann_3d"),
             ("hartmann6", "Hartmann_6d"),
             ("hartmann30", "Hartmann_30d"),
-            ("branin_currin_fixed_noise", "BraninCurrin_fixed_noise"),
-            ("branin_currin30_fixed_noise", "BraninCurrin_fixed_noise_30d"),
+            ("branin_currin_observed_noise", "BraninCurrin_observed_noise"),
+            ("branin_currin30_observed_noise", "BraninCurrin_observed_noise_30d"),
             ("levy4", "Levy_4d"),
         ]
         for registry_key, problem_name in expected_names:
             problem = get_problem(problem_name=registry_key)
             self.assertEqual(problem.name, problem_name)
 
     def test_no_duplicates(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_surrogate_problems.py` & `ax-platform-0.4.0/ax/benchmark/tests/problems/test_surrogate_problems.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,34 +1,44 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import numpy as np
 from ax.benchmark.benchmark import compute_score_trace
+from ax.benchmark.benchmark_problem import BenchmarkProblemProtocol
 from ax.core.runner import Runner
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.benchmark_stubs import get_moo_surrogate, get_soo_surrogate
 
 
 class TestSurrogateProblems(TestCase):
+    def test_conforms_to_protocol(self) -> None:
+        sbp = get_soo_surrogate()
+        self.assertIsInstance(sbp, BenchmarkProblemProtocol)
+
+        mbp = get_moo_surrogate()
+        self.assertIsInstance(mbp, BenchmarkProblemProtocol)
+
     def test_lazy_instantiation(self) -> None:
 
         # test instantiation from init
         sbp = get_soo_surrogate()
         # test __repr__ method
 
         expected_repr = (
             "SOOSurrogateBenchmarkProblem(name=test, "
             "optimization_config=OptimizationConfig(objective=Objective(metric_name="
             '"branin", '
             "minimize=False), "
-            "outcome_constraints=[]), num_trials=6, infer_noise=False, "
-            "tracking_metrics=[])"
+            "outcome_constraints=[]), num_trials=6, is_noiseless=True, "
+            "observe_noise_stds=True, noise_stds=0.0, tracking_metrics=[])"
         )
         self.assertEqual(repr(sbp), expected_repr)
 
         self.assertIsNone(sbp._runner)
         # sets runner
         self.assertIsInstance(sbp.runner, Runner)
```

### Comparing `ax-platform-0.3.7/ax/benchmark/tests/test_surrogate_runner.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_transform_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,62 +1,88 @@
+#!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from unittest.mock import MagicMock
+# pyre-strict
 
-import torch
-from ax.benchmark.problems.surrogate import SurrogateRunner
-from ax.core.arm import Arm
+from unittest import mock
+
+import numpy as np
+from ax.core.data import Data
+from ax.core.experiment import Experiment
+from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
-from ax.core.trial import Trial
-from ax.modelbridge.transforms.int_to_float import IntToFloat
-from ax.modelbridge.transforms.log import Log
+from ax.modelbridge.base import ModelBridge
+from ax.modelbridge.transforms.utils import (
+    ClosestLookupDict,
+    derelativize_optimization_config_with_raw_status_quo,
+)
 from ax.utils.common.testutils import TestCase
-from ax.utils.common.typeutils import checked_cast, not_none
+from ax.utils.testing.core_stubs import get_multi_objective_optimization_config
+
+OBSERVATION_DATA = [
+    Observation(
+        features=ObservationFeatures(parameters={"x": 2.0, "y": 10.0}),
+        data=ObservationData(
+            means=np.array([1.0, 2.0, 6.0]),
+            covariance=np.array([[1.0, 2.0, 0.0], [3.0, 4.0, 0.0], [0.0, 0.0, 4.0]]),
+            metric_names=["m1", "m2", "m3"],
+        ),
+        arm_name="1_1",
+    )
+]
+
 
+class TransformUtilsTest(TestCase):
+    def test_closest_lookup_dict(self) -> None:
+        # test empty lookup
+        d = ClosestLookupDict()
+        with self.assertRaises(RuntimeError):
+            d[0]
+        # basic test
+        keys = (1.0, 2, 4)
+        vals = ("a", "b", "c")
+        d = ClosestLookupDict(zip(keys, vals))
+        for k, v in zip(keys, vals):
+            self.assertEqual(d[k], v)
+        self.assertEqual(d[2.5], "b")
+        self.assertEqual(d[0], "a")
+        self.assertEqual(d[6], "c")
+        with self.assertRaises(ValueError):
+            # pyre-fixme[6]: For 1st param expected `Number` but got `str`.
+            d["str_key"] = 3
 
-class TestSurrogateRunner(TestCase):
-    def test_surrogate_runner(self) -> None:
-        surrogate = MagicMock()
-        surrogate.predict = MagicMock(return_value=(torch.zeros(1, 1), 0))
-        # Construct a search space with log-scale parameters.
-        search_space = SearchSpace(
+    @mock.patch(
+        "ax.modelbridge.base.observations_from_data",
+        autospec=True,
+        return_value=(OBSERVATION_DATA),
+    )
+    def test_derelativize_optimization_config_with_raw_status_quo(self, _) -> None:
+        optimization_config = get_multi_objective_optimization_config()
+        dummy_search_space = SearchSpace(
             parameters=[
-                RangeParameter("x", ParameterType.FLOAT, 0.0, 5.0),
-                RangeParameter("y", ParameterType.FLOAT, 1.0, 10.0, log_scale=True),
-                RangeParameter("z", ParameterType.INT, 1.0, 5.0, log_scale=True),
+                RangeParameter("x", ParameterType.FLOAT, 0, 20),
+                RangeParameter("y", ParameterType.FLOAT, 0, 20),
             ]
         )
-
-        runner = SurrogateRunner(
-            name="test runner",
-            surrogate=surrogate,
-            datasets=[],
-            search_space=search_space,
-            metric_names=["dummy metric"],
-        )
-        self.assertEqual(runner.name, "test runner")
-        self.assertIs(runner.surrogate, surrogate)
-        self.assertEqual(runner.metric_names, ["dummy metric"])
-        # Check that the transforms are set up correctly.
-        transforms = not_none(runner.transforms)
-        self.assertEqual(len(transforms), 2)
-        self.assertIsInstance(transforms[0], IntToFloat)
-        self.assertIsInstance(transforms[1], Log)
-        self.assertEqual(
-            checked_cast(IntToFloat, transforms[0]).transform_parameters, {"z"}
+        modelbridge = ModelBridge(
+            search_space=dummy_search_space,
+            model=None,
+            transforms=[],
+            experiment=Experiment(dummy_search_space, "test"),
+            data=Data(),
+            optimization_config=optimization_config,
+            status_quo_name="1_1",
         )
-        self.assertEqual(
-            checked_cast(Log, transforms[1]).transform_parameters, {"y", "z"}
+        new_opt_config = derelativize_optimization_config_with_raw_status_quo(
+            optimization_config=optimization_config,
+            modelbridge=modelbridge,
+            observations=OBSERVATION_DATA,
         )
-        # Check that the evaluation works correctly with the transformed parameters.
-        trial = Trial(experiment=MagicMock())
-        trial.add_arm(Arm({"x": 2.5, "y": 10.0, "z": 1.0}, name="0_0"))
-        run_output = runner.run(trial)
-        self.assertEqual(run_output["dummy metric"]["0_0"], 0.0)
-        self.assertIsInstance(run_output["dummy metric"]["0_0"], float)
-        surrogate.predict.assert_called_once()
-        X = surrogate.predict.call_args[1]["X"]
-        self.assertTrue(torch.allclose(X, torch.tensor([[2.5, 1.0, 0.0]])))
+        expected_bound_values = {"m1": 0.9975, "m2": 1.995, "m3": 5.985}
+        for oc in new_opt_config.all_constraints:
+            self.assertFalse(oc.relative)
+            expected_bound_value = expected_bound_values[oc.metric.name]
+            self.assertEqual(oc.bound, expected_bound_value)
```

### Comparing `ax-platform-0.3.7/ax/core/__init__.py` & `ax-platform-0.4.0/ax/core/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 # flake8: noqa F401
 from ax.core.arm import Arm
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
 from ax.core.metric import Metric
```

### Comparing `ax-platform-0.3.7/ax/core/arm.py` & `ax-platform-0.4.0/ax/core/arm.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 import hashlib
 import json
 from typing import Optional
 
 from ax.core.types import TParameterization
 from ax.utils.common.base import SortableBase
 from ax.utils.common.equality import equality_typechecker
-from ax.utils.common.typeutils import numpy_type_to_python_type
+from ax.utils.common.typeutils_nonnative import numpy_type_to_python_type
 
 
 class Arm(SortableBase):
     """Base class for defining arms.
 
     Randomization in experiments assigns units to a given arm. Thus, the arm
     encapsulates the parametrization needed by the unit.
```

### Comparing `ax-platform-0.3.7/ax/core/base_trial.py` & `ax-platform-0.4.0/ax/core/base_trial.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from abc import ABC, abstractmethod, abstractproperty
+from copy import deepcopy
 from datetime import datetime, timedelta
 from enum import Enum
 from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union
 
 from ax.core.arm import Arm
 from ax.core.data import Data
 from ax.core.formatting_utils import data_and_evaluations_from_raw_data
@@ -401,15 +404,15 @@
 
         # Default to experiment runner if trial doesn't have one
         self.assign_runner()
 
         if self._runner is None:
             raise ValueError("No runner set on trial or experiment.")
 
-        self._run_metadata = not_none(self._runner).run(self)
+        self.update_run_metadata(not_none(self._runner).run(self))
 
         if not_none(self._runner).staging_required:
             self.mark_staged()
         else:
             self.mark_running()
         return self
 
@@ -845,21 +848,62 @@
             sample_sizes=sample_sizes or {},
             data_type=self.experiment.default_data_type,
             start_time=metadata.get("start_time"),
             end_time=metadata.get("end_time"),
         )
         return evaluations, data
 
+    def _raise_cant_attach_if_completed(self) -> None:
+        """
+        Helper method used by `validate_can_attach_data` to raise an error if
+        the user tries to attach data to a completed trial. Subclasses such as
+        `Trial` override this by suggesting a remediation.
+        """
+        raise UnsupportedError(
+            f"Trial {self.index} already has status 'COMPLETED', so data cannot "
+            "be attached."
+        )
+
     def _validate_can_attach_data(self) -> None:
         """Determines whether a trial is in a state that can be attached data."""
         if self.status.is_completed:
-            raise UnsupportedError(
-                f"Trial {self.index} has already been completed with data."
-                "To add more data to it (for example, for a different metric), "
-                "use `Trial.update_trial_data()` or "
-                "BatchTrial.update_batch_trial_data()."
-            )
+            self._raise_cant_attach_if_completed()
         if self.status.is_abandoned or self.status.is_failed:
             raise UnsupportedError(
                 f"Trial {self.index} has been marked {self.status.name}, so it "
                 "no longer expects data."
             )
+
+    def _update_trial_attrs_on_clone(
+        self,
+        new_trial: BaseTrial,
+    ) -> None:
+        """Updates attributes of the trial that are not copied over when cloning
+        a trial.
+
+        Args:
+            new_trial: The cloned trial.
+            new_experiment: The experiment that the cloned trial belongs to.
+            new_status: The new status of the cloned trial.
+        """
+        new_trial._run_metadata = deepcopy(self._run_metadata)
+        new_trial._stop_metadata = deepcopy(self._stop_metadata)
+        new_trial._num_arms_created = self._num_arms_created
+        new_trial.runner = self._runner.clone() if self._runner else None
+
+        # Set status and reason accordingly.
+        if self.status == TrialStatus.CANDIDATE:
+            return
+        if self.status == TrialStatus.STAGED:
+            new_trial.mark_staged()
+            return
+        # Other statuses require the state first be set to `RUNNING`.
+        new_trial.mark_running(no_runner_required=True)
+        if self.status == TrialStatus.RUNNING:
+            return
+        if self.status == TrialStatus.ABANDONED:
+            new_trial.mark_abandoned(reason=self.abandoned_reason)
+            return
+        if self.status == TrialStatus.FAILED:
+            new_trial.mark_failed(reason=self.failed_reason)
+            return
+        new_trial.mark_as(self.status)
```

### Comparing `ax-platform-0.3.7/ax/core/batch_trial.py` & `ax-platform-0.4.0/ax/core/batch_trial.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import warnings
 
 from collections import defaultdict, OrderedDict
-from copy import deepcopy
 from dataclasses import dataclass
 from datetime import datetime
 from enum import Enum
 from logging import Logger
 from typing import (
     DefaultDict,
     Dict,
@@ -477,18 +478,15 @@
         # intuitively should just depend on the arms.
         sufficient_factors = all(len(arm.parameters or []) >= 2 for arm in self.arms)
         if not sufficient_factors:
             return False
         param_levels: DefaultDict[str, Dict[Union[str, float], int]] = defaultdict(dict)
         for arm in self.arms:
             for param_name, param_value in arm.parameters.items():
-                # Expected `Union[float, str]` for 2nd anonymous parameter to call
-                # `dict.__setitem__` but got `Optional[Union[bool, float, str]]`.
-                # pyre-fixme[6]: Expected `Union[float, str]` for 1st param but got `...
-                param_levels[param_name][param_value] = 1
+                param_levels[param_name][not_none(param_value)] = 1
         param_cardinality = 1
         for param_values in param_levels.values():
             param_cardinality *= len(param_values)
         return len(self.arms) == param_cardinality
 
     def run(self) -> BatchTrial:
         return checked_cast(BatchTrial, super().run())
@@ -597,18 +595,15 @@
 
         if (self._status_quo is not None) and include_sq:
             sq_weight = self._status_quo_weight_override
             new_trial.set_status_quo_with_weight(
                 self._status_quo.clone(),
                 weight=sq_weight,
             )
-        new_trial.runner = self._runner.clone() if self._runner else None
-        new_trial._run_metadata = deepcopy(self._run_metadata)
-        new_trial._stop_metadata = deepcopy(self._stop_metadata)
-        new_trial._num_arms_created = self._num_arms_created
+        self._update_trial_attrs_on_clone(new_trial=new_trial)
         return new_trial
 
     def attach_batch_trial_data(
         self,
         raw_data: Dict[str, TEvaluationOutcome],
         sample_sizes: Optional[Dict[str, int]] = None,
         metadata: Optional[Dict[str, Union[str, int]]] = None,
```

### Comparing `ax-platform-0.3.7/ax/core/data.py` & `ax-platform-0.4.0/ax/core/data.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import json
 from abc import abstractmethod
 from functools import reduce
 from hashlib import md5
+from io import StringIO
 from typing import Any, Dict, Iterable, List, Optional, Set, Type, TypeVar, Union
 
 import numpy as np
 import pandas as pd
 from ax.core.types import TFidelityTrialEvaluation, TTrialEvaluation
 from ax.utils.common.base import Base
 from ax.utils.common.serialization import (
@@ -196,15 +199,15 @@
         Used for storage.
         """
         # Extract `df` only if present, since certain inputs to this fn, e.g.
         # SQAData.structure_metadata_json, don't have a `df` attribute.
         if "df" in args and not isinstance(args["df"], pd.DataFrame):
             # NOTE: Need dtype=False, otherwise infers arm_names like
             # "4_1" should be int 41.
-            args["df"] = pd.read_json(args["df"]["value"], dtype=False)
+            args["df"] = pd.read_json(StringIO(args["df"]["value"]), dtype=False)
         return extract_init_args(args=args, class_=cls)
 
     @property
     def true_df(self) -> pd.DataFrame:
         """Return the `DataFrame` being used as the source of truth (avoid using
         except for caching).
         """
@@ -264,23 +267,22 @@
     ) -> TBaseData:
         """Combines multiple objects into one (with the concatenated
         underlying dataframe).
 
         Args:
             data: Iterable of Ax objects of this class to combine.
         """
-        incompatible_types = {
-            type(datum) for datum in data if not isinstance(datum, cls)
-        }
-        if incompatible_types:
-            raise TypeError(
-                f"All data objects must be instances of class {cls}. Got "
-                f"{incompatible_types}."
-            )
-        dfs = [datum.df for datum in data]
+        dfs = []
+        for datum in data:
+            if not isinstance(datum, cls):
+                raise TypeError(
+                    f"All data objects must be instances of class {cls}. Got "
+                    f"{type(datum)}."
+                )
+            dfs.append(datum.df)
 
         if len(dfs) == 0:
             return cls()
 
         return cls(df=pd.concat(dfs, axis=0, sort=True))
 
     @classmethod
```

### Comparing `ax-platform-0.3.7/ax/core/experiment.py` & `ax-platform-0.4.0/ax/core/experiment.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import logging
 import re
 import warnings
 from collections import defaultdict, OrderedDict
 from datetime import datetime
@@ -308,21 +310,14 @@
         return len(abandoned)
 
     @property
     def optimization_config(self) -> Optional[OptimizationConfig]:
         """The experiment's optimization config."""
         return self._optimization_config
 
-    @property
-    def is_moo_problem(self) -> bool:
-        """Whether the experiment's optimization config contains multiple objectives."""
-        if self.optimization_config is None:
-            return False
-        return not_none(self.optimization_config).is_moo_problem
-
     @optimization_config.setter
     def optimization_config(self, optimization_config: OptimizationConfig) -> None:
         if (
             len(self.trials) > 0
             and getattr(self, "_optimization_config", None) is not None
             and self.immutable_search_space_and_opt_config
         ):
@@ -339,14 +334,21 @@
         if any(
             isinstance(metric, MapMetric)
             for metric in optimization_config.metrics.values()
         ):
             self._default_data_type = DataType.MAP_DATA
 
     @property
+    def is_moo_problem(self) -> bool:
+        """Whether the experiment's optimization config contains multiple objectives."""
+        if self.optimization_config is None:
+            return False
+        return not_none(self.optimization_config).is_moo_problem
+
+    @property
     def data_by_trial(self) -> Dict[int, OrderedDict[int, Data]]:
         """Data stored on the experiment, indexed by trial index and storage time.
 
         First key is trial index and second key is storage time in milliseconds.
         For a given trial, data is ordered by storage time, so first added data
         will appear first in the list.
         """
@@ -928,19 +930,21 @@
 
     def lookup_data_for_trial(
         self,
         trial_index: int,
     ) -> Tuple[Data, int]:
         """Lookup stored data for a specific trial.
 
-        Returns latest data object, and its storage timestamp, present for this trial.
-        Returns empty data and -1 if no data present.
+        Returns latest data object and its storage timestamp present for this trial.
+        Returns empty data and -1 if no data is present. In particular, this method
+        will not fetch data from metrics - to do that, use `fetch_data()` instead.
 
         Args:
             trial_index: The index of the trial to lookup data for.
+
         Returns:
             The requested data object, and its storage timestamp in milliseconds.
         """
         try:
             trial_data_dict = self._data_by_trial[trial_index]
         except KeyError:
             return (self.default_data_constructor(), -1)
@@ -952,31 +956,32 @@
         trial_data = trial_data_dict[storage_time]
         return trial_data, storage_time
 
     def lookup_data(
         self,
         trial_indices: Optional[Iterable[int]] = None,
     ) -> Data:
-        """Lookup data for all trials on this experiment and for either the
-        specified metrics or all metrics currently on the experiment, if `metrics`
-        argument is not specified.
+        """Lookup stored data for trials on this experiment.
+
+        For each trial, returns latest data object present for this trial.
+        Returns empty data if no data is present. In particular, this method
+        will not fetch data from metrics - to do that, use `fetch_data()` instead.
 
         Args:
-            trial_indices: Indices of trials, for which to fetch data.
+            trial_indices: Indices of trials for which to fetch data. If omitted,
+                lookup data for all trials on the experiment.
+
         Returns:
-            Data for the experiment.
+            Data for the trials on the experiment.
         """
         data_by_trial = []
         trial_indices = trial_indices or list(self.trials.keys())
         for trial_index in trial_indices:
-            data_by_trial.append(
-                self.lookup_data_for_trial(
-                    trial_index=trial_index,
-                )[0]
-            )
+            trial_data, _ = self.lookup_data_for_trial(trial_index=trial_index)
+            data_by_trial.append(trial_data)
         if not data_by_trial:
             return self.default_data_constructor()
         last_data = data_by_trial[-1]
         last_data_type = type(last_data)
         data = last_data_type.from_multiple_data(data_by_trial)
         return data
 
@@ -1536,14 +1541,18 @@
         properties: Optional[Dict[str, Any]] = None,
         trial_indices: Optional[List[int]] = None,
         data: Optional[Data] = None,
     ) -> Experiment:
         r"""
         Return a copy of this experiment with some attributes replaced.
 
+        NOTE: This method only retains the latest data attached to the experiment.
+        This is the same data that would be accessed using common APIs such as
+        ``Experiment.lookup_data()``.
+
         Args:
             search_space: New search space. If None, it uses the cloned search space
                 of the original experiment.
             name: New experiment name. If None, it adds cloned_experiment_  prefix
                 to the original experiment name.
             optimization_config: New optimization config. If None, it clones the same
                 optimization_config from the orignal experiment.
@@ -1555,15 +1564,15 @@
             is_test: Whether the cloned experiment should be considered a test. If None,
                 it uses the same value.
             properties: New properties dictionary. If None, it uses a copy of the
                 same properties.
             trial_indices: If specified, only clones the specified trials. If None,
                 clones all trials.
             data: If specified, attach this data to the cloned experiment. If None,
-                clones the data attached to the original experiment if
+                clones the latest data attached to the original experiment if
                 the experiment has any data.
         """
         search_space = (
             self.search_space.clone() if (search_space is None) else search_space
         )
         name = (
             "cloned_experiment_" + self.name
@@ -1605,47 +1614,44 @@
             description=description,
             is_test=is_test,
             experiment_type=self.experiment_type,
             properties=properties,
             default_data_type=self._default_data_type,
         )
 
-        datas = []
-        # clone only the specified trials
+        # Clone only the specified trials.
         original_trial_indices = self.trials.keys()
-        # pyre-fixme[9]: trial_indices has type `Optional[List[int]]`; used as
-        #  `Set[int]`.
-        trial_indices = (
+        trial_indices_to_keep = (
             set(original_trial_indices) if trial_indices is None else set(trial_indices)
         )
-        if (
-            # pyre-fixme[16]: `Optional` has no attribute `difference`.
-            len(trial_indices_diff := trial_indices.difference(original_trial_indices))
-            > 0
+        if trial_indices_diff := trial_indices_to_keep.difference(
+            original_trial_indices
         ):
             warnings.warn(
                 f"Trials indexed with {trial_indices_diff} are not a part "
                 "of the original experiment. ",
                 stacklevel=2,
             )
-        # pyre-fixme[16]: `Optional` has no attribute `intersection`.
-        for trial_index in trial_indices.intersection(original_trial_indices):
+
+        data_by_trial = {}
+        for trial_index in trial_indices_to_keep.intersection(original_trial_indices):
             trial = self.trials[trial_index]
             if isinstance(trial, BatchTrial) or isinstance(trial, Trial):
                 trial.clone_to(cloned_experiment)
-                trial_data, storage_time = self.lookup_data_for_trial(trial_index)
-                if (trial_data is not None) and (storage_time is not None):
-                    datas.append(trial_data)
+                trial_data, timestamp = self.lookup_data_for_trial(trial_index)
+                if timestamp != -1:
+                    data_by_trial[trial_index] = OrderedDict([(timestamp, trial_data)])
             else:
                 raise NotImplementedError(f"Cloning of {type(trial)} is not supported.")
-
-        if (data is None) and (len(datas) > 0):
-            data = self.default_data_constructor.from_multiple_data(datas)
         if data is not None:
+            # If user passed in data, use it.
             cloned_experiment.attach_data(data)
+        else:
+            # Otherwise, attach the data extracted from the original experiment.
+            cloned_experiment._data_by_trial = data_by_trial
 
         return cloned_experiment
 
     @property
     def metric_config_summary_df(self) -> pd.DataFrame:
         """Creates a dataframe with information about each metric in the
         experiment. The resulting dataframe has one row per metric, and the
```

### Comparing `ax-platform-0.3.7/ax/core/formatting_utils.py` & `ax-platform-0.4.0/ax/core/formatting_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from enum import Enum
 from typing import cast, Dict, List, Optional, Tuple, Type, Union
 
 import numpy as np
 from ax.core.data import Data
 from ax.core.map_data import MapData
 from ax.core.types import (
     TEvaluationOutcome,
     TMapTrialEvaluation,
     TTrialEvaluation,
     validate_evaluation_outcome,
 )
 from ax.exceptions.core import UserInputError
-from ax.utils.common.typeutils import numpy_type_to_python_type
+from ax.utils.common.typeutils_nonnative import numpy_type_to_python_type
 
 
 # -------------------- Data formatting utils. ---------------------
 
 
 class DataType(Enum):
     DATA = 1
```

### Comparing `ax-platform-0.3.7/ax/core/generation_strategy_interface.py` & `ax-platform-0.4.0/ax/core/generation_strategy_interface.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,108 +1,111 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+from __future__ import annotations
+
 from abc import ABC, abstractmethod
+
 from typing import List, Optional
 
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
+from ax.exceptions.core import AxError, UnsupportedError
 from ax.utils.common.base import Base
 from ax.utils.common.typeutils import not_none
 
 
 class GenerationStrategyInterface(ABC, Base):
-    _name: Optional[str]
-    # All generator runs created through this generation strategy, in chronological
-    # order.
-    _generator_runs: List[GeneratorRun]
+    """Interface for all generation strategies: standard Ax
+    ``GenerationStrategy``, as well as non-standard (e.g. remote, external)
+    generation strategies.
+
+    NOTE: Currently in Beta; please do not use without discussion with the Ax
+    developers.
+    """
+
+    _name: str
     # Experiment, for which this generation strategy has generated trials, if
     # it exists.
     _experiment: Optional[Experiment] = None
 
+    def __init__(self, name: str) -> None:
+        self._name = name
+
     @abstractmethod
     def gen_for_multiple_trials_with_multiple_models(
         self,
         experiment: Experiment,
-        num_generator_runs: int,
         data: Optional[Data] = None,
+        # TODO[drfreund, danielcohennyc, mgarrard]: Update the format of the arguments
+        # below as we find the right one.
+        num_generator_runs: int = 1,
         n: int = 1,
     ) -> List[List[GeneratorRun]]:
-        """Produce GeneratorRuns for multiple trials at once with the possibility of
-        ensembling, or using multiple models per trial, getting multiple
-        GeneratorRuns per trial.
+        """Produce ``GeneratorRun``-s for multiple trials at once with the possibility
+        of joining ``GeneratorRun``-s from multiple models into one ``BatchTrial``.
 
         Args:
-            experiment: Experiment, for which the generation strategy is producing
-                a new generator run in the course of `gen`, and to which that
+            experiment: ``Experiment``, for which the generation strategy is producing
+                a new generator run in the course of ``gen``, and to which that
                 generator run will be added as trial(s). Information stored on the
                 experiment (e.g., trial statuses) is used to determine which model
                 will be used to produce the generator run returned from this method.
-            data: Optional data to be passed to the underlying model's `gen`, which
+            data: Optional data to be passed to the underlying model's ``gen``, which
                 is called within this method and actually produces the resulting
-                generator run. By default, data is all data on the `experiment`.
+                generator run. By default, data is all data on the ``experiment``.
             n: Integer representing how many trials should be in the generator run
                 produced by this method. NOTE: Some underlying models may ignore
                 the ``n`` and produce a model-determined number of arms. In that
                 case this method will also output a generator run with number of
                 arms that can differ from ``n``.
             pending_observations: A map from metric name to pending
                 observations for that metric, used by some models to avoid
                 resuggesting points that are currently being evaluated.
 
         Returns:
-            A list of lists of lists generator runs. Each outer list represents
-            a trial being suggested and  each inner list represents a generator
-            run for that trial.
+            A list of lists of ``GeneratorRun``-s. Each outer list item represents
+            a ``(Batch)Trial`` being suggested, with a list of ``GeneratorRun``-s for
+            that trial.
         """
-        pass
+        # When implementing your subclass' override for this method, don't forget
+        # to consider using "pending points", corresponding to arms in trials that
+        # are currently running / being evaluated/
+        ...
+
+    @abstractmethod
+    def clone_reset(self) -> GenerationStrategyInterface:
+        """Returns a clone of this generation strategy with all state reset."""
+        ...
 
     @property
     def name(self) -> str:
-        """Name of this generation strategy. Defaults to a combination of model
-        names provided in generation steps.
-        """
-        if self._name is not None:
-            return not_none(self._name)
-
-        self._name = f"GenerationStrategy {self.db_id}"
-        return not_none(self._name)
-
-    @name.setter
-    def name(self, name: str) -> None:
-        """Set generation strategy name."""
-        self._name = name
+        """Name of this generation strategy."""
+        return self._name
 
     @property
     def experiment(self) -> Experiment:
         """Experiment, currently set on this generation strategy."""
         if self._experiment is None:
-            raise ValueError("No experiment set on generation strategy.")
+            raise AxError("No experiment set on generation strategy.")
         return not_none(self._experiment)
 
     @experiment.setter
     def experiment(self, experiment: Experiment) -> None:
         """If there is an experiment set on this generation strategy as the
         experiment it has been generating generator runs for, check if the
         experiment passed in is the same as the one saved and log an information
         statement if its not. Set the new experiment on this generation strategy.
         """
-        if self._experiment is None or experiment._name == self.experiment._name:
-            self._experiment = experiment
-        else:
-            raise ValueError(
+        if self._experiment is not None and experiment._name != self.experiment._name:
+            raise UnsupportedError(
                 "This generation strategy has been used for experiment "
                 f"{self.experiment._name} so far; cannot reset experiment"
-                f" to {experiment._name}. If this is a new optimization, "
+                f" to {experiment._name}. If this is a new experiment, "
                 "a new generation strategy should be created instead."
             )
-
-    @property
-    def last_generator_run(self) -> Optional[GeneratorRun]:
-        """Latest generator run produced by this generation strategy.
-        Returns None if no generator runs have been produced yet.
-        """
-        # Used to restore current model when decoding a serialized GS.
-        return self._generator_runs[-1] if self._generator_runs else None
+        self._experiment = experiment
```

### Comparing `ax-platform-0.3.7/ax/core/generator_run.py` & `ax-platform-0.4.0/ax/core/generator_run.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import copy
 from collections import OrderedDict
 from dataclasses import dataclass
 from datetime import datetime
 from enum import Enum
@@ -350,20 +352,22 @@
 
     def clone(self) -> GeneratorRun:
         """Return a deep copy of a GeneratorRun."""
         cand_metadata = self.candidate_metadata_by_arm_signature
         generator_run = GeneratorRun(
             arms=[a.clone() for a in self.arms],
             weights=self.weights[:] if self.weights is not None else None,
-            optimization_config=self.optimization_config.clone()
-            if self.optimization_config is not None
-            else None,
-            search_space=self.search_space.clone()
-            if self.search_space is not None
-            else None,
+            optimization_config=(
+                self.optimization_config.clone()
+                if self.optimization_config is not None
+                else None
+            ),
+            search_space=(
+                self.search_space.clone() if self.search_space is not None else None
+            ),
             model_predictions=copy.deepcopy(self.model_predictions),
             best_arm_predictions=copy.deepcopy(self.best_arm_predictions),
             type=self.generator_run_type,
             fit_time=self.fit_time,
             gen_time=self.gen_time,
             model_key=self._model_key,
             model_kwargs=self._model_kwargs,
```

### Comparing `ax-platform-0.3.7/ax/core/map_data.py` & `ax-platform-0.4.0/ax/core/map_data.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from logging import Logger
 from typing import Any, Dict, Generic, Iterable, List, Optional, Sequence, Type, TypeVar
 
 import numpy as np
 
@@ -249,17 +251,19 @@
         subset_metrics: Optional[Iterable[str]] = None,
     ) -> MapData:
         """Downcast instances of Data into instances of MapData with empty
         map_key_infos if necessary then combine as usual (filling in empty cells with
         default values).
         """
         map_datas = [
-            MapData(df=datum.df, map_key_infos=[])
-            if not isinstance(datum, MapData)
-            else datum
+            (
+                MapData(df=datum.df, map_key_infos=[])
+                if not isinstance(datum, MapData)
+                else datum
+            )
             for datum in data
         ]
 
         return MapData.from_multiple_map_data(
             data=map_datas, subset_metrics=subset_metrics
         )
```

### Comparing `ax-platform-0.3.7/ax/core/map_metric.py` & `ax-platform-0.4.0/ax/core/map_metric.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Type
 
 from ax.core.map_data import MapData, MapKeyInfo
 from ax.core.metric import Metric, MetricFetchE
 from ax.utils.common.result import Result
```

### Comparing `ax-platform-0.3.7/ax/core/metric.py` & `ax-platform-0.4.0/ax/core/metric.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import traceback
 import warnings
 
 from dataclasses import dataclass
 from datetime import timedelta
@@ -470,17 +472,19 @@
         if len(oks) < len(results):
             errs: List[Err[Data, MetricFetchE]] = [
                 result for result in results.values() if isinstance(result, Err)
             ]
 
             # TODO[mpolson64] Raise all errors in a group via PEP 654
             exceptions = [
-                err.err.exception
-                if err.err.exception is not None
-                else Exception(err.err.message)
+                (
+                    err.err.exception
+                    if err.err.exception is not None
+                    else Exception(err.err.message)
+                )
                 for err in errs
             ]
 
             raise UnwrapError(errs) from (
                 exceptions[0] if len(exceptions) == 1 else Exception(exceptions)
             )
 
@@ -529,17 +533,19 @@
                 for metric_name, result in results.items()
                 if isinstance(result, Err) and metric_name in critical_metric_names
             ]
 
             if len(critical_errs) > 0:
                 # TODO[mpolson64] Raise all errors in a group via PEP 654
                 exceptions = [
-                    err.err.exception
-                    if err.err.exception is not None
-                    else Exception(err.err.message)
+                    (
+                        err.err.exception
+                        if err.err.exception is not None
+                        else Exception(err.err.message)
+                    )
                     for err in critical_errs
                 ]
                 raise UnwrapError(critical_errs) from (
                     exceptions[0] if len(exceptions) == 1 else Exception(exceptions)
                 )
 
         data = [ok.ok for ok in oks]
@@ -568,17 +574,19 @@
         if len(oks) < len(flattened):
             errs: List[Err[Data, MetricFetchE]] = [
                 result for result in flattened if isinstance(result, Err)
             ]
 
             # TODO[mpolson64] Raise all errors in a group via PEP 654
             exceptions = [
-                err.err.exception
-                if err.err.exception is not None
-                else Exception(err.err.message)
+                (
+                    err.err.exception
+                    if err.err.exception is not None
+                    else Exception(err.err.message)
+                )
                 for err in errs
             ]
             raise UnwrapError(errs) from (
                 exceptions[0] if len(exceptions) == 1 else Exception(exceptions)
             )
 
         data = [ok.ok for ok in oks]
```

### Comparing `ax-platform-0.3.7/ax/core/objective.py` & `ax-platform-0.4.0/ax/core/objective.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import warnings
 from logging import Logger
 from typing import Any, Iterable, List, Optional, Tuple
 
 from ax.core.metric import Metric
+from ax.exceptions.core import UserInputError
 from ax.utils.common.base import SortableBase
 from ax.utils.common.logger import get_logger
 from ax.utils.common.typeutils import not_none
 
 logger: Logger = get_logger(__name__)
 
 
@@ -28,44 +31,35 @@
     def __init__(self, metric: Metric, minimize: Optional[bool] = None) -> None:
         """Create a new objective.
 
         Args:
             metric: The metric to be optimized.
             minimize: If True, minimize metric. If None, will be set based on the
                 `lower_is_better` property of the metric (if that is not specified,
-                will raise a DeprecationWarning).
+                will raise a `UserInputError`).
 
         """
         lower_is_better = metric.lower_is_better
         if minimize is None:
             if lower_is_better is None:
-                warnings.warn(
-                    f"Defaulting to `minimize=False` for metric {metric.name} not "
-                    + "specifying `lower_is_better` property. This is a wild guess. "
-                    + "Specify either `lower_is_better` on the metric, or specify "
-                    + "`minimize` explicitly. This will become an error in the future.",
-                    DeprecationWarning,
+                raise UserInputError(
+                    f"Metric {metric.name} does not specify `lower_is_better` "
+                    "and `minimize` is not specified. At least one of these "
+                    "must be specified."
                 )
-                minimize = False
             else:
                 minimize = lower_is_better
-        if lower_is_better is not None:
-            if lower_is_better and not minimize:
-                warnings.warn(
-                    f"Attempting to maximize metric {metric.name} with property "
-                    "`lower_is_better=True`."
-                )
-            elif not lower_is_better and minimize:
-                warnings.warn(
-                    f"Attempting to minimize metric {metric.name} with property "
-                    "`lower_is_better=False`."
-                )
-        self._metric = metric
-        # pyre-fixme[4]: Attribute must be annotated.
-        self.minimize = not_none(minimize)
+        elif lower_is_better is not None and lower_is_better != minimize:
+            raise UserInputError(
+                f"Metric {metric.name} specifies {lower_is_better=}, "
+                "which doesn't match the specified optimization direction "
+                f"{minimize=}."
+            )
+        self._metric: Metric = metric
+        self.minimize: bool = not_none(minimize)
 
     @property
     def metric(self) -> Metric:
         """Get the objective metric."""
         return self._metric
 
     @property
@@ -124,26 +118,25 @@
         if objectives is None:
             if "metrics" not in extra_kwargs:
                 raise ValueError(
                     "Must either specify `objectives` or `metrics` "
                     "as input to `MultiObjective` constructor."
                 )
             metrics = extra_kwargs["metrics"]
-            minimize = extra_kwargs.get("minimize", False)
+            minimize = extra_kwargs.get("minimize", None)
             warnings.warn(
                 "Passing `metrics` and `minimize` as input to the `MultiObjective` "
                 "constructor will soon be deprecated. Instead, pass a list of "
                 "`objectives`. This will become an error in the future.",
                 DeprecationWarning,
+                stacklevel=2,
             )
             objectives = []
             for metric in metrics:
-                lower_is_better = metric.lower_is_better or False
-                _minimize = not lower_is_better if minimize else lower_is_better
-                objectives.append(Objective(metric=metric, minimize=_minimize))
+                objectives.append(Objective(metric=metric, minimize=minimize))
 
         # pyre-fixme[4]: Attribute must be annotated.
         self._objectives = not_none(objectives)
 
         # For now, assume all objectives are weighted equally.
         # This might be used in the future to change emphasis on the
         # relative focus of the exploration during the optimization.
```

### Comparing `ax-platform-0.3.7/ax/core/observation.py` & `ax-platform-0.4.0/ax/core/observation.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,31 +1,34 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import json
+import warnings
 from copy import deepcopy
 from typing import Dict, Iterable, List, Optional, Set, Tuple
 
 import ax.core.experiment as experiment
 
 import numpy as np
 import pandas as pd
 from ax.core.arm import Arm
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
 from ax.core.map_data import MapData
 from ax.core.types import TCandidateMetadata, TParameterization
 from ax.utils.common.base import Base
 from ax.utils.common.constants import Keys
-from ax.utils.common.typeutils import not_none
+from ax.utils.common.typeutils import checked_cast, not_none
 
 
 TIME_COLS = {"start_time", "end_time"}
 
 OBS_COLS: Set[str] = {
     "arm_name",
     "trial_index",
@@ -309,14 +312,21 @@
 
         obs_parameters = experiment.arms_by_name[arm_name].parameters.copy()
         if obs_parameters:
             obs_kwargs["parameters"] = obs_parameters
         for f, val in features.items():
             if f in OBS_KWARGS:
                 obs_kwargs[f] = val
+        # add start and end time of trial if the start and end time
+        # is the same for all metrics and arms
+        for col in TIME_COLS:
+            if col in d.columns:
+                times = d[col]
+                if times.nunique() == 1 and not times.isnull().any():
+                    obs_kwargs[col] = times.iloc[0]
         fidelities = features.get("fidelities")
         if fidelities is not None:
             obs_parameters.update(json.loads(fidelities))
 
         for map_key in map_keys:
             if map_key in obs_parameters or map_keys_as_parameters:
                 obs_parameters[map_key] = features[map_key]
@@ -332,20 +342,34 @@
                 ),
                 arm_name=arm_name,
             )
         )
     return observations
 
 
-def get_feature_cols(data: Data) -> List[str]:
-    return list(OBS_COLS.intersection(data.df.columns))
-
+def get_feature_cols(data: Data, is_map_data: bool = False) -> List[str]:
+    feature_cols = OBS_COLS.intersection(data.df.columns)
+    # note we use this check, rather than isinstance, since
+    # only some Modelbridges (e.g. MapTorchModelBridge)
+    # use observations_from_map_data, which is required
+    # to properly handle MapData features (e.g. fidelity).
+    if is_map_data:
+        data = checked_cast(MapData, data)
+        feature_cols = feature_cols.union(data.map_keys)
+
+    for column in TIME_COLS:
+        if column in feature_cols and len(data.df[column].unique()) > 1:
+            warnings.warn(
+                f"`{column} is not consistent and being discarded from "
+                "observation data",
+                stacklevel=5,
+            )
+            feature_cols.discard(column)
 
-def get_feature_cols_from_map_data(map_data: MapData) -> List[str]:
-    return list(OBS_COLS.intersection(map_data.df.columns).union(map_data.map_keys))
+    return list(feature_cols)
 
 
 def observations_from_data(
     experiment: experiment.Experiment, data: Data, include_abandoned: bool = False
 ) -> List[Observation]:
     """Convert Data to observations.
 
@@ -452,15 +476,15 @@
     if limit_rows_per_metric is not None or limit_rows_per_group is not None:
         map_data = map_data.subsample(
             map_key=map_data.map_keys[0],
             limit_rows_per_metric=limit_rows_per_metric,
             limit_rows_per_group=limit_rows_per_group,
             include_first_last=True,
         )
-    feature_cols = get_feature_cols_from_map_data(map_data)
+    feature_cols = get_feature_cols(map_data, is_map_data=True)
     observations = []
     arm_name_only = len(feature_cols) == 1  # there will always be an arm name
     # One DataFrame where all rows have all features.
     isnull = map_data.map_df[feature_cols].isnull()
     isnull_any = isnull.any(axis=1)
     incomplete_df_cols = isnull[isnull_any].any()
 
@@ -472,17 +496,19 @@
 
     if set(feature_cols) == set(complete_feature_cols):
         complete_df = map_data.map_df
         incomplete_df = None
     else:
         # The groupby and filter is expensive, so do it only if we have to.
         grouped = map_data.map_df.groupby(
-            by=complete_feature_cols
-            if len(complete_feature_cols) > 1
-            else complete_feature_cols[0]
+            by=(
+                complete_feature_cols
+                if len(complete_feature_cols) > 1
+                else complete_feature_cols[0]
+            )
         )
         complete_df = grouped.filter(lambda r: ~r[feature_cols].isnull().any().any())
         incomplete_df = grouped.filter(lambda r: r[feature_cols].isnull().any().any())
 
     # Get Observations from complete_df
     observations.extend(
         _observations_from_dataframe(
```

### Comparing `ax-platform-0.3.7/ax/core/optimization_config.py` & `ax-platform-0.4.0/ax/core/optimization_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from itertools import groupby
 from logging import Logger
 from typing import Dict, List, Optional
 
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective, ScalarizedObjective
 from ax.core.outcome_constraint import (
```

### Comparing `ax-platform-0.3.7/ax/core/outcome_constraint.py` & `ax-platform-0.4.0/ax/core/outcome_constraint.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import logging
 from typing import Dict, Iterable, List, Optional, Tuple
 
 from ax.core.metric import Metric
 from ax.core.types import ComparisonOp
```

### Comparing `ax-platform-0.3.7/ax/core/parameter.py` & `ax-platform-0.4.0/ax/core/parameter.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from abc import ABCMeta, abstractmethod, abstractproperty
 from copy import deepcopy
 from enum import Enum
 from math import inf
-from typing import Dict, List, Optional, Tuple, Type, Union
+from typing import cast, Dict, List, Optional, Tuple, Type, Union
 from warnings import warn
 
-from ax.core.types import TParamValue, TParamValueList
-from ax.exceptions.core import AxWarning, UserInputError
+from ax.core.types import TNumeric, TParamValue, TParamValueList
+from ax.exceptions.core import AxParameterWarning, UserInputError
 from ax.utils.common.base import SortableBase
 from ax.utils.common.typeutils import not_none
+from pyre_extensions import assert_is_instance
 
 # Tolerance for floating point comparisons. This is relatively permissive,
 # and allows for serializing at rather low numerical precision.
 # TODO: Do a more comprehensive audit of how floating point precision issues
 # may creep up and implement a more principled fix
 EPS = 1.5e-7
 MAX_VALUES_CHOICE_PARAM = 1000
@@ -75,15 +78,15 @@
             return param_type
     raise ValueError(f"No Ax parameter type corresponding to {python_type}.")
 
 
 class Parameter(SortableBase, metaclass=ABCMeta):
     _is_fidelity: bool = False
     _name: str
-    _target_value: Optional[TParamValue] = None
+    _target_value: TParamValue = None
     _parameter_type: ParameterType
 
     def cast(self, value: TParamValue) -> TParamValue:
         if value is None:
             return None
         return self.python_type(value)
 
@@ -119,15 +122,15 @@
     @property
     def is_hierarchical(self) -> bool:
         return isinstance(self, (ChoiceParameter, FixedParameter)) and bool(
             self._dependents
         )
 
     @property
-    def target_value(self) -> Optional[TParamValue]:
+    def target_value(self) -> TParamValue:
         return self._target_value
 
     @property
     def parameter_type(self) -> ParameterType:
         return self._parameter_type
 
     @property
@@ -228,15 +231,15 @@
         parameter_type: ParameterType,
         lower: float,
         upper: float,
         log_scale: bool = False,
         logit_scale: bool = False,
         digits: Optional[int] = None,
         is_fidelity: bool = False,
-        target_value: Optional[TParamValue] = None,
+        target_value: TParamValue = None,
     ) -> None:
         """Initialize RangeParameter
 
         Args:
             name: Name of the parameter.
             parameter_type: Enum indicating the type of parameter
                 value (e.g. string, int).
@@ -253,102 +256,115 @@
         if is_fidelity and (target_value is None):
             raise UserInputError(
                 "`target_value` should not be None for the fidelity parameter: "
                 "{}".format(name)
             )
 
         self._name = name
+        if parameter_type not in (ParameterType.INT, ParameterType.FLOAT):
+            raise UserInputError("RangeParameter type must be int or float.")
         self._parameter_type = parameter_type
         self._digits = digits
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._lower = self.cast(lower)
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._upper = self.cast(upper)
+        self._lower: TNumeric = not_none(self.cast(lower))
+        self._upper: TNumeric = not_none(self.cast(upper))
         self._log_scale = log_scale
         self._logit_scale = logit_scale
         self._is_fidelity = is_fidelity
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._target_value = self.cast(target_value)
+        self._target_value: Optional[TNumeric] = self.cast(target_value)
 
         self._validate_range_param(
             parameter_type=parameter_type,
             lower=lower,
             upper=upper,
             log_scale=log_scale,
             logit_scale=logit_scale,
         )
 
-    def cardinality(self) -> float:
+    def cardinality(self) -> TNumeric:
         if self.parameter_type == ParameterType.FLOAT:
             return inf
 
-        return self.upper - self.lower + 1
+        return int(self.upper) - int(self.lower) + 1
 
     def _validate_range_param(
         self,
-        lower: TParamValue,
-        upper: TParamValue,
+        lower: TNumeric,
+        upper: TNumeric,
         log_scale: bool,
         logit_scale: bool,
         parameter_type: Optional[ParameterType] = None,
     ) -> None:
         if parameter_type and parameter_type not in (
             ParameterType.INT,
             ParameterType.FLOAT,
         ):
             raise UserInputError("RangeParameter type must be int or float.")
-        # pyre-fixme[58]: `>=` is not supported for operand types `Union[None, bool,
-        #  float, int, str]` and `Union[None, bool, float, int, str]`.
+
+        upper = float(upper)
         if lower >= upper:
             raise UserInputError(
                 f"Upper bound of {self.name} must be strictly larger than lower."
                 f"Got: ({lower}, {upper})."
             )
-        # pyre-fixme[58]: `-` is not supported for operand types `Union[None, bool,
-        #  float, int, str]` and `Union[None, bool, float, int, str]`.
         width: float = upper - lower
         if width < 100 * EPS:
             raise UserInputError(
                 f"Parameter range ({width}) is very small and likely "
                 "to cause numerical errors. Consider reparameterizing your "
                 "problem by scaling the parameter."
             )
         if log_scale and logit_scale:
             raise UserInputError("Can't use both log and logit.")
-        # pyre-fixme[58]: `<=` is not supported for operand types `Union[None, bool,
-        #  float, int, str]` and `int`.
         if log_scale and lower <= 0:
             raise UserInputError("Cannot take log when min <= 0.")
-        # pyre-fixme[58]: `<=` is not supported for operand types `Union[None, bool,
-        #  float, int, str]` and `int`.
         if logit_scale and (lower <= 0 or upper >= 1):
             raise UserInputError("Logit requires lower > 0 and upper < 1")
         if not (self.is_valid_type(lower)) or not (self.is_valid_type(upper)):
             raise UserInputError(
                 f"[{lower}, {upper}] is an invalid range for this parameter."
             )
 
     @property
-    def upper(self) -> float:
+    def upper(self) -> TNumeric:
         """Upper bound of the parameter range.
 
         Value is cast to parameter type upon set and also validated
         to ensure the bound is strictly greater than lower bound.
         """
         return self._upper
 
+    @upper.setter
+    def upper(self, value: TNumeric) -> None:
+        self._validate_range_param(
+            lower=self.lower,
+            upper=value,
+            log_scale=self.log_scale,
+            logit_scale=self.logit_scale,
+        )
+        self._upper = not_none(self.cast(value))
+
     @property
-    def lower(self) -> float:
+    def lower(self) -> TNumeric:
         """Lower bound of the parameter range.
 
         Value is cast to parameter type upon set and also validated
         to ensure the bound is strictly less than upper bound.
         """
         return self._lower
 
+    @lower.setter
+    def lower(self, value: TNumeric) -> None:
+        self._validate_range_param(
+            lower=value,
+            upper=self.upper,
+            log_scale=self.log_scale,
+            logit_scale=self.logit_scale,
+        )
+        self._lower = not_none(self.cast(value))
+
     @property
     def digits(self) -> Optional[int]:
         """Number of digits to round values to for float type.
 
         Upper and lower bound are re-cast after this property is changed.
         """
         return self._digits
@@ -375,34 +391,32 @@
             upper: New value for the upper bound.
         """
         if lower is None:
             lower = self._lower
         if upper is None:
             upper = self._upper
 
-        cast_lower = self.cast(lower)
-        cast_upper = self.cast(upper)
+        cast_lower = not_none(self.cast(lower))
+        cast_upper = not_none(self.cast(upper))
         self._validate_range_param(
             lower=cast_lower,
             upper=cast_upper,
             log_scale=self.log_scale,
             logit_scale=self.logit_scale,
         )
         self._lower = cast_lower
         self._upper = cast_upper
         return self
 
     def set_digits(self, digits: int) -> RangeParameter:
         self._digits = digits
 
         # Re-scale min and max to new digits definition
-        cast_lower = self.cast(self._lower)
-        cast_upper = self.cast(self._upper)
-        # pyre-fixme[58]: `>=` is not supported for operand types `Union[None, bool,
-        #  float, int, str]` and `Union[None, bool, float, int, str]`.
+        cast_lower = not_none(self.cast(self._lower))
+        cast_upper = not_none(self.cast(self._upper))
         if cast_lower >= cast_upper:
             raise UserInputError(
                 f"Lower bound {cast_lower} is >= upper bound {cast_upper}."
             )
 
         self._lower = cast_lower
         self._upper = cast_upper
@@ -445,17 +459,15 @@
         for Int parameters.
         """
         if not (isinstance(value, float) or isinstance(value, int)):
             return False
 
         # This might have issues with ints > 2^24
         if self.parameter_type is ParameterType.INT:
-            # pyre-fixme[6]: Expected `Union[_SupportsIndex, bytearray, bytes, str,
-            #  typing.SupportsFloat]` for 1st param but got `Union[None, float, str]`.
-            return isinstance(value, int) or float(value).is_integer()
+            return isinstance(value, int) or float(not_none(value)).is_integer()
         return True
 
     def clone(self) -> RangeParameter:
         return RangeParameter(
             name=self._name,
             parameter_type=self._parameter_type,
             lower=self._lower,
@@ -463,21 +475,20 @@
             log_scale=self._log_scale,
             logit_scale=self._logit_scale,
             digits=self._digits,
             is_fidelity=self._is_fidelity,
             target_value=self._target_value,
         )
 
-    def cast(self, value: TParamValue) -> TParamValue:
+    def cast(self, value: TParamValue) -> Optional[TNumeric]:
         if value is None:
             return None
         if self.parameter_type is ParameterType.FLOAT and self._digits is not None:
-            # pyre-fixme[6]: Expected `None` for 2nd param but got `Optional[int]`.
-            return round(float(value), self._digits)
-        return self.python_type(value)
+            return round(float(value), not_none(self._digits))
+        return assert_is_instance(self.python_type(value), TNumeric)
 
     def __repr__(self) -> str:
         ret_val = self._base_repr()
 
         if self._digits is not None:
             ret_val += f", digits={self._digits}"
 
@@ -520,15 +531,15 @@
         self,
         name: str,
         parameter_type: ParameterType,
         values: List[TParamValue],
         is_ordered: Optional[bool] = None,
         is_task: bool = False,
         is_fidelity: bool = False,
-        target_value: Optional[TParamValue] = None,
+        target_value: TParamValue = None,
         sort_values: Optional[bool] = None,
         dependents: Optional[Dict[TParamValue, List[str]]] = None,
     ) -> None:
         if (is_fidelity or is_task) and (target_value is None):
             ptype = "fidelity" if is_fidelity else "task"
             raise UserInputError(
                 f"`target_value` should not be None for the {ptype} parameter: "
@@ -551,37 +562,34 @@
             )
         # Remove duplicate values.
         # Using dict to deduplicate here since set doesn't preserve order but dict does.
         if len(values) != len(dict_values := dict.fromkeys(values)):
             warn(
                 f"Duplicate values found for ChoiceParameter {name}. "
                 "Initializing the parameter with duplicate values removed. ",
-                AxWarning,
+                AxParameterWarning,
                 stacklevel=2,
             )
             values = list(dict_values)
-        self._values: List[TParamValue] = self._cast_values(values)
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._is_ordered = (
+
+        self._is_ordered: bool = (
             is_ordered
             if is_ordered is not None
             else self._get_default_bool_and_warn(param_string="is_ordered")
         )
         # sort_values defaults to True if the parameter is not a string
         self._sort_values: bool = (
             sort_values
             if sort_values is not None
             else self._get_default_bool_and_warn(param_string="sort_values")
         )
         if self.sort_values:
-            # pyre-ignore[6]: values/self._values expects List[Union[None, bool, float,
-            # int, str]] but sorted() takes/returns
-            # List[Variable[_typeshed.SupportsLessThanT (bound to
-            # _typeshed.SupportsLessThan)]]
-            self._values = self._cast_values(sorted(values))
+            values = cast(List[TParamValue], sorted([not_none(v) for v in values]))
+        self._values: List[TParamValue] = self._cast_values(values)
+
         if dependents:
             for value in dependents:
                 if value not in self.values:
                     raise UserInputError(
                         f"Value {value} in `dependents` "
                         f"argument is not among the parameter values: {self.values}."
                     )
@@ -592,15 +600,17 @@
     def _get_default_bool_and_warn(self, param_string: str) -> bool:
         default_bool = self._parameter_type != ParameterType.STRING
         warn(
             f'`{param_string}` is not specified for `ChoiceParameter` "{self._name}". '
             f"Defaulting to `{default_bool}` for parameters of `ParameterType` "
             f"{self.parameter_type.name}. To override this behavior (or avoid this "
             f"warning), specify `{param_string}` during `ChoiceParameter` "
-            "construction."
+            "construction.",
+            AxParameterWarning,
+            stacklevel=3,
         )
         return default_bool
 
     def cardinality(self) -> float:
         return len(self.values)
 
     @property
@@ -708,15 +718,15 @@
 
     def __init__(
         self,
         name: str,
         parameter_type: ParameterType,
         value: TParamValue,
         is_fidelity: bool = False,
-        target_value: Optional[TParamValue] = None,
+        target_value: TParamValue = None,
         dependents: Optional[Dict[TParamValue, List[str]]] = None,
     ) -> None:
         """Initialize FixedParameter
 
         Args:
             name: Name of the parameter.
             parameter_type: Enum indicating the type of parameter
@@ -731,19 +741,17 @@
             raise UserInputError(
                 "`target_value` should not be None for the fidelity parameter: "
                 "{}".format(name)
             )
 
         self._name = name
         self._parameter_type = parameter_type
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._value = self.cast(value)
+        self._value: TParamValue = self.cast(value)
         self._is_fidelity = is_fidelity
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._target_value = self.cast(target_value)
+        self._target_value: TParamValue = self.cast(target_value)
         # NOTE: We don't need to check that dependent parameters actually exist as
         # that is done in `HierarchicalSearchSpace` constructor.
         if dependents:
             if len(dependents) > 1 or next(iter(dependents.keys())) != self.value:
                 raise UserInputError(
                     "The only expected key in `dependents` for fixed parameter "
                     f"{self.name}: {self.value}; got: {dependents}."
```

### Comparing `ax-platform-0.3.7/ax/core/parameter_constraint.py` & `ax-platform-0.4.0/ax/core/parameter_constraint.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,19 +17,18 @@
 
 class ParameterConstraint(SortableBase):
     """Base class for linear parameter constraints.
 
     Constraints are expressed using a map from parameter name to weight
     followed by a bound.
 
-    The constraint is satisfied if w * v <= b where:
+    The constraint is satisfied if sum_i(w_i * v_i) <= b where:
         w is the vector of parameter weights.
         v is a vector of parameter values.
         b is the specified bound.
-        * is the dot product operator.
     """
 
     def __init__(self, constraint_dict: Dict[str, float], bound: float) -> None:
         """Initialize ParameterConstraint
 
         Args:
             constraint_dict: Map from parameter name to weight.
```

### Comparing `ax-platform-0.3.7/ax/core/parameter_distribution.py` & `ax-platform-0.4.0/ax/core/parameter_distribution.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from copy import deepcopy
 from importlib import import_module
 from typing import Any, Dict, List, Optional, TYPE_CHECKING
 
 from ax.exceptions.core import UserInputError
```

### Comparing `ax-platform-0.3.7/ax/core/risk_measures.py` & `ax-platform-0.4.0/ax/core/risk_measures.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from copy import deepcopy
 from typing import Dict, List, Union
 
 from ax.utils.common.base import SortableBase
 from ax.utils.common.equality import equality_typechecker
```

### Comparing `ax-platform-0.3.7/ax/core/runner.py` & `ax-platform-0.4.0/ax/core/runner.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from abc import ABC, abstractmethod
 from typing import Any, Dict, Iterable, List, Optional, Set, TYPE_CHECKING
 
 from ax.utils.common.base import Base
 from ax.utils.common.serialization import SerializationMixin
```

### Comparing `ax-platform-0.3.7/ax/core/search_space.py` & `ax-platform-0.4.0/ax/core/search_space.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 # pyre-strict
 
 from __future__ import annotations
 
+import math
 import warnings
 from dataclasses import dataclass, field
 from functools import reduce
 from logging import Logger
 from random import choice, uniform
 from typing import Callable, Dict, Hashable, List, Mapping, Optional, Set, Tuple, Union
 
@@ -30,19 +31,20 @@
 from ax.core.parameter_constraint import (
     OrderConstraint,
     ParameterConstraint,
     SumConstraint,
 )
 from ax.core.parameter_distribution import ParameterDistribution
 from ax.core.types import TParameterization
-from ax.exceptions.core import UnsupportedError, UserInputError
+from ax.exceptions.core import AxWarning, UnsupportedError, UserInputError
 from ax.utils.common.base import Base
 from ax.utils.common.constants import Keys
 from ax.utils.common.logger import get_logger
 from ax.utils.common.typeutils import not_none
+from scipy.special import expit, logit
 
 
 logger: Logger = get_logger(__name__)
 PARAMETER_DF_COLNAMES: Mapping[Hashable, str] = {
     "name": "Name",
     "type": "Type",
     "domain": "Domain",
@@ -234,16 +236,15 @@
                         f"{value} is not a valid value for "
                         f"parameter {self.parameters[name]}"
                     )
                 return False
 
         # parameter constraints only accept numeric parameters
         numerical_param_dict = {
-            # pyre-fixme[6]: Expected `typing.Union[...oat]` but got `unknown`.
-            name: float(value)
+            name: float(not_none(value))
             for name, value in parameterization.items()
             if self.parameters[name].is_numeric
         }
 
         for constraint in self._parameter_constraints:
             if not constraint.check(numerical_param_dict):
                 if raise_error:
@@ -493,63 +494,73 @@
 
         return obs_feats
 
     def flatten_observation_features(
         self,
         observation_features: core.observation.ObservationFeatures,
         inject_dummy_values_to_complete_flat_parameterization: bool = False,
+        use_random_dummy_values: bool = False,
     ) -> core.observation.ObservationFeatures:
         """Flatten observation features that were previously cast to the hierarchical
         structure of the given search space; return the newly flattened observation
         features. This method re-injects parameter values that were removed from
         observation features during casting (as they are saved in observation features
         metadata).
 
         Args:
             observation_features: Observation features corresponding to one point
                 to flatten.
             inject_dummy_values_to_complete_flat_parameterization: Whether to inject
-                values for parameters that are not in the parameterization if they
-                are not recorded in the observation features' metadata (this can
-                happen if e.g. the point wasn't generated by Ax but attached manually).
+                values for parameters that are not in the parameterization.
+                This will be used to complete the parameterization after re-injecting
+                the parameters that are recorded in the metadata (for parameters
+                that were generated by Ax).
+            use_random_dummy_values: Whether to use random values for missing
+                parameters. If False, we set the values to the middle of
+                the corresponding parameter domain range.
         """
         obs_feats = observation_features
-        if obs_feats.metadata and Keys.FULL_PARAMETERIZATION in obs_feats.metadata:
-            # NOTE: We could just use the full parameterization as stored;
-            # opting for a safer option of only injecting parameters that were
-            # removed, but not altering those that are present if they have different
-            # values in full parameterization as stored in metadata.
+        has_full_parameterization = Keys.FULL_PARAMETERIZATION in (
+            obs_feats.metadata or {}
+        )
+
+        if obs_feats.parameters == {} and not has_full_parameterization:
+            # Return as is if the observation feature does not have any parameters.
+            return obs_feats
+
+        if has_full_parameterization:
+            # If full parameterization is recorded, use it to fill in missing values.
             full_parameterization = not_none(obs_feats.metadata)[
                 Keys.FULL_PARAMETERIZATION
             ]
             obs_feats.parameters = {**full_parameterization, **obs_feats.parameters}
-            return obs_feats
 
-        if obs_feats.parameters == {}:
-            # Return as is if the observation feature does not have any parameters.
-            return obs_feats
-
-        if inject_dummy_values_to_complete_flat_parameterization:
-            # To cast a parameterization to flattened search space, inject dummy values
-            # for parameters that were not present in it.
-            dummy_values_to_inject = (
-                self._gen_dummy_values_to_complete_flat_parameterization(
-                    observation_features=obs_feats
+        if len(obs_feats.parameters) < len(self.parameters):
+            if inject_dummy_values_to_complete_flat_parameterization:
+                # Inject dummy values for parameters missing from the parameterization.
+                dummy_values_to_inject = (
+                    self._gen_dummy_values_to_complete_flat_parameterization(
+                        observation_features=obs_feats,
+                        use_random_dummy_values=use_random_dummy_values,
+                    )
+                )
+                obs_feats.parameters = {
+                    **dummy_values_to_inject,
+                    **obs_feats.parameters,
+                }
+            else:
+                # The parameterization is still incomplete.
+                warnings.warn(
+                    f"Cannot flatten observation features {obs_feats} as full "
+                    "parameterization is not recorded in metadata and "
+                    "`inject_dummy_values_to_complete_flat_parameterization` is "
+                    "set to False.",
+                    AxWarning,
+                    stacklevel=2,
                 )
-            )
-            obs_feats.parameters = {**dummy_values_to_inject, **obs_feats.parameters}
-            return obs_feats
-
-        # We did not have the full parameterization stored, so we either return the
-        # observation features as given without change, or we inject dummy values if
-        # that behavior was requested via the opt-in flag.
-        warnings.warn(
-            f"Cannot flatten observation features {obs_feats} as full "
-            "parameterization is not recorded in metadata."
-        )
         return obs_feats
 
     def check_membership(
         self,
         parameterization: TParameterization,
         raise_error: bool = False,
         check_all_parameters_present: bool = True,
@@ -660,37 +671,42 @@
         check_all_parameters_present: bool = True,
     ) -> TParameterization:
         """Cast parameterization (of an arm, observation features, etc.) to the
         hierarchical structure of this search space.
 
         Args:
             parameters: Parameterization to cast to hierarchical structure.
-            check_all_parameters_present: Whether to raise an error if a paramete
-                 that is expected to be present (according to values of other
-                 parameters and the hierarchical structure of the search space)
-                 is not specified.
+            check_all_parameters_present: Whether to raise an error if a parameter
+                that is expected to be present (according to values of other
+                parameters and the hierarchical structure of the search space)
+                is not specified. When this is False, if a parameter is missing,
+                its dependents will not be included in the returned parameterization.
         """
         error_msg_prefix: str = (
             f"Parameterization {parameters} violates the hierarchical structure "
             f"of the search space: {self.hierarchical_structure_str}."
         )
 
         def _find_applicable_parameters(root: Parameter) -> Set[str]:
             applicable = {root.name}
             if check_all_parameters_present and root.name not in parameters:
                 raise RuntimeError(
                     error_msg_prefix
                     + f"Parameter '{root.name}' not in parameterization to cast."
                 )
 
-            if not root.is_hierarchical:
+            # Return if the root parameter is not hierarchical or if it is not
+            # in the parameterization to cast.
+            if not root.is_hierarchical or root.name not in parameters:
                 return applicable
 
+            # Find the dependents of the current root parameter.
+            root_val = parameters[root.name]
             for val, deps in root.dependents.items():
-                if parameters[root.name] == val:
+                if root_val == val:
                     for dep in deps:
                         applicable.update(_find_applicable_parameters(root=self[dep]))
 
             return applicable
 
         applicable_paramers = _find_applicable_parameters(root=self.root)
         if check_all_parameters_present and not all(
@@ -787,27 +803,46 @@
                 f"Parameters {self._all_parameter_names - visited} are not reachable "
                 "from the root. Please check that the hierachical search space provided"
                 " is represented as a valid tree with a single root."
             )
         logger.debug(f"Visited all parameters in the tree: {visited}.")
 
     def _gen_dummy_values_to_complete_flat_parameterization(
-        self, observation_features: core.observation.ObservationFeatures
+        self,
+        observation_features: core.observation.ObservationFeatures,
+        use_random_dummy_values: bool,
     ) -> Dict[str, TParamValue]:
         dummy_values_to_inject = {}
         for param_name, param in self.parameters.items():
             if param_name in observation_features.parameters:
                 continue
             if isinstance(param, FixedParameter):
                 dummy_values_to_inject[param_name] = param.value
             elif isinstance(param, ChoiceParameter):
-                dummy_values_to_inject[param_name] = choice(param.values)
+                if use_random_dummy_values:
+                    dummy_value = choice(param.values)
+                else:
+                    dummy_value = param.values[len(param.values) // 2]
+                dummy_values_to_inject[param_name] = dummy_value
             elif isinstance(param, RangeParameter):
-                val = uniform(param.lower, param.upper)
+                lower, upper = float(param.lower), float(param.upper)
+                if use_random_dummy_values:
+                    val = uniform(lower, upper)
+                elif param.log_scale:
+                    log_lower, log_upper = math.log10(lower), math.log10(upper)
+                    log_mid = (log_upper + log_lower) / 2.0
+                    val = math.pow(10, log_mid)
+                elif param.logit_scale:
+                    logit_lower, logit_upper = logit(lower).item(), logit(upper).item()
+                    logit_mid = (logit_upper + logit_lower) / 2.0
+                    val = expit(logit_mid).item()
+                else:
+                    val = (upper + lower) / 2.0
                 if param.parameter_type is ParameterType.INT:
+                    # This makes the distribution uniform after casting to int.
                     val += 0.5
                 dummy_values_to_inject[param_name] = param.cast(val)
             else:
                 raise NotImplementedError(
                     f"Unhandled parameter type on parameter {param}."
                 )
         return dummy_values_to_inject
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_arm.py` & `ax-platform-0.4.0/ax/core/tests/test_arm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.arm import Arm
 from ax.utils.common.testutils import TestCase
 
 
 class ArmTest(TestCase):
-    def setUp(self) -> None:
-        pass
-
     def test_Init(self) -> None:
         arm = Arm(parameters={"y": 0.25, "x": 0.75, "z": 75})
         self.assertEqual(str(arm), "Arm(parameters={'y': 0.25, 'x': 0.75, 'z': 75})")
 
         arm = Arm(parameters={"y": 0.25, "x": 0.75, "z": 75}, name="status_quo")
         self.assertEqual(
             str(arm),
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_batch_trial.py` & `ax-platform-0.4.0/ax/core/tests/test_batch_trial.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from time import sleep
 from unittest import mock
 from unittest.mock import patch, PropertyMock
 
 import numpy as np
 from ax.core.arm import Arm
 from ax.core.base_trial import TrialStatus
 from ax.core.batch_trial import BatchTrial, GeneratorRunStruct
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun, GeneratorRunType
 from ax.core.parameter import FixedParameter, ParameterType
 from ax.core.search_space import SearchSpace
+from ax.exceptions.core import UnsupportedError
 from ax.runners.synthetic import SyntheticRunner
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast
 from ax.utils.testing.core_stubs import (
     get_abandoned_arm,
     get_arm,
     get_arm_weights1,
@@ -41,14 +44,24 @@
         weights = get_weights()
         self.status_quo = arms[0]
         self.sq_weight = weights[0]
         self.arms = arms[1:]
         self.weights = weights[1:]
         self.batch.add_arms_and_weights(arms=self.arms, weights=self.weights)
 
+    def test__validate_can_attach_data(self) -> None:
+        self.batch.mark_running(no_runner_required=True)
+        self.batch.mark_completed()
+
+        expected_msg = (
+            "Trial 0 already has status 'COMPLETED', so data cannot be attached."
+        )
+        with self.assertRaisesRegex(UnsupportedError, expected_msg):
+            self.batch._validate_can_attach_data()
+
     def test_Eq(self) -> None:
         new_batch_trial = self.experiment.new_batch_trial()
         self.assertNotEqual(self.batch, new_batch_trial)
 
         abandoned_arm = get_abandoned_arm()
         abandoned_arm_2 = get_abandoned_arm()
         self.assertEqual(abandoned_arm, abandoned_arm_2)
@@ -414,14 +427,16 @@
         new_experiment.status_quo = None
         batch.clone_to(new_experiment)
         new_batch_trial_1 = checked_cast(BatchTrial, new_experiment.trials[0])
 
         self.assertEqual(new_batch_trial_0.index, 1)
         # Set index to original trial's value for equality check.
         new_batch_trial_0._index = batch.index
+        new_batch_trial_0._time_created = batch._time_created
+        new_batch_trial_1._time_created = batch._time_created
         self.assertEqual(new_batch_trial_0, batch)
         self.assertEqual(new_batch_trial_1, batch)
 
         # make sure modifying the cloned batch trial does not affect original one
         new_batch_trial_1.add_arm(
             Arm(name="new_arm", parameters={"w": 2.6, "x": 2, "y": "baz", "z": True})
         )
@@ -429,18 +444,20 @@
         self.assertEqual(len(batch.arms), 2)
 
         # cloning a trial that has status quo arm
         status_quo = Arm(
             name="status_quo", parameters={"w": 0.0, "x": 1, "y": "foo", "z": True}
         )
         batch.set_status_quo_and_optimize_power(status_quo)
+        batch.mark_running(no_runner_required=True)
         new_batch_trial = batch.clone_to()
         self.assertEqual(new_batch_trial.index, 2)
-        # Set index to original trial's value for equality check.
+        # Set index & time_created to original trial's value for equality check.
         new_batch_trial._index = batch.index
+        new_batch_trial._time_created = batch._time_created
         self.assertEqual(new_batch_trial, batch)
 
     def test_Runner(self) -> None:
         # Verify BatchTrial without runner will fail
         with self.assertRaises(ValueError):
             self.batch.run()
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_data.py` & `ax-platform-0.4.0/ax/core/tests/test_data.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import random
 
 import pandas as pd
 from ax.core.data import (
     clone_without_metrics,
     custom_data_class,
     Data,
@@ -15,14 +17,15 @@
 )
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.timeutils import current_timestamp_in_millis
 
 
 class DataTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.df_hash = "3dd7ab8c67942d43c78ea4af05bbb1c4"
         self.df = pd.DataFrame(
             [
                 {
                     "arm_name": "0_0",
                     "mean": 2.0,
                     "sem": 0.2,
@@ -284,14 +287,18 @@
                             "year": "2018-09-20",
                         }
                     ]
                 )
             )
             Data.from_multiple_data([data_elt_A, data_elt_B])
 
+    def test_from_multiple_with_generator(self) -> None:
+        data = Data.from_multiple_data(Data(df=self.df) for _ in range(2))
+        self.assertEqual(len(data.df), 2 * len(self.df))
+
     def test_GetFilteredResults(self) -> None:
         data = Data(df=self.df)
         # pyre-fixme[6]: For 1st param expected `Dict[str, typing.Any]` but got `str`.
         # pyre-fixme[6]: For 2nd param expected `Dict[str, typing.Any]` but got `str`.
         actual_filtered = data.get_filtered_results(arm_name="0_0", metric_name="a")
         # Create new Data to replicate timestamp casting.
         expected_filtered = Data(
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_experiment.py` & `ax-platform-0.4.0/ax/core/tests/test_experiment.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
+from collections import OrderedDict
 from typing import Dict, List, Type
 from unittest.mock import MagicMock, patch
 
 import pandas as pd
 from ax.core import BatchTrial, Trial
 from ax.core.arm import Arm
 from ax.core.base_trial import TrialStatus
@@ -34,14 +37,15 @@
 from ax.exceptions.core import UnsupportedError
 from ax.metrics.branin import BraninMetric
 from ax.modelbridge.registry import Models
 from ax.runners.synthetic import SyntheticRunner
 from ax.service.ax_client import AxClient
 from ax.service.utils.instantiation import ObjectiveProperties
 from ax.utils.common.constants import EXPERIMENT_IS_TEST_WARNING, Keys
+from ax.utils.common.random import set_rng_seed
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast
 from ax.utils.testing.core_stubs import (
     get_arm,
     get_branin_arms,
     get_branin_experiment,
     get_branin_experiment_with_multi_objective,
@@ -66,25 +70,28 @@
 DUMMY_ABANDONED_REASON = "test abandoned reason"
 DUMMY_ARM_NAME = "test_arm_name"
 
 
 class TestMetric(Metric):
     """Shell metric class for testing."""
 
+    __test__ = False
+
     pass
 
 
 class SyntheticRunnerWithMetadataKeys(SyntheticRunner):
     @property
     def run_metadata_report_keys(self) -> List[str]:
         return [DUMMY_RUN_METADATA_KEY]
 
 
 class ExperimentTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_experiment()
 
     def _setupBraninExperiment(self, n: int) -> Experiment:
         exp = Experiment(
             name="test3",
             search_space=get_branin_search_space(),
             tracking_metrics=[BraninMetric(name="b", param_names=["x1", "x2"])],
@@ -1032,15 +1039,20 @@
 
         new_status_quo = Arm({"x1": 1.0, "x2": 1.0})
 
         cloned_experiment = experiment.clone_with(
             search_space=larger_search_space,
             status_quo=new_status_quo,
         )
+        self.assertEqual(cloned_experiment._data_by_trial, experiment._data_by_trial)
         self.assertEqual(len(cloned_experiment.trials), 2)
+        for trial_index in cloned_experiment.trials.keys():
+            cloned_trial = cloned_experiment.trials[trial_index]
+            original_trial = experiment.trials[trial_index]
+            self.assertEqual(cloned_trial.status, original_trial.status)
         x1 = checked_cast(
             RangeParameter, cloned_experiment.search_space.parameters["x1"]
         )
         self.assertEqual(x1.lower, -10.0)
         self.assertEqual(x1.upper, 10.0)
         x2 = checked_cast(
             ChoiceParameter, cloned_experiment.search_space.parameters["x2"]
@@ -1096,20 +1108,32 @@
             num_trials=5, num_fetches=3, num_complete=4
         )
         cloned_experiment = experiment.clone_with(
             search_space=larger_search_space,
             status_quo=new_status_quo,
         )
         new_data = cloned_experiment.lookup_data()
+        self.assertNotEqual(cloned_experiment._data_by_trial, experiment._data_by_trial)
         self.assertIsInstance(new_data, MapData)
+        expected_data_by_trial = {}
+        for trial_index in experiment.trials:
+            if original_trial_data := experiment._data_by_trial.get(trial_index, None):
+                expected_data_by_trial[trial_index] = OrderedDict(
+                    list(original_trial_data.items())[-1:]
+                )
+        self.assertEqual(cloned_experiment.data_by_trial, expected_data_by_trial)
 
         experiment = get_experiment()
         cloned_experiment = experiment.clone_with()
         self.assertEqual(cloned_experiment.name, "cloned_experiment_" + experiment.name)
         cloned_experiment._name = experiment.name
+
+        # the clone_experiment._time_created field is set as datetime.now().
+        # for it to be equal we need to update it to match experiment.
+        cloned_experiment._time_created = experiment._time_created
         self.assertEqual(cloned_experiment, experiment)
 
     def test_metric_summary_df(self) -> None:
         experiment = Experiment(
             name="test_experiment",
             search_space=SearchSpace(parameters=[]),
             optimization_config=MultiObjectiveOptimizationConfig(
@@ -1193,14 +1217,15 @@
             ordered=True,
         )
         pd.testing.assert_frame_equal(df, expected_df)
 
 
 class ExperimentWithMapDataTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_experiment_with_map_data_type()
 
     def _setupBraninExperiment(self, n: int, incremental: bool = False) -> Experiment:
         exp = get_branin_experiment_with_timestamp_map_metric()
         batch = exp.new_batch_trial()
         batch.add_arms_and_weights(arms=get_branin_arms(n=n, seed=0))
         batch.run()
@@ -1382,18 +1407,22 @@
             old_df.drop(["arm_name", "trial_index"], axis=1),
             new_df.drop(["arm_name", "trial_index"], axis=1),
         )
 
     @fast_botorch_optimize
     def test_batch_with_multiple_generator_runs(self) -> None:
         exp = get_branin_experiment()
-        sobol = Models.SOBOL(experiment=exp, search_space=exp.search_space)
+        # set seed to avoid transient errors caused by duplicate arms,
+        # which leads to fewer arms in the trial than expected.
+        seed = 0
+        sobol = Models.SOBOL(experiment=exp, search_space=exp.search_space, seed=seed)
         exp.new_batch_trial(generator_runs=[sobol.gen(n=7)]).run().complete()
 
         data = exp.fetch_data()
+        set_rng_seed(seed)
         gp = Models.BOTORCH_MODULAR(
             experiment=exp, search_space=exp.search_space, data=data
         )
         ts = Models.EMPIRICAL_BAYES_THOMPSON(
             experiment=exp, search_space=exp.search_space, data=data
         )
         exp.new_batch_trial(generator_runs=[gp.gen(n=3), ts.gen(n=1)]).run().complete()
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_formatting_utils.py` & `ax-platform-0.4.0/ax/core/tests/test_formatting_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.formatting_utils import raw_data_to_evaluation
 from ax.exceptions.core import UserInputError
 from ax.utils.common.testutils import TestCase
 
 
 class TestRawDataToEvaluation(TestCase):
     def test_raw_data_is_not_dict_of_dicts(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_generator_run.py` & `ax-platform-0.4.0/ax/core/tests/test_generator_run.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.arm import Arm
 from ax.core.generator_run import GeneratorRun
 from ax.exceptions.core import UnsupportedError
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import (
     get_arms,
     get_model_predictions,
@@ -19,14 +21,15 @@
 
 GENERATOR_RUN_STR = "GeneratorRun(3 arms, total weight 3.0)"
 GENERATOR_RUN_STR_PLUS_1 = "GeneratorRun(3 arms, total weight 4.0)"
 
 
 class GeneratorRunTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.model_predictions = get_model_predictions()
         self.optimization_config = get_optimization_config()
         self.search_space = get_search_space()
 
         self.arms = get_arms()
         self.weights = [2, 1, 1]
         self.unweighted_run = GeneratorRun(
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_map_data.py` & `ax-platform-0.4.0/ax/core/tests/test_map_data.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,21 +1,24 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import pandas as pd
 from ax.core.data import Data
 from ax.core.map_data import MapData, MapKeyInfo
 from ax.utils.common.testutils import TestCase
 
 
 class MapDataTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.df = pd.DataFrame(
             [
                 {
                     "arm_name": "0_0",
                     "epoch": 0,
                     "mean": 2.0,
                     "sem": 0.2,
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_map_metric.py` & `ax-platform-0.4.0/ax/core/tests/test_map_metric.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.map_metric import MapMetric
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_map_data
 
 
 METRIC_STRING = "MapMetric('m1')"
 
 
 class MapMetricTest(TestCase):
-    def setUp(self) -> None:
-        pass
-
     def test_Init(self) -> None:
         metric = MapMetric(name="m1", lower_is_better=False)
         self.assertEqual(str(metric), METRIC_STRING)
 
     def test_Eq(self) -> None:
         metric1 = MapMetric(name="m1", lower_is_better=False)
         metric2 = MapMetric(name="m1", lower_is_better=False)
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_metric.py` & `ax-platform-0.4.0/ax/core/tests/test_metric.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.metric import Metric, MetricFetchE
 from ax.utils.common.result import Err
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import (
     get_branin_metric,
     get_data,
     get_experiment,
     get_factorial_metric,
 )
 
 
 class TestMetric(Metric):
+    __test__ = False
+
     pass
 
 
 METRIC_STRING = "Metric('m1')"
 
 
 class MetricTest(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_multi_type_experiment.py` & `ax-platform-0.4.0/ax/core/tests/test_multi_type_experiment.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,24 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.metrics.branin import BraninMetric
 from ax.runners.synthetic import SyntheticRunner
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_arms, get_multi_type_experiment
 
 
 class MultiTypeExperimentTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_multi_type_experiment()
 
     def test_MTExperimentFlow(self) -> None:
         self.assertTrue(self.experiment.supports_trial_type("type1"))
         self.assertTrue(self.experiment.supports_trial_type("type2"))
         self.assertFalse(self.experiment.supports_trial_type(None))
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_objective.py` & `ax-platform-0.4.0/ax/core/tests/test_objective.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-import warnings
+# pyre-strict
 
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective, ScalarizedObjective
+from ax.exceptions.core import UserInputError
 from ax.utils.common.testutils import TestCase
 
 
 class ObjectiveTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.metrics = {
             "m1": Metric(name="m1"),
             "m2": Metric(name="m2", lower_is_better=True),
             "m3": Metric(name="m3", lower_is_better=False),
         }
         self.objectives = {
-            "o1": Objective(metric=self.metrics["m1"]),
+            "o1": Objective(metric=self.metrics["m1"], minimize=True),
             "o2": Objective(metric=self.metrics["m2"], minimize=True),
             "o3": Objective(metric=self.metrics["m3"], minimize=False),
         }
         self.objective = Objective(metric=self.metrics["m1"], minimize=False)
         self.multi_objective = MultiObjective(
             objectives=[
                 self.objectives["o1"],
@@ -32,85 +34,84 @@
             ]
         )
         self.scalarized_objective = ScalarizedObjective(
             metrics=[self.metrics["m1"], self.metrics["m2"]], minimize=True
         )
 
     def test_Init(self) -> None:
+        with self.assertRaisesRegex(UserInputError, "does not specify"):
+            Objective(metric=self.metrics["m1"]),
+        with self.assertRaisesRegex(
+            UserInputError, "doesn't match the specified optimization direction"
+        ):
+            Objective(metric=self.metrics["m2"], minimize=False)
         with self.assertRaises(ValueError):
             ScalarizedObjective(
                 metrics=[self.metrics["m1"], self.metrics["m2"]], weights=[1.0]
             )
         with self.assertRaisesRegex(
             ValueError,
             "Metric with name m2 specifies `lower_is_better` = "
             "True, which doesn't match the specified optimization direction.",
         ):
             # Should fail since m2 specifies lower_is_better=True
             ScalarizedObjective(
                 metrics=[self.metrics["m1"], self.metrics["m2"]],
                 minimize=False,
             )
-        warnings.resetwarnings()
-        warnings.simplefilter("always", append=True)
-        with warnings.catch_warnings(record=True) as ws:
-            Objective(metric=self.metrics["m1"])
-            self.assertTrue(any(issubclass(w.category, DeprecationWarning) for w in ws))
-            self.assertTrue(
-                any("Defaulting to `minimize=False`" in str(w.message) for w in ws)
-            )
-        with warnings.catch_warnings(record=True) as ws:
-            Objective(Metric(name="m4", lower_is_better=True), minimize=False)
-            self.assertTrue(any("Attempting to maximize" in str(w.message) for w in ws))
-        with warnings.catch_warnings(record=True) as ws:
-            Objective(Metric(name="m4", lower_is_better=False), minimize=True)
-            self.assertTrue(any("Attempting to minimize" in str(w.message) for w in ws))
         self.assertEqual(
             self.objective.get_unconstrainable_metrics(), [self.metrics["m1"]]
         )
 
     def test_MultiObjective(self) -> None:
         with self.assertRaises(NotImplementedError):
             # pyre-fixme[7]: Expected `None` but got `Metric`.
             return self.multi_objective.metric
 
         self.assertEqual(self.multi_objective.metrics, list(self.metrics.values()))
         minimizes = [obj.minimize for obj in self.multi_objective.objectives]
-        self.assertEqual(minimizes, [False, True, False])
+        self.assertEqual(minimizes, [True, True, False])
         weights = [mw[1] for mw in self.multi_objective.objective_weights]
         self.assertEqual(weights, [1.0, 1.0, 1.0])
         self.assertEqual(self.multi_objective.clone(), self.multi_objective)
         self.assertEqual(
             str(self.multi_objective),
             (
                 "MultiObjective(objectives="
-                '[Objective(metric_name="m1", minimize=False), '
+                '[Objective(metric_name="m1", minimize=True), '
                 'Objective(metric_name="m2", minimize=True), '
                 'Objective(metric_name="m3", minimize=False)])'
             ),
         )
         self.assertEqual(
             self.multi_objective.get_unconstrainable_metrics(),
             [self.metrics["m1"], self.metrics["m2"], self.metrics["m3"]],
         )
 
     def test_MultiObjectiveBackwardsCompatibility(self) -> None:
-        multi_objective = MultiObjective(
-            metrics=[self.metrics["m1"], self.metrics["m2"], self.metrics["m3"]]
-        )
+        metrics = [
+            Metric(name="m1", lower_is_better=False),
+            self.metrics["m2"],
+            self.metrics["m3"],
+        ]
+        multi_objective = MultiObjective(metrics=metrics)
         minimizes = [obj.minimize for obj in multi_objective.objectives]
-        self.assertEqual(multi_objective.metrics, list(self.metrics.values()))
+        self.assertEqual(multi_objective.metrics, metrics)
         self.assertEqual(minimizes, [False, True, False])
 
         multi_objective_min = MultiObjective(
-            metrics=[self.metrics["m1"], self.metrics["m2"], self.metrics["m3"]],
+            metrics=[
+                Metric(name="m1"),
+                Metric(name="m2"),
+                Metric(name="m3", lower_is_better=True),
+            ],
             minimize=True,
         )
         minimizes = [obj.minimize for obj in multi_objective_min.objectives]
-        self.assertEqual(minimizes, [True, False, True])
+        self.assertEqual(minimizes, [True, True, True])
 
     def test_ScalarizedObjective(self) -> None:
         with self.assertRaises(NotImplementedError):
             # pyre-fixme[7]: Expected `None` but got `Metric`.
             return self.scalarized_objective.metric
 
         self.assertEqual(
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_observation.py` & `ax-platform-0.4.0/ax/core/tests/test_observation.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from unittest.mock import Mock, PropertyMock
 
 import numpy as np
 import pandas as pd
 from ax.core.arm import Arm
 from ax.core.base_trial import TrialStatus
@@ -21,15 +23,17 @@
     ObservationFeatures,
     observations_from_data,
     observations_from_map_data,
     recombine_observations,
     separate_observations,
 )
 from ax.core.trial import Trial
+from ax.core.types import TParameterization
 from ax.utils.common.testutils import TestCase
+from ax.utils.common.typeutils import not_none
 
 
 class ObservationsTest(TestCase):
     def test_ObservationFeatures(self) -> None:
         t = np.datetime64("now")
         attrs = {
             "parameters": {"x": 0, "y": "a"},
@@ -656,14 +660,123 @@
             self.assertEqual(obs.data.metric_names, obsd_truth["metric_names"][i])
             self.assertTrue(np.array_equal(obs.data.means, obsd_truth["means"][i]))
             self.assertTrue(
                 np.array_equal(obs.data.covariance, obsd_truth["covariance"][i])
             )
             self.assertEqual(obs.arm_name, cname_truth[i])
 
+    def test_ObservationsFromDataWithDifferentTimesSingleTrial(self) -> None:
+        params0: TParameterization = {"x": 0, "y": "a"}
+        params1: TParameterization = {"x": 1, "y": "a"}
+        truth = [
+            {
+                "arm_name": "0_0",
+                "parameters": params0,
+                "mean": 2.0,
+                "sem": 2.0,
+                "trial_index": 0,
+                "metric_name": "a",
+                "start_time": "2024-03-20 08:45:00",
+                "end_time": "2024-03-20 08:47:00",
+            },
+            {
+                "arm_name": "0_0",
+                "parameters": params0,
+                "mean": 3.0,
+                "sem": 3.0,
+                "trial_index": 0,
+                "metric_name": "b",
+                "start_time": "2024-03-20 08:45:00",
+            },
+            {
+                "arm_name": "0_1",
+                "parameters": params1,
+                "mean": 4.0,
+                "sem": 4.0,
+                "trial_index": 0,
+                "metric_name": "a",
+                "start_time": "2024-03-20 08:43:00",
+                "end_time": "2024-03-20 08:46:00",
+            },
+            {
+                "arm_name": "0_1",
+                "parameters": params1,
+                "mean": 5.0,
+                "sem": 5.0,
+                "trial_index": 0,
+                "metric_name": "b",
+                "start_time": "2024-03-20 08:45:00",
+                "end_time": "2024-03-20 08:46:00",
+            },
+        ]
+        arms_by_name = {
+            "0_0": Arm(name="0_0", parameters=params0),
+            "0_1": Arm(name="0_1", parameters=params1),
+        }
+        experiment = Mock()
+        experiment._trial_indices_by_status = {status: set() for status in TrialStatus}
+        trials = {
+            0: BatchTrial(experiment, GeneratorRun(arms=list(arms_by_name.values())))
+        }
+        type(experiment).arms_by_name = PropertyMock(return_value=arms_by_name)
+        type(experiment).trials = PropertyMock(return_value=trials)
+
+        df = pd.DataFrame(truth)[
+            [
+                "arm_name",
+                "trial_index",
+                "mean",
+                "sem",
+                "metric_name",
+                "start_time",
+                "end_time",
+            ]
+        ]
+        data = Data(df=df)
+        observations = observations_from_data(experiment, data)
+
+        self.assertEqual(len(observations), 2)
+        # Get them in the order we want for tests below
+        if observations[0].features.parameters["x"] == 1:
+            observations.reverse()
+
+        obs_truth = {
+            "arm_name": ["0_0", "0_1"],
+            "parameters": [{"x": 0, "y": "a"}, {"x": 1, "y": "a"}],
+            "metric_names": [["a", "b"], ["a", "b"]],
+            "means": [np.array([2.0, 3.0]), np.array([4.0, 5.0])],
+            "covariance": [np.diag([4.0, 9.0]), np.diag([16.0, 25.0])],
+        }
+
+        for i, obs in enumerate(observations):
+            self.assertEqual(obs.features.parameters, obs_truth["parameters"][i])
+            self.assertEqual(
+                obs.features.trial_index,
+                0,
+            )
+            self.assertEqual(obs.data.metric_names, obs_truth["metric_names"][i])
+            self.assertTrue(np.array_equal(obs.data.means, obs_truth["means"][i]))
+            self.assertTrue(
+                np.array_equal(obs.data.covariance, obs_truth["covariance"][i])
+            )
+            self.assertEqual(obs.arm_name, obs_truth["arm_name"][i])
+            self.assertEqual(obs.arm_name, obs_truth["arm_name"][i])
+            if i == 0:
+                self.assertEqual(
+                    not_none(obs.features.start_time).strftime("%Y-%m-%d %X"),
+                    "2024-03-20 08:45:00",
+                )
+                self.assertIsNone(obs.features.end_time)
+            else:
+                self.assertIsNone(obs.features.start_time)
+                self.assertEqual(
+                    not_none(obs.features.end_time).strftime("%Y-%m-%d %X"),
+                    "2024-03-20 08:46:00",
+                )
+
     def test_SeparateObservations(self) -> None:
         obs_arm_name = "0_0"
         obs = Observation(
             features=ObservationFeatures(parameters={"x": 20}),
             data=ObservationData(
                 means=np.array([1]), covariance=np.array([[2]]), metric_names=["a"]
             ),
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_optimization_config.py` & `ax-platform-0.4.0/ax/core/tests/test_optimization_config.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective, ScalarizedObjective
 from ax.core.optimization_config import (
     _NO_RISK_MEASURE,
     MultiObjectiveOptimizationConfig,
     OptimizationConfig,
 )
@@ -37,14 +39,15 @@
     "outcome_constraints=[OutcomeConstraint(m3 >= -0.25%), "
     "OutcomeConstraint(m3 <= 0.25%)], objective_thresholds=[])"
 )
 
 
 class OptimizationConfigTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.metrics = {
             "m1": Metric(name="m1"),
             "m2": Metric(name="m2"),
             "m3": Metric(name="m3"),
             "m4": Metric(name="m4"),
         }
         self.objective = Objective(metric=self.metrics["m1"], minimize=False)
@@ -261,25 +264,26 @@
             ),
             config3,
         )
 
 
 class MultiObjectiveOptimizationConfigTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.metrics = {
             "m1": Metric(name="m1", lower_is_better=True),
             "m2": Metric(name="m2", lower_is_better=False),
             "m3": Metric(name="m3", lower_is_better=False),
         }
         self.objectives = {
             "o1": Objective(metric=self.metrics["m1"]),
             "o2": Objective(metric=self.metrics["m2"], minimize=False),
             "o3": Objective(metric=self.metrics["m3"], minimize=False),
         }
-        self.objective = Objective(metric=self.metrics["m1"], minimize=False)
+        self.objective = Objective(metric=self.metrics["m1"], minimize=True)
         self.multi_objective = MultiObjective(
             objectives=[self.objectives["o1"], self.objectives["o2"]]
         )
         self.multi_objective_just_m2 = MultiObjective(
             objectives=[self.objectives["o2"]]
         )
         self.scalarized_objective = ScalarizedObjective(
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_outcome_constraint.py` & `ax-platform-0.4.0/ax/core/tests/test_outcome_constraint.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 from ax.core.metric import Metric
 from ax.core.outcome_constraint import (
     CONSTRAINT_WARNING_MESSAGE,
     LOWER_BOUND_MISMATCH,
     ObjectiveThreshold,
@@ -20,14 +22,15 @@
 
 
 OUTCOME_CONSTRAINT_PATH = "ax.core.outcome_constraint"
 
 
 class OutcomeConstraintTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.minimize_metric = Metric(name="bar", lower_is_better=True)
         self.maximize_metric = Metric(name="baz", lower_is_better=False)
         self.bound = 0
         simple_metric = Metric(name="foo")
         self.constraint = OutcomeConstraint(
             metric=simple_metric, op=ComparisonOp.GEQ, bound=self.bound
         )
@@ -99,14 +102,15 @@
         self.assertFalse(oc._validate_constraint()[0])
         oc = OutcomeConstraint(metric, bound=-3, relative=True, op=ComparisonOp.GEQ)
         self.assertFalse(oc._validate_constraint()[0])
 
 
 class ObjectiveThresholdTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.minimize_metric = Metric(name="bar", lower_is_better=True)
         self.maximize_metric = Metric(name="baz", lower_is_better=False)
         self.ambiguous_metric = Metric(name="buz")
         self.bound = 0
         self.threshold = ObjectiveThreshold(
             metric=self.maximize_metric, op=ComparisonOp.GEQ, bound=self.bound
         )
@@ -171,14 +175,15 @@
                 relative=False,
             ).relative
         )
 
 
 class ScalarizedOutcomeConstraintTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.metrics = [
             Metric(name="m1", lower_is_better=True),
             Metric(name="m2", lower_is_better=True),
             Metric(name="m3", lower_is_better=True),
         ]
         self.weights = [0.1, 0.3, 0.6]
         self.bound = 0
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_parameter.py` & `ax-platform-0.4.0/ax/core/tests/test_parameter.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+from typing import cast, List
+
 from ax.core.parameter import (
     _get_parameter_type,
     ChoiceParameter,
     EPS,
     FixedParameter,
     ParameterType,
     RangeParameter,
@@ -15,14 +19,15 @@
 from ax.exceptions.core import AxWarning, UserInputError
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import not_none
 
 
 class RangeParameterTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.param1 = RangeParameter(
             name="x",
             parameter_type=ParameterType.FLOAT,
             lower=1,
             upper=3,
             log_scale=True,
             digits=5,
@@ -203,14 +208,15 @@
                 "parameter_type": "int",
             },
         )
 
 
 class ChoiceParameterTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.param1 = ChoiceParameter(
             name="x", parameter_type=ParameterType.STRING, values=["foo", "bar", "baz"]
         )
         self.param1_repr = (
             "ChoiceParameter(name='x', parameter_type=STRING, "
             "values=['foo', 'bar', 'baz'], is_ordered=False, sort_values=False)"
         )
@@ -293,15 +299,15 @@
         )
         self.assertTrue(bool_param.is_ordered)
         int_param = ChoiceParameter(
             name="x", parameter_type=ParameterType.INT, values=[2, 1, 3]
         )
         self.assertTrue(int_param.is_ordered)
         self.assertListEqual(
-            int_param.values, sorted(int_param.values)  # pyre-fixme[6]
+            int_param.values, sorted(cast(List[int], int_param.values))
         )
         float_param = ChoiceParameter(
             name="x", parameter_type=ParameterType.FLOAT, values=[1.5, 2.5, 3.5]
         )
         self.assertTrue(float_param.is_ordered)
         string_param = ChoiceParameter(
             name="x", parameter_type=ParameterType.STRING, values=["foo", "bar", "baz"]
@@ -502,14 +508,15 @@
                 values=["foo", "bar", "foo"],
             )
         self.assertEqual(p.values, ["foo", "bar"])
 
 
 class FixedParameterTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.param1 = FixedParameter(
             name="x", parameter_type=ParameterType.BOOL, value=True
         )
         self.param1_repr = "FixedParameter(name='x', parameter_type=BOOL, value=True)"
         self.param2 = FixedParameter(
             name="y", parameter_type=ParameterType.STRING, value="foo"
         )
@@ -625,14 +632,15 @@
                 "parameter_type": "string",
             },
         )
 
 
 class ParameterEqualityTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.fixed_parameter = FixedParameter(
             name="x", parameter_type=ParameterType.BOOL, value=True
         )
         self.choice_parameter = ChoiceParameter(
             name="x", parameter_type=ParameterType.STRING, values=["foo", "bar", "baz"]
         )
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_parameter_constraint.py` & `ax-platform-0.4.0/ax/core/tests/test_parameter_constraint.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.parameter import (
     ChoiceParameter,
     FixedParameter,
     ParameterType,
     RangeParameter,
 )
 from ax.core.parameter_constraint import (
@@ -17,14 +19,15 @@
     SumConstraint,
 )
 from ax.utils.common.testutils import TestCase
 
 
 class ParameterConstraintTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.constraint = ParameterConstraint(
             constraint_dict={"x": 2.0, "y": -3.0}, bound=6.0
         )
         self.constraint_repr = "ParameterConstraint(2.0*x + -3.0*y <= 6.0)"
 
     def test_Eq(self) -> None:
         constraint1 = ParameterConstraint(
@@ -92,14 +95,15 @@
             constraint_dict={"y": -3.0, "x": 2.0}, bound=6.0
         )
         self.assertTrue(constraint1 < constraint2)
 
 
 class OrderConstraintTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.x = RangeParameter("x", ParameterType.INT, lower=0, upper=1)
         self.y = RangeParameter("y", ParameterType.INT, lower=0, upper=1)
         self.constraint = OrderConstraint(
             lower_parameter=self.x, upper_parameter=self.y
         )
         self.constraint_repr = "OrderConstraint(x <= y)"
 
@@ -147,14 +151,15 @@
         z = ChoiceParameter("z", ParameterType.STRING, ["a", "b", "c"])
         with self.assertRaises(ValueError):
             self.constraint = OrderConstraint(lower_parameter=self.x, upper_parameter=z)
 
 
 class SumConstraintTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.x = RangeParameter("x", ParameterType.INT, lower=-5, upper=5)
         self.y = RangeParameter("y", ParameterType.INT, lower=-5, upper=5)
         self.constraint1 = SumConstraint(
             parameters=[self.x, self.y], is_upper_bound=True, bound=5
         )
         self.constraint2 = SumConstraint(
             parameters=[self.x, self.y], is_upper_bound=False, bound=-5
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_parameter_distribution.py` & `ax-platform-0.4.0/ax/core/tests/test_parameter_distribution.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.parameter_distribution import ParameterDistribution
 from ax.core.search_space import RobustSearchSpace
 from ax.exceptions.core import UserInputError
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 from scipy.stats._continuous_distns import norm_gen
 from scipy.stats._distn_infrastructure import rv_frozen
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_risk_measures.py` & `ax-platform-0.4.0/ax/core/tests/test_risk_measures.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.risk_measures import RiskMeasure
 from ax.utils.common.testutils import TestCase
 
 
 class TestRiskMeasure(TestCase):
     def test_risk_measure(self) -> None:
         rm = RiskMeasure(
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_runner.py` & `ax-platform-0.4.0/ax/core/tests/test_runner.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 from ax.core.base_trial import BaseTrial
 from ax.core.runner import Runner
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_batch_trial, get_trial
 
@@ -16,14 +18,15 @@
     # pyre-fixme[3]: Return type must be annotated.
     def run(self, trial: BaseTrial):
         return {"metadatum": f"value_for_trial_{trial.index}"}
 
 
 class RunnerTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.dummy_runner = DummyRunner()
         self.trials = [get_trial(), get_batch_trial()]
 
     def test_base_runner_staging_required(self) -> None:
         self.assertFalse(self.dummy_runner.staging_required)
 
     def test_base_runner_stop(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_search_space.py` & `ax-platform-0.4.0/ax/core/tests/test_search_space.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 import warnings
 from random import choice
 from typing import cast, List
 from unittest import mock
 
 import pandas as pd
-
 from ax.core.arm import Arm
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import (
     ChoiceParameter,
     FixedParameter,
     Parameter,
     ParameterType,
@@ -30,17 +31,19 @@
 from ax.core.search_space import (
     HierarchicalSearchSpace,
     RobustSearchSpace,
     RobustSearchSpaceDigest,
     SearchSpace,
     SearchSpaceDigest,
 )
+from ax.core.types import TParameterization
 from ax.exceptions.core import UnsupportedError, UserInputError
 from ax.utils.common.constants import Keys
 from ax.utils.common.testutils import TestCase
+from ax.utils.common.typeutils import checked_cast
 from ax.utils.testing.core_stubs import (
     get_hierarchical_search_space,
     get_l2_reg_weight_parameter,
     get_lr_parameter,
     get_model_parameter,
     get_num_boost_rounds_parameter,
     get_parameter_constraint,
@@ -49,14 +52,15 @@
 TOTAL_PARAMS = 6
 TUNABLE_PARAMS = 4
 RANGE_PARAMS = 3
 
 
 class SearchSpaceTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.a = RangeParameter(
             name="a", parameter_type=ParameterType.FLOAT, lower=0.5, upper=5.5
         )
         self.b = RangeParameter(
             name="b", parameter_type=ParameterType.INT, lower=2, upper=10
         )
         self.c = ChoiceParameter(
@@ -487,14 +491,15 @@
             }
         )
         pd.testing.assert_frame_equal(df, expected_df)
 
 
 class SearchSpaceDigestTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.kwargs = {
             "feature_names": ["a", "b", "c"],
             "bounds": [(0.0, 1.0), (0, 2), (0, 4)],
             "ordinal_features": [1],
             "categorical_features": [2],
             "discrete_choices": {1: [0, 1, 2], 2: [0, 0.25, 4.0]},
             "task_features": [3],
@@ -521,14 +526,15 @@
             ssd = SearchSpaceDigest(
                 **{k: v for k, v in self.kwargs.items() if k != arg}
             )
 
 
 class RobustSearchSpaceDigestTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.kwargs = {
             "sample_param_perturbations": lambda: 1,
             "sample_environmental": lambda: 2,
             "environmental_variables": ["a"],
             "multiplicative": False,
         }
 
@@ -544,14 +550,15 @@
             rssd = RobustSearchSpaceDigest(
                 **{k: v for k, v in self.kwargs.items() if k != arg}
             )
 
 
 class HierarchicalSearchSpaceTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.model_parameter = get_model_parameter()
         self.lr_parameter = get_lr_parameter()
         self.l2_reg_weight_parameter = get_l2_reg_weight_parameter()
         self.num_boost_rounds_parameter = get_num_boost_rounds_parameter()
         self.hss_1 = get_hierarchical_search_space()
         self.use_linear_parameter = ChoiceParameter(
             name="use_linear",  # Contrived!
@@ -615,21 +622,20 @@
             parameters={
                 "model": "XGBoost",
                 "learning_rate": 0.01,
                 "l2_reg_weight": 0.0001,
                 "num_boost_rounds": 12,
             }
         )
-        self.hss_1_arm_missing_param = Arm(
-            parameters={
-                "model": "Linear",
-                "l2_reg_weight": 0.0001,
-                "num_boost_rounds": 12,
-            }
-        )
+        self.hss_1_missing_params: TParameterization = {
+            "model": "Linear",
+            "l2_reg_weight": 0.0001,
+            "num_boost_rounds": 12,
+        }
+        self.hss_1_arm_missing_param = Arm(parameters=self.hss_1_missing_params)
         self.hss_1_arm_1_cast = Arm(
             parameters={
                 "model": "Linear",
                 "learning_rate": 0.01,
                 "l2_reg_weight": 0.0001,
             }
         )
@@ -753,14 +759,15 @@
         )
         self.assertTrue(
             str(self.hss_with_constraints).startswith("HierarchicalSearchSpace")
         )
         self.assertTrue(str(flattened_hss_with_constraints).startswith("SearchSpace"))
 
     def test_cast_arm(self) -> None:
+        # This uses _cast_parameterization with check_all_parameters_present=True.
         self.assertEqual(  # Check one subtree.
             self.hss_1._cast_arm(arm=self.hss_1_arm_1_flat),
             self.hss_1_arm_1_cast,
         )
         self.assertEqual(  # Check other subtree.
             self.hss_1._cast_arm(arm=self.hss_1_arm_2_flat),
             self.hss_1_arm_2_cast,
@@ -769,14 +776,15 @@
             self.hss_1._cast_arm(arm=self.hss_1_arm_1_cast),
             self.hss_1_arm_1_cast,
         )
         with self.assertRaises(RuntimeError):
             self.hss_1._cast_arm(arm=self.hss_1_arm_missing_param)
 
     def test_cast_observation_features(self) -> None:
+        # This uses _cast_parameterization with check_all_parameters_present=False.
         # Ensure that during casting, full parameterization is saved
         # in metadata and actual parameterization is cast to HSS.
         hss_1_obs_feats_1 = ObservationFeatures.from_arm(arm=self.hss_1_arm_1_flat)
         hss_1_obs_feats_1_cast = self.hss_1.cast_observation_features(
             observation_features=hss_1_obs_feats_1
         )
         self.assertEqual(  # Check one subtree.
@@ -792,42 +800,86 @@
         # manipulated during casting).
         hss_1_obs_feats_1_cast.metadata = None
         self.assertEqual(
             hss_1_obs_feats_1_cast,
             ObservationFeatures.from_arm(arm=self.hss_1_arm_1_cast),
         )
 
+    def test_cast_parameterization(self) -> None:
+        # NOTE: This is also tested in test_cast_arm & test_cast_observation_features.
+        with self.assertRaisesRegex(RuntimeError, "not in parameterization to cast"):
+            self.hss_1._cast_parameterization(
+                parameters=self.hss_1_missing_params,
+                check_all_parameters_present=True,
+            )
+        # An active leaf param is missing, it'll get ignored. There's an inactive
+        # leaf param, that'll just get filtered out.
+        self.assertEqual(
+            self.hss_1._cast_parameterization(
+                parameters=self.hss_1_missing_params,
+                check_all_parameters_present=False,
+            ),
+            {"l2_reg_weight": 0.0001, "model": "Linear"},
+        )
+        # A hierarchical param is missing, all its dependents will be ignored.
+        # In this case, it is the root param, so we'll have empty parameterization.
+        self.assertEqual(
+            self.hss_1._cast_parameterization(
+                parameters={
+                    "l2_reg_weight": 0.0001,
+                    "num_boost_rounds": 12,
+                },
+                check_all_parameters_present=False,
+            ),
+            {},
+        )
+
     def test_flatten_observation_features(self) -> None:
         # Ensure that during casting, full parameterization is saved
         # in metadata and actual parameterization is cast to HSS; during
         # flattening, parameterization in metadata is used ot inject back
         # the parameters removed during casting.
         hss_1_obs_feats_1 = ObservationFeatures.from_arm(arm=self.hss_1_arm_1_flat)
         hss_1_obs_feats_1_cast = self.hss_1.cast_observation_features(
             observation_features=hss_1_obs_feats_1
         )
-        hss_1_obs_feats_1_flattened = self.hss_1.flatten_observation_features(
-            observation_features=hss_1_obs_feats_1_cast
-        )
-        self.assertEqual(  # Cast-flatten roundtrip.
-            hss_1_obs_feats_1.parameters,
-            hss_1_obs_feats_1_flattened.parameters,
-        )
-        self.assertEqual(  # Check that both cast and flattened have full params.
-            hss_1_obs_feats_1_cast.metadata.get(Keys.FULL_PARAMETERIZATION),
-            hss_1_obs_feats_1_flattened.metadata.get(Keys.FULL_PARAMETERIZATION),
-        )
+        for inject_dummy in (True, False):
+            hss_1_obs_feats_1_flattened = self.hss_1.flatten_observation_features(
+                observation_features=hss_1_obs_feats_1_cast,
+                inject_dummy_values_to_complete_flat_parameterization=inject_dummy,
+            )
+            self.assertEqual(  # Cast-flatten roundtrip.
+                hss_1_obs_feats_1.parameters,
+                hss_1_obs_feats_1_flattened.parameters,
+            )
+            self.assertEqual(  # Check that both cast and flattened have full params.
+                hss_1_obs_feats_1_cast.metadata.get(Keys.FULL_PARAMETERIZATION),
+                hss_1_obs_feats_1_flattened.metadata.get(Keys.FULL_PARAMETERIZATION),
+            )
         # Check that flattening observation features without metadata does nothing.
+        # Does not warn here since it already has all parameters.
         with warnings.catch_warnings(record=True) as ws:
             self.assertEqual(
                 self.hss_1.flatten_observation_features(
                     observation_features=hss_1_obs_feats_1
                 ),
                 hss_1_obs_feats_1,
             )
+        self.assertFalse(
+            any("Cannot flatten observation features" in str(w.message) for w in ws)
+        )
+        # This one warns since it is missing some parameters.
+        obs_ft_missing = ObservationFeatures.from_arm(arm=self.hss_1_arm_missing_param)
+        with warnings.catch_warnings(record=True) as ws:
+            self.assertEqual(
+                self.hss_1.flatten_observation_features(
+                    observation_features=obs_ft_missing
+                ),
+                obs_ft_missing,
+            )
         self.assertTrue(
             any("Cannot flatten observation features" in str(w.message) for w in ws)
         )
         # Check that no warning is raised if the observation feature doesn't
         # have parameterization (for trial-index only features).
         obs_ft = ObservationFeatures(parameters={}, trial_index=0)
         with warnings.catch_warnings(record=True) as ws:
@@ -836,27 +888,28 @@
                 obs_ft,
             )
         self.assertFalse(
             any("Cannot flatten observation features" in str(w.message) for w in ws)
         )
 
     @mock.patch(f"{HierarchicalSearchSpace.__module__}.uniform", return_value=0.6)
-    def test_flatten_observation_features_inject_dummy_parameter_values(
+    def test_flatten_observation_features_inject_dummy_parameter_values_with_random(
         self, mock_uniform: mock.MagicMock
     ) -> None:
         # Case 1: Linear arm
         hss_obs_feats = ObservationFeatures.from_arm(arm=self.hss_1_arm_1_cast)
         hss_obs_feats_flattened = self.hss_1.flatten_observation_features(
-            observation_features=hss_obs_feats
+            observation_features=hss_obs_feats, use_random_dummy_values=True
         )
         mock_uniform.assert_not_called()
         self.assertNotIn("num_boost_rounds", hss_obs_feats_flattened.parameters)
         flattened_with_dummies = self.hss_1.flatten_observation_features(
             observation_features=hss_obs_feats,
             inject_dummy_values_to_complete_flat_parameterization=True,
+            use_random_dummy_values=True,
         ).parameters
         mock_uniform.assert_called()
         self.assertIn("num_boost_rounds", flattened_with_dummies)
         self.assertEqual(
             flattened_with_dummies["num_boost_rounds"],
             1,  # int(0.6 + 0.5) = floor(0.6 + 0.5) = 1
         )
@@ -865,23 +918,24 @@
             int,
         )
 
         # Case 2: XGBoost arm
         mock_uniform.reset_mock()
         hss_obs_feats = ObservationFeatures.from_arm(arm=self.hss_1_arm_2_cast)
         hss_obs_feats_flattened = self.hss_1.flatten_observation_features(
-            observation_features=hss_obs_feats
+            observation_features=hss_obs_feats, use_random_dummy_values=True
         )
         mock_uniform.assert_not_called()
         self.assertNotIn("learning_rate", hss_obs_feats_flattened.parameters)
         self.assertNotIn("l2_reg_weight", hss_obs_feats_flattened.parameters)
         flattened_with_dummies = (
             self.hss_1.flatten_observation_features(
                 observation_features=hss_obs_feats,
                 inject_dummy_values_to_complete_flat_parameterization=True,
+                use_random_dummy_values=True,
             )
         ).parameters
         mock_uniform.assert_called()
         self.assertIn("learning_rate", flattened_with_dummies)
         self.assertIn("l2_reg_weight", flattened_with_dummies)
         self.assertEqual(
             flattened_with_dummies["learning_rate"],
@@ -897,36 +951,105 @@
             f"{HierarchicalSearchSpace.__module__}.choice", wraps=choice
         ) as mock_choice:
             flattened_only_dummies = self.hss_2.flatten_observation_features(
                 observation_features=ObservationFeatures(
                     parameters={"num_boost_rounds": 12}
                 ),
                 inject_dummy_values_to_complete_flat_parameterization=True,
+                use_random_dummy_values=True,
             ).parameters
             self.assertEqual(
                 mock_choice.call_args_list,
                 [mock.call([False, True]), mock.call(["Linear", "XGBoost"])],
             )
         self.assertEqual(
             set(flattened_only_dummies.keys()), set(self.hss_2.parameters.keys())
         )
 
         # Case 4: test setting of fixed parameters
         flattened_only_dummies = self.hss_with_fixed.flatten_observation_features(
             observation_features=ObservationFeatures(parameters={"use_linear": True}),
             inject_dummy_values_to_complete_flat_parameterization=True,
+            use_random_dummy_values=True,
         ).parameters
         self.assertEqual(
             set(flattened_only_dummies.keys()),
             set(self.hss_with_fixed.parameters.keys()),
         )
 
+    def test_flatten_observation_features_inject_dummy_parameter_values_non_random(
+        self,
+    ) -> None:
+        """Check behavior with different parameter cases:
+        Choice, Int-Range, Log-Range & Logit-Range.
+
+        HSS2 has enough parameters that we can modify to test all cases.
+        - `use_linear` is bool-Choice
+        - `model` is string-Choice
+        - `learning_rate` is Range, can be made log-scale
+        - `l2_reg_weight` is Range, can be made logit-scale
+        - `num_boost_rounds` is Int-Range.
+        """
+        checked_cast(
+            RangeParameter, self.hss_2.parameters["learning_rate"]
+        )._log_scale = True
+        checked_cast(
+            RangeParameter, self.hss_2.parameters["l2_reg_weight"]
+        )._logit_scale = True
+        # This has no other parameters on it, so they should all be set to
+        # middle value in their respective domains.
+        obs_ft = ObservationFeatures(parameters={"use_linear": False})
+        flat_obs_ft = self.hss_2.flatten_observation_features(
+            observation_features=obs_ft,
+            inject_dummy_values_to_complete_flat_parameterization=True,
+            use_random_dummy_values=False,
+        )
+        expected_parameters = {
+            "use_linear": False,
+            "model": "XGBoost",  # has two values, so it will be index 1.
+            "learning_rate": 0.01,  # middle of 0.1 & 0.001 in log-scale.
+            "l2_reg_weight": 0.00010004052867652256,  # middle of range in logit-scale.
+            "num_boost_rounds": 15,  # mid-point of 10 & 20.
+        }
+        self.assertDictEqual(flat_obs_ft.parameters, expected_parameters)
+
+    def test_flatten_observation_features_full_and_dummy(self) -> None:
+        # Test flattening when both full features & inject dummy values
+        # are specified. This is relevant if the full parameterization
+        # is from some subset of the search space.
+        # Get an obs feat with hss_1 parameterization in the metadata.
+        hss_1_obs_feats_1 = ObservationFeatures.from_arm(arm=self.hss_1_arm_1_flat)
+        hss_1_obs_feats_1_cast = self.hss_1.cast_observation_features(
+            observation_features=hss_1_obs_feats_1
+        )
+        hss_1_flat_params = hss_1_obs_feats_1.parameters
+        # Flatten it using hss_2, which requires an additional parameter.
+        # This will work but miss a parameter.
+        self.assertEqual(
+            self.hss_2.flatten_observation_features(
+                observation_features=hss_1_obs_feats_1_cast,
+                inject_dummy_values_to_complete_flat_parameterization=False,
+            ).parameters,
+            hss_1_flat_params,
+        )
+        # Now try with inject dummy. This will add the mising param.
+        hss_2_flat = self.hss_2.flatten_observation_features(
+            observation_features=hss_1_obs_feats_1_cast,
+            inject_dummy_values_to_complete_flat_parameterization=True,
+        ).parameters
+        self.assertNotEqual(hss_2_flat, hss_1_flat_params)
+        self.assertEqual(
+            {k: hss_2_flat[k] for k in hss_1_flat_params}, hss_1_flat_params
+        )
+        self.assertEqual(set(hss_2_flat.keys()), set(self.hss_2.parameters.keys()))
+
 
 class TestRobustSearchSpace(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.a = RangeParameter(
             name="a", parameter_type=ParameterType.FLOAT, lower=0.5, upper=5.5
         )
         self.b = RangeParameter(
             name="b", parameter_type=ParameterType.INT, lower=2, upper=10
         )
         self.c = ChoiceParameter(
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_trial.py` & `ax-platform-0.4.0/ax/core/tests/test_trial.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import itertools
 from copy import deepcopy
 from unittest import mock
 from unittest.mock import Mock, patch
 
 import pandas as pd
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.data import Data
 from ax.core.generator_run import GeneratorRun, GeneratorRunType
 from ax.core.runner import Runner
-from ax.exceptions.core import UserInputError
+from ax.exceptions.core import UnsupportedError, UserInputError
 from ax.runners.synthetic import SyntheticRunner
 from ax.utils.common.result import Ok
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import (
     get_arms,
     get_branin_arms,
     get_experiment,
@@ -39,14 +41,15 @@
         ]
     )
 )
 
 
 class TrialTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.mock_supports_trial_type = mock.patch(
             f"{get_experiment.__module__}.Experiment.supports_trial_type",
             return_value=True,
         )
         self.mock_supports_trial_type.start()
         self.experiment = get_experiment()
         self.trial = self.experiment.new_trial(ttl_seconds=123, trial_type="foo")
@@ -54,14 +57,24 @@
         self.trial.update_stop_metadata(metadata={"bar": "baz"})
         self.arm = get_arms()[0]
         self.trial.add_arm(self.arm)
 
     def tearDown(self) -> None:
         self.mock_supports_trial_type.stop()
 
+    def test__validate_can_attach_data(self) -> None:
+        self.trial.mark_running(no_runner_required=True)
+        self.trial.mark_completed()
+
+        expected_msg = (
+            "Trial 0 has already been completed with data. To add more data to "
+        )
+        with self.assertRaisesRegex(UnsupportedError, expected_msg):
+            self.trial._validate_can_attach_data()
+
     def test_eq(self) -> None:
         new_trial = self.experiment.new_trial()
         self.assertNotEqual(self.trial, new_trial)
 
     def test_basic_properties(self) -> None:
         self.assertEqual(self.experiment, self.trial.experiment)
         self.assertEqual(self.trial.index, 0)
@@ -148,14 +161,29 @@
         self.trial.run()
         self.trial.mark_failed(reason=fail_reason)
 
         self.assertTrue(self.trial.status.is_failed)
         self.assertTrue(self.trial.did_not_complete)
         self.assertEqual(self.trial.failed_reason, fail_reason)
 
+    def test_trial_run_does_not_overwrite_existing_metadata(self) -> None:
+        self.trial.runner = SyntheticRunner(dummy_metadata="y")
+        self.trial.update_run_metadata({"orig_metadata": "x"})
+        self.trial.run()
+        self.assertDictEqual(
+            self.trial.run_metadata,
+            {
+                "name": "test_0",
+                "orig_metadata": "x",
+                "dummy_metadata": "y",
+                # this is set in setUp
+                "foo": "bar",
+            },
+        )
+
     def test_mark_as(self) -> None:
         for terminal_status in (
             TrialStatus.ABANDONED,
             TrialStatus.FAILED,
             TrialStatus.COMPLETED,
             TrialStatus.EARLY_STOPPED,
         ):
@@ -332,24 +360,47 @@
             UserInputError,
             "Raw data does not conform to the expected structure.",
         ):
             map_trial.update_trial_data(raw_data=[["aa", {"m1": 1.0}]])
 
     def test_clone_to(self) -> None:
         # cloned trial attached to the same experiment
+        self.trial.mark_running(no_runner_required=True)
         new_trial = self.trial.clone_to()
         self.assertIs(new_trial.experiment, self.trial.experiment)
         # Test equality of all attributes except index, time_created, and experiment.
         for k, v in new_trial.__dict__.items():
-            if k in ["_index", "_time_created", "_experiment"]:
+            if k in ["_index", "_time_created", "_experiment", "_time_run_started"]:
                 continue
             self.assertEqual(v, self.trial.__dict__[k])
 
         # cloned trial attached to a new experiment
         new_experiment = get_experiment()
         new_trial = self.trial.clone_to(new_experiment)
         self.assertEqual(new_trial, self.trial)
 
         # make sure updating cloned trial doesn't affect original one
         new_trial._status = TrialStatus.COMPLETED
         self.assertTrue(new_trial.status.is_completed)
         self.assertFalse(self.trial.status.is_completed)
+
+    def test_update_trial_status_on_clone(self) -> None:
+        for status in [
+            TrialStatus.CANDIDATE,
+            TrialStatus.STAGED,
+            TrialStatus.RUNNING,
+            TrialStatus.EARLY_STOPPED,
+            TrialStatus.COMPLETED,
+            TrialStatus.FAILED,
+            TrialStatus.ABANDONED,
+        ]:
+            self.trial._failed_reason = self.trial._abandoned_reason = None
+            if status != TrialStatus.CANDIDATE:
+                self.trial.mark_as(
+                    status=status, unsafe=True, no_runner_required=True, reason="test"
+                )
+            test_trial = self.trial.clone_to()
+            # Overwrite unimportant attrs before equality check.
+            test_trial._index = self.trial.index
+            test_trial._time_created = self.trial._time_created
+            test_trial._time_staged = self.trial._time_staged
+            self.assertEqual(self.trial, test_trial)
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_types.py` & `ax-platform-0.4.0/ax/core/tests/test_types.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.types import (
     merge_model_predict,
     validate_evaluation_outcome,
     validate_floatlike,
     validate_map_dict,
     validate_param_value,
     validate_parameterization,
@@ -15,14 +17,15 @@
     validate_trial_evaluation,
 )
 from ax.utils.common.testutils import TestCase
 
 
 class TypesTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.num_arms = 2
         mu = {"m1": [0.0, 0.5], "m2": [0.1, 0.6]}
         cov = {
             "m1": {"m1": [0.0, 0.0], "m2": [0.0, 0.0]},
             "m2": {"m1": [0.0, 0.0], "m2": [0.0, 0.0]},
         }
         self.predict = (mu, cov)
```

### Comparing `ax-platform-0.3.7/ax/core/tests/test_utils.py` & `ax-platform-0.4.0/ax/core/tests/test_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from unittest.mock import patch
 
 import numpy as np
 import pandas as pd
 from ax.core.arm import Arm
 from ax.core.base_trial import TrialStatus
-
 from ax.core.data import Data
 from ax.core.generator_run import GeneratorRun
 from ax.core.metric import Metric
 from ax.core.objective import Objective
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.outcome_constraint import OutcomeConstraint
@@ -40,14 +41,15 @@
     get_robust_branin_experiment,
 )
 from pyre_extensions import none_throws
 
 
 class UtilsTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_experiment()
         self.arm = Arm({"x": 5, "y": "foo", "z": True, "w": 5})
         self.trial = self.experiment.new_trial(GeneratorRun([self.arm]))
         self.experiment_2 = get_experiment()
         self.batch_trial = self.experiment_2.new_batch_trial(GeneratorRun([self.arm]))
         self.batch_trial.set_status_quo_with_weight(self.experiment_2.status_quo, 1)
         self.obs_feat = ObservationFeatures.from_arm(
@@ -152,15 +154,15 @@
                 },
             ]
         )
 
         self.data = Data(df=self.df)
 
         self.optimization_config = OptimizationConfig(
-            objective=Objective(metric=Metric(name="a")),
+            objective=Objective(metric=Metric(name="a"), minimize=False),
             outcome_constraints=[
                 OutcomeConstraint(
                     metric=Metric(name="b"),
                     op=ComparisonOp.GEQ,
                     bound=0,
                     relative=False,
                 )
@@ -282,14 +284,99 @@
                 {
                     "tracking": [self.obs_feat],
                     "m2": [self.obs_feat],
                     "m1": [self.obs_feat],
                 },
             )
 
+    def test_get_pending_observation_features_multi_trial(self) -> None:
+        # Pending observations should be none if there aren't any.
+        self.assertIsNone(get_pending_observation_features(self.experiment))
+        self.trial.mark_running(no_runner_required=True)
+        # Now that the trial is deployed, it should become a pending trial on the
+        # experiment and appear as pending for all metrics.
+
+        # With `fetch_data` on trial returning data for metric "m2", that metric
+        # should no longer have pending observation features.
+        with patch.object(
+            self.trial,
+            "lookup_data",
+            return_value=Data.from_evaluations(
+                {self.trial.arm.name: {"m2": (1, 0)}}, trial_index=self.trial.index
+            ),
+        ):
+            self.assertEqual(
+                get_pending_observation_features(self.experiment),
+                {"tracking": [self.obs_feat], "m2": [], "m1": [self.obs_feat]},
+            )
+
+        # Make sure that trial_index is set correctly
+        other_obs_feat = ObservationFeatures.from_arm(arm=self.trial.arm, trial_index=1)
+        other_trial = self.experiment.new_trial(GeneratorRun([self.arm]))
+        other_trial.mark_running(no_runner_required=True)
+
+        with patch.object(
+            self.trial,
+            "lookup_data",
+            return_value=Data.from_evaluations(
+                {self.trial.arm.name: {"m2": (1, 0)}}, trial_index=self.trial.index
+            ),
+        ):
+            with patch.object(
+                other_trial,
+                "lookup_data",
+                return_value=Data.from_evaluations(
+                    {other_trial.arm.name: {"m2": (1, 0), "tracking": (1, 0)}},
+                    trial_index=other_trial.index,
+                ),
+            ):
+                pending = get_pending_observation_features(self.experiment)
+                print(pending)
+                self.assertEqual(
+                    pending,
+                    {
+                        "tracking": [self.obs_feat],
+                        "m2": [],
+                        "m1": [self.obs_feat, other_obs_feat],
+                    },
+                )
+
+    def test_get_pending_observation_features_out_of_design(self) -> None:
+        # Pending observations should be none if there aren't any.
+        self.assertIsNone(get_pending_observation_features(self.experiment))
+        self.trial.mark_running(no_runner_required=True)
+        # Now that the trial is deployed, it should become a pending trial on the
+        # experiment and appear as pending for all metrics.
+        with patch.object(
+            self.experiment.search_space,
+            "check_membership",
+            return_value=False,
+        ):
+            self.assertIsNone(
+                get_pending_observation_features(
+                    self.experiment, include_out_of_design_points=False
+                ),
+            )
+
+        with patch.object(
+            self.experiment.search_space,
+            "check_membership",
+            return_value=False,
+        ):
+            self.assertEqual(
+                get_pending_observation_features(
+                    self.experiment, include_out_of_design_points=True
+                ),
+                {
+                    "tracking": [self.obs_feat],
+                    "m2": [self.obs_feat],
+                    "m1": [self.obs_feat],
+                },
+            )
+
     def test_get_pending_observation_features_hss(self) -> None:
         # Pending observations should be none if there aren't any.
         self.assertIsNone(get_pending_observation_features(self.hss_exp))
         self.hss_trial.mark_running(no_runner_required=True)
         # Now that the trial is deployed, it should become a pending trial on the
         # experiment and appear as pending for all metrics.
         pending = get_pending_observation_features(self.hss_exp)
```

### Comparing `ax-platform-0.3.7/ax/core/trial.py` & `ax-platform-0.4.0/ax/core/trial.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from __future__ import annotations
+# pyre-strict
 
-from copy import deepcopy
+from __future__ import annotations
 
 from functools import partial
 
 from logging import Logger
 
 from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union
 
 from ax.core.arm import Arm
 from ax.core.base_trial import BaseTrial, immutable_once_run
 from ax.core.data import Data
 from ax.core.generator_run import GeneratorRun, GeneratorRunType
 from ax.core.types import TCandidateMetadata, TEvaluationOutcome
+from ax.exceptions.core import UnsupportedError
 from ax.utils.common.docutils import copy_doc
 from ax.utils.common.logger import _round_floats_for_logging, get_logger
 from ax.utils.common.typeutils import not_none
+from pyre_extensions import override
 
 logger: Logger = get_logger(__name__)
 
 ROUND_FLOATS_IN_LOGS_TO_DECIMAL_PLACES: int = 6
 
 # pyre-fixme[5]: Global expression must be annotated.
 round_floats_for_logging = partial(
@@ -122,17 +124,19 @@
             generator_run=GeneratorRun(
                 arms=[arm],
                 type=GeneratorRunType.MANUAL.name,
                 # pyre-ignore[6]: In call `GeneratorRun.__init__`, for 3rd parameter
                 # `candidate_metadata_by_arm_signature`
                 # expected `Optional[Dict[str, Optional[Dict[str, typing.Any]]]]`
                 # but got `Optional[Dict[str, Dict[str, typing.Any]]]`
-                candidate_metadata_by_arm_signature=None
-                if candidate_metadata is None
-                else {arm.signature: candidate_metadata.copy()},
+                candidate_metadata_by_arm_signature=(
+                    None
+                    if candidate_metadata is None
+                    else {arm.signature: candidate_metadata.copy()}
+                ),
             )
         )
 
     @immutable_once_run
     def add_generator_run(
         self, generator_run: GeneratorRun, multiplier: float = 1.0
     ) -> Trial:
@@ -343,13 +347,22 @@
         """
         experiment = self._experiment if experiment is None else experiment
         new_trial = experiment.new_trial(
             ttl_seconds=self.ttl_seconds, trial_type=self.trial_type
         )
         if self.generator_run is not None:
             new_trial.add_generator_run(self.generator_run.clone())
-        new_trial._run_metadata = deepcopy(self._run_metadata)
-        new_trial._stop_metadata = deepcopy(self._stop_metadata)
-        new_trial._num_arms_created = self._num_arms_created
-        new_trial.runner = self._runner.clone() if self._runner else None
-
+        self._update_trial_attrs_on_clone(new_trial=new_trial)
         return new_trial
+
+    @override
+    def _raise_cant_attach_if_completed(self) -> None:
+        """
+        Helper method used by `validate_can_attach_data` to raise an error if
+        the user tries to attach data to a completed trial. Subclasses such as
+        `Trial` override this by suggesting a remediation.
+        """
+        raise UnsupportedError(
+            f"Trial {self.index} has already been completed with data. "
+            "To add more data to it (for example, for a different metric), "
+            f"use `{self.__class__.__name__}.update_trial_data()`."
+        )
```

### Comparing `ax-platform-0.3.7/ax/core/types.py` & `ax-platform-0.4.0/ax/core/types.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 from typing import (
     Any,
     Callable,
     DefaultDict,
     Dict,
     Hashable,
```

### Comparing `ax-platform-0.3.7/ax/core/utils.py` & `ax-platform-0.4.0/ax/core/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import defaultdict
 from typing import Dict, Iterable, List, NamedTuple, Optional, Set, Tuple
 
 import numpy as np
 from ax.core.arm import Arm
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.batch_trial import BatchTrial
@@ -17,15 +19,14 @@
 from ax.core.objective import MultiObjective
 
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
 
 from ax.core.trial import Trial
 from ax.core.types import ComparisonOp
-from ax.utils.common.typeutils import not_none
 from pyre_extensions import none_throws
 
 TArmTrial = Tuple[str, int]
 
 # Threshold for switching to pending points extraction based on trial status.
 MANY_TRIALS_IN_EXPERIMENT = 100
 
@@ -266,53 +267,64 @@
         pending for that metric (i.e. do not have evaluation data for that metric).
         If there are no pending features for any of the metrics, return is None.
     """
 
     def _is_in_design(arm: Arm) -> bool:
         return experiment.search_space.check_membership(parameterization=arm.parameters)
 
-    pending_features = {}
+    pending_features = {metric_name: [] for metric_name in experiment.metrics}
+
+    def create_observation_feature(
+        arm: Arm,
+        trial_index: int,
+        trial: BaseTrial,
+    ) -> Optional[ObservationFeatures]:
+        if not include_out_of_design_points and not _is_in_design(arm=arm):
+            return None
+        return ObservationFeatures.from_arm(
+            arm=arm,
+            trial_index=trial_index,
+            metadata=trial._get_candidate_metadata(arm_name=arm.name),
+        )
+
     # Note that this assumes that if a metric appears in fetched data, the trial is
     # not pending for the metric. Where only the most recent data matters, this will
     # work, but may need to add logic to check previously added data objects, too.
     for trial_index, trial in experiment.trials.items():
-        dat = trial.lookup_data()
+        if trial.status.is_deployed:
+            metric_names_in_data = set(trial.lookup_data().df.metric_name.values)
+        else:
+            metric_names_in_data = set()
+
         for metric_name in experiment.metrics:
             if metric_name not in pending_features:
                 pending_features[metric_name] = []
 
             if trial.status.is_abandoned or (
                 trial.status.is_deployed
-                and metric_name not in dat.df.metric_name.values
+                and metric_name not in metric_names_in_data
                 and trial.arms is not None
             ):
                 for arm in trial.arms:
-                    # Do not add out-of-design points unless requested.
-                    if not include_out_of_design_points and not _is_in_design(arm=arm):
-                        continue
-                    pending_features[metric_name].append(
-                        ObservationFeatures.from_arm(
-                            arm=arm,
-                            trial_index=trial_index,
-                            metadata=trial._get_candidate_metadata(arm_name=arm.name),
-                        )
-                    )
+                    if feature := create_observation_feature(
+                        arm=arm,
+                        trial_index=trial_index,
+                        trial=trial,
+                    ):
+                        pending_features[metric_name].append(feature)
 
             # Also add abandoned arms as pending for all metrics.
             if isinstance(trial, BatchTrial):
                 for arm in trial.abandoned_arms:
-                    if not include_out_of_design_points and not _is_in_design(arm=arm):
-                        continue
-                    not_none(pending_features.get(metric_name)).append(
-                        ObservationFeatures.from_arm(
-                            arm=arm,
-                            trial_index=trial_index,
-                            metadata=trial._get_candidate_metadata(arm_name=arm.name),
-                        )
-                    )
+                    if feature := create_observation_feature(
+                        arm=arm,
+                        trial_index=trial_index,
+                        trial=trial,
+                    ):
+                        pending_features[metric_name].append(feature)
 
     return pending_features if any(x for x in pending_features.values()) else None
 
 
 # TODO: allow user to pass search space which overrides that on the experiment
 # (to use for the `include_out_of_design_points` check)
 def get_pending_observation_features_based_on_trial_status(
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/strategies/__init__.py` & `ax-platform-0.4.0/ax/early_stopping/strategies/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.early_stopping.strategies.base import (
     BaseEarlyStoppingStrategy,
     EarlyStoppingTrainingData,
     ModelBasedEarlyStoppingStrategy,
 )
 from ax.early_stopping.strategies.logical import (
     AndEarlyStoppingStrategy,
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/strategies/base.py` & `ax-platform-0.4.0/ax/early_stopping/strategies/base.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 from logging import Logger
 from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple, Type
 
 import numpy as np
@@ -66,15 +68,14 @@
         self,
         metric_names: Optional[Iterable[str]] = None,
         seconds_between_polls: int = 300,
         min_progression: Optional[float] = None,
         max_progression: Optional[float] = None,
         min_curves: Optional[int] = None,
         trial_indices_to_ignore: Optional[List[int]] = None,
-        true_objective_metric_name: Optional[str] = None,
         normalize_progressions: bool = False,
     ) -> None:
         """A BaseEarlyStoppingStrategy class.
 
         Args:
             metric_names: The names of the metrics the strategy will interact with.
                 If no metric names are provided the objective metric is assumed.
@@ -87,17 +88,14 @@
             max_progression: Do not stop trials that have passed `max_progression`.
                 Useful if we prefer finishing a trial that are already near completion.
             min_curves: Trials will not be stopped until a number of trials
                 `min_curves` have completed with curve data attached. That is, if
                 `min_curves` trials are completed but their curve data was not
                 successfully retrieved, further trials may not be early-stopped.
             trial_indices_to_ignore: Trial indices that should not be early stopped.
-            true_objective_metric_name: The actual objective to be optimized; used in
-                situations where early stopping uses a proxy objective (such as training
-                loss instead of eval loss) for stopping decisions.
             normalize_progressions: Normalizes the progression column of the MapData df
                 by dividing by the max. If the values were originally in [0, `prog_max`]
                 (as we would expect), the transformed values will be in [0, 1]. Useful
                 for inferring the max progression and allows `min_progression` to be
                 specified in the transformed space. IMPORTANT: Typically, `min_curves`
                 should be > 0 to ensure that at least one trial has completed and that
                 we have a reliable approximation for `prog_max`.
@@ -106,15 +104,14 @@
             raise ValueError("`seconds_between_polls may not be less than 0.")
         self.metric_names = metric_names
         self.seconds_between_polls = seconds_between_polls
         self.min_progression = min_progression
         self.max_progression = max_progression
         self.min_curves = min_curves
         self.trial_indices_to_ignore = trial_indices_to_ignore
-        self.true_objective_metric_name = true_objective_metric_name
         self.normalize_progressions = normalize_progressions
 
     @abstractmethod
     def should_stop_trials_early(
         self,
         trial_indices: Set[int],
         experiment: Experiment,
@@ -416,15 +413,14 @@
         self,
         metric_names: Optional[Iterable[str]] = None,
         seconds_between_polls: int = 300,
         min_progression: Optional[float] = None,
         max_progression: Optional[float] = None,
         min_curves: Optional[int] = None,
         trial_indices_to_ignore: Optional[List[int]] = None,
-        true_objective_metric_name: Optional[str] = None,
         normalize_progressions: bool = False,
         min_progression_modeling: Optional[float] = None,
     ) -> None:
         """A ModelBasedEarlyStoppingStrategy class.
 
         Args:
             metric_names: The names of the metrics the strategy will interact with.
@@ -438,17 +434,14 @@
             max_progression: Do not stop trials that have passed `max_progression`.
                 Useful if we prefer finishing a trial that are already near completion.
             min_curves: Trials will not be stopped until a number of trials
                 `min_curves` have completed with curve data attached. That is, if
                 `min_curves` trials are completed but their curve data was not
                 successfully retrieved, further trials may not be early-stopped.
             trial_indices_to_ignore: Trial indices that should not be early stopped.
-            true_objective_metric_name: The actual objective to be optimized; used in
-                situations where early stopping uses a proxy objective (such as training
-                loss instead of eval loss) for stopping decisions.
             normalize_progressions: Normalizes the progression column of the MapData df
                 by dividing by the max. If the values were originally in [0, `prog_max`]
                 (as we would expect), the transformed values will be in [0, 1]. Useful
                 for inferring the max progression and allows `min_progression` to be
                 specified in the transformed space. IMPORTANT: Typically, `min_curves`
                 should be > 0 to ensure that at least one trial has completed and that
                 we have a reliable approximation for `prog_max`.
@@ -459,15 +452,14 @@
         super().__init__(
             metric_names=metric_names,
             seconds_between_polls=seconds_between_polls,
             min_progression=min_progression,
             max_progression=max_progression,
             min_curves=min_curves,
             trial_indices_to_ignore=trial_indices_to_ignore,
-            true_objective_metric_name=true_objective_metric_name,
             normalize_progressions=normalize_progressions,
         )
         self.min_progression_modeling = min_progression_modeling
 
     def _check_validity_and_get_data(
         self, experiment: Experiment, metric_names: List[str]
     ) -> Optional[MapData]:
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/strategies/logical.py` & `ax-platform-0.4.0/ax/early_stopping/strategies/logical.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from functools import reduce
 from typing import Any, Dict, Optional, Sequence, Set
 
 from ax.core.experiment import Experiment
 from ax.early_stopping.strategies.base import BaseEarlyStoppingStrategy
 from ax.exceptions.core import UserInputError
 
 
 class LogicalEarlyStoppingStrategy(BaseEarlyStoppingStrategy):
     def __init__(
         self,
         left: BaseEarlyStoppingStrategy,
         right: BaseEarlyStoppingStrategy,
         seconds_between_polls: int = 300,
-        true_objective_metric_name: Optional[str] = None,
     ) -> None:
         super().__init__(
             seconds_between_polls=seconds_between_polls,
-            true_objective_metric_name=true_objective_metric_name,
         )
 
         self.left = left
         self.right = right
 
 
 class AndEarlyStoppingStrategy(LogicalEarlyStoppingStrategy):
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/strategies/percentile.py` & `ax-platform-0.4.0/ax/early_stopping/strategies/percentile.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple
 
 import numpy as np
 import pandas as pd
 from ax.core.experiment import Experiment
 from ax.early_stopping.strategies.base import BaseEarlyStoppingStrategy
@@ -28,15 +30,14 @@
         metric_names: Optional[Iterable[str]] = None,
         seconds_between_polls: int = 300,
         percentile_threshold: float = 50.0,
         min_progression: Optional[float] = 10,
         max_progression: Optional[float] = None,
         min_curves: Optional[int] = 5,
         trial_indices_to_ignore: Optional[List[int]] = None,
-        true_objective_metric_name: Optional[str] = None,
         normalize_progressions: bool = False,
     ) -> None:
         """Construct a PercentileEarlyStoppingStrategy instance.
 
         Args:
             metric_names: A (length-one) list of name of the metric to observe. If
                 None will default to the objective metric on the Experiment's
@@ -56,17 +57,14 @@
             max_progression: Do not stop trials that have passed `max_progression`.
                 Useful if we prefer finishing a trial that are already near completion.
             min_curves: Trials will not be stopped until a number of trials
                 `min_curves` have completed with curve data attached. That is, if
                 `min_curves` trials are completed but their curve data was not
                 successfully retrieved, further trials may not be early-stopped.
             trial_indices_to_ignore: Trial indices that should not be early stopped.
-            true_objective_metric_name: The actual objective to be optimized; used in
-                situations where early stopping uses a proxy objective (such as training
-                loss instead of eval loss) for stopping decisions.
             normalize_progressions: Normalizes the progression column of the MapData df
                 by dividing by the max. If the values were originally in [0, `prog_max`]
                 (as we would expect), the transformed values will be in [0, 1]. Useful
                 for inferring the max progression and allows `min_progression` to be
                 specified in the transformed space. IMPORTANT: Typically, `min_curves`
                 should be > 0 to ensure that at least one trial has completed and that
                 we have a reliable approximation for `prog_max`.
@@ -74,15 +72,14 @@
         super().__init__(
             metric_names=metric_names,
             seconds_between_polls=seconds_between_polls,
             trial_indices_to_ignore=trial_indices_to_ignore,
             min_progression=min_progression,
             max_progression=max_progression,
             min_curves=min_curves,
-            true_objective_metric_name=true_objective_metric_name,
             normalize_progressions=normalize_progressions,
         )
 
         self.percentile_threshold = percentile_threshold
 
         if metric_names is not None and len(list(metric_names)) > 1:
             raise UnsupportedError(
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/strategies/threshold.py` & `ax-platform-0.4.0/ax/early_stopping/strategies/threshold.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple
 
 import pandas as pd
 from ax.core.experiment import Experiment
 from ax.early_stopping.strategies.base import BaseEarlyStoppingStrategy
 from ax.exceptions.core import UnsupportedError
@@ -25,15 +27,14 @@
         metric_names: Optional[Iterable[str]] = None,
         seconds_between_polls: int = 300,
         metric_threshold: float = 0.2,
         min_progression: Optional[float] = 10,
         max_progression: Optional[float] = None,
         min_curves: Optional[int] = 5,
         trial_indices_to_ignore: Optional[List[int]] = None,
-        true_objective_metric_name: Optional[str] = None,
         normalize_progressions: bool = False,
     ) -> None:
         """Construct a ThresholdEarlyStoppingStrategy instance.
 
         Args
             metric_names: A (length-one) list of name of the metric to observe. If
                 None will default to the objective metric on the Experiment's
@@ -49,29 +50,25 @@
             max_progression: Do not stop trials that have passed `max_progression`.
                 Useful if we prefer finishing a trial that are already near completion.
             min_curves: Trials will not be stopped until a number of trials
                 `min_curves` have completed with curve data attached. That is, if
                 `min_curves` trials are completed but their curve data was not
                 successfully retrieved, further trials may not be early-stopped.
             trial_indices_to_ignore: Trial indices that should not be early stopped.
-            true_objective_metric_name: The actual objective to be optimized; used in
-                situations where early stopping uses a proxy objective (such as training
-                loss instead of eval loss) for stopping decisions.
             normalize_progressions: Normalizes the progression column of the MapData df
                 by dividing by the max. If the values were originally in [0, `prog_max`]
                 (as we would expect), the transformed values will be in [0, 1]. Useful
                 for inferring the max progression and allows `min_progression` to be
                 specified in the transformed space. IMPORTANT: Typically, `min_curves`
                 should be > 0 to ensure that at least one trial has completed and that
                 we have a reliable approximation for `prog_max`.
         """
         super().__init__(
             metric_names=metric_names,
             seconds_between_polls=seconds_between_polls,
-            true_objective_metric_name=true_objective_metric_name,
             min_progression=min_progression,
             max_progression=max_progression,
             min_curves=min_curves,
             trial_indices_to_ignore=trial_indices_to_ignore,
             normalize_progressions=normalize_progressions,
         )
         self.metric_threshold = metric_threshold
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/tests/test_strategies.py` & `ax-platform-0.4.0/ax/early_stopping/tests/test_strategies.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from typing import Any, cast, Dict, Optional, Set
 
 import numpy as np
 from ax.core import OptimizationConfig
 from ax.core.base_trial import TrialStatus
 from ax.core.experiment import Experiment
@@ -279,23 +281,14 @@
             min_progression=3,
         )
         should_stop = early_stopping_strategy.should_stop_trials_early(
             trial_indices=idcs, experiment=exp
         )
         self.assertEqual(should_stop, {})
 
-        # True objective metric name
-        self.assertIsNone(
-            early_stopping_strategy.true_objective_metric_name
-        )  # default none
-        early_stopping_strategy.true_objective_metric_name = "true_obj_metric"
-        self.assertEqual(
-            early_stopping_strategy.true_objective_metric_name, "true_obj_metric"
-        )
-
     def test_percentile_early_stopping_strategy(self) -> None:
         self._test_percentile_early_stopping_strategy(non_objective_metric=False)
 
     def test_percentile_early_stopping_strategy_non_objective_metric(self) -> None:
         self._test_percentile_early_stopping_strategy(non_objective_metric=True)
 
         with self.assertRaisesRegex(
@@ -425,17 +418,17 @@
         # test case 1
         exp = get_test_map_data_experiment(num_trials=5, num_fetches=3, num_complete=5)
         # manually "unalign" timestamps to simulate real-world scenario
         # where each curve reports results at different steps
         data = checked_cast(MapData, exp.fetch_data())
 
         unaligned_timestamps = [0, 1, 4, 1, 2, 3, 1, 3, 4, 0, 1, 2, 0, 2, 4]
-        data.map_df.loc[
-            data.map_df["metric_name"] == "branin_map", "timestamp"
-        ] = unaligned_timestamps
+        data.map_df.loc[data.map_df["metric_name"] == "branin_map", "timestamp"] = (
+            unaligned_timestamps
+        )
         exp.attach_data(data=data)
 
         """
         Dataframe after interpolation:
                     0           1          2           3          4
         timestamp
         0          146.138620         NaN        NaN  143.375669  65.033535
@@ -468,17 +461,17 @@
         # manually "unalign" timestamps to simulate real-world scenario
         # where each curve reports results at different steps
         data = checked_cast(MapData, exp.fetch_data())
         data.map_df.sort_values(by=["metric_name", "arm_name"], inplace=True)
         data.map_df.reset_index(drop=True, inplace=True)
 
         unaligned_timestamps = [0, 1, 4, 1, 2, 3, 1, 3, 4, 0, 1, 2, 0, 2, 4]
-        data.map_df.loc[
-            data.map_df["metric_name"] == "branin_map", "timestamp"
-        ] = unaligned_timestamps
+        data.map_df.loc[data.map_df["metric_name"] == "branin_map", "timestamp"] = (
+            unaligned_timestamps
+        )
         # manually remove timestamps 1 and 2 for arm 3
         df = data.map_df
         df.drop(
             df.index[
                 (df["metric_name"] == "branin_map")
                 & (df["trial_index"] == 3)
                 & (df["timestamp"].isin([1.0, 2.0]))
```

### Comparing `ax-platform-0.3.7/ax/early_stopping/utils.py` & `ax-platform-0.4.0/ax/early_stopping/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import defaultdict
 from logging import Logger
 from typing import Dict, List, Optional, Tuple
 
 import pandas as pd
 from ax.core.base_trial import TrialStatus
 from ax.core.experiment import Experiment
```

### Comparing `ax-platform-0.3.7/ax/exceptions/constants.py` & `ax-platform-0.4.0/ax/exceptions/constants.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 TS_MIN_WEIGHT_ERROR = """\
 No arms generated by Thompson Sampling had weight > min_weight. \
 The minimum weight required is {min_weight:2.4}, and the \
 maximum weight of any arm generated is {max_weight:2.4}.
 """
 
 TS_NO_FEASIBLE_ARMS_ERROR = """\
```

### Comparing `ax-platform-0.3.7/ax/exceptions/core.py` & `ax-platform-0.4.0/ax/exceptions/core.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 class AxError(Exception):
     """Base Ax exception.
 
     All exceptions derived from AxError need to define a custom error message.
     Additionally, exceptions can define a hint property that provides additional
     guidance as to how to remedy the error.
@@ -21,31 +23,27 @@
     def __str__(self) -> str:
         return " ".join([self.message, getattr(self, "hint", "")]).rstrip()
 
 
 class UserInputError(AxError):
     """Raised when the user passes in an invalid input"""
 
-    pass
-
 
 class UnsupportedError(AxError):
     """Raised when an unsupported request is made.
 
     UnsupportedError may seem similar to NotImplementedError (NIE).
     It differs in the following ways:
 
     1. UnsupportedError is not used for abstract methods, which
         is the official NIE use case.
     2. UnsupportedError indicates an intentional and permanent lack of support.
         It should not be used for TODO (another common use case of NIE).
     """
 
-    pass
-
 
 class UnsupportedPlotError(AxError):
     """Raised when plotting functionality is not supported for the
     given configurations.
     """
 
     def __init__(self, message: str) -> None:
@@ -65,39 +63,41 @@
     def __init__(
         self, message: str, hint: str = "", exposures_unavailable: bool = False
     ) -> None:
         super().__init__(message=message, hint=hint)
         self.exposures_unavailable = exposures_unavailable
 
 
+class MetricDataNotReadyError(AxError):
+    """Raised when trying to pull metric data from a trial that has
+    not finished running.
+    """
+
+    pass
+
+
 class NoDataError(AxError):
     """Raised when no data is found for experiment in underlying data store.
 
     Useful to distinguish data failure reasons in automated analyses.
     """
 
-    pass
-
 
 class DataRequiredError(AxError):
     """Raised when more observed data is needed by the model to continue the
     optimization.
 
     Useful to distinguish when user needs to wait to request more trials until
     more data is available.
     """
 
-    pass
-
 
 class MisconfiguredExperiment(AxError):
     """Raised when experiment has incomplete or incorrect information."""
 
-    pass
-
 
 class OptimizationComplete(AxError):
     """Raised when you hit SearchSpaceExhausted and GenerationStrategyComplete."""
 
     def __init__(self, message: str) -> None:
         super().__init__(
             message=message
@@ -121,22 +121,18 @@
 
     This exception replaces ValueError raised by code when an objects is not
     found in the database. In order to maintain backwards compatibility
     ObjectNotFoundError inherits from ValueError. Dependency on ValueError
     may be removed in the future.
     """
 
-    pass
-
 
 class ExperimentNotFoundError(ObjectNotFoundError):
     """Raised when an experiment is not found in the database."""
 
-    pass
-
 
 class SearchSpaceExhausted(OptimizationComplete):
     """Raised when using an algorithm that deduplicates points and no more
     new points can be sampled from the search space."""
 
     def __init__(self, message: str) -> None:
         super().__init__(
@@ -144,16 +140,14 @@
             or "No more new points could be sampled in the search space."
         )
 
 
 class IncompatibleDependencyVersion(AxError):
     """Raise when an imcompatible dependency version is installed."""
 
-    pass
-
 
 class AxWarning(Warning):
     """Base Ax warning.
 
     All warnings derived from AxWarning need to define a custom warning message.
     Additionally, warnings can define a hint property that provides additional
     guidance as to how to remedy the warning.
@@ -167,8 +161,10 @@
     def __str__(self) -> str:
         return " ".join([self.message, getattr(self, "hint", "")]).rstrip()
 
 
 class AxStorageWarning(AxWarning):
     """Ax warning used for storage related concerns."""
 
-    pass
+
+class AxParameterWarning(AxWarning):
+    """Ax warning used for concerns related to parameter setups."""
```

### Comparing `ax-platform-0.3.7/ax/exceptions/data_provider.py` & `ax-platform-0.4.0/ax/exceptions/data_provider.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Iterable
 
 
 class DataProviderError(Exception):
     """Base Exception for Ax DataProviders.
 
     The type of the data provider must be included.
```

### Comparing `ax-platform-0.3.7/ax/exceptions/generation_strategy.py` & `ax-platform-0.4.0/ax/exceptions/generation_strategy.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,20 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import typing  # noqa F401, this is to enable type-checking
 from typing import Optional
 
 from ax.exceptions.core import AxError, OptimizationComplete
 
 
-class MaxParallelismReachedException(AxError):
+class AxGenerationException(AxError):
+    """Raised when there is an issue with the generation strategy."""
+
+    pass
+
+
+class MaxParallelismReachedException(AxGenerationException):
     """Special exception indicating that maximum number of trials running in
     parallel set on a given step (as `GenerationStep.max_parallelism`) has been
     reached. Upon getting this exception, users should wait until more trials
     are completed with data, to generate new trials.
     """
 
     def __init__(
@@ -53,15 +61,15 @@
     """Special exception indicating that the generation strategy is repeatedly
     suggesting previously sampled points.
     """
 
     pass
 
 
-class GenerationStrategyMisconfiguredException(AxError):
+class GenerationStrategyMisconfiguredException(AxGenerationException):
     """Special exception indicating that the generation strategy is misconfigured."""
 
     def __init__(self, error_info: Optional[str]) -> None:
         super().__init__(
             "This GenerationStrategy was unable to be initialized properly. Please "
             + "check the documentation, and adjust the configuration accordingly. "
             + f"{error_info}"
```

### Comparing `ax-platform-0.3.7/ax/exceptions/storage.py` & `ax-platform-0.4.0/ax/exceptions/storage.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from ax.exceptions.core import AxError
 
 
 class JSONDecodeError(AxError):
     """Raised when an error occurs during JSON decoding."""
```

### Comparing `ax-platform-0.3.7/ax/global_stopping/strategies/base.py` & `ax-platform-0.4.0/ax/global_stopping/strategies/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from abc import ABC, abstractmethod
 from typing import Any, Tuple
 
 from ax.core.base_trial import TrialStatus
 from ax.core.experiment import Experiment
 from ax.utils.common.base import Base
```

### Comparing `ax-platform-0.3.7/ax/global_stopping/strategies/improvement.py` & `ax-platform-0.4.0/ax/global_stopping/strategies/improvement.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Dict, List, Optional, Tuple
 
 import numpy as np
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.data import Data
 from ax.core.experiment import Experiment
@@ -16,16 +18,18 @@
     OptimizationConfig,
 )
 from ax.core.outcome_constraint import ObjectiveThreshold
 from ax.core.trial import Trial
 from ax.core.types import ComparisonOp
 from ax.global_stopping.strategies.base import BaseGlobalStoppingStrategy
 from ax.modelbridge.modelbridge_utils import observed_hypervolume
-from ax.plot.pareto_utils import get_tensor_converter_model
-from ax.service.utils.best_point import fill_missing_thresholds_from_nadir
+from ax.plot.pareto_utils import (
+    get_tensor_converter_model,
+    infer_reference_point_from_experiment,
+)
 from ax.utils.common.logger import get_logger
 from ax.utils.common.typeutils import checked_cast, not_none
 
 
 logger: Logger = get_logger(__name__)
 
 
@@ -72,14 +76,15 @@
         super().__init__(
             min_trials=min_trials,
             inactive_when_pending_trials=inactive_when_pending_trials,
         )
         self.window_size = window_size
         self.improvement_bar = improvement_bar
         self.hv_by_trial: Dict[int, float] = {}
+        self._inferred_objective_thresholds: Optional[List[ObjectiveThreshold]] = None
 
     def _should_stop_optimization(
         self,
         experiment: Experiment,
         trial_to_check: Optional[int] = None,
         objective_thresholds: Optional[List[ObjectiveThreshold]] = None,
         **kwargs: Dict[str, Any],
@@ -98,14 +103,17 @@
             experiment: The experiment to apply the strategy on.
             trial_to_check: The trial in the experiment at which we want to check
                 for stopping. If None, we check at the latest trial.
             objective_thresholds: Custom objective thresholds to use as reference pooint
                 when computing hv of the pareto front against. This is used only in the
                 MOO setting. If not specified, the objective thresholds on the
                 experiment's optimization config will be used for the purpose.
+                If no thresholds are provided, they are automatically inferred. They are
+                only inferred once for each instance of the strategy (i.e. inferred
+                thresholds don't update with additional data).
             kwargs: Unused.
 
         Returns:
             A Tuple with a boolean determining whether the optimization should stop,
             and a str declaring the reason for stopping.
         """
 
@@ -132,29 +140,44 @@
             message = (
                 "There are not enough completed trials to make a stopping decision "
                 f"(completed: {num_completed_trials}, required: {min_required_trials})."
             )
             return stop, message
 
         if isinstance(experiment.optimization_config, MultiObjectiveOptimizationConfig):
+            if objective_thresholds is None:
+                # self._inferred_objective_thresholds is cached and only computed once.
+                if self._inferred_objective_thresholds is None:
+                    # only infer reference point if there is data on the experiment.
+                    data = experiment.fetch_data()
+                    if not data.df.empty:
+                        # We infer the nadir reference point to be used by the GSS.
+                        self._inferred_objective_thresholds = (
+                            infer_reference_point_from_experiment(
+                                experiment=experiment, data=data
+                            )
+                        )
+                # TODO: move this out into a separate infer_objective_thresholds
+                # instance method or property that handles the caching.
+                objective_thresholds = self._inferred_objective_thresholds
             return self._should_stop_moo(
                 experiment=experiment,
                 trial_to_check=trial_to_check,
-                objective_thresholds=objective_thresholds,
+                objective_thresholds=not_none(objective_thresholds),
             )
         else:
             return self._should_stop_single_objective(
                 experiment=experiment, trial_to_check=trial_to_check
             )
 
     def _should_stop_moo(
         self,
         experiment: Experiment,
         trial_to_check: int,
-        objective_thresholds: Optional[List[ObjectiveThreshold]] = None,
+        objective_thresholds: List[ObjectiveThreshold],
     ) -> Tuple[bool, str]:
         """
         This is the "should_stop_optimization" method of this class, specialized
         to MOO experiments.
 
         It computes the (feasible) hypervolume of the Pareto front at
         `trial_to_check` trial and `window_size` trials before, and suggest to stop the
@@ -180,21 +203,14 @@
                 and a str declaring the reason for stopping.
         """
         reference_trial_index = trial_to_check - self.window_size + 1
         data_df = experiment.fetch_data().df
         data_df_reference = data_df[data_df["trial_index"] <= reference_trial_index]
         data_df = data_df[data_df["trial_index"] <= trial_to_check]
 
-        optimization_config = checked_cast(
-            MultiObjectiveOptimizationConfig, experiment.optimization_config
-        ).clone_with_args(objective_thresholds=objective_thresholds)
-        objective_thresholds = fill_missing_thresholds_from_nadir(
-            experiment=experiment, optimization_config=optimization_config
-        )
-
         # Computing or retrieving HV at "window_size" iteration before
         if reference_trial_index in self.hv_by_trial:
             hv_reference = self.hv_by_trial[reference_trial_index]
         else:
             mb_reference = get_tensor_converter_model(
                 experiment=experiment, data=Data(data_df_reference)
             )
```

### Comparing `ax-platform-0.3.7/ax/global_stopping/tests/tests_strategies.py` & `ax-platform-0.4.0/ax/global_stopping/tests/test_strategies.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Tuple
 
 import numpy as np
 import pandas as pd
 from ax.core.arm import Arm
+from ax.core.base_trial import TrialStatus
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective
 from ax.core.optimization_config import (
     MultiObjectiveOptimizationConfig,
@@ -60,29 +63,33 @@
         self.assertFalse(stop)
         self.assertEqual(
             message,
             f"There are only 1 completed trials, but {gss} has a minumum of 2.",
         )
 
         # Check that we properly count completed trials.
-        for _ in range(4):
-            checked_cast(BatchTrial, exp.trials[0]).clone()
-        exp.trials[3].mark_running(no_runner_required=True).mark_completed()
+        for i in range(4):
+            trial = checked_cast(BatchTrial, exp.trials[0]).clone_to()
+            if i < 3:
+                trial._status = TrialStatus.CANDIDATE
         stop, message = gss.should_stop_optimization(experiment=exp)
         self.assertFalse(stop)
         self.assertEqual(
             message,
             "There are not enough completed trials to "
             "make a stopping decision (completed: 2, required: 3).",
         )
 
         # Should raise ValueError if trying to check an invalid trial
-        with self.assertRaises(ValueError):
+        with self.assertRaisesRegex(
+            ValueError,
+            r"trial_to_check is larger than the total number of trials \(=4\).",
+        ):
             stop, message = gss.should_stop_optimization(
-                experiment=exp, trial_to_check=4
+                experiment=exp, trial_to_check=5
             )
 
     def _get_arm(self) -> Arm:
         """generates random arm in [0,1]^2."""
         return Arm(parameters={"x": np.random.rand(), "y": np.random.rand()})
 
     def _get_data_for_trial(
@@ -258,28 +265,28 @@
             (0.4, 0.5, 0.6),
             (0.3, 0.4, 0.0),
             (0.6, 0.1, 0.3),
             (0.9, 0.1, 0.1),
         ]
         exp = self._create_multi_objective_experiment(metric_values=metric_values)
         gss = ImprovementGlobalStoppingStrategy(
-            min_trials=3, window_size=3, improvement_bar=0.1
+            min_trials=3, window_size=3, improvement_bar=0.3
         )
         stop, message = gss.should_stop_optimization(experiment=exp, trial_to_check=4)
         self.assertFalse(stop)
         self.assertEqual(message, "")
 
         stop, message = gss.should_stop_optimization(experiment=exp, trial_to_check=5)
         self.assertTrue(stop)
 
         self.assertEqual(
             message,
-            "The improvement in hypervolume in the past 3 trials (=0.000) is "
-            "less than improvement_bar (=0.1) times the hypervolume at the "
-            "start of the window (=0.055).",
+            "The improvement in hypervolume in the past 3 trials (=0.289) is "
+            "less than improvement_bar (=0.3) times the hypervolume at the "
+            "start of the window (=0.104).",
         )
 
         # Now we select a very far custom reference point against which the pareto front
         # has not increased in hypervolume at trial 4. Hence, it should stop the
         # optimization at this trial.
         gss = ImprovementGlobalStoppingStrategy(
             min_trials=3, window_size=3, improvement_bar=0.1
```

### Comparing `ax-platform-0.3.7/ax/health_check/search_space.py` & `ax-platform-0.4.0/ax/health_check/search_space.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/health_check/tests/test_search_space.py` & `ax-platform-0.4.0/ax/health_check/tests/test_search_space.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List
 
 from ax.core import SearchSpace
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.types import TParameterization
 from ax.health_check.search_space import search_space_update_recommendation
 from ax.utils.common.testutils import TestCase
```

### Comparing `ax-platform-0.3.7/ax/metrics/botorch_test_problem.py` & `ax-platform-0.4.0/ax/metrics/botorch_test_problem.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Optional
 
 import pandas as pd
 from ax.core.base_trial import BaseTrial
 from ax.core.data import Data
 from ax.core.metric import Metric, MetricFetchE, MetricFetchResult
 from ax.utils.common.result import Err, Ok
```

### Comparing `ax-platform-0.3.7/ax/metrics/branin.py` & `ax-platform-0.4.0/ax/metrics/branin.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.metrics.noisy_function import NoisyFunctionMetric
 from ax.utils.common.typeutils import checked_cast
 from ax.utils.measurement.synthetic_functions import aug_branin, branin
 
 
 class BraninMetric(NoisyFunctionMetric):
```

### Comparing `ax-platform-0.3.7/ax/metrics/branin_map.py` & `ax-platform-0.4.0/ax/metrics/branin_map.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import math
 from collections import defaultdict
 from random import random
 from typing import Any, Iterable, Mapping, Optional
```

### Comparing `ax-platform-0.3.7/ax/metrics/chemistry.py` & `ax-platform-0.4.0/ax/metrics/chemistry.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Classes for optimizing yields from chemical reactions.
 
 References
 
 .. [Perera2018]
     D. Perera, J. W. Tucker, S. Brahmbhatt, C. Helal, A. Chong, W. Farrell,
```

### Comparing `ax-platform-0.3.7/ax/metrics/chemistry_data.zip` & `ax-platform-0.4.0/ax/metrics/chemistry_data.zip`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/metrics/curve.py` & `ax-platform-0.4.0/ax/metrics/curve.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Metrics that allow to retrieve curves of partial results.
 Typically used to retrieve partial learning curves of ML training jobs.
 """
 
 from __future__ import annotations
```

### Comparing `ax-platform-0.3.7/ax/metrics/dict_lookup.py` & `ax-platform-0.4.0/ax/metrics/dict_lookup.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import pandas as pd
 from ax.core.base_trial import BaseTrial
```

### Comparing `ax-platform-0.3.7/ax/metrics/factorial.py` & `ax-platform-0.4.0/ax/metrics/factorial.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Tuple
 
 import numpy as np
 import pandas as pd
 from ax.core.base_trial import BaseTrial
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
```

### Comparing `ax-platform-0.3.7/ax/metrics/hartmann6.py` & `ax-platform-0.4.0/ax/metrics/hartmann6.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.metrics.noisy_function import NoisyFunctionMetric
 from ax.utils.common.typeutils import checked_cast
 from ax.utils.measurement.synthetic_functions import aug_hartmann6, hartmann6
 
 
 class Hartmann6Metric(NoisyFunctionMetric):
```

### Comparing `ax-platform-0.3.7/ax/metrics/jenatton.py` & `ax-platform-0.4.0/ax/benchmark/metrics/jenatton.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,67 +1,107 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+from __future__ import annotations
+
 from typing import Any, Optional
 
+import numpy as np
 import pandas as pd
+from ax.benchmark.metrics.base import BenchmarkMetricBase, GroundTruthMetricMixin
 from ax.core.base_trial import BaseTrial
 from ax.core.data import Data
-from ax.core.metric import Metric, MetricFetchE, MetricFetchResult
+from ax.core.metric import MetricFetchE, MetricFetchResult
 from ax.utils.common.result import Err, Ok
 from ax.utils.common.typeutils import not_none
 
 
-class JenattonMetric(Metric):
+class JenattonMetric(BenchmarkMetricBase):
+    """Jenatton metric for hierarchical search spaces."""
+
+    has_ground_truth: bool = True
+
     def __init__(
         self,
         name: str = "jenatton",
-        infer_noise: bool = True,
+        noise_std: float = 0.0,
+        observe_noise_sd: bool = False,
     ) -> None:
         super().__init__(name=name)
-        self.infer_noise = infer_noise
-
-    @staticmethod
-    def _f(
-        x1: Optional[int] = None,
-        x2: Optional[int] = None,
-        x3: Optional[int] = None,
-        x4: Optional[float] = None,
-        x5: Optional[float] = None,
-        x6: Optional[float] = None,
-        x7: Optional[float] = None,
-        r8: Optional[float] = None,
-        r9: Optional[float] = None,
-    ) -> float:
-        if x1 == 0:
-            if x2 == 0:
-                return not_none(x4) ** 2 + 0.1 + not_none(r8)
-            else:
-                return not_none(x5) ** 2 + 0.2 + not_none(r8)
-        else:
-            if x3 == 0:
-                return not_none(x6) ** 2 + 0.3 + not_none(r9)
-            else:
-                return not_none(x7) ** 2 + 0.4 + not_none(r9)
+        self.noise_std = noise_std
+        self.observe_noise_sd = observe_noise_sd
+        self.lower_is_better = True
 
     def fetch_trial_data(self, trial: BaseTrial, **kwargs: Any) -> MetricFetchResult:
         try:
-            # pyre-ignore [6]
-            mean = [self._f(**arm.parameters) for _, arm in trial.arms_by_name.items()]
+            mean = [
+                jenatton_test_function(**arm.parameters)  # pyre-ignore [6]
+                for _, arm in trial.arms_by_name.items()
+            ]
+            if self.noise_std != 0:
+                mean = [m + self.noise_std * np.random.randn() for m in mean]
             df = pd.DataFrame(
                 {
                     "arm_name": [name for name, _ in trial.arms_by_name.items()],
                     "metric_name": self.name,
                     "mean": mean,
-                    "sem": None if self.infer_noise else 0,
+                    "sem": self.noise_std if self.observe_noise_sd else None,
                     "trial_index": trial.index,
                 }
             )
-
             return Ok(value=Data(df=df))
 
         except Exception as e:
             return Err(
                 MetricFetchE(message=f"Failed to fetch {self.name}", exception=e)
             )
+
+    def make_ground_truth_metric(self) -> GroundTruthJenattonMetric:
+        return GroundTruthJenattonMetric(original_metric=self)
+
+
+class GroundTruthJenattonMetric(JenattonMetric, GroundTruthMetricMixin):
+    def __init__(self, original_metric: JenattonMetric) -> None:
+        """
+        Args:
+            original_metric: The original JenattonMetric to which this metric
+                corresponds.
+        """
+        super().__init__(
+            name=self.get_ground_truth_name(original_metric),
+            noise_std=0.0,
+            observe_noise_sd=False,
+        )
+
+
+def jenatton_test_function(
+    x1: Optional[int] = None,
+    x2: Optional[int] = None,
+    x3: Optional[int] = None,
+    x4: Optional[float] = None,
+    x5: Optional[float] = None,
+    x6: Optional[float] = None,
+    x7: Optional[float] = None,
+    r8: Optional[float] = None,
+    r9: Optional[float] = None,
+) -> float:
+    """Jenatton test function for hierarchical search spaces.
+
+    This function is taken from:
+
+    R. Jenatton, C. Archambeau, J. González, and M. Seeger. Bayesian
+    optimization with tree-structured dependencies. ICML 2017.
+    """
+    if x1 == 0:
+        if x2 == 0:
+            return not_none(x4) ** 2 + 0.1 + not_none(r8)
+        else:
+            return not_none(x5) ** 2 + 0.2 + not_none(r8)
+    else:
+        if x3 == 0:
+            return not_none(x6) ** 2 + 0.3 + not_none(r9)
+        else:
+            return not_none(x7) ** 2 + 0.4 + not_none(r9)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `ax-platform-0.3.7/ax/metrics/noisy_function.py` & `ax-platform-0.4.0/ax/metrics/noisy_function.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Callable, List, Optional
 
 import numpy as np
 import pandas as pd
 from ax.core.base_trial import BaseTrial
```

### Comparing `ax-platform-0.3.7/ax/metrics/noisy_function_map.py` & `ax-platform-0.4.0/ax/metrics/noisy_function_map.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from logging import Logger
 
 from typing import Any, Iterable, Mapping, Optional
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/metrics/sklearn.py` & `ax-platform-0.4.0/ax/metrics/sklearn.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from copy import deepcopy
 from enum import Enum
 from functools import lru_cache
 from math import sqrt
 from typing import Any, Dict, Tuple
```

### Comparing `ax-platform-0.3.7/ax/metrics/tests/test_chemistry.py` & `ax-platform-0.4.0/ax/metrics/tests/test_chemistry.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from enum import Enum
 from unittest import mock
 
 import numpy as np
 import pandas as pd
 from ax.core.arm import Arm
 from ax.core.generator_run import GeneratorRun
```

### Comparing `ax-platform-0.3.7/ax/metrics/tests/test_curve.py` & `ax-platform-0.4.0/ax/metrics/tests/test_curve.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from ax.metrics.curve import AbstractCurveMetric
 from ax.utils.common.testutils import TestCase
 
 
 class AbstractCurveMetricTest(TestCase):
     def test_AbstractCurveMetric(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/metrics/tests/test_dict_lookup.py` & `ax-platform-0.4.0/ax/metrics/tests/test_dict_lookup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 from ax.core.arm import Arm
 from ax.core.generator_run import GeneratorRun
 from ax.metrics.dict_lookup import DictLookupMetric
 from ax.utils.common.result import Err
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_trial
 
 
 class TestDictLookupMetric(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.lookup_dict = {
             (1.0, 2.0): 3.0,
             (2.0, 3.0): 4.0,
             (3.0, 4.0): 5.0,
         }
         self.param_names = ["p1", "p2"]
         self.metric = DictLookupMetric(
```

### Comparing `ax-platform-0.3.7/ax/metrics/tests/test_noisy_function.py` & `ax-platform-0.4.0/ax/metrics/tests/test_noisy_function.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 
 from ax.metrics.noisy_function import GenericNoisyFunctionMetric
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_trial
```

### Comparing `ax-platform-0.3.7/ax/metrics/tests/test_sklearn.py` & `ax-platform-0.4.0/ax/metrics/tests/test_sklearn.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from contextlib import ExitStack
 from enum import Enum
 from math import sqrt
 from unittest import mock
 
 import numpy as np
 from ax.core.arm import Arm
```

### Comparing `ax-platform-0.3.7/ax/metrics/torchx.py` & `ax-platform-0.4.0/ax/metrics/torchx.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, cast
 
 import pandas as pd
 from ax.core import Trial
 from ax.core.base_trial import BaseTrial
 from ax.core.data import Data
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/__init__.py` & `ax-platform-0.4.0/ax/modelbridge/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,33 +1,32 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 # flake8: noqa F401
 from ax.modelbridge import transforms
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.factory import (
     get_factorial,
-    get_GPEI,
     get_sobol,
     get_thompson,
     get_uniform,
     Models,
 )
 from ax.modelbridge.map_torch import MapTorchModelBridge
 from ax.modelbridge.torch import TorchModelBridge
 
 __all__ = [
     "MapTorchModelBridge",
     "ModelBridge",
     "Models",
     "TorchModelBridge",
     "get_factorial",
-    "get_GPEI",
-    "get_GPKG",
     "get_sobol",
     "get_thompson",
     "get_uniform",
     "transforms",
 ]
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/base.py` & `ax-platform-0.4.0/ax/modelbridge/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 import time
 import warnings
 from abc import ABC
 from collections import OrderedDict
 from copy import deepcopy
 from dataclasses import dataclass, field
@@ -54,15 +56,15 @@
 class GenResults:
     observation_features: List[ObservationFeatures]
     weights: List[float]
     best_observation_features: Optional[ObservationFeatures] = None
     gen_metadata: Dict[str, Any] = field(default_factory=dict)
 
 
-class ModelBridge(ABC):
+class ModelBridge(ABC):  # noqa: B024 -- ModelBridge doesn't have any abstract methods.
     """The main object for using models in Ax.
 
     ModelBridge specifies 3 methods for using models:
 
     - predict: Make model predictions. This method is not optimized for
       speed and so should be used primarily for plotting or similar tasks
       and not inside an optimization loop.
@@ -425,21 +427,20 @@
                 )
             sq_obs = [
                 obs for obs in self._training_data if obs.arm_name == status_quo_name
             ]
 
             if len(sq_obs) == 0:
                 logger.warning(f"Status quo {status_quo_name} not present in data")
+            elif len(sq_obs) > 1:
+                logger.warning(
+                    f"Status quo {status_quo_name} found in data with multiple "
+                    "features. Use status_quo_features to specify which to use."
+                )
             else:
-                if len(sq_obs) > 1:
-                    logger.warning(
-                        f"Status quo {status_quo_name} found in data with multiple "
-                        "features. Use status_quo_features to specify which to use."
-                        " Defaulting to the first observation."
-                    )
                 self._status_quo = sq_obs[0]
 
         elif status_quo_features is not None:
             sq_obs = [
                 obs
                 for obs in self._training_data
                 if (obs.features.parameters == status_quo_features.parameters)
@@ -456,20 +457,20 @@
                 self._status_quo = sq_obs[0]
 
     @property
     def status_quo_data_by_trial(self) -> Optional[Dict[int, ObservationData]]:
         """A map of trial index to the status quo observation data of each trial"""
         return _get_status_quo_by_trial(
             observations=self._training_data,
-            status_quo_name=None
-            if self._status_quo is None
-            else self._status_quo.arm_name,
-            status_quo_features=None
-            if self._status_quo is None
-            else self._status_quo.features,
+            status_quo_name=(
+                None if self._status_quo is None else self._status_quo.arm_name
+            ),
+            status_quo_features=(
+                None if self._status_quo is None else self._status_quo.features
+            ),
         )
 
     @property
     def status_quo(self) -> Optional[Observation]:
         """Observation corresponding to status quo, if any."""
         return self._status_quo
 
@@ -685,14 +686,15 @@
                         f"Metrics {outcomes} is not a subset of {self.outcomes}."
                     )
             optimization_config = optimization_config.clone()
 
         # TODO(T34225037): replace deepcopy with native clone() in Ax
         pending_observations = deepcopy(pending_observations)
         fixed_features = deepcopy(fixed_features)
+        search_space = search_space.clone()
 
         # Transform
         for t in self.transforms.values():
             search_space = t.transform_search_space(search_space)
             if optimization_config is not None:
                 optimization_config = t.transform_optimization_config(
                     optimization_config=optimization_config,
@@ -842,17 +844,17 @@
         optimization_config = None if immutable else base_gen_args.optimization_config
         gr = GeneratorRun(
             arms=arms,
             weights=gen_results.weights,
             optimization_config=optimization_config,
             search_space=None if immutable else base_gen_args.search_space,
             model_predictions=model_predictions,
-            best_arm_predictions=None
-            if best_arm is None
-            else (best_arm, best_point_predictions),
+            best_arm_predictions=(
+                None if best_arm is None else (best_arm, best_point_predictions)
+            ),
             fit_time=self.fit_time_since_gen,
             gen_time=time.monotonic() - t_gen_start,
             model_key=self._model_key,
             model_kwargs=self._model_kwargs,
             bridge_kwargs=self._bridge_kwargs,
             gen_metadata=gen_results.gen_metadata,
             model_state_after_gen=self._get_serialized_model_state(),
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/cross_validation.py` & `ax-platform-0.4.0/ax/modelbridge/cross_validation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+import warnings
 from abc import ABC, abstractmethod
 from collections import defaultdict
 from copy import deepcopy
 from enum import Enum
 from functools import partial
 
 from logging import Logger
 from numbers import Number
 from typing import Any, Callable, Dict, Iterable, List, NamedTuple, Optional, Set, Tuple
 
 import numpy as np
 from ax.core.experiment import Experiment
-from ax.core.observation import Observation, ObservationData, observations_from_data
+from ax.core.observation import Observation, ObservationData, recombine_observations
 from ax.core.optimization_config import OptimizationConfig
 from ax.modelbridge.base import ModelBridge, unwrap_observation_data
 from ax.utils.common.logger import get_logger
 
 from ax.utils.stats.model_fit_stats import (
     _correlation_coefficient,
     _fisher_exact_test_p,
@@ -33,14 +36,15 @@
     _wmape,
     coefficient_of_determination,
     compute_model_fit_metrics,
     mean_of_the_standardized_error,
     ModelFitMetricProtocol,
     std_of_the_standardized_error,
 )
+from botorch.exceptions.warnings import InputDataWarning
 
 logger: Logger = get_logger(__name__)
 
 CVDiagnostics = Dict[str, Dict[str, float]]
 
 MEAN_PREDICTION_CI = "Mean prediction CI"
 MAPE = "MAPE"
@@ -164,19 +168,28 @@
             (
                 cv_training_data,
                 cv_test_points,
                 search_space,
             ) = model._transform_inputs_for_cv(
                 cv_training_data=cv_training_data, cv_test_points=cv_test_points
             )
-            cv_test_predictions = model._cross_validate(
-                search_space=search_space,
-                cv_training_data=cv_training_data,
-                cv_test_points=cv_test_points,
-            )
+            with warnings.catch_warnings():
+                # Since each CV fold removes points from the training data, the
+                # remaining observations will not pass the standardization test.
+                # To avoid confusing users with this warning, we filter it out.
+                warnings.filterwarnings(
+                    "ignore",
+                    message="Input data is not standardized.",
+                    category=InputDataWarning,
+                )
+                cv_test_predictions = model._cross_validate(
+                    search_space=search_space,
+                    cv_training_data=cv_training_data,
+                    cv_test_points=cv_test_points,
+                )
             # Get test observations in transformed space
             cv_test_data = deepcopy(cv_test_data)
             for t in model.transforms.values():
                 cv_test_data = t.transform_observations(cv_test_data)
         # Form CVResult objects
         for i, obs in enumerate(cv_test_data):
             result.append(CVResult(observed=obs, predicted=cv_test_predictions[i]))
@@ -498,35 +511,36 @@
         generalization: Boolean indicating whether to compute the generalization
             metrics on cross-validation data or on the training data. The latter
             helps diagnose problems with model training, rather than generalization.
         untransform: Boolean indicating whether to untransform model predictions
             before calcualting the model fit metrics. False by default as models
             are trained in transformed space and model fit should be
             evaluated in transformed space.
+
     Returns:
         A nested dictionary mapping from the *model fit* metric names and the
         *experimental metric* names to the values of the model fit metrics.
 
         Example for an imaginary AutoML experiment that seeks to minimize the test
         error after training an expensive model, with respect to hyper-parameters:
 
         ```
         model_fit_dict = compute_model_fit_metrics_from_modelbridge(model_bridge, exp)
         model_fit_dict["coefficient_of_determination"]["test error"] =
             `coefficient of determination of the test error predictions`
         ```
     """
-    y_obs, y_pred, se_pred = (
-        _predict_on_cross_validation_data(
-            model_bridge=model_bridge, untransform=untransform
-        )
+    predict_func = (
+        _predict_on_cross_validation_data
         if generalization
-        else _predict_on_training_data(model_bridge=model_bridge, experiment=experiment)
+        else _predict_on_training_data
+    )
+    y_obs, y_pred, se_pred = predict_func(
+        model_bridge=model_bridge, untransform=untransform
     )
-
     if fit_metrics_dict is None:
         fit_metrics_dict = {
             "coefficient_of_determination": coefficient_of_determination,
             "mean_of_the_standardized_error": mean_of_the_standardized_error,
             "std_of_the_standardized_error": std_of_the_standardized_error,
         }
 
@@ -536,64 +550,84 @@
         se_pred=se_pred,
         fit_metrics_dict=fit_metrics_dict,
     )
 
 
 def _predict_on_training_data(
     model_bridge: ModelBridge,
-    experiment: Experiment,
-) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray],]:
+    untransform: bool = False,
+) -> Tuple[
+    Dict[str, np.ndarray],
+    Dict[str, np.ndarray],
+    Dict[str, np.ndarray],
+]:
     """Makes predictions on the training data of a given experiment using a ModelBridge
     and returning the observed values, and the corresponding predictive means and
     predictive standard deviations of the model, in transformed space.
 
     NOTE: This is a helper function for `compute_model_fit_metrics_from_modelbridge`.
 
     Args:
         model_bridge: A ModelBridge object with which to make predictions.
-        experiment: The experiment with whose data to compute the model fit metrics.
+        untransform: Boolean indicating whether to untransform model predictions.
 
     Returns:
         A tuple containing three dictionaries for 1) observed metric values, and the
         model's associated 2) predictive means and 3) predictive standard deviations.
     """
-    data = experiment.lookup_data()
-    observations = observations_from_data(
-        experiment=experiment, data=data
-    )  # List[Observation]
-    observation_features = [obs.features for obs in observations]
+    observations = model_bridge.get_training_data()  # List[Observation]
+
+    # NOTE: the following up to the end of the untransform block could be replaced
+    # with model_bridge's public predict / private _batch_predict method, if the
+    # latter had a boolean untransform flag.
 
-    # Transform observation features
-    observation_features = deepcopy(observation_features)
+    # Transform observations -- this will transform both obs data and features
     for t in model_bridge.transforms.values():
-        observation_features = t.transform_observation_features(observation_features)
+        observations = t.transform_observations(observations)
+
+    observation_features = [obs.features for obs in observations]
 
     # Make predictions in transformed space
     observation_data_pred = model_bridge._predict(observation_features)
 
+    if untransform:
+        # Apply reverse transforms, in reverse order
+        pred_observations = recombine_observations(
+            observation_features=observation_features,
+            observation_data=observation_data_pred,
+        )
+        for t in reversed(list(model_bridge.transforms.values())):
+            pred_observations = t.untransform_observations(pred_observations)
+
+        observation_data_pred = [obs.data for obs in pred_observations]
+
     mean_predicted, cov_predicted = unwrap_observation_data(observation_data_pred)
     mean_observed = [
         obs.data.means_dict for obs in observations
     ]  # List[Dict[str, float]]
 
-    metric_names = list(data.metric_names)
+    metric_names = observations[0].data.metric_names
     mean_observed = _list_of_dicts_to_dict_of_lists(
         list_of_dicts=mean_observed, keys=metric_names
     )
     # converting dictionary values to arrays
     mean_observed = {k: np.array(v) for k, v in mean_observed.items()}
     mean_predicted = {k: np.array(v) for k, v in mean_predicted.items()}
     std_predicted = {m: np.sqrt(np.array(cov_predicted[m][m])) for m in cov_predicted}
     return mean_observed, mean_predicted, std_predicted
 
 
 def _predict_on_cross_validation_data(
     model_bridge: ModelBridge,
     untransform: bool = False,
-) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray],]:
+) -> Tuple[
+    Dict[str, np.ndarray],
+    Dict[str, np.ndarray],
+    Dict[str, np.ndarray],
+]:
     """Makes leave-one-out cross-validation predictions on the training data of the
     ModelBridge and returns the observed values, and the corresponding predictive means
     and predictive standard deviations of the model as numpy arrays,
     in transformed space.
 
     NOTE: This is a helper function for `compute_model_fit_metrics_from_modelbridge`.
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/discrete.py` & `ax-platform-0.4.0/ax/modelbridge/discrete.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, Set, Tuple
 
 from ax.core.observation import (
     Observation,
     ObservationData,
     ObservationFeatures,
     separate_observations,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/dispatch_utils.py` & `ax-platform-0.4.0/ax/modelbridge/dispatch_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 import warnings
 from math import ceil
 from typing import Any, cast, Dict, Optional, Type, Union
 
 import torch
 from ax.core.experiment import Experiment
@@ -75,14 +77,15 @@
     ] = None,
     no_winsorization: bool = False,
     should_deduplicate: bool = False,
     verbose: Optional[bool] = None,
     disable_progbar: Optional[bool] = None,
     jit_compile: Optional[bool] = None,
     derelativize_with_raw_status_quo: bool = False,
+    fit_out_of_design: bool = False,
 ) -> GenerationStep:
     """Shortcut for creating a BayesOpt generation step."""
     model_kwargs = model_kwargs or {}
 
     winsorization_transform_config = _get_winsorization_transform_config(
         winsorization_config=winsorization_config,
         no_winsorization=no_winsorization,
@@ -92,14 +95,15 @@
     derelativization_transform_config = {
         "use_raw_status_quo": derelativize_with_raw_status_quo
     }
     model_kwargs["transform_configs"] = model_kwargs.get("transform_configs", {})
     model_kwargs["transform_configs"][
         "Derelativize"
     ] = derelativization_transform_config
+    model_kwargs["fit_out_of_design"] = fit_out_of_design
 
     if not no_winsorization:
         _, default_bridge_kwargs = model.view_defaults()
         default_transforms = default_bridge_kwargs["transforms"]
         transforms = model_kwargs.get("transforms", default_transforms)
         model_kwargs["transforms"] = [cast(Type[Transform], Winsorize)] + transforms
         if winsorization_transform_config is not None:
@@ -181,15 +185,15 @@
             else:
                 num_ordered_parameters += 1
         elif isinstance(parameter, RangeParameter):
             num_ordered_parameters += 1
             if parameter.parameter_type == ParameterType.FLOAT:
                 all_range_parameters_are_discrete = False
             else:
-                num_param_discrete_values = int(parameter.upper - parameter.lower) + 1
+                num_param_discrete_values = parameter.cardinality()
                 num_possible_points *= num_param_discrete_values
 
         if should_enumerate_param:
             num_enumerated_combinations *= not_none(num_param_discrete_values)
         else:
             all_parameters_are_enumerated = False
 
@@ -306,14 +310,15 @@
     should_deduplicate: bool = False,
     use_saasbo: bool = False,
     verbose: Optional[bool] = None,
     disable_progbar: Optional[bool] = None,
     jit_compile: Optional[bool] = None,
     experiment: Optional[Experiment] = None,
     suggested_model_override: Optional[ModelRegistryBase] = None,
+    fit_out_of_design: bool = False,
 ) -> GenerationStrategy:
     """Select an appropriate generation strategy based on the properties of
     the search space and expected settings of the experiment, such as number of
     arms per trial, optimization algorithm settings, expected number of trials
     in the experiment, etc.
 
     Args:
@@ -406,14 +411,15 @@
         experiment: If specified, ``_experiment`` attribute of the generation strategy
             will be set to this experiment (useful for associating a generation
             strategy with a given experiment before it's first used to ``gen`` with
             that experiment). Can also provide `optimization_config` if it is not
             provided as an arg to this function.
         suggested_model_override: If specified, this model will be used for the GP
             step and automatic selection will be skipped.
+        fit_out_of_design: Whether to include out-of-design points in the model.
     """
     if experiment is not None and optimization_config is None:
         optimization_config = experiment.optimization_config
 
     suggested_model = suggested_model_override or _suggest_gp_model(
         search_space=search_space,
         num_trials=num_trials,
@@ -501,14 +507,19 @@
         if jit_compile is not None and not model_is_saasbo:
             logger.warning(
                 f"Overriding `jit_compile = {jit_compile}` to `None` for "
                 "non-SAASBO GP step."
             )
             jit_compile = None
 
+        model_kwargs: Dict[str, Any] = {
+            "torch_device": torch_device,
+            "fit_out_of_design": fit_out_of_design,
+        }
+
         # Create `generation_strategy`, adding first Sobol step
         # if `num_remaining_initialization_trials` is > 0.
         if num_remaining_initialization_trials > 0:
             steps.append(
                 _make_sobol_step(
                     num_trials=num_remaining_initialization_trials,
                     min_trials_observed=min_sobol_trials_observed,
@@ -521,15 +532,15 @@
         steps.append(
             _make_botorch_step(
                 model=suggested_model,
                 winsorization_config=winsorization_config,
                 derelativize_with_raw_status_quo=derelativize_with_raw_status_quo,
                 no_winsorization=no_winsorization,
                 max_parallelism=bo_parallelism,
-                model_kwargs={"torch_device": torch_device},
+                model_kwargs=model_kwargs,
                 should_deduplicate=should_deduplicate,
                 verbose=verbose,
                 disable_progbar=disable_progbar,
                 jit_compile=jit_compile,
             ),
         )
         gs = GenerationStrategy(steps=steps)
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/factory.py` & `ax-platform-0.4.0/ax/modelbridge/factory.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
-from typing import Any, Dict, List, Optional, Type
+from typing import Dict, List, Optional, Type
 
 import torch
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.multi_type_experiment import MultiTypeExperiment
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
@@ -239,15 +241,14 @@
     transforms: List[Type[Transform]] = Cont_X_trans + Y_trans,
     transform_configs: Optional[Dict[str, TConfig]] = None,
     model_constructor: TModelConstructor = get_and_fit_model,
     model_predictor: TModelPredictor = predict_from_model,
     acqf_constructor: TAcqfConstructor = get_qLogNEI,
     acqf_optimizer: TOptimizer = scipy_optimizer,  # pyre-ignore[9]
     refit_on_cv: bool = False,
-    refit_on_update: bool = True,
     optimization_config: Optional[OptimizationConfig] = None,
 ) -> TorchModelBridge:
     """Instantiates a BotorchModel."""
     if data.df.empty:
         raise ValueError("`BotorchModel` requires non-empty data.")
     return checked_cast(
         TorchModelBridge,
@@ -260,15 +261,14 @@
             transforms=transforms,
             transform_configs=transform_configs,
             model_constructor=model_constructor,
             model_predictor=model_predictor,
             acqf_constructor=acqf_constructor,
             acqf_optimizer=acqf_optimizer,
             refit_on_cv=refit_on_cv,
-            refit_on_update=refit_on_update,
             optimization_config=optimization_config,
         ),
     )
 
 
 def get_GPEI(
     experiment: Experiment,
@@ -288,47 +288,14 @@
             search_space=search_space or experiment.search_space,
             torch_dtype=dtype,
             torch_device=device,
         ),
     )
 
 
-def get_GPKG(
-    experiment: Experiment,
-    data: Data,
-    search_space: Optional[SearchSpace] = None,
-    cost_intercept: float = 0.01,
-    dtype: torch.dtype = torch.double,
-    device: torch.device = DEFAULT_TORCH_DEVICE,
-    transforms: List[Type[Transform]] = Cont_X_trans + Y_trans,
-    transform_configs: Optional[Dict[str, TConfig]] = None,
-    **kwargs: Any,
-) -> TorchModelBridge:
-    """Instantiates a GP model that generates points with KG."""
-    if search_space is None:
-        search_space = experiment.search_space
-    if data.df.empty:
-        raise ValueError("GP+KG BotorchModel requires non-empty data.")
-
-    inputs = {
-        "search_space": search_space,
-        "experiment": experiment,
-        "data": data,
-        "cost_intercept": cost_intercept,
-        "torch_dtype": dtype,
-        "torch_device": device,
-        "transforms": transforms,
-        "transform_configs": transform_configs,
-    }
-
-    if any(p.is_fidelity for k, p in experiment.parameters.items()):
-        inputs["linear_truncated"] = kwargs.get("linear_truncated", True)
-    return checked_cast(TorchModelBridge, Models.GPKG(**inputs))  # pyre-ignore: [16]
-
-
 # TODO[Lena]: how to instantiate MTGP through the enum? The Multi-type MTGP requires
 # a MultiTypeExperiment, so we would need validation for that, but more importantly,
 # we need to create `trial_index_to_type` as in the factory function below.
 # Maybe `MultiTypeExperiment` could have that mapping as a property?
 def get_MTGP_LEGACY(
     experiment: Experiment,
     data: Data,
@@ -450,47 +417,14 @@
             min_weight=min_weight,
             uniform_weights=uniform_weights,
             fit_out_of_design=True,
         ),
     )
 
 
-def get_GPMES(
-    experiment: Experiment,
-    data: Data,
-    search_space: Optional[SearchSpace] = None,
-    cost_intercept: float = 0.01,
-    dtype: torch.dtype = torch.double,
-    device: torch.device = DEFAULT_TORCH_DEVICE,
-    transforms: List[Type[Transform]] = Cont_X_trans + Y_trans,
-    transform_configs: Optional[Dict[str, TConfig]] = None,
-    **kwargs: Any,
-) -> TorchModelBridge:
-    """Instantiates a GP model that generates points with MES."""
-    if search_space is None:
-        search_space = experiment.search_space
-    if data.df.empty:
-        raise ValueError("GP + MES BotorchModel requires non-empty data.")
-
-    inputs = {
-        "search_space": search_space,
-        "experiment": experiment,
-        "data": data,
-        "cost_intercept": cost_intercept,
-        "torch_dtype": dtype,
-        "torch_device": device,
-        "transforms": transforms,
-        "transform_configs": transform_configs,
-    }
-
-    if any(p.is_fidelity for k, p in experiment.parameters.items()):
-        inputs["linear_truncated"] = kwargs.get("linear_truncated", True)
-    return checked_cast(TorchModelBridge, Models.GPMES(**inputs))  # pyre-ignore: [16]
-
-
 def get_MOO_EHVI(
     experiment: Experiment,
     data: Data,
     search_space: Optional[SearchSpace] = None,
     dtype: torch.dtype = torch.double,
     device: Optional[torch.device] = None,
     optimization_config: Optional[OptimizationConfig] = None,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/generation_node.py` & `ax-platform-0.4.0/ax/modelbridge/generation_node.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import dataclass, field
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Tuple, Union
 
 # Module-level import to avoid circular dependency b/w this file and
@@ -18,18 +20,17 @@
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import UserInputError
-
 from ax.exceptions.generation_strategy import GenerationStrategyRepeatedPoints
 from ax.modelbridge.base import ModelBridge
-from ax.modelbridge.cross_validation import BestModelSelector, CVDiagnostics, CVResult
+from ax.modelbridge.cross_validation import BestModelSelector
 from ax.modelbridge.model_spec import FactoryFunctionModelSpec, ModelSpec
 from ax.modelbridge.registry import ModelRegistryBase
 from ax.modelbridge.transition_criterion import (
     MaxGenerationParallelism,
     MaxTrials,
     MinTrials,
     TransitionCriterion,
@@ -75,28 +76,25 @@
             assume that the optimization converged when the model can no longer suggest
             unique arms.
         node_name: A unique name for the GenerationNode. Used for storage purposes.
         transition_criteria: List of TransitionCriterion, each of which describes a
             condition that must be met before completing a GenerationNode. All `is_met`
             must evaluateTrue for the GenerationStrategy to move on to the next
             GenerationNode.
-        gen_unlimited_trials: If True the number of trials that can be generated from
-            this GenerationNode is unlimited.
 
     Note for developers: by "model" here we really mean an Ax ModelBridge object, which
     contains an Ax Model under the hood. We call it "model" here to simplify and focus
     on explaining the logic of GenerationStep and GenerationStrategy.
     """
 
     # Required options:
     model_specs: List[ModelSpec]
     # TODO: Move `should_deduplicate` to `ModelSpec` if possible, and make optional
     should_deduplicate: bool
     _node_name: str
-    _gen_unlimited_trials: bool = True
 
     # Optional specifications
     _model_spec_to_gen_from: Optional[ModelSpec] = None
     # TODO: @mgarrard should this be a dict criterion_class name -> criterion mapping?
     _transition_criteria: Optional[Sequence[TransitionCriterion]]
 
     # [TODO] Handle experiment passing more eloquently by enforcing experiment
@@ -108,27 +106,25 @@
     def __init__(
         self,
         node_name: str,
         model_specs: List[ModelSpec],
         best_model_selector: Optional[BestModelSelector] = None,
         should_deduplicate: bool = False,
         transition_criteria: Optional[Sequence[TransitionCriterion]] = None,
-        gen_unlimited_trials: bool = True,
     ) -> None:
         self._node_name = node_name
         # While `GenerationNode` only handles a single `ModelSpec` in the `gen`
         # and `_pick_fitted_model_to_gen_from` methods, we validate the
         # length of `model_specs` in `_pick_fitted_model_to_gen_from` in order
         # to not require all `GenerationNode` subclasses to override an `__init__`
         # method to bypass that validation.
         self.model_specs = model_specs
         self.best_model_selector = best_model_selector
         self.should_deduplicate = should_deduplicate
         self._transition_criteria = transition_criteria
-        self._gen_unlimited_trials = gen_unlimited_trials
 
     @property
     def node_name(self) -> str:
         """Returns the unique name of this GenerationNode"""
         return self._node_name
 
     @property
@@ -139,62 +135,22 @@
         if self._model_spec_to_gen_from is not None:
             return self._model_spec_to_gen_from
 
         self._model_spec_to_gen_from = self._pick_fitted_model_to_gen_from()
         return self._model_spec_to_gen_from
 
     @property
-    def model_enum(self) -> ModelRegistryBase:
-        """model_enum from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.model_enum
-
-    @property
-    def model_kwargs(self) -> Optional[Dict[str, Any]]:
-        """model_kwargs from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.model_kwargs
-
-    @property
-    def model_gen_kwargs(self) -> Optional[Dict[str, Any]]:
-        """model_gen_kwargs from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.model_gen_kwargs
-
-    @property
-    def model_cv_kwargs(self) -> Optional[Dict[str, Any]]:
-        """model_cv_kwargs from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.model_cv_kwargs
-
-    @property
-    def fitted_model(self) -> ModelBridge:
-        """fitted_model from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.fitted_model
-
-    @property
-    def fixed_features(self) -> Optional[ObservationFeatures]:
-        """fixed_features from self.model_spec_to_gen_from for convenience"""
-        if len({model_spec.fixed_features for model_spec in self.model_specs}) == 1:
-            return self.model_specs[0].fixed_features
-
-        return self.model_spec_to_gen_from.fixed_features
-
-    @property
-    def cv_results(self) -> Optional[List[CVResult]]:
-        """cv_results from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.cv_results
-
-    @property
-    def diagnostics(self) -> Optional[CVDiagnostics]:
-        """diagnostics from self.model_spec_to_gen_from for convenience"""
-        return self.model_spec_to_gen_from.diagnostics
-
-    @property
     def model_to_gen_from_name(self) -> Optional[str]:
         """Returns the name of the model that will be used for gen, if there is one.
         Otherwise, returns None.
         """
-        return self.model_spec_to_gen_from.model_key
+        if self._model_spec_to_gen_from is not None:
+            return self._model_spec_to_gen_from.model_key
+        else:
+            return None
 
     @property
     def generation_strategy(self) -> modelbridge.generation_strategy.GenerationStrategy:
         """Returns a backpointer to the GenerationStrategy, useful for obtaining the
         experiment associated with this GenerationStrategy"""
         # TODO: @mgarrard remove this property once we make experiment a required
         # arguement on GenerationStrategy
@@ -213,28 +169,25 @@
 
     @property
     def experiment(self) -> Experiment:
         """Returns the experiment associated with this GenerationStrategy"""
         return self.generation_strategy.experiment
 
     @property
-    def gen_unlimited_trials(self) -> bool:
-        """If True, this GenerationNode can generate unlimited trials."""
-        return self._gen_unlimited_trials
-
-    @property
     def is_completed(self) -> bool:
         """Returns True if this GenerationNode is complete and should transition to
         the next node.
         """
         return self.should_transition_to_next_node(raise_data_required_error=False)[0]
 
     @property
     def _unique_id(self) -> str:
-        """Returns a unique id for this GenerationNode"""
+        """Returns a unique (w.r.t. parent class: ``GenerationStrategy``) id
+        for this GenerationNode. Used for SQL storage.
+        """
         return self.node_name
 
     @property
     def _fitted_model(self) -> Optional[ModelBridge]:
         """Private property to return optional fitted_model from
         self.model_spec_to_gen_from for convenience. If no model is fit,
         will return None. If using the non-private `fitted_model` property,
@@ -250,77 +203,81 @@
         optimization_config: Optional[OptimizationConfig] = None,
         **kwargs: Any,
     ) -> None:
         """Fits the specified models to the given experiment + data using
         the model kwargs set on each corresponding model spec and the kwargs
         passed to this method.
 
-        NOTE: Local kwargs take precedence over the ones stored in
-        ``ModelSpec.model_kwargs``.
+        Args:
+            experiment: The experiment to fit the model to.
+            data: The experiment data used to fit the model.
+            search_space: An optional overwrite for the experiment search space.
+            optimization_config: An optional overwrite for the experiment
+                optimization config.
+            kwargs: Additional keyword arguments to pass to the model's
+                ``fit`` method. NOTE: Local kwargs take precedence over the ones
+                stored in ``ModelSpec.model_kwargs``.
         """
         self._model_spec_to_gen_from = None
         for model_spec in self.model_specs:
             model_spec.fit(  # Stores the fitted model as `model_spec._fitted_model`
                 experiment=experiment,
                 data=data,
                 search_space=search_space,
                 optimization_config=optimization_config,
                 **kwargs,
             )
 
+    # TODO [drfreund]: Move this up to `GenerationNodeInterface` once implemented.
     def gen(
         self,
         n: Optional[int] = None,
         pending_observations: Optional[Dict[str, List[ObservationFeatures]]] = None,
         max_gen_draws_for_deduplication: int = MAX_GEN_DRAWS,
         arms_by_signature_for_deduplication: Optional[Dict[str, Arm]] = None,
         **model_gen_kwargs: Any,
     ) -> GeneratorRun:
-        """Picks a fitted model, from which to generate candidates (via
-        ``self._pick_fitted_model_to_gen_from``) and generates candidates
-        from it. Uses the ``model_gen_kwargs`` set on the selected ``ModelSpec``
-        alongside any kwargs passed in to this function (with local kwargs)
-        taking precedent.
+        """This method generates candidates using `self._gen` and handles deduplication
+        of generated candidates if `self.should_deduplicate=True`.
+
+        NOTE: Models must have been fit prior to calling ``gen``.
+        NOTE: Some underlying models may ignore the ``n`` argument and produce a
+            model-determined number of arms. In that case this method will also output
+            a generator run with number of arms that may differ from ``n``.
 
         Args:
-            n: Optional nteger representing how many arms should be in the generator
+            n: Optional integer representing how many arms should be in the generator
                 run produced by this method. When this is ``None``, ``n`` will be
                 determined by the ``ModelSpec`` that we are generating from.
             pending_observations: A map from metric name to pending
                 observations for that metric, used by some models to avoid
                 resuggesting points that are currently being evaluated.
-            max_gen_draws_for_deduplication: TODO
+            max_gen_draws_for_deduplication: Maximum number of attempts for generating
+                new candidates without duplicates. If non-duplicate candidates are not
+                generated with these attempts, a ``GenerationStrategyRepeatedPoints``
+                exception will be raised.
+            arms_by_signature_for_deduplication: A dictionary mapping arm signatures to
+                the arms, to be used for deduplicating newly generated arms.
             model_gen_kwargs: Keyword arguments, passed through to ``ModelSpec.gen``;
                 these override any pre-specified in ``ModelSpec.model_gen_kwargs``.
 
-        NOTE: Models must have been fit prior to calling ``gen``.
-        NOTE: Some underlying models may ignore the ``n`` argument and produce a
-            model-determined number of arms. In that case this method will also output
-            a generator run with number of arms (that can differ from ``n``).
+        Returns:
+            A ``GeneratorRun`` containing the newly generated candidates.
         """
-        model_spec = self.model_spec_to_gen_from
         should_generate_run = True
         generator_run = None
         n_gen_draws = 0
         # Keep generating until each of `generator_run.arms` is not a duplicate
         # of a previous arm, if `should_deduplicate is True`
         while should_generate_run:
-            generator_run = model_spec.gen(
-                # If `n` is not specified, ensure that the `None` value does not
-                # override the one set in `model_spec.model_gen_kwargs`.
-                n=model_spec.model_gen_kwargs.get("n")
-                if n is None and model_spec.model_gen_kwargs
-                else n,
-                # For `pending_observations`, prefer the input to this function, as
-                # `pending_observations` are dynamic throughout the experiment and thus
-                # unlikely to be specified in `model_spec.model_gen_kwargs`.
+            generator_run = self._gen(
+                n=n,
                 pending_observations=pending_observations,
                 **model_gen_kwargs,
             )
-
             should_generate_run = (
                 self.should_deduplicate
                 and arms_by_signature_for_deduplication
                 and any(
                     arm.signature in arms_by_signature_for_deduplication
                     for arm in generator_run.arms
                 )
@@ -340,14 +297,53 @@
         assert generator_run is not None, (
             "The GeneratorRun is None which is an unexpected state of this"
             " GenerationStrategy. This occured on GenerationNode: {self.node_name}."
         )
         generator_run._generation_node_name = self.node_name
         return generator_run
 
+    def _gen(
+        self,
+        n: Optional[int] = None,
+        pending_observations: Optional[Dict[str, List[ObservationFeatures]]] = None,
+        **model_gen_kwargs: Any,
+    ) -> GeneratorRun:
+        """Picks a fitted model, from which to generate candidates (via
+        ``self._pick_fitted_model_to_gen_from``) and generates candidates
+        from it. Uses the ``model_gen_kwargs`` set on the selected ``ModelSpec``
+        alongside any kwargs passed in to this function (with local kwargs)
+        taking precedent.
+
+        Args:
+            n: Optional integer representing how many arms should be in the generator
+                run produced by this method. When this is ``None``, ``n`` will be
+                determined by the ``ModelSpec`` that we are generating from.
+            pending_observations: A map from metric name to pending
+                observations for that metric, used by some models to avoid
+                resuggesting points that are currently being evaluated.
+            model_gen_kwargs: Keyword arguments, passed through to ``ModelSpec.gen``;
+                these override any pre-specified in ``ModelSpec.model_gen_kwargs``.
+
+        Returns:
+            A ``GeneratorRun`` containing the newly generated candidates.
+        """
+        model_spec = self.model_spec_to_gen_from
+        if n is None and model_spec.model_gen_kwargs:
+            # If `n` is not specified, ensure that the `None` value does not
+            # override the one set in `model_spec.model_gen_kwargs`.
+            n = model_spec.model_gen_kwargs.get("n", None)
+        return model_spec.gen(
+            n=n,
+            # For `pending_observations`, prefer the input to this function, as
+            # `pending_observations` are dynamic throughout the experiment and thus
+            # unlikely to be specified in `model_spec.model_gen_kwargs`.
+            pending_observations=pending_observations,
+            **model_gen_kwargs,
+        )
+
     # ------------------------- Model selection logic helpers. -------------------------
 
     def _pick_fitted_model_to_gen_from(self) -> ModelSpec:
         """Select one model to generate from among the fitted models on this
         generation node.
 
         NOTE: In base ``GenerationNode`` class, this method does the following:
@@ -374,20 +370,17 @@
     @property
     def trials_from_node(self) -> Set[int]:
         """Returns a dictionary mapping a GenerationNode to the trials it generated.
 
         Returns:
             Set[int]: A set containing all the trials indices generated by this node.
         """
-        # TODO: @mgarrard simplify this method after generation_node_name added to
-        # BaseTrial
         trials_from_node = set()
         for _idx, trial in self.experiment.trials.items():
-            generator_runs_from_trial = trial.generator_runs
-            for gr in generator_runs_from_trial:
+            for gr in trial.generator_runs:
                 if (
                     gr._generation_node_name is not None
                     and gr._generation_node_name == self.node_name
                 ):
                     trials_from_node.add(trial.index)
         return trials_from_node
 
@@ -401,75 +394,71 @@
             raise_data_required_error: Whether to raise ``DataRequiredError`` in the
                 case detailed above. Not raising the error is useful if just looking to
                 check how many generator runs (to be made into trials) can be produced,
                 but not actually producing them yet.
         Returns:
             bool: Whether we should transition to the next node.
         """
-        if self.gen_unlimited_trials and len(self.transition_criteria) == 0:
+        # if no transition criteria are defined, this node can generate unlimited trials
+        if len(self.transition_criteria) == 0:
             return False, None
 
-        transition_blocking_criterion = [
-            criterion
-            for criterion in self.transition_criteria
-            if criterion.block_transition_if_unmet
+        transition_blocking = [
+            tc for tc in self.transition_criteria if tc.block_transition_if_unmet
         ]
-        all_transition_blocking_criteria_are_met = all(
-            transition_criterion.is_met(
-                self.experiment,
+        transition_blocking_met = all(
+            tc.is_met(
+                experiment=self.experiment,
                 trials_from_node=self.trials_from_node,
             )
-            for transition_criterion in transition_blocking_criterion
+            for tc in transition_blocking
         )
         # Raise any necessary generation errors: for any met criterion,
         # call its `block_continued_generation_error` method if not all
         # transition-blocking criteria are met. The method might not raise an
         # error, depending on its implementation on given criterion, so the error
         # from the first met one that does block continued generation, will be raised.
-        if not all_transition_blocking_criteria_are_met:
-            for criterion in self.transition_criteria:
+        if not transition_blocking_met:
+            for tc in self.transition_criteria:
                 if (
-                    criterion.is_met(
-                        self.experiment, trials_from_node=self.trials_from_node
-                    )
+                    tc.is_met(self.experiment, trials_from_node=self.trials_from_node)
                     and raise_data_required_error
                 ):
-                    criterion.block_continued_generation_error(
+                    tc.block_continued_generation_error(
                         node_name=self.node_name,
                         model_name=self.model_to_gen_from_name,
                         experiment=self.experiment,
                         trials_from_node=self.trials_from_node,
                     )
 
         # Determine transition state
-        if (
-            len(transition_blocking_criterion) > 0
-            and all_transition_blocking_criteria_are_met
-        ):
-            transition_nodes = [
-                criterion.transition_to
-                for criterion in transition_blocking_criterion
-                if criterion._transition_to is not None
+        if len(transition_blocking) > 0 and transition_blocking_met:
+            next_nodes = [
+                c.transition_to
+                for c in transition_blocking
+                if c._transition_to is not None
             ]
-            if len(set(transition_nodes)) > 1:
+            if len(set(next_nodes)) > 1:
                 # TODO: support intelligent selection between multiple transition nodes
                 raise NotImplementedError(
-                    "Cannot currently select between multiple transition nodes."
+                    "Cannot currently select between multiple nodes to transition to."
                 )
-            return True, transition_nodes[0]
+            else:
+                return True, next_nodes[0]
+
         return False, None
 
     def generator_run_limit(self, supress_generation_errors: bool = True) -> int:
         """How many generator runs can this generation strategy generate right now,
         assuming each one of them becomes its own trial. Only considers
         `transition_criteria` that are TrialBasedCriterion.
 
         Returns:
-              - the number of generator runs that can currently be produced, with -1
-                meaning unlimited generator runs,
+            The number of generator runs that can currently be produced, with -1
+            meaning unlimited generator runs.
         """
         # TODO: @mgarrard Should we consider returning `None` if there is no limit?
         # TODO:@mgarrard Should we instead have `raise_generation_error`? The name
         # of this method doesn't suggest that it would raise errors by default, since
         # it's just finding out the limit according to the name. I know we want the
         # errors in some cases, so we could call the flag `raise_error_if_cannot_gen` or
         # something like that : )
@@ -500,33 +489,25 @@
                     criterion.block_continued_generation_error(
                         node_name=self.node_name,
                         model_name=self.model_to_gen_from_name,
                         experiment=self.experiment,
                         trials_from_node=self.trials_from_node,
                     )
         if len(gen_blocking_criterion_delta_from_threshold) == 0:
-            if not self.gen_unlimited_trials:
-                logger.warning(
-                    "Even though this node is not flagged for generation of unlimited "
-                    "trials, there are no generation blocking criterion, therefore, "
-                    "unlimited trials will be generated."
-                )
             return -1
         return min(gen_blocking_criterion_delta_from_threshold)
 
     def __repr__(self) -> str:
         "String representation of this GenerationNode"
         # add model specs
         str_rep = f"{self.__class__.__name__}(model_specs="
         model_spec_str = str(self.model_specs).replace("\n", " ").replace("\t", "")
         str_rep += model_spec_str
 
-        # add node name, gen_unlimited_trials, and transition_criteria
         str_rep += f", node_name={self.node_name}"
-        str_rep += f", gen_unlimited_trials={str(self.gen_unlimited_trials)}"
         str_rep += f", transition_criteria={str(self.transition_criteria)}"
 
         return f"{str_rep})"
 
 
 @dataclass
 class GenerationStep(GenerationNode, SortableBase):
@@ -666,25 +647,23 @@
         # to ensure that requirements related to num_trials and unlimited trials
         # are met. MinimumTrialsInStatus can be used enforce the min_trials_observed
         # requirement, and override MaxTrials if enforce flag is set to true. We set
         # `transition_to` is set in `GenerationStrategy` constructor,
         # because only then is the order of the generation steps actually known.
         transition_criteria = []
         if self.num_trials != -1:
-            gen_unlimited_trials = False
             transition_criteria.append(
                 MaxTrials(
                     threshold=self.num_trials,
                     not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
                     block_gen_if_met=self.enforce_num_trials,
                     block_transition_if_unmet=True,
                 )
             )
-        else:
-            gen_unlimited_trials = True
+
         if self.min_trials_observed > 0:
             transition_criteria.append(
                 MinTrials(
                     only_in_statuses=[
                         TrialStatus.COMPLETED,
                         TrialStatus.EARLY_STOPPED,
                     ],
@@ -700,24 +679,20 @@
                     only_in_statuses=[TrialStatus.RUNNING],
                     block_gen_if_met=True,
                     block_transition_if_unmet=False,
                     transition_to=None,
                 )
             )
 
-        if len(self.completion_criteria) > 0:
-            gen_unlimited_trials = False
-
         transition_criteria += self.completion_criteria
         super().__init__(
             node_name=f"GenerationStep_{str(self.index)}",
             model_specs=[model_spec],
             should_deduplicate=self.should_deduplicate,
             transition_criteria=transition_criteria,
-            gen_unlimited_trials=gen_unlimited_trials,
         )
 
     @property
     def model_spec(self) -> ModelSpec:
         """Returns the first model_spec from the model_specs attribute."""
         return self.model_specs[0]
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/generation_strategy.py` & `ax-platform-0.4.0/ax/modelbridge/generation_strategy.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from copy import deepcopy
 from functools import wraps
-
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar
 
 import pandas as pd
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generation_strategy_interface import GenerationStrategyInterface
@@ -23,23 +24,23 @@
     get_pending_observation_features_based_on_trial_status,
 )
 from ax.exceptions.core import DataRequiredError, UnsupportedError, UserInputError
 from ax.exceptions.generation_strategy import (
     GenerationStrategyCompleted,
     GenerationStrategyMisconfiguredException,
 )
-
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.generation_node import GenerationNode, GenerationStep
 from ax.modelbridge.model_spec import FactoryFunctionModelSpec
 from ax.modelbridge.modelbridge_utils import get_fixed_features_from_experiment
 from ax.modelbridge.registry import _extract_model_state_after_gen, ModelRegistryBase
 from ax.modelbridge.transition_criterion import TrialBasedCriterion
 from ax.utils.common.logger import _round_floats_for_logging, get_logger
-from ax.utils.common.typeutils import checked_cast, not_none
+from ax.utils.common.typeutils import checked_cast, checked_cast_list, not_none
+from pyre_extensions import none_throws
 
 logger: Logger = get_logger(__name__)
 
 
 MAX_CONDITIONS_GENERATED = 10000
 MAX_GEN_DRAWS = 5
 MAX_GEN_DRAWS_EXCEEDED_MESSAGE = (
@@ -48,23 +49,22 @@
     "likely been fully explored, or that the sweep has converged."
 )
 T = TypeVar("T")
 
 
 def step_based_gs_only(f: Callable[..., T]) -> Callable[..., T]:
     """
-    For use as a decorator on functions only implemented for GenerationStep based
-    GenerationStrategies. Mainly useful for older GenerationStrategies.
+    For use as a decorator on functions only implemented for ``GenerationStep``-based
+    ``GenerationStrategies``. Mainly useful for older ``GenerationStrategies``.
     """
 
     @wraps(f)
     def impl(
         self: "GenerationStrategy", *args: List[Any], **kwargs: Dict[str, Any]
     ) -> T:
-
         if self.is_node_based:
             raise UnsupportedError(
                 f"{f.__name__} is not supported for GenerationNode based"
                 " GenerationStrategies."
             )
         return f(self, *args, **kwargs)
 
@@ -90,122 +90,106 @@
             should use nodes. Notably, either, but not both, of `nodes` and `steps`
             must be provided.
         steps: A list of `GenerationStep` describing steps of this strategy.
         name: An optional name for this generaiton strategy. If not specified,
             strategy's name will be names of its nodes' models joined with '+'.
     """
 
-    _name: Optional[str]
     _nodes: List[GenerationNode]
     _curr: GenerationNode  # Current node in the strategy.
     # Whether all models in this GS are in Models registry enum.
     _uses_registered_models: bool
     # All generator runs created through this generation strategy, in chronological
     # order.
     _generator_runs: List[GeneratorRun]
     # Experiment, for which this generation strategy has generated trials, if
     # it exists.
     _experiment: Optional[Experiment] = None
     # Trial indices as last seen by the model; updated in `_model` property setter.
-    # pyre-fixme[4]: Attribute must be annotated.
-    _seen_trial_indices_by_status = None
     _model: Optional[ModelBridge] = None  # Current model.
 
     def __init__(
         self,
         steps: Optional[List[GenerationStep]] = None,
         name: Optional[str] = None,
         nodes: Optional[List[GenerationNode]] = None,
     ) -> None:
-        self._name = name
-        self._uses_registered_models = True
-        self._generator_runs = []
-
-        # validate that only one of steps or nodes is provided
+        # Validate that one and only one of steps or nodes is provided
         if not ((steps is None) ^ (nodes is None)):
             raise GenerationStrategyMisconfiguredException(
                 error_info="GenerationStrategy must contain either steps or nodes."
             )
+
         # pyre-ignore[8]
-        self._nodes = steps if steps is not None else nodes
-        node_based_strategy = self.is_node_based
+        self._nodes = none_throws(nodes if steps is None else steps)
 
-        if isinstance(steps, list) and not node_based_strategy:
+        # Validate correctness of steps list or nodes graph
+        if isinstance(steps, list) and all(
+            isinstance(s, GenerationStep) for s in steps
+        ):
             self._validate_and_set_step_sequence(steps=self._nodes)
-        elif isinstance(nodes, list) and node_based_strategy:
+        elif isinstance(nodes, list) and self.is_node_based:
             self._validate_and_set_node_graph(nodes=nodes)
         else:
+            # TODO[mgarrard]: Allow mix of nodes and steps
             raise GenerationStrategyMisconfiguredException(
-                error_info="Steps must either be a GenerationStep list or a "
-                + "GenerationNode list."
+                "`GenerationStrategy` inputs are:\n"
+                "`steps` (list of `GenerationStep`) or\n"
+                "`nodes` (list of `GenerationNode`)."
             )
+
+        # Log warning if the GS uses a non-registered (factory function) model.
         self._uses_registered_models = not any(
             isinstance(ms, FactoryFunctionModelSpec)
             for node in self._nodes
             for ms in node.model_specs
         )
         if not self._uses_registered_models:
             logger.info(
                 "Using model via callable function, "
                 "so optimization is not resumable if interrupted."
             )
-        self._seen_trial_indices_by_status = None
+        self._generator_runs = []
         # Set name to an explicit value ahead of time to avoid
         # adding properties during equality checks
-        self._name = self.name
+        super().__init__(name=name or self._make_default_name())
 
     @property
     def is_node_based(self) -> bool:
-        """Whether this strategy consists of GenerationNodes or GenerationSteps.
-        This is useful for determining initialization properties and other logic.
+        """Whether this strategy consists of GenerationNodes only.
+        This is useful for determining initialization properties and
+        other logic.
         """
-        if any(isinstance(n, GenerationStep) for n in self._nodes):
-            return False
-        return True
+        return not any(isinstance(n, GenerationStep) for n in self._nodes) and all(
+            isinstance(n, GenerationNode) for n in self._nodes
+        )
 
     @property
     def name(self) -> str:
         """Name of this generation strategy. Defaults to a combination of model
-        names provided in generation steps.
+        names provided in generation steps, set at the time of the
+        ``GenerationStrategy`` creation.
         """
-        if self._name is not None:
-            return not_none(self._name)
-
-        factory_names = (node.model_spec_to_gen_from.model_key for node in self._nodes)
-        # Trim the "get_" beginning of the factory function if it's there.
-        factory_names = (n[4:] if n[:4] == "get_" else n for n in factory_names)
-        self._name = "+".join(factory_names)
-        return not_none(self._name)
+        return self._name
 
     @name.setter
     def name(self, name: str) -> None:
         """Set generation strategy name."""
         self._name = name
 
     @property
     @step_based_gs_only
     def model_transitions(self) -> List[int]:
-        """List of trial indices where a transition happened from one model to
-        another."""
-        # TODO @mgarrard to support GenerationNodes here, which is non-trival
-        # since nodes are dynamic and may only support past model_transitions
-        gen_changes = []
-        for node in self._nodes:
-            for criterion in node.transition_criteria:
-                if (
-                    isinstance(criterion, TrialBasedCriterion)
-                    and criterion.criterion_class == "MaxTrials"
-                ):
-                    gen_changes.append(criterion.threshold)
-
-        # if the last node has unlimited generation, do not remeove the last
-        # transition point in the list
-        if self._nodes[-1].gen_unlimited_trials:
-            return [sum(gen_changes[: i + 1]) for i in range(len(gen_changes))]
-        return [sum(gen_changes[: i + 1]) for i in range(len(gen_changes))][:-1]
+        """[DEPRECATED]List of trial indices where a transition happened from one model
+        to another.
+        """
+        raise DeprecationWarning(
+            "`model_transitions` is no longer supported. Please refer to `model_key` "
+            "field on generator runs for similar information if needed."
+        )
 
     @property
     def current_step(self) -> GenerationStep:
         """Current generation step."""
         if not isinstance(self._curr, GenerationStep):
             raise TypeError(
                 "The current object is not a GenerationStep, you may be looking "
@@ -270,14 +254,22 @@
                 "This generation strategy has been used for experiment "
                 f"{self.experiment._name} so far; cannot reset experiment"
                 f" to {experiment._name}. If this is a new optimization, "
                 "a new generation strategy should be created instead."
             )
 
     @property
+    def last_generator_run(self) -> Optional[GeneratorRun]:
+        """Latest generator run produced by this generation strategy.
+        Returns None if no generator runs have been produced yet.
+        """
+        # Used to restore current model when decoding a serialized GS.
+        return self._generator_runs[-1] if self._generator_runs else None
+
+    @property
     def uses_non_registered_models(self) -> bool:
         """Whether this generation strategy involves models that are not
         registered and therefore cannot be stored."""
         return not self._uses_registered_models
 
     @property
     def trials_as_df(self) -> Optional[pd.DataFrame]:
@@ -327,117 +319,17 @@
                 "Trial Status",
                 "Arm Parameterizations",
             ]
         )
 
     @property
     def optimization_complete(self) -> bool:
-        """Checks whether all nodes are completed in the generation strategy and
-        the current node is not an AEPsych node.
-        """
+        """Checks whether all nodes are completed in the generation strategy."""
         return all(node.is_completed for node in self._nodes)
 
-    @step_based_gs_only
-    def _validate_and_set_step_sequence(self, steps: List[GenerationStep]) -> None:
-        """Initialize and validate the steps provided to this GenerationStrategy.
-
-        Some GenerationStrategies are composed of GenerationStep objects, but we also
-        need to initialize the correct GenerationNode representation for these steps.
-        This function validates:
-            1. That only the last step has num_trials=-1, which indicates unlimited
-               trial generation is possible.
-            2. That each step's num_trials attrivute is either positive or -1
-            3. That each step's max_parallelism attribute is either None or positive
-        It then sets the corect TransitionCriterion and node_name attributes on the
-        underlying GenerationNode objects.
-        """
-        for idx, step in enumerate(steps):
-            if step.num_trials == -1 and len(step.completion_criteria) < 1:
-                if idx < len(self._steps) - 1:
-                    raise UserInputError(
-                        "Only last step in generation strategy can have "
-                        "`num_trials` set to -1 to indicate that the model in "
-                        "the step shouldbe used to generate new trials "
-                        "indefinitely unless completion critera present."
-                    )
-            elif step.num_trials < 1 and step.num_trials != -1:
-                raise UserInputError(
-                    "`num_trials` must be positive or -1 (indicating unlimited) "
-                    "for all generation steps."
-                )
-            if step.max_parallelism is not None and step.max_parallelism < 1:
-                raise UserInputError(
-                    "Maximum parallelism should be None (if no limit) or "
-                    f"a positive number. Got: {step.max_parallelism} for "
-                    f"step {step.model_name}."
-                )
-
-            step._node_name = f"GenerationStep_{str(idx)}"
-            step.index = idx
-
-            # Set transition_to field for all but the last step, which remains
-            # null.
-            if idx != len(self._steps):
-                for transition_criteria in step.transition_criteria:
-                    if (
-                        transition_criteria.criterion_class
-                        != "MaxGenerationParallelism"
-                    ):
-                        transition_criteria._transition_to = (
-                            f"GenerationStep_{str(idx + 1)}"
-                        )
-            step._generation_strategy = self
-        self._curr = steps[0]
-
-    def _validate_and_set_node_graph(self, nodes: List[GenerationNode]) -> None:
-        """Initialize and validate the node graph provided to this GenerationStrategy.
-
-        This function validates:
-            1. That all nodes have unique names.
-            2. That there is at least one node with a transition_to field.
-            3. That all `transition_to` attributes on a TransitionCriterion point to
-                another node in the same GenerationStrategy.
-            4. Warns if no nodes contain a transition criterion
-        """
-        node_names = []
-        for node in self._nodes:
-            # validate that all node names are unique
-            if node.node_name in node_names:
-                raise GenerationStrategyMisconfiguredException(
-                    error_info="All node names in a GenerationStrategy "
-                    + "must be unique."
-                )
-
-            node_names.append(node.node_name)
-            node._generation_strategy = self
-
-        # validate `transition_criterion`
-        contains_a_transition_to_argument = False
-        for node in self._nodes:
-            for transition_criteria in node.transition_criteria:
-                if transition_criteria.transition_to is not None:
-                    contains_a_transition_to_argument = True
-                    if transition_criteria.transition_to not in node_names:
-                        raise GenerationStrategyMisconfiguredException(
-                            error_info=f"`transition_to` argument "
-                            f"{transition_criteria.transition_to} does not "
-                            "correspond to any node in this GenerationStrategy."
-                        )
-
-        # validate that at least one node has transition_to field
-        if len(self._nodes) > 1 and not contains_a_transition_to_argument:
-            logger.warning(
-                "None of the nodes in this GenerationStrategy "
-                "contain a `transition_to` argument in their transition_criteria. "
-                "Therefore, the GenerationStrategy will not be able to "
-                "move from one node to another. Please add a "
-                "`transition_to` argument."
-            )
-        self._curr = nodes[0]
-
     @property
     @step_based_gs_only
     def _steps(self) -> List[GenerationStep]:
         """List of generation steps."""
         return self._nodes  # pyre-ignore[7]
 
     def gen(
@@ -550,42 +442,136 @@
             return 0, True
 
         # if the generation strategy is not complete, optimization is not complete
         return self._curr.generator_run_limit(), False
 
     def clone_reset(self) -> GenerationStrategy:
         """Copy this generation strategy without it's state."""
-        if self.is_node_based:
-            nodes = deepcopy(self._nodes)
-            for n in nodes:
-                # Unset the generation strategy back-pointer, so the nodes are not
-                # associated with any generation strategy.
-                n._generation_strategy = None
-            return GenerationStrategy(name=self.name, nodes=nodes)
-
-        steps = deepcopy(self._steps)
-        for s in steps:
-            # Unset the generation strategy back-pointer, so the steps are not
+        cloned_nodes = deepcopy(self._nodes)
+        for n in cloned_nodes:
+            # Unset the generation strategy back-pointer, so the nodes are not
             # associated with any generation strategy.
-            s._generation_strategy = None
-        return GenerationStrategy(name=self.name, steps=steps)
+            n._generation_strategy = None
+        if self.is_node_based:
+            return GenerationStrategy(name=self.name, nodes=cloned_nodes)
+
+        return GenerationStrategy(
+            name=self.name, steps=checked_cast_list(GenerationStep, cloned_nodes)
+        )
 
     def _unset_non_persistent_state_fields(self) -> None:
         """Utility for testing convenience: unset fields of generation strategy
         that are set during candidate generation; these fields are not persisted
         during storage. To compare a pre-storage and a reloaded generation
         strategies; call this utility on the pre-storage one first. The rest
         of the fields should be identical.
         """
-        self._seen_trial_indices_by_status = None
         self._model = None
         for s in self._nodes:
             s._model_spec_to_gen_from = None
 
     @step_based_gs_only
+    def _validate_and_set_step_sequence(self, steps: List[GenerationStep]) -> None:
+        """Initialize and validate the steps provided to this GenerationStrategy.
+
+        Some GenerationStrategies are composed of GenerationStep objects, but we also
+        need to initialize the correct GenerationNode representation for these steps.
+        This function validates:
+            1. That only the last step has num_trials=-1, which indicates unlimited
+               trial generation is possible.
+            2. That each step's num_trials attrivute is either positive or -1
+            3. That each step's max_parallelism attribute is either None or positive
+        It then sets the corect TransitionCriterion and node_name attributes on the
+        underlying GenerationNode objects.
+        """
+        for idx, step in enumerate(steps):
+            if step.num_trials == -1 and len(step.completion_criteria) < 1:
+                if idx < len(self._steps) - 1:
+                    raise UserInputError(
+                        "Only last step in generation strategy can have "
+                        "`num_trials` set to -1 to indicate that the model in "
+                        "the step shouldbe used to generate new trials "
+                        "indefinitely unless completion critera present."
+                    )
+            elif step.num_trials < 1 and step.num_trials != -1:
+                raise UserInputError(
+                    "`num_trials` must be positive or -1 (indicating unlimited) "
+                    "for all generation steps."
+                )
+            if step.max_parallelism is not None and step.max_parallelism < 1:
+                raise UserInputError(
+                    "Maximum parallelism should be None (if no limit) or "
+                    f"a positive number. Got: {step.max_parallelism} for "
+                    f"step {step.model_name}."
+                )
+
+            step._node_name = f"GenerationStep_{str(idx)}"
+            step.index = idx
+
+            # Set transition_to field for all but the last step, which remains
+            # null.
+            if idx != len(self._steps):
+                for transition_criteria in step.transition_criteria:
+                    if (
+                        transition_criteria.criterion_class
+                        != "MaxGenerationParallelism"
+                    ):
+                        transition_criteria._transition_to = (
+                            f"GenerationStep_{str(idx + 1)}"
+                        )
+            step._generation_strategy = self
+        self._curr = steps[0]
+
+    def _validate_and_set_node_graph(self, nodes: List[GenerationNode]) -> None:
+        """Initialize and validate the node graph provided to this GenerationStrategy.
+
+        This function validates:
+            1. That all nodes have unique names.
+            2. That there is at least one node with a transition_to field.
+            3. That all `transition_to` attributes on a TransitionCriterion point to
+                another node in the same GenerationStrategy.
+            4. Warns if no nodes contain a transition criterion
+        """
+        node_names = []
+        for node in self._nodes:
+            # validate that all node names are unique
+            if node.node_name in node_names:
+                raise GenerationStrategyMisconfiguredException(
+                    error_info="All node names in a GenerationStrategy "
+                    + "must be unique."
+                )
+
+            node_names.append(node.node_name)
+            node._generation_strategy = self
+
+        # validate `transition_criterion`
+        contains_a_transition_to_argument = False
+        for node in self._nodes:
+            for transition_criteria in node.transition_criteria:
+                if transition_criteria.transition_to is not None:
+                    contains_a_transition_to_argument = True
+                    if transition_criteria.transition_to not in node_names:
+                        raise GenerationStrategyMisconfiguredException(
+                            error_info=f"`transition_to` argument "
+                            f"{transition_criteria.transition_to} does not "
+                            "correspond to any node in this GenerationStrategy."
+                        )
+
+        # validate that at least one node has transition_to field
+        if len(self._nodes) > 1 and not contains_a_transition_to_argument:
+            logger.warning(
+                "None of the nodes in this GenerationStrategy "
+                "contain a `transition_to` argument in their transition_criteria. "
+                "Therefore, the GenerationStrategy will not be able to "
+                "move from one node to another. Please add a "
+                "`transition_to` argument."
+            )
+        self._curr = nodes[0]
+
+    @step_based_gs_only
     def _step_repr(self, step_str_rep: str) -> str:
         """Return the string representation of the steps in a GenerationStrategy
         composed of GenerationSteps.
         """
         step_str_rep += "steps=["
         remaining_trials = "subsequent" if len(self._nodes) > 1 else "all"
         for step in self._nodes:
@@ -602,14 +588,36 @@
                 model_name = "model with unknown name"
 
             step_str_rep += f"{model_name} for {num_trials} trials, "
         step_str_rep = step_str_rep[:-2]
         step_str_rep += "])"
         return step_str_rep
 
+    def _make_default_name(self) -> str:
+        """Make a default name for this generation strategy; used when no name is passed
+        to the constructor. For node-based generation strategies, the name is
+        constructed by joining together the names of the nodes set on this
+        generation strategy. For step-based generation strategies, the model keys
+        of the underlying model specs are used.
+        Note: This should only be called once the nodes are set.
+        """
+        if not self._nodes:
+            raise UnsupportedError(
+                "Cannot make a default name for a generation strategy with no nodes "
+                "set yet."
+            )
+        # TODO: Simplify this after updating GStep names to represent underlying models.
+        if self.is_node_based:
+            node_names = (node.node_name for node in self._nodes)
+        else:
+            node_names = (node.model_spec_to_gen_from.model_key for node in self._nodes)
+            # Trim the "get_" beginning of the factory function if it's there.
+            node_names = (n[4:] if n[:4] == "get_" else n for n in node_names)
+        return "+".join(node_names)
+
     def __repr__(self) -> str:
         """String representation of this generation strategy."""
         gs_str = f"GenerationStrategy(name='{self.name}', "
         if not self.is_node_based:
             return self._step_repr(gs_str)
         gs_str += f"nodes={str(self._nodes)})"
         return gs_str
@@ -711,15 +719,14 @@
                 strategy will obtain the data via ``experiment.lookup_data``.
         """
         data = self.experiment.lookup_data() if data is None else data
         # If last generator run's index matches the current node, extract
         # model state from last generator run and pass it to the model
         # being instantiated in this function.
         model_state_on_lgr = self._get_model_state_from_last_generator_run()
-
         if not data.df.empty:
             trial_indices_in_data = sorted(data.df["trial_index"].unique())
             logger.debug(f"Fitting model with data for trials: {trial_indices_in_data}")
 
         self._curr.fit(experiment=self.experiment, data=data, **model_state_on_lgr)
         self._model = self._curr._fitted_model
 
@@ -739,23 +746,24 @@
         Returns:
             Whether generation strategy moved to the next node.
         """
         move_to_next_node, next_node = self._curr.should_transition_to_next_node(
             raise_data_required_error=raise_data_required_error
         )
         if move_to_next_node:
-            assert (
-                self._curr.gen_unlimited_trials is False
-            ), "This node should never attempt to transition since"
-            " it can generate unlimited trials"
             if self.optimization_complete:
                 raise GenerationStrategyCompleted(
                     f"Generation strategy {self} generated all the trials as "
                     "specified in its nodes."
                 )
+            if next_node is None:
+                # If the last node did not specify which node to transition to,
+                # move to the next node in the list.
+                current_node_index = self._nodes.index(self._curr)
+                next_node = self._nodes[current_node_index + 1].node_name
             for node in self._nodes:
                 if node.node_name == next_node:
                     self._curr = node
                     # Moving to the next node also entails unsetting this GS's model
                     # (since new node's model will be initialized for the first time;
                     # this is done in `self._fit_current_model).
                     self._model = None
@@ -765,15 +773,21 @@
         lgr = self.last_generator_run
         # NOTE: This will not be easily compatible with `GenerationNode`;
         # will likely need to find last generator run per model. Not a problem
         # for now though as GS only allows `GenerationStep`-s for now.
         # Potential solution: store generator runs on `GenerationNode`-s and
         # split them per-model there.
         model_state_on_lgr = {}
-        model_on_curr = self._curr.model_enum
+        # Need to check if model_spec_to_gen_from is none to account for
+        # ExternalGenerationNodes which leverage models from outside Ax.
+        model_on_curr = (
+            self._curr.model_spec_to_gen_from.model_enum
+            if self._curr.model_spec_to_gen_from
+            else None
+        )
         if lgr is None:
             return model_state_on_lgr
 
         if not self.is_node_based:
             grs_equal = lgr._generation_step_index == self.current_step_index
         else:
             grs_equal = lgr._generation_node_name == self._curr.node_name
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/map_torch.py` & `ax-platform-0.4.0/ax/modelbridge/map_torch.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, List, Optional, Tuple, Type
 
 import numpy as np
 
 import torch
 from ax.core.data import Data
 from ax.core.experiment import Experiment
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/model_spec.py` & `ax-platform-0.4.0/ax/modelbridge/model_spec.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import json
 import warnings
 from copy import deepcopy
 from dataclasses import dataclass
 from typing import Any, Callable, Dict, List, Optional, Tuple
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/modelbridge_utils.py` & `ax-platform-0.4.0/ax/modelbridge/modelbridge_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import warnings
 
 from copy import deepcopy
 from functools import partial
 
@@ -54,15 +56,15 @@
 )
 from ax.core.types import TBounds, TCandidateMetadata
 
 from ax.core.utils import (  # noqa F402: Temporary import for backward compatibility.
     get_pending_observation_features,  # noqa F401
     get_pending_observation_features_based_on_trial_status,  # noqa F401
 )
-from ax.exceptions.core import DataRequiredError, UnsupportedError, UserInputError
+from ax.exceptions.core import DataRequiredError, UserInputError
 from ax.modelbridge.transforms.base import Transform
 from ax.modelbridge.transforms.utils import (
     derelativize_optimization_config_with_raw_status_quo,
 )
 from ax.models.torch.botorch_moo_defaults import pareto_frontier_evaluator
 from ax.models.torch.frontier_utils import (
     get_weighted_mc_objective_and_objective_thresholds,
@@ -164,15 +166,25 @@
     if data.df.empty:
         raise ValueError("MultiObjectiveOptimization requires non-empty data.")
 
 
 def extract_parameter_constraints(
     parameter_constraints: List[ParameterConstraint], param_names: List[str]
 ) -> TBounds:
-    """Extract parameter constraints."""
+    """Convert Ax parameter constraints into a tuple of NumPy arrays representing the
+    system of linear inequality constraints.
+
+    Args:
+        parameter_constraints: A list of parameter constraint objects.
+        param_names: A list of parameter names.
+
+    Returns:
+        An optional tuple of NumPy arrays (A, b) representing the system of linear
+        inequality constraints A x < b.
+    """
     if len(parameter_constraints) == 0:
         return None
     A = np.zeros((len(parameter_constraints), len(param_names)))
     b = np.zeros((len(parameter_constraints), 1))
     for i, c in enumerate(parameter_constraints):
         b[i, 0] = c.bound
         for name, val in c.constraint_dict.items():
@@ -185,15 +197,16 @@
 ) -> SearchSpaceDigest:
     """Extract basic parameter properties from a search space.
 
     This is typically called with the transformed search space and makes certain
     assumptions regarding the parameters being transformed.
 
     For ChoiceParameters:
-    * The choices are assumed to be numerical. ChoiceEncode and OrderedChoiceEncode
+    * The choices are assumed to be numerical. ChoiceEncode
+    and OrderedChoiceToIntegerRange
     transforms handle this.
     * If is_task, its index is added to task_features.
     * If ordered, its index is added to ordinal_features.
     * Otherwise, its index is added to categorical_features.
     * In all cases, the choices are added to discrete_choices.
     * The minimum and maximum value are added to the bounds.
     * The target_value is added to target_values.
@@ -674,15 +687,14 @@
     modelbridge: modelbridge_module.torch.TorchModelBridge,
     observation_features: List[ObservationFeatures],
     observation_data: Optional[List[ObservationData]] = None,
     objective_thresholds: Optional[TRefPoint] = None,
     optimization_config: Optional[MultiObjectiveOptimizationConfig] = None,
     arm_names: Optional[List[Optional[str]]] = None,
     use_model_predictions: bool = True,
-    transform_outcomes_and_configs: Optional[bool] = None,
 ) -> Tuple[List[Observation], Tensor, Tensor, Optional[Tensor]]:
     """Helper that applies transforms and calls ``frontier_evaluator``.
 
     Returns the ``frontier_evaluator`` configs in addition to the Pareto
     observations.
 
     Args:
@@ -696,52 +708,23 @@
             specified in ``optimization_config``, if necessary.
         optimization_config: Multi-objective optimization config.
         arm_names: Arm names for each observation in ``observation_features``.
         use_model_predictions: If ``True``, will use model predictions at
             ``observation_features`` to compute Pareto front. If ``False``,
             will use ``observation_data`` directly to compute Pareto front, ignoring
             ``observation_features``.
-        transform_outcomes_and_configs: Deprecated and must be ``False`` if provided.
-            Previously, if ``True``, would transform the optimization
-            config, observation features and observation data, before calling
-            ``frontier_evaluator``, then will untransform all of the above before
-            returning the observations.
 
     Returns: Four-item tuple of:
           - frontier_observations: Observations of points on the pareto frontier,
           - f: n x m tensor representation of the Pareto frontier values where n is the
             length of frontier_observations and m is the number of metrics,
           - obj_w: m tensor of objective weights,
           - obj_t: m tensor of objective thresholds corresponding to Y, or None if no
             objective thresholds used.
     """
-    if transform_outcomes_and_configs is None:
-        warnings.warn(
-            "FYI: The default behavior of `get_pareto_frontier_and_configs` when "
-            "`transform_outcomes_and_configs` is not specified has changed. Previously,"
-            " the default was `transform_outcomes_and_configs=True`; now this argument "
-            "is deprecated and behavior is as if "
-            "`transform_outcomes_and_configs=False`. You did not specify "
-            "`transform_outcomes_and_configs`, so this warning requires no action.",
-            stacklevel=2,
-        )
-    elif transform_outcomes_and_configs:
-        raise UnsupportedError(
-            "`transform_outcomes_and_configs=True` is no longer supported, and the "
-            "`transform_outcomes_and_configs` argument is deprecated. Please do not "
-            "specify this argument."
-        )
-    else:
-        warnings.warn(
-            "You passed `transform_outcomes_and_configs=False`. Specifying "
-            "`transform_outcomes_and_configs` at all is deprecated because `False` is "
-            "now the only allowed behavior. In the future, this will become an error.",
-            DeprecationWarning,
-            stacklevel=2,
-        )
     # Input validation
     if use_model_predictions:
         if observation_data is not None:
             warnings.warn(
                 "You provided `observation_data` when `use_model_predictions` is True; "
                 "`observation_data` will not be used.",
                 stacklevel=2,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/pairwise.py` & `ax-platform-0.4.0/ax/modelbridge/pairwise.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import List, Optional, Tuple
 
 import numpy as np
 import torch
 from ax.core.observation import ObservationData, ObservationFeatures
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/prediction_utils.py` & `ax-platform-0.4.0/ax/modelbridge/prediction_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.modelbridge import ModelBridge
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/random.py` & `ax-platform-0.4.0/ax/modelbridge/random.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional
 
 from ax.core.experiment import Experiment
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.base import GenResults, ModelBridge
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/registry.py` & `ax-platform-0.4.0/ax/modelbridge/registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 Module containing a registry of standard models (and generators, samplers etc.)
 such as Sobol generator, GP+EI, Thompson sampler, etc.
 
 Use of `Models` enum allows for serialization and reinstantiation of models and
 generation strategies from generator runs they produced. To reinstantiate a model
 from generator run, use `get_model_from_generator_run` utility from this module.
@@ -29,15 +31,18 @@
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.discrete import DiscreteModelBridge
 from ax.modelbridge.random import RandomModelBridge
 from ax.modelbridge.torch import TorchModelBridge
 from ax.modelbridge.transforms.base import Transform
 from ax.modelbridge.transforms.centered_unit_x import CenteredUnitX
-from ax.modelbridge.transforms.choice_encode import ChoiceEncode, OrderedChoiceEncode
+from ax.modelbridge.transforms.choice_encode import (
+    ChoiceEncode,
+    OrderedChoiceToIntegerRange,
+)
 from ax.modelbridge.transforms.convert_metric_names import ConvertMetricNames
 from ax.modelbridge.transforms.derelativize import Derelativize
 from ax.modelbridge.transforms.int_range_to_choice import IntRangeToChoice
 from ax.modelbridge.transforms.int_to_float import IntToFloat
 from ax.modelbridge.transforms.ivw import IVW
 from ax.modelbridge.transforms.log import Log
 from ax.modelbridge.transforms.logit import Logit
@@ -55,16 +60,14 @@
 from ax.models.discrete.full_factorial import FullFactorialGenerator
 from ax.models.discrete.thompson import ThompsonSampler
 from ax.models.random.alebo_initializer import ALEBOInitializer
 from ax.models.random.sobol import SobolGenerator
 from ax.models.random.uniform import UniformGenerator
 from ax.models.torch.alebo import ALEBO
 from ax.models.torch.botorch import BotorchModel
-from ax.models.torch.botorch_kg import KnowledgeGradient
-from ax.models.torch.botorch_mes import MaxValueEntropySearch
 from ax.models.torch.botorch_modular.model import (
     BoTorchModel as ModularBoTorchModel,
     SurrogateSpec,
 )
 from ax.models.torch.botorch_moo import MultiObjectiveBotorchModel
 from ax.models.torch.cbo_sac import SACBO
 from ax.models.torch.fully_bayesian import (
@@ -83,15 +86,15 @@
 from botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP
 from botorch.models.fully_bayesian_multitask import SaasFullyBayesianMultiTaskGP
 
 logger: Logger = get_logger(__name__)
 
 Cont_X_trans: List[Type[Transform]] = [
     RemoveFixed,
-    OrderedChoiceEncode,
+    OrderedChoiceToIntegerRange,
     OneHot,
     IntToFloat,
     Log,
     Logit,
     UnitX,
 ]
 
@@ -172,26 +175,14 @@
     ),
     "GPEI": ModelSetup(
         bridge_class=TorchModelBridge,
         model_class=BotorchModel,
         transforms=Cont_X_trans + Y_trans,
         standard_bridge_kwargs=STANDARD_TORCH_BRIDGE_KWARGS,
     ),
-    "GPKG": ModelSetup(
-        bridge_class=TorchModelBridge,
-        model_class=KnowledgeGradient,
-        transforms=Cont_X_trans + Y_trans,
-        standard_bridge_kwargs=STANDARD_TORCH_BRIDGE_KWARGS,
-    ),
-    "GPMES": ModelSetup(
-        bridge_class=TorchModelBridge,
-        model_class=MaxValueEntropySearch,
-        transforms=Cont_X_trans + Y_trans,
-        standard_bridge_kwargs=STANDARD_TORCH_BRIDGE_KWARGS,
-    ),
     "EB": ModelSetup(
         bridge_class=DiscreteModelBridge,
         model_class=EmpiricalBayesThompsonSampler,
         transforms=TS_trans,
     ),
     "Factorial": ModelSetup(
         bridge_class=DiscreteModelBridge,
@@ -466,16 +457,14 @@
     For instance, `Models.SOBOL(search_space=search_space, scramble=False)`
     will instantiate a `RandomModelBridge(search_space=search_space)`
     with a `SobolGenerator(scramble=False)` underlying model.
     """
 
     SOBOL = "Sobol"
     GPEI = "GPEI"
-    GPKG = "GPKG"
-    GPMES = "GPMES"
     FACTORIAL = "Factorial"
     SAASBO = "SAASBO"
     FULLYBAYESIAN = "FullyBayesian"
     FULLYBAYESIANMOO = "FullyBayesianMOO"
     SAAS_MTGP = "SAAS_MTGP"
     FULLYBAYESIAN_MTGP = "FullyBayesian_MTGP"
     FULLYBAYESIANMOO_MTGP = "FullyBayesianMOO_MTGP"
@@ -599,24 +588,28 @@
 
 
 def _encode_callables_as_references(kwarg_dict: Dict[str, Any]) -> Dict[str, Any]:
     """Converts callables to references of form <module>.<qualname>, and returns
     the resulting dictionary.
     """
     return {
-        k: {"is_callable_as_path": True, "value": callable_to_reference(v)}
-        if isfunction(v)
-        else v
+        k: (
+            {"is_callable_as_path": True, "value": callable_to_reference(v)}
+            if isfunction(v)
+            else v
+        )
         for k, v in kwarg_dict.items()
     }
 
 
 def _decode_callables_from_references(kwarg_dict: Dict[str, Any]) -> Dict[str, Any]:
     """Retrieves callables from references of form <module>.<qualname>, and returns
     the resulting dictionary.
     """
     return {
-        k: callable_from_reference(checked_cast(str, v.get("value")))
-        if isinstance(v, dict) and v.get("is_callable_as_path", False)
-        else v
+        k: (
+            callable_from_reference(checked_cast(str, v.get("value")))
+            if isinstance(v, dict) and v.get("is_callable_as_path", False)
+            else v
+        )
         for k, v in kwarg_dict.items()
     }
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/strategies/alebo.py` & `ax-platform-0.4.0/ax/modelbridge/strategies/alebo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Optional
 
 import numpy as np
 import torch
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.search_space import SearchSpace
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/strategies/rembo.py` & `ax-platform-0.4.0/ax/modelbridge/strategies/rembo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from collections import defaultdict
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 import numpy as np
 import torch
 from ax.core.data import Data
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_aepsych_criterion.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_aepsych_criterion.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest.mock import patch
 
 import pandas as pd
 from ax.core.base_trial import TrialStatus
 from ax.core.data import Data
 from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
 from ax.modelbridge.registry import Models
@@ -70,15 +72,17 @@
             # We have seen three "yes" and three "no"
             self.assertTrue(
                 generation_strategy._maybe_move_to_next_step(
                     raise_data_required_error=False
                 )
             )
 
-            self.assertEqual(generation_strategy._curr.model_enum, Models.GPEI)
+            self.assertEqual(
+                generation_strategy._curr.model_spec_to_gen_from.model_enum, Models.GPEI
+            )
 
     def test_many_criteria(self) -> None:
         criteria = [
             MinimumPreferenceOccurances(metric_name="m1", threshold=3),
             MinTrials(only_in_statuses=[TrialStatus.COMPLETED], threshold=5),
         ]
 
@@ -144,8 +148,10 @@
             # "no"
             self.assertTrue(
                 generation_strategy._maybe_move_to_next_step(
                     raise_data_required_error=False
                 )
             )
 
-            self.assertEqual(generation_strategy._curr.model_enum, Models.GPEI)
+            self.assertEqual(
+                generation_strategy._curr.model_spec_to_gen_from.model_enum, Models.GPEI
+            )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_alebo_strategy.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_alebo_strategy.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 import numpy as np
 import pandas as pd
 import torch
 from ax.core.data import Data
 from ax.modelbridge.strategies.alebo import (
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_base_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_base_modelbridge.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
 from typing import Any, List
 from unittest import mock
 from unittest.mock import Mock
 
 import numpy as np
 from ax.core.arm import Arm
@@ -150,15 +152,15 @@
         modelbridge._gen = mock.MagicMock(
             "ax.modelbridge.base.ModelBridge._gen",
             autospec=True,
             return_value=GenResults(
                 observation_features=[get_observation1trans().features], weights=[2]
             ),
         )
-        oc = OptimizationConfig(objective=Objective(metric=Metric(name="test_metric")))
+        oc = get_optimization_config_no_constraints()
         modelbridge._set_kwargs_to_save(
             model_key="TestModel", model_kwargs={}, bridge_kwargs={}
         )
         # Test input error when generating 0 candidates.
         with self.assertRaisesRegex(UserInputError, "Attempted to generate"):
             modelbridge.gen(n=0)
         gr = modelbridge.gen(
@@ -316,15 +318,15 @@
         modelbridge = ModelBridge(
             search_space=ss,
             model=Model(),
             optimization_config=oc,
             fit_tracking_metrics=False,
         )
         new_oc = OptimizationConfig(
-            objective=Objective(metric=Metric(name="test_metric2"))
+            objective=Objective(metric=Metric(name="test_metric2"), minimize=False),
         )
         with self.assertRaisesRegex(UnsupportedError, "fit_tracking_metrics"):
             modelbridge.gen(n=1, optimization_config=new_oc)
 
     @mock.patch(
         "ax.modelbridge.base.observations_from_data",
         autospec=True,
@@ -572,22 +574,25 @@
         sobol = Models.SOBOL(search_space=exp.search_space)
         exp.new_batch_trial(sobol.gen(5)).set_status_quo_and_optimize_power(
             status_quo=exp.status_quo
         ).run()
 
         # create data where metrics vary in start and end times
         data = get_non_monolithic_branin_moo_data()
-        bridge = ModelBridge(
-            experiment=exp,
-            data=data,
-            model=Model(),
-            search_space=exp.search_space,
-        )
+        with warnings.catch_warnings(record=True) as ws:
+            bridge = ModelBridge(
+                experiment=exp,
+                data=data,
+                model=Model(),
+                search_space=exp.search_space,
+            )
         # just testing it doesn't error
         bridge.gen(5)
+        self.assertTrue(any("start_time" in str(w.message) for w in ws))
+        self.assertTrue(any("end_time" in str(w.message) for w in ws))
         # pyre-fixme[16]: Optional type has no attribute `arm_name`.
         self.assertEqual(bridge.status_quo.arm_name, "status_quo")
 
     @mock.patch(
         "ax.modelbridge.base.observations_from_data",
         autospec=True,
         return_value=(
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_cross_validation.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_cross_validation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List
 from unittest import mock
 
 import numpy as np
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.optimization_config import (
     MultiObjectiveOptimizationConfig,
     OptimizationConfig,
 )
-
 from ax.core.outcome_constraint import OutcomeConstraint
 from ax.core.parameter import FixedParameter, ParameterType
 from ax.core.search_space import SearchSpace
 from ax.core.types import ComparisonOp
 from ax.modelbridge.cross_validation import (
     assess_model_fit,
     compute_diagnostics,
@@ -34,14 +35,15 @@
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_experiment
 from ax.utils.testing.modeling_stubs import get_observation1trans, get_observation2trans
 
 
 class CrossValidationTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.training_data = [
             Observation(
                 features=ObservationFeatures(parameters={"x": 2.0}, trial_index=0),
                 data=ObservationData(
                     means=np.array([2.0, 4.0]),
                     covariance=np.array([[1.0, 2.0], [3.0, 4.0]]),
                     metric_names=["a", "b"],
@@ -199,17 +201,15 @@
         # Test ModelBridge._cross_validate was called correctly.
         z = ma._cross_validate.mock_calls
         self.assertEqual(len(z), 3)
         ma._cross_validate.assert_called_with(**self.transformed_cv_input_dict)
 
         # Test selector
 
-        # pyre-fixme[3]: Return type must be annotated.
-        # pyre-fixme[2]: Parameter must be annotated.
-        def test_selector(obs):
+        def test_selector(obs: Observation) -> bool:
             return obs.features.parameters["x"] != 4.0
 
         result = cross_validate(model=ma, folds=-1, test_selector=test_selector)
         self.assertEqual(len(result), 3)
         z = ma.cross_validate.mock_calls[5:]
         self.assertEqual(len(z), 2)
         all_test = np.hstack(
@@ -263,18 +263,17 @@
 
     def test_cross_validate_gives_a_useful_error_for_model_with_no_data(self) -> None:
         exp = get_branin_experiment()
         sobol = Models.SOBOL(experiment=exp, search_space=exp.search_space)
         with self.assertRaisesRegex(ValueError, "no training data"):
             cross_validate(model=sobol)
 
-    # pyre-fixme[3]: Return type must be annotated.
     def test_cross_validate_raises_not_implemented_error_for_non_cv_model_with_data(
         self,
-    ):
+    ) -> None:
         exp = get_branin_experiment(with_batch=True)
         exp.trials[0].run().complete()
         sobol = Models.SOBOL(
             experiment=exp, search_space=exp.search_space, data=exp.fetch_data()
         )
         with self.assertRaises(NotImplementedError):
             cross_validate(model=sobol)
@@ -342,35 +341,40 @@
         assess_model_fit_result = assess_model_fit(
             diagnostics=diag,
             significance_level=0.05,
         )
 
         # Test single objective
         optimization_config = OptimizationConfig(
-            objective=Objective(metric=Metric("a"))
+            objective=Objective(metric=Metric("a"), minimize=True)
         )
         has_good_fit = has_good_opt_config_model_fit(
             optimization_config=optimization_config,
             assess_model_fit_result=assess_model_fit_result,
         )
         self.assertFalse(has_good_fit)
 
         # Test multi objective
         optimization_config = MultiObjectiveOptimizationConfig(
-            objective=MultiObjective(metrics=[Metric("a"), Metric("b")])
+            objective=MultiObjective(
+                objectives=[
+                    Objective(Metric("a"), minimize=False),
+                    Objective(Metric("b"), minimize=False),
+                ]
+            )
         )
         has_good_fit = has_good_opt_config_model_fit(
             optimization_config=optimization_config,
             assess_model_fit_result=assess_model_fit_result,
         )
         self.assertFalse(has_good_fit)
 
         # Test constraints
         optimization_config = OptimizationConfig(
-            objective=Objective(metric=Metric("a")),
+            objective=Objective(metric=Metric("a"), minimize=False),
             outcome_constraints=[
                 OutcomeConstraint(metric=Metric("b"), op=ComparisonOp.GEQ, bound=0.1)
             ],
         )
         has_good_fit = has_good_opt_config_model_fit(
             optimization_config=optimization_config,
             assess_model_fit_result=assess_model_fit_result,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_discrete_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_discrete_modelbridge.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 from unittest.mock import Mock
 
 import numpy as np
 from ax.core.metric import Metric
 from ax.core.objective import Objective
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
@@ -24,14 +26,15 @@
 from ax.modelbridge.discrete import _get_parameter_values, DiscreteModelBridge
 from ax.models.discrete_base import DiscreteModel
 from ax.utils.common.testutils import TestCase
 
 
 class DiscreteModelBridgeTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.parameters = [
             ChoiceParameter("x", ParameterType.FLOAT, values=[0, 1]),
             ChoiceParameter("y", ParameterType.STRING, values=["foo", "bar"]),
             FixedParameter("z", ParameterType.BOOL, value=True),
         ]
         parameter_constraints = []
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_dispatch_utils.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_dispatch_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 import warnings
 from typing import Any, Dict
 
 import torch
 from ax.core.objective import MultiObjective
 from ax.core.optimization_config import MultiObjectiveOptimizationConfig
@@ -54,14 +56,15 @@
             self.assertEqual(sobol_gpei._steps[0].model, Models.SOBOL)
             self.assertEqual(sobol_gpei._steps[0].num_trials, 5)
             self.assertEqual(sobol_gpei._steps[1].model, Models.BOTORCH_MODULAR)
             expected_model_kwargs: Dict[str, Any] = {
                 "torch_device": None,
                 "transforms": expected_transforms,
                 "transform_configs": expected_transform_configs,
+                "fit_out_of_design": False,
             }
             self.assertEqual(sobol_gpei._steps[1].model_kwargs, expected_model_kwargs)
             device = torch.device("cpu")
             sobol_gpei = choose_generation_strategy(
                 search_space=get_branin_search_space(),
                 verbose=True,
                 torch_device=device,
@@ -115,15 +118,20 @@
             )
             self.assertEqual(sobol_gpei._steps[0].model, Models.SOBOL)
             self.assertEqual(sobol_gpei._steps[0].num_trials, 5)
             self.assertEqual(sobol_gpei._steps[1].model, Models.BOTORCH_MODULAR)
             model_kwargs = not_none(sobol_gpei._steps[1].model_kwargs)
             self.assertEqual(
                 set(model_kwargs.keys()),
-                {"torch_device", "transforms", "transform_configs"},
+                {
+                    "torch_device",
+                    "transforms",
+                    "transform_configs",
+                    "fit_out_of_design",
+                },
             )
             self.assertGreater(len(model_kwargs["transforms"]), 0)
         with self.subTest("Sobol (we can try every option)"):
             sobol = choose_generation_strategy(
                 search_space=get_factorial_search_space(), num_trials=1000
             )
             self.assertEqual(sobol._steps[0].model, Models.SOBOL)
@@ -192,28 +200,30 @@
             self.assertEqual(bo_mixed._steps[0].model, Models.SOBOL)
             self.assertEqual(bo_mixed._steps[0].num_trials, 6)
             self.assertEqual(bo_mixed._steps[1].model, Models.BO_MIXED)
             expected_model_kwargs = {
                 "torch_device": None,
                 "transforms": [Winsorize] + Mixed_transforms + Y_trans,
                 "transform_configs": expected_transform_configs,
+                "fit_out_of_design": False,
             }
             self.assertEqual(bo_mixed._steps[1].model_kwargs, expected_model_kwargs)
         with self.subTest("BO_MIXED (mixed search space)"):
             ss = get_branin_search_space(with_choice_parameter=True)
             # pyre-fixme[16]: `Parameter` has no attribute `_is_ordered`.
             ss.parameters["x2"]._is_ordered = False
             bo_mixed_2 = choose_generation_strategy(search_space=ss)
             self.assertEqual(bo_mixed_2._steps[0].model, Models.SOBOL)
             self.assertEqual(bo_mixed_2._steps[0].num_trials, 5)
             self.assertEqual(bo_mixed_2._steps[1].model, Models.BO_MIXED)
             expected_model_kwargs = {
                 "torch_device": None,
                 "transforms": [Winsorize] + Mixed_transforms + Y_trans,
                 "transform_configs": expected_transform_configs,
+                "fit_out_of_design": False,
             }
             self.assertEqual(bo_mixed._steps[1].model_kwargs, expected_model_kwargs)
         with self.subTest("BO_MIXED (mixed multi-objective optimization)"):
             search_space = get_branin_search_space(with_choice_parameter=True)
             search_space.parameters["x2"]._is_ordered = False
             optimization_config = MultiObjectiveOptimizationConfig(
                 objective=MultiObjective(objectives=[])
@@ -223,15 +233,20 @@
             )
             self.assertEqual(moo_mixed._steps[0].model, Models.SOBOL)
             self.assertEqual(moo_mixed._steps[0].num_trials, 5)
             self.assertEqual(moo_mixed._steps[1].model, Models.BO_MIXED)
             model_kwargs = not_none(moo_mixed._steps[1].model_kwargs)
             self.assertEqual(
                 set(model_kwargs.keys()),
-                {"torch_device", "transforms", "transform_configs"},
+                {
+                    "torch_device",
+                    "transforms",
+                    "transform_configs",
+                    "fit_out_of_design",
+                },
             )
             self.assertGreater(len(model_kwargs["transforms"]), 0)
         with self.subTest("SAASBO"):
             sobol_fullybayesian = choose_generation_strategy(
                 search_space=get_branin_search_space(),
                 use_batch_trials=True,
                 num_initialization_trials=3,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_factory.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_factory.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 import pandas as pd
 import torch
 from ax.core.data import Data
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import MultiObjectiveOptimizationConfig
 from ax.core.outcome_constraint import ComparisonOp, ObjectiveThreshold
-from ax.core.parameter import RangeParameter
 from ax.modelbridge.discrete import DiscreteModelBridge
 from ax.modelbridge.factory import (
     get_botorch,
     get_empirical_bayes_thompson,
     get_factorial,
     get_GPEI,
-    get_GPKG,
-    get_GPMES,
     get_MOO_EHVI,
     get_MOO_NEHVI,
     get_MOO_PAREGO,
     get_MOO_RS,
     get_MTGP_LEGACY,
     get_MTGP_NEHVI,
     get_MTGP_PAREGO,
@@ -34,26 +33,26 @@
     get_thompson,
     get_uniform,
 )
 from ax.modelbridge.random import RandomModelBridge
 from ax.modelbridge.torch import TorchModelBridge
 from ax.models.discrete.eb_thompson import EmpiricalBayesThompsonSampler
 from ax.models.discrete.thompson import ThompsonSampler
-from ax.models.winsorization_config import WinsorizationConfig
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import (
     get_branin_experiment,
     get_branin_experiment_with_multi_objective,
     get_branin_optimization_config,
     get_factorial_experiment,
     get_multi_type_experiment,
     get_multi_type_experiment_with_multi_objective,
 )
 from ax.utils.testing.mock import fast_botorch_optimize
 from botorch.models.multitask import MultiTaskGP
+from botorch.optim.optimize import optimize_acqf, optimize_acqf_list
 
 
 # pyre-fixme[3]: Return type must be annotated.
 def get_multi_obj_exp_and_opt_config():
     multi_obj_exp = get_branin_experiment_with_multi_objective(with_batch=True)
     # pyre-fixme[16]: Optional type has no attribute `objective`.
     metrics = multi_obj_exp.optimization_config.objective.metrics
@@ -131,108 +130,14 @@
         sobol_run = sobol.gen(n=1)
         t = exp.new_batch_trial().add_generator_run(sobol_run)
         t.run().mark_completed()
 
         with self.assertRaises(ValueError):
             get_MTGP_LEGACY(experiment=exp, data=exp.fetch_data(), trial_index=0)
 
-    @fast_botorch_optimize
-    def test_GPKG(self) -> None:
-        """Tests GPKG instantiation."""
-        exp = get_branin_experiment(with_batch=True)
-        with self.assertRaises(ValueError):
-            get_GPKG(experiment=exp, data=exp.fetch_data())
-        exp.trials[0].run().mark_completed()
-        gpkg = get_GPKG(experiment=exp, data=exp.fetch_data())
-        self.assertIsInstance(gpkg, TorchModelBridge)
-
-        # Check that .gen returns without failure
-        gr = gpkg.gen(n=1)
-        self.assertEqual(len(gr.arms), 1)
-
-        # test transform_configs with winsorization
-        configs = {
-            "Winsorize": {
-                "winsorization_config": WinsorizationConfig(
-                    lower_quantile_margin=0.1,
-                    upper_quantile_margin=0.1,
-                )
-            }
-        }
-        gpkg_win = get_GPKG(
-            experiment=exp,
-            data=exp.fetch_data(),
-            # pyre-fixme[6]: For 3rd param expected `Optional[Dict[str, Dict[str,
-            #  Union[None, Dict[str, typing.Any], OptimizationConfig,
-            #  AcquisitionFunction, float, int, str]]]]` but got `Dict[str, Dict[str,
-            #  WinsorizationConfig]]`.
-            transform_configs=configs,
-        )
-        self.assertIsInstance(gpkg_win, TorchModelBridge)
-        self.assertEqual(gpkg_win._transform_configs, configs)
-
-        # test multi-fidelity optimization
-        exp.parameters["x2"] = RangeParameter(
-            name="x2",
-            parameter_type=exp.parameters["x2"].parameter_type,
-            lower=-5.0,
-            upper=10.0,
-            is_fidelity=True,
-            target_value=10.0,
-        )
-        gpkg_mf = get_GPKG(experiment=exp, data=exp.fetch_data())
-        self.assertIsInstance(gpkg_mf, TorchModelBridge)
-
-    @fast_botorch_optimize
-    def test_GPMES(self) -> None:
-        """Tests GPMES instantiation."""
-        exp = get_branin_experiment(with_batch=True)
-        with self.assertRaises(ValueError):
-            get_GPMES(experiment=exp, data=exp.fetch_data())
-        exp.trials[0].run()
-        gpmes = get_GPMES(experiment=exp, data=exp.fetch_data())
-        self.assertIsInstance(gpmes, TorchModelBridge)
-
-        # Check that .gen returns without failure
-        gr = gpmes.gen(n=1)
-        self.assertEqual(len(gr.arms), 1)
-
-        # test transform_configs with winsorization
-        configs = {
-            "Winsorize": {
-                "winsorization_config": WinsorizationConfig(
-                    lower_quantile_margin=0.1,
-                    upper_quantile_margin=0.1,
-                )
-            }
-        }
-        gpmes_win = get_GPMES(
-            experiment=exp,
-            data=exp.fetch_data(),
-            # pyre-fixme[6]: For 3rd param expected `Optional[Dict[str, Dict[str,
-            #  Union[None, Dict[str, typing.Any], OptimizationConfig,
-            #  AcquisitionFunction, float, int, str]]]]` but got `Dict[str, Dict[str,
-            #  WinsorizationConfig]]`.
-            transform_configs=configs,
-        )
-        self.assertIsInstance(gpmes_win, TorchModelBridge)
-        self.assertEqual(gpmes_win._transform_configs, configs)
-
-        # test multi-fidelity optimization
-        exp.parameters["x2"] = RangeParameter(
-            name="x2",
-            parameter_type=exp.parameters["x2"].parameter_type,
-            lower=-5.0,
-            upper=10.0,
-            is_fidelity=True,
-            target_value=10.0,
-        )
-        gpmes_mf = get_GPMES(experiment=exp, data=exp.fetch_data())
-        self.assertIsInstance(gpmes_mf, TorchModelBridge)
-
     def test_model_kwargs(self) -> None:
         """Tests that model kwargs are passed correctly."""
         exp = get_branin_experiment()
         sobol = get_sobol(
             search_space=exp.search_space, init_position=2, scramble=False, seed=239
         )
         self.assertIsInstance(sobol, RandomModelBridge)
@@ -283,17 +188,16 @@
         uniform = get_uniform(exp.search_space)
         self.assertIsInstance(uniform, RandomModelBridge)
         uniform_run = uniform.gen(n=5)
         self.assertEqual(len(uniform_run.arms), 5)
 
 
 class ModelBridgeFactoryTestMultiObjective(TestCase):
-    # pyre-fixme[3]: Return type must be annotated.
     # pyre-fixme[2]: Parameter must be annotated.
-    def test_single_objective_error(self, factory_fn=get_MOO_RS):
+    def test_single_objective_error(self, factory_fn=get_MOO_RS) -> None:
         single_obj_exp = get_branin_experiment(with_batch=True)
         with self.assertRaises(ValueError):
             factory_fn(
                 experiment=single_obj_exp,
                 data=single_obj_exp.fetch_data(),
             )
 
@@ -317,15 +221,20 @@
             {
                 "acquisition_function_kwargs": {
                     "random_scalarization": True,
                 }
             },
             moo_rs._default_model_gen_options,
         )
-        moo_rs_run = moo_rs.gen(n=2)
+        with mock.patch(
+            "ax.models.torch.botorch_moo_defaults.optimize_acqf_list",
+            wraps=optimize_acqf_list,
+        ) as mock_optimize_acqf_list:
+            moo_rs_run = moo_rs.gen(n=2)
+        mock_optimize_acqf_list.assert_called()
         self.assertEqual(len(moo_rs_run.arms), 2)
 
     @fast_botorch_optimize
     def test_MOO_PAREGO(self) -> None:
         self.test_single_objective_error(get_MOO_PAREGO)
         multi_obj_exp = self.test_data_error_and_get_multi_obj_exp(get_MOO_PAREGO)
         moo_parego = get_MOO_PAREGO(
@@ -347,31 +256,38 @@
             mock_infer_ot.assert_not_called()
         self.assertEqual(len(moo_parego_run.arms), 2)
 
     @fast_botorch_optimize
     def test_MOO_EHVI(self) -> None:
         self.test_single_objective_error(get_MOO_EHVI)
         multi_obj_exp, optimization_config = get_multi_obj_exp_and_opt_config()
-        # ValueError: MultiObjectiveOptimization requires non-empty data.
-        with self.assertRaises(ValueError):
+        with self.assertRaisesRegex(
+            ValueError,
+            "MultiObjectiveOptimization requires non-empty data.",
+        ):
             get_MOO_EHVI(
                 experiment=multi_obj_exp,
                 data=multi_obj_exp.fetch_data(),
                 optimization_config=optimization_config,
             )
 
         multi_obj_exp.trials[0].run().mark_completed()
         moo_ehvi = get_MOO_EHVI(
             experiment=multi_obj_exp,
             data=multi_obj_exp.fetch_data(),
             optimization_config=optimization_config,
         )
         self.assertIsInstance(moo_ehvi, TorchModelBridge)
-        moo_ehvi_run = moo_ehvi.gen(n=1)
+        with mock.patch(
+            "ax.models.torch.botorch_defaults.optimize_acqf", wraps=optimize_acqf
+        ) as mock_optimize_acqf:
+            moo_ehvi_run = moo_ehvi.gen(n=1)
         self.assertEqual(len(moo_ehvi_run.arms), 1)
+        mock_optimize_acqf.assert_called_once()
+        self.assertTrue(mock_optimize_acqf.call_args.kwargs["sequential"])
 
     @fast_botorch_optimize
     def test_MTGP_PAREGO(self) -> None:
         """Tests MTGP ParEGO instantiation."""
         self.test_single_objective_error(get_MTGP_PAREGO)
         multi_obj_exp, optimization_config = get_multi_obj_exp_and_opt_config()
         with self.assertRaises(ValueError):
@@ -395,19 +311,24 @@
             optimization_config=optimization_config,
         )
         self.assertIsInstance(mt_ehvi, TorchModelBridge)
         # pyre-fixme[16]: Optional type has no attribute `model`.
         self.assertIsInstance(mt_ehvi.model.model.models[0], MultiTaskGP)
         task_covar_factor = mt_ehvi.model.model.models[0].task_covar_module.covar_factor
         self.assertEqual(task_covar_factor.shape, torch.Size([2, 2]))
-        mt_ehvi_run = mt_ehvi.gen(
-            n=1,
-            fixed_features=ObservationFeatures(parameters={}, trial_index=1),
-        )
+        with mock.patch(
+            "ax.models.torch.botorch_moo_defaults.optimize_acqf_list",
+            wraps=optimize_acqf_list,
+        ) as mock_optimize_acqf_list:
+            mt_ehvi_run = mt_ehvi.gen(
+                n=1,
+                fixed_features=ObservationFeatures(parameters={}, trial_index=1),
+            )
         self.assertEqual(len(mt_ehvi_run.arms), 1)
+        mock_optimize_acqf_list.assert_called_once()
 
         # Bad index given
         with self.assertRaises(ValueError):
             get_MTGP_PAREGO(
                 experiment=multi_obj_exp,
                 data=multi_obj_exp.fetch_data(),
                 trial_index=999,
@@ -439,16 +360,21 @@
         multi_obj_exp.trials[0].run()
         moo_ehvi = get_MOO_NEHVI(
             experiment=multi_obj_exp,
             data=multi_obj_exp.fetch_data(),
             optimization_config=optimization_config,
         )
         self.assertIsInstance(moo_ehvi, TorchModelBridge)
-        moo_ehvi_run = moo_ehvi.gen(n=1)
+        with mock.patch(
+            "ax.models.torch.botorch_defaults.optimize_acqf", wraps=optimize_acqf
+        ) as mock_optimize_acqf:
+            moo_ehvi_run = moo_ehvi.gen(n=1)
         self.assertEqual(len(moo_ehvi_run.arms), 1)
+        mock_optimize_acqf.assert_called_once()
+        self.assertTrue(mock_optimize_acqf.call_args.kwargs["sequential"])
 
     @fast_botorch_optimize
     def test_MOO_with_more_outcomes_than_thresholds(self) -> None:
         experiment = get_branin_experiment_with_multi_objective(
             has_optimization_config=False
         )
         metric_c = Metric(name="c", lower_is_better=False)
@@ -535,18 +461,23 @@
             optimization_config=optimization_config,
         )
         self.assertIsInstance(mt_ehvi, TorchModelBridge)
         # pyre-fixme[16]: Optional type has no attribute `model`.
         self.assertIsInstance(mt_ehvi.model.model.models[0], MultiTaskGP)
         task_covar_factor = mt_ehvi.model.model.models[0].task_covar_module.covar_factor
         self.assertEqual(task_covar_factor.shape, torch.Size([2, 2]))
-        mt_ehvi_run = mt_ehvi.gen(
-            n=1,
-            fixed_features=ObservationFeatures(parameters={}, trial_index=1),
-        )
+        with mock.patch(
+            "ax.models.torch.botorch_defaults.optimize_acqf", wraps=optimize_acqf
+        ) as mock_optimize_acqf:
+            mt_ehvi_run = mt_ehvi.gen(
+                n=1,
+                fixed_features=ObservationFeatures(parameters={}, trial_index=1),
+            )
+        mock_optimize_acqf.assert_called_once()
+        self.assertTrue(mock_optimize_acqf.call_args.kwargs["sequential"])
         self.assertEqual(len(mt_ehvi_run.arms), 1)
 
         # Bad index given
         with self.assertRaises(ValueError):
             get_MTGP_NEHVI(
                 experiment=multi_obj_exp,
                 data=multi_obj_exp.fetch_data(),
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_generation_node.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_generation_node.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-import logging
+# pyre-strict
+
 from logging import Logger
 from unittest.mock import patch, PropertyMock
 
 from ax.core.base_trial import TrialStatus
-
 from ax.core.observation import ObservationFeatures
 from ax.exceptions.core import UserInputError
 from ax.modelbridge.cross_validation import (
     DiagnosticCriterion,
     MetricAggregation,
     SingleDiagnosticBestModelSelector,
 )
@@ -28,14 +28,15 @@
 from ax.utils.testing.mock import fast_botorch_optimize
 
 logger: Logger = get_logger(__name__)
 
 
 class TestGenerationNode(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.sobol_model_spec = ModelSpec(
             model_enum=Models.SOBOL,
             model_kwargs={"init_position": 3},
             model_gen_kwargs={"some_gen_kwarg": "some_value"},
         )
         self.sobol_generation_node = GenerationNode(
             node_name="test", model_specs=[self.sobol_model_spec]
@@ -108,52 +109,69 @@
                             parameters={},
                             trial_index=0,
                         ),
                     },
                 ),
             ],
         )
+        self.assertIsNone(node.model_to_gen_from_name)
         dat = self.branin_experiment.lookup_data()
         node.fit(
             experiment=self.branin_experiment,
             data=dat,
         )
-        self.assertEqual(node.model_enum, node.model_specs[0].model_enum)
-        self.assertEqual(node.model_kwargs, node.model_specs[0].model_kwargs)
-        self.assertEqual(node.model_gen_kwargs, node.model_specs[0].model_gen_kwargs)
-        self.assertEqual(node.model_cv_kwargs, node.model_specs[0].model_cv_kwargs)
-        self.assertEqual(node.fixed_features, node.model_specs[0].fixed_features)
-        self.assertEqual(node.cv_results, node.model_specs[0].cv_results)
-        self.assertEqual(node.diagnostics, node.model_specs[0].diagnostics)
+        self.assertEqual(
+            node.model_spec_to_gen_from.model_enum, node.model_specs[0].model_enum
+        )
+        self.assertEqual(
+            node.model_spec_to_gen_from.model_kwargs, node.model_specs[0].model_kwargs
+        )
+        self.assertEqual(node.model_to_gen_from_name, "GPEI")
+        self.assertEqual(
+            node.model_spec_to_gen_from.model_gen_kwargs,
+            node.model_specs[0].model_gen_kwargs,
+        )
+        self.assertEqual(
+            node.model_spec_to_gen_from.model_cv_kwargs,
+            node.model_specs[0].model_cv_kwargs,
+        )
+        self.assertEqual(
+            node.model_spec_to_gen_from.fixed_features,
+            node.model_specs[0].fixed_features,
+        )
+        self.assertEqual(
+            node.model_spec_to_gen_from.cv_results, node.model_specs[0].cv_results
+        )
+        self.assertEqual(
+            node.model_spec_to_gen_from.diagnostics, node.model_specs[0].diagnostics
+        )
         self.assertEqual(node.node_name, "test")
-        self.assertEqual(node.gen_unlimited_trials, True)
         self.assertEqual(node._unique_id, "test")
 
     def test_node_string_representation(self) -> None:
         node = GenerationNode(
             node_name="test",
             model_specs=[
                 ModelSpec(
                     model_enum=Models.GPEI,
                     model_kwargs={},
                     model_gen_kwargs={},
                 ),
             ],
-            gen_unlimited_trials=False,
             transition_criteria=[
                 MaxTrials(threshold=5, only_in_statuses=[TrialStatus.RUNNING])
             ],
         )
         string_rep = str(node)
         self.assertEqual(
             string_rep,
             (
                 "GenerationNode(model_specs=[ModelSpec(model_enum=GPEI,"
                 " model_kwargs={}, model_gen_kwargs={}, model_cv_kwargs={},"
-                " )], node_name=test, gen_unlimited_trials=False, "
+                " )], node_name=test, "
                 "transition_criteria=[MaxTrials({'threshold': 5, "
                 "'only_in_statuses': [<enum 'TrialStatus'>.RUNNING], "
                 "'not_in_statuses': None, 'transition_to': None, "
                 "'block_transition_if_unmet': True, 'block_gen_if_met': False, "
                 "'use_all_trials_in_exp': False})])"
             ),
         )
@@ -168,74 +186,23 @@
                     model_gen_kwargs={
                         "n": 2,
                         "fixed_features": ObservationFeatures(parameters={"x": 0}),
                     },
                 ),
             ],
         )
-        self.assertEqual(node.fixed_features, ObservationFeatures(parameters={"x": 0}))
-
-    def test_multiple_same_fixed_features(self) -> None:
-        node = GenerationNode(
-            node_name="test",
-            model_specs=[
-                ModelSpec(
-                    model_enum=Models.GPEI,
-                    model_kwargs={},
-                    model_gen_kwargs={
-                        "n": 2,
-                        "fixed_features": ObservationFeatures(parameters={"x": 0}),
-                    },
-                ),
-                ModelSpec(
-                    model_enum=Models.GPEI,
-                    model_kwargs={},
-                    model_gen_kwargs={
-                        "n": 3,
-                        "fixed_features": ObservationFeatures(parameters={"x": 0}),
-                    },
-                ),
-            ],
-        )
-        self.assertEqual(node.fixed_features, ObservationFeatures(parameters={"x": 0}))
-
-    def test_generator_run_limit_unlimited_without_flag(self) -> None:
-        """This tests checks that when the `gen_unlimited_trials` flag is false
-        but there are no generation blocking criteria, then the generator run limit
-        is set to -1 and a warning is logged.
-        """
-        node = GenerationNode(
-            node_name="test",
-            model_specs=[
-                ModelSpec(
-                    model_enum=Models.GPEI,
-                    model_kwargs={},
-                    model_gen_kwargs={
-                        "n": -1,
-                        "fixed_features": ObservationFeatures(parameters={"x": 0}),
-                    },
-                ),
-            ],
-            gen_unlimited_trials=False,
+        self.assertEqual(
+            node.model_spec_to_gen_from.fixed_features,
+            ObservationFeatures(parameters={"x": 0}),
         )
-        warning_msg = (
-            "Even though this node is not flagged for generation of unlimited "
-            "trials, there are no generation blocking criterion, therefore, "
-            "unlimited trials will be generated."
-        )
-        with self.assertLogs(GenerationNode.__module__, logging.WARNING) as logger:
-            self.assertEqual(node.generator_run_limit(), -1)
-            self.assertTrue(
-                any(warning_msg in output for output in logger.output),
-                logger.output,
-            )
 
 
 class TestGenerationStep(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.model_kwargs = {"init_position": 5}
         self.sobol_generation_step = GenerationStep(
             model=Models.SOBOL,
             num_trials=5,
             model_kwargs=self.model_kwargs,
         )
         self.model_spec = ModelSpec(
@@ -297,29 +264,30 @@
         self.assertEqual(self.sobol_generation_step.model_spec, self.model_spec)
         self.assertEqual(self.sobol_generation_step._unique_id, "-1")
 
 
 class TestGenerationNodeWithBestModelSelector(TestCase):
     @fast_botorch_optimize
     def setUp(self) -> None:
+        super().setUp()
         self.branin_experiment = get_branin_experiment()
         sobol = Models.SOBOL(search_space=self.branin_experiment.search_space)
         sobol_run = sobol.gen(n=20)
         self.branin_experiment.new_batch_trial().add_generator_run(
             sobol_run
         ).run().mark_completed()
         data = self.branin_experiment.fetch_data()
 
         ms_gpei = ModelSpec(model_enum=Models.GPEI)
         ms_gpei.fit(experiment=self.branin_experiment, data=data)
 
-        ms_gpkg = ModelSpec(model_enum=Models.GPKG)
-        ms_gpkg.fit(experiment=self.branin_experiment, data=data)
+        ms_botorch = ModelSpec(model_enum=Models.BOTORCH_MODULAR)
+        ms_botorch.fit(experiment=self.branin_experiment, data=data)
 
-        self.fitted_model_specs = [ms_gpei, ms_gpkg]
+        self.fitted_model_specs = [ms_gpei, ms_botorch]
 
         self.model_selection_node = GenerationNode(
             node_name="test",
             model_specs=self.fitted_model_specs,
             best_model_selector=SingleDiagnosticBestModelSelector(
                 diagnostic="Fisher exact test p",
                 # pyre-fixme[6]: For 2nd param expected `DiagnosticCriterion` but
@@ -342,19 +310,7 @@
         # Currently, `ModelSelectionNode` should just pick the first model
         # spec as the one to generate from.
         # TODO[adamobeng]: Test correct behavior here when implemented.
         self.assertEqual(gr._model_key, "GPEI")
 
         # test model_to_gen_from_name property
         self.assertEqual(self.model_selection_node.model_to_gen_from_name, "GPEI")
-
-    def test_fixed_features_is_from_model_to_gen_from(self) -> None:
-        self.model_selection_node.model_specs[0].fixed_features = ObservationFeatures(
-            parameters={"x": 0}
-        )
-        self.model_selection_node.model_specs[1].fixed_features = ObservationFeatures(
-            parameters={"x": 1}
-        )
-        self.assertEqual(
-            self.model_selection_node.fixed_features,
-            self.model_selection_node.model_spec_to_gen_from.fixed_features,
-        )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_generation_strategy.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_generation_strategy.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from typing import cast, List
 from unittest import mock
 from unittest.mock import MagicMock, patch
 
 from ax.core.arm import Arm
 from ax.core.base_trial import TrialStatus
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, FixedParameter, Parameter, ParameterType
-from ax.core.search_space import SearchSpace
+from ax.core.search_space import HierarchicalSearchSpace, SearchSpace
 from ax.core.utils import (
     get_pending_observation_features_based_on_trial_status as get_pending,
 )
 from ax.exceptions.core import DataRequiredError, UnsupportedError, UserInputError
 from ax.exceptions.generation_strategy import (
     GenerationStrategyCompleted,
     GenerationStrategyMisconfiguredException,
@@ -44,28 +46,29 @@
     MaxTrials,
     MinTrials,
 )
 from ax.models.random.sobol import SobolGenerator
 from ax.utils.common.equality import same_elements
 from ax.utils.common.mock import mock_patch_method_original
 from ax.utils.common.testutils import TestCase
-from ax.utils.common.typeutils import not_none
+from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.testing.core_stubs import (
     get_branin_data,
     get_branin_experiment,
     get_choice_parameter,
     get_data,
     get_experiment_with_multi_objective,
     get_hierarchical_search_space_experiment,
 )
 from ax.utils.testing.mock import fast_botorch_optimize
 
 
 class TestGenerationStrategy(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.gr = GeneratorRun(arms=[Arm(parameters={"x1": 1, "x2": 2})])
 
         # Mock out slow GPEI.
         self.torch_model_bridge_patcher = patch(
             f"{TorchModelBridge.__module__}.TorchModelBridge", spec=True
         )
         self.mock_torch_model_bridge = self.torch_model_bridge_patcher.start()
@@ -123,14 +126,59 @@
                     Models.SOBOL,
                     num_trials=-1,
                     should_deduplicate=True,
                 )
             ]
         )
 
+        # Set up the node-based generation strategy for testing.
+        self.sobol_criterion = [
+            MaxTrials(
+                threshold=5,
+                transition_to="GPEI_node",
+                block_gen_if_met=True,
+                only_in_statuses=None,
+                not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
+            )
+        ]
+        self.gpei_criterion = [
+            MaxTrials(
+                threshold=2,
+                transition_to=None,
+                block_gen_if_met=True,
+                only_in_statuses=None,
+                not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
+            )
+        ]
+        self.sobol_model_spec = ModelSpec(
+            model_enum=Models.SOBOL,
+            model_kwargs=self.step_model_kwargs,
+            model_gen_kwargs={},
+        )
+        self.gpei_model_spec = ModelSpec(
+            model_enum=Models.GPEI,
+            model_kwargs=self.step_model_kwargs,
+            model_gen_kwargs={},
+        )
+        self.sobol_node = GenerationNode(
+            node_name="sobol_node",
+            transition_criteria=self.sobol_criterion,
+            model_specs=[self.sobol_model_spec],
+        )
+        self.gpei_node = GenerationNode(
+            node_name="GPEI_node",
+            transition_criteria=self.gpei_criterion,
+            model_specs=[self.gpei_model_spec],
+        )
+
+        self.sobol_GPEI_GS_nodes = GenerationStrategy(
+            name="Sobol+GPEI_Nodes",
+            nodes=[self.sobol_node, self.gpei_node],
+        )
+
     def tearDown(self) -> None:
         self.torch_model_bridge_patcher.stop()
         self.discrete_model_bridge_patcher.stop()
         self.registry_setup_dict_patcher.stop()
 
     def test_unique_step_names(self) -> None:
         """This tests the name of the steps on generation strategy. The name is
@@ -143,15 +191,25 @@
                 GenerationStep(model=Models.GPEI, num_trials=-1),
             ]
         )
         self.assertEqual(gs._steps[0].node_name, "GenerationStep_0")
         self.assertEqual(gs._steps[1].node_name, "GenerationStep_1")
 
     def test_name(self) -> None:
-        self.sobol_GS.name = "SomeGSName"
+        self.assertEqual(self.sobol_GS._name, "Sobol")
+        self.assertEqual(
+            GenerationStrategy(
+                steps=[
+                    GenerationStep(model=Models.SOBOL, num_trials=5),
+                    GenerationStep(model=Models.GPEI, num_trials=-1),
+                ],
+            ).name,
+            "Sobol+GPEI",
+        )
+        self.sobol_GS._name = "SomeGSName"
         self.assertEqual(self.sobol_GS.name, "SomeGSName")
 
     def test_validation(self) -> None:
         # num_trials can be positive or -1.
         with self.assertRaises(UserInputError):
             GenerationStrategy(
                 steps=[
@@ -238,18 +296,18 @@
                         ),
                     ],
                 )
             ]
         )
         self.assertEqual(
             str(gs3),
-            "GenerationStrategy(name='Sobol', nodes=[GenerationNode("
+            "GenerationStrategy(name='test', nodes=[GenerationNode("
             "model_specs=[ModelSpec(model_enum=Sobol, "
             "model_kwargs={}, model_gen_kwargs={}, model_cv_kwargs={},"
-            " )], node_name=test, gen_unlimited_trials=True, "
+            " )], node_name=test, "
             "transition_criteria=[])])",
         )
 
     def test_equality(self) -> None:
         gs1 = GenerationStrategy(
             name="Sobol+GPEI",
             steps=[
@@ -307,15 +365,14 @@
             gs.gen(exp)
         # Make sure Sobol is used to generate the 6th point.
         self.assertIsInstance(gs._model, RandomModelBridge)
 
     def test_sobol_GPEI_strategy(self) -> None:
         exp = get_branin_experiment()
         self.assertEqual(self.sobol_GPEI_GS.name, "Sobol+GPEI")
-        self.assertEqual(self.sobol_GPEI_GS.model_transitions, [5])
         for i in range(7):
             g = self.sobol_GPEI_GS.gen(exp)
             exp.new_trial(generator_run=g).run()
             self.assertEqual(len(self.sobol_GPEI_GS._generator_runs), i + 1)
             if i > 4:
                 self.mock_torch_model_bridge.assert_called()
             else:
@@ -380,15 +437,14 @@
                     model=Models.GPEI,
                     num_trials=-1,
                     model_kwargs=self.step_model_kwargs,
                 ),
             ]
         )
         self.assertEqual(sobol_GPEI_generation_strategy.name, "Sobol+GPEI")
-        self.assertEqual(sobol_GPEI_generation_strategy.model_transitions, [5])
         exp.new_trial(generator_run=sobol_GPEI_generation_strategy.gen(exp)).run()
         for i in range(1, 15):
             g = sobol_GPEI_generation_strategy.gen(exp)
             exp.new_trial(generator_run=g).run()
             if i > 4:
                 self.assertIsInstance(
                     sobol_GPEI_generation_strategy.model, TorchModelBridge
@@ -428,15 +484,14 @@
                     model_kwargs=self.step_model_kwargs,
                 ),
             ]
         )
         self.assertEqual(
             factorial_thompson_generation_strategy.name, "Factorial+Thompson"
         )
-        self.assertEqual(factorial_thompson_generation_strategy.model_transitions, [1])
         mock_model_bridge = self.mock_discrete_model_bridge.return_value
 
         # Initial factorial batch.
         exp.new_batch_trial(factorial_thompson_generation_strategy.gen(experiment=exp))
         args, kwargs = mock_model_bridge._set_kwargs_to_save.call_args
         self.assertEqual(kwargs.get("model_key"), "Factorial")
 
@@ -488,15 +543,14 @@
                 ),
                 GenerationStep(
                     model=Models.GPEI, num_trials=6, model_kwargs=self.step_model_kwargs
                 ),
             ],
         )
         self.assertEqual(sobol_GPEI_generation_strategy.name, "Sobol+GPEI")
-        self.assertEqual(sobol_GPEI_generation_strategy.model_transitions, [1])
         gr = sobol_GPEI_generation_strategy.gen(exp, n=2)
         exp.new_batch_trial(generator_run=gr).run()
         for i in range(1, 8):
             if i == 7:
                 # Check completeness error message.
                 with self.assertRaises(GenerationStrategyCompleted):
                     g = sobol_GPEI_generation_strategy.gen(exp, n=2)
@@ -543,38 +597,28 @@
                 GenerationStep(model=Models.SOBOL, num_trials=3),
             ]
         )
         # No trials yet, so the DF will be None.
         self.assertIsNone(sobol_generation_strategy.trials_as_df)
         # Now the trial should appear in the DF.
         trial = exp.new_trial(sobol_generation_strategy.gen(experiment=exp))
-        # pyre-fixme[16]: Optional type has no attribute `empty`.
-        self.assertFalse(sobol_generation_strategy.trials_as_df.empty)
-        self.assertEqual(
-            # pyre-fixme[16]: Optional type has no attribute `head`.
-            sobol_generation_strategy.trials_as_df.head()["Trial Status"][0],
-            "CANDIDATE",
-        )
+        trials_df = not_none(sobol_generation_strategy.trials_as_df)
+        self.assertFalse(trials_df.empty)
+        self.assertEqual(trials_df.head()["Trial Status"][0], "CANDIDATE")
         # Changes in trial status should be reflected in the DF.
         trial._status = TrialStatus.RUNNING
-        self.assertEqual(
-            sobol_generation_strategy.trials_as_df.head()["Trial Status"][0], "RUNNING"
-        )
+        trials_df = not_none(sobol_generation_strategy.trials_as_df)
+        self.assertEqual(trials_df.head()["Trial Status"][0], "RUNNING")
         # Check that rows are present for step 0 and 1 after moving to step 1
         for _i in range(3):
             # attach necessary trials to fill up the Generation Strategy
             trial = exp.new_trial(sobol_generation_strategy.gen(experiment=exp))
-        self.assertEqual(
-            sobol_generation_strategy.trials_as_df.head()["Generation Step"][0],
-            "GenerationStep_0",
-        )
-        self.assertEqual(
-            sobol_generation_strategy.trials_as_df.head()["Generation Step"][2],
-            "GenerationStep_1",
-        )
+        trials_df = not_none(sobol_generation_strategy.trials_as_df)
+        self.assertEqual(trials_df.head()["Generation Step"][0], "GenerationStep_0")
+        self.assertEqual(trials_df.head()["Generation Step"][2], "GenerationStep_1")
 
         # construct the same GS as above but directly with nodes
         sobol_model_spec = ModelSpec(
             model_enum=Models.SOBOL,
             model_kwargs={},
             model_gen_kwargs={},
         )
@@ -588,56 +632,47 @@
                             threshold=2,
                             not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
                             block_gen_if_met=True,
                             block_transition_if_unmet=True,
                             transition_to="sobol_3_trial",
                         )
                     ],
-                    gen_unlimited_trials=False,
                 ),
                 GenerationNode(
                     node_name="sobol_3_trial",
                     model_specs=[sobol_model_spec],
                     transition_criteria=[
                         MaxTrials(
                             threshold=2,
                             not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
                             block_gen_if_met=True,
                             block_transition_if_unmet=True,
                             transition_to=None,
                         )
                     ],
-                    gen_unlimited_trials=False,
                 ),
             ]
         )
         self.assertIsNone(node_gs.trials_as_df)
         # Now the trial should appear in the DF.
         trial = exp.new_trial(node_gs.gen(experiment=exp))
-
-        self.assertFalse(node_gs.trials_as_df.empty)
-        self.assertEqual(
-            node_gs.trials_as_df.head()["Trial Status"][0],
-            "CANDIDATE",
-        )
+        trials_df = not_none(node_gs.trials_as_df)
+        self.assertFalse(trials_df.empty)
+        self.assertEqual(trials_df.head()["Trial Status"][0], "CANDIDATE")
         # Changes in trial status should be reflected in the DF.
         trial._status = TrialStatus.RUNNING
-        self.assertEqual(node_gs.trials_as_df.head()["Trial Status"][0], "RUNNING")
+        trials_df = not_none(node_gs.trials_as_df)
+        self.assertEqual(trials_df.head()["Trial Status"][0], "RUNNING")
         # Check that rows are present for step 0 and 1 after moving to step 1
         for _i in range(3):
             # attach necessary trials to fill up the Generation Strategy
             trial = exp.new_trial(node_gs.gen(experiment=exp))
-        self.assertEqual(
-            node_gs.trials_as_df.head()["Generation Node"][0],
-            "sobol_2_trial",
-        )
-        self.assertEqual(
-            node_gs.trials_as_df.head()["Generation Node"][2],
-            "sobol_3_trial",
-        )
+        trials_df = not_none(node_gs.trials_as_df)
+        self.assertEqual(trials_df.head()["Generation Node"][0], "sobol_2_trial")
+        self.assertEqual(trials_df.head()["Generation Node"][2], "sobol_3_trial")
 
     def test_max_parallelism_reached(self) -> None:
         exp = get_branin_experiment()
         sobol_generation_strategy = GenerationStrategy(
             steps=[GenerationStep(model=Models.SOBOL, num_trials=5, max_parallelism=1)]
         )
         exp.new_trial(
@@ -776,19 +811,17 @@
             # contain all parameters of the flat search space.
             with patch.object(
                 RandomModelBridge, "_fit"
             ) as mock_model_fit, patch.object(RandomModelBridge, "gen"):
                 self.sobol_GS.gen(experiment=experiment)
                 mock_model_fit.assert_called_once()
                 observations = mock_model_fit.call_args[1].get("observations")
-                all_parameter_names = (
-                    # pyre-fixme[16]: `SearchSpace` has no attribute
-                    #  `_all_parameter_names`.
-                    experiment.search_space._all_parameter_names.copy()
-                )
+                all_parameter_names = checked_cast(
+                    HierarchicalSearchSpace, experiment.search_space
+                )._all_parameter_names.copy()
                 # One of the parameter names is modified by transforms (because it's
                 # one-hot encoded).
                 all_parameter_names.remove("model")
                 all_parameter_names.add("model_OH_PARAM_")
                 for obs in observations:
                     for p_name in all_parameter_names:
                         self.assertIn(p_name, obs.features.parameters)
@@ -1071,15 +1104,15 @@
                     ),
                     GenerationNode(
                         node_name="node_1",
                         model_specs=[sobol_model_spec],
                     ),
                 ],
             )
-        # check error raised if transition to arguemnt is not valid
+        # check error raised if transition to argument is not valid
         with self.assertRaisesRegex(
             GenerationStrategyMisconfiguredException, "`transition_to` argument"
         ):
             GenerationStrategy(
                 nodes=[
                     GenerationNode(
                         node_name="node_1",
@@ -1091,15 +1124,15 @@
                         model_specs=[sobol_model_spec],
                     ),
                 ],
             )
 
         # check error raised if provided both steps and nodes
         with self.assertRaisesRegex(
-            GenerationStrategyMisconfiguredException, "either steps or nodes"
+            GenerationStrategyMisconfiguredException, "contain either steps or nodes"
         ):
             GenerationStrategy(
                 nodes=[
                     GenerationNode(
                         node_name="node_1",
                         transition_criteria=node_1_criterion,
                         model_specs=[sobol_model_spec],
@@ -1121,15 +1154,15 @@
                         model_kwargs=self.step_model_kwargs,
                     ),
                 ],
             )
 
         # check error raised if provided both steps and nodes under node list
         with self.assertRaisesRegex(
-            GenerationStrategyMisconfiguredException, "must either be a GenerationStep"
+            GenerationStrategyMisconfiguredException, "`GenerationStrategy` inputs are:"
         ):
             GenerationStrategy(
                 nodes=[
                     GenerationNode(
                         node_name="node_1",
                         transition_criteria=node_1_criterion,
                         model_specs=[sobol_model_spec],
@@ -1166,66 +1199,21 @@
             self.assertTrue(
                 any(warning_msg in output for output in logger.output),
                 logger.output,
             )
 
     def test_gs_with_generation_nodes(self) -> None:
         "Simple test of a SOBOL + GPEI GenerationStrategy composed of GenerationNodes"
-        sobol_criterion = [
-            MaxTrials(
-                threshold=5,
-                transition_to="GPEI_node",
-                block_gen_if_met=True,
-                only_in_statuses=None,
-                not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
-            )
-        ]
-        gpei_criterion = [
-            MaxTrials(
-                threshold=2,
-                transition_to=None,
-                block_gen_if_met=True,
-                only_in_statuses=None,
-                not_in_statuses=[TrialStatus.FAILED, TrialStatus.ABANDONED],
-            )
-        ]
-        sobol_model_spec = ModelSpec(
-            model_enum=Models.SOBOL,
-            model_kwargs=self.step_model_kwargs,
-            model_gen_kwargs={},
-        )
-        gpei_model_spec = ModelSpec(
-            model_enum=Models.GPEI,
-            model_kwargs=self.step_model_kwargs,
-            model_gen_kwargs={},
-        )
-        sobol_node = GenerationNode(
-            node_name="sobol_node",
-            transition_criteria=sobol_criterion,
-            model_specs=[sobol_model_spec],
-            gen_unlimited_trials=False,
-        )
-        gpei_node = GenerationNode(
-            node_name="GPEI_node",
-            transition_criteria=gpei_criterion,
-            model_specs=[gpei_model_spec],
-            gen_unlimited_trials=False,
-        )
-
-        sobol_GPEI_GS_nodes = GenerationStrategy(
-            name="Sobol+GPEI_Nodes",
-            nodes=[sobol_node, gpei_node],
-        )
         exp = get_branin_experiment()
-        self.assertEqual(sobol_GPEI_GS_nodes.name, "Sobol+GPEI_Nodes")
+        self.assertEqual(self.sobol_GPEI_GS_nodes.name, "Sobol+GPEI_Nodes")
 
         for i in range(7):
-            g = sobol_GPEI_GS_nodes.gen(exp)
+            g = self.sobol_GPEI_GS_nodes.gen(exp)
             exp.new_trial(generator_run=g).run()
-            self.assertEqual(len(sobol_GPEI_GS_nodes._generator_runs), i + 1)
+            self.assertEqual(len(self.sobol_GPEI_GS_nodes._generator_runs), i + 1)
             if i > 4:
                 self.mock_torch_model_bridge.assert_called()
             else:
                 self.assertEqual(g._model_key, "Sobol")
                 mkw = g._model_kwargs
                 self.assertIsNotNone(mkw)
                 if i > 0:
@@ -1266,14 +1254,74 @@
                 # Generated points are randomized, so just checking that they are there.
                 self.assertIn("generated_points", ms)
                 # Remove the randomized generated points to compare the rest.
                 ms = ms.copy()
                 del ms["generated_points"]
                 self.assertEqual(ms, {"init_position": i + 1})
 
+    def test_clone_reset_nodes(self) -> None:
+        """Test that node-based generation strategy is appropriately reset
+        when cloned with `clone_reset`.
+        """
+        exp = get_branin_experiment()
+        for i in range(7):
+            g = self.sobol_GPEI_GS_nodes.gen(exp)
+            exp.new_trial(generator_run=g).run()
+            self.assertEqual(len(self.sobol_GPEI_GS_nodes._generator_runs), i + 1)
+        gs_clone = self.sobol_GPEI_GS_nodes.clone_reset()
+        self.assertEqual(gs_clone.name, self.sobol_GPEI_GS_nodes.name)
+        self.assertEqual(gs_clone._generator_runs, [])
+
+    def test_gs_with_nodes_and_blocking_criteria(self) -> None:
+        sobol_model_spec = ModelSpec(
+            model_enum=Models.SOBOL,
+            model_kwargs=self.step_model_kwargs,
+            model_gen_kwargs={},
+        )
+        sobol_node_with_criteria = GenerationNode(
+            node_name="test",
+            model_specs=[sobol_model_spec],
+            transition_criteria=[
+                MaxTrials(
+                    threshold=3,
+                    block_gen_if_met=True,
+                    block_transition_if_unmet=True,
+                    transition_to="GPEI_node",
+                ),
+                MinTrials(
+                    threshold=2,
+                    only_in_statuses=[TrialStatus.COMPLETED],
+                    block_gen_if_met=False,
+                    block_transition_if_unmet=True,
+                    transition_to="GPEI_node",
+                ),
+            ],
+        )
+        gpei_model_spec = ModelSpec(
+            model_enum=Models.GPEI,
+            model_kwargs=self.step_model_kwargs,
+            model_gen_kwargs={},
+        )
+        gpei_node = GenerationNode(
+            node_name="GPEI_node",
+            model_specs=[gpei_model_spec],
+        )
+        gs = GenerationStrategy(
+            name="Sobol+GPEI_Nodes",
+            nodes=[sobol_node_with_criteria, gpei_node],
+        )
+        exp = get_branin_experiment()
+        for _ in range(5):
+            trial = exp.new_trial(
+                generator_run=gs.gen(n=1, experiment=exp, data=exp.lookup_data())
+            )
+            trial.mark_running(no_runner_required=True)
+            exp.attach_data(get_branin_data(trials=[trial]))
+            trial.mark_completed()
+
     def test_step_based_gs_only(self) -> None:
         """Test the step_based_gs_only decorator"""
         sobol_model_spec = ModelSpec(
             model_enum=Models.SOBOL,
             model_kwargs={},
             model_gen_kwargs={"n": 2},
         )
@@ -1334,16 +1382,14 @@
         )
         gs2 = GenerationStrategy(
             steps=[
                 GenerationStep(model=Models.SOBOL, num_trials=5),
                 GenerationStep(model=Models.GPEI, num_trials=-1),
             ]
         )
-        print(gs1)
-        print(gs2)
         self.assertEqual(gs1, gs2)
 
     # ------------- Testing helpers (put tests above this line) -------------
 
     def _run_GS_for_N_rounds(
         self, gs: GenerationStrategy, exp: Experiment, num_rounds: int
     ) -> List[int]:
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_map_torch_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_map_torch_modelbridge.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 import numpy as np
 
 import torch
 
 from ax.core.base_trial import TrialStatus
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_model_fit_metrics.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_model_fit_metrics.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,50 +1,61 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+import warnings
+from itertools import product
 from typing import cast, Dict
 
+import numpy as np
 from ax.core.experiment import Experiment
 from ax.core.objective import Objective
 from ax.core.optimization_config import OptimizationConfig
 from ax.metrics.branin import BraninMetric
-from ax.modelbridge.cross_validation import compute_model_fit_metrics_from_modelbridge
+from ax.modelbridge.cross_validation import (
+    _predict_on_cross_validation_data,
+    _predict_on_training_data,
+    compute_model_fit_metrics_from_modelbridge,
+)
 from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
 from ax.modelbridge.registry import Models
 from ax.runners.synthetic import SyntheticRunner
 from ax.service.scheduler import get_fitted_model_bridge, Scheduler, SchedulerOptions
 from ax.utils.common.constants import Keys
 from ax.utils.common.testutils import TestCase
+from ax.utils.stats.model_fit_stats import _entropy_via_kde, entropy_of_observations
 from ax.utils.testing.core_stubs import get_branin_search_space
 
 NUM_SOBOL = 5
 
 
 class TestModelBridgeFitMetrics(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         # setting up experiment and generation strategy
         self.runner = SyntheticRunner()
         self.branin_experiment = Experiment(
             name="branin_test_experiment",
             search_space=get_branin_search_space(),
             runner=self.runner,
             optimization_config=OptimizationConfig(
                 objective=Objective(
                     metric=BraninMetric(name="branin", param_names=["x1", "x2"]),
                     minimize=True,
                 ),
             ),
             is_test=True,
         )
-        self.branin_experiment._properties[
-            Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF
-        ] = True
+        self.branin_experiment._properties[Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF] = (
+            True
+        )
         self.generation_strategy = GenerationStrategy(
             steps=[
                 GenerationStep(
                     model=Models.SOBOL, num_trials=NUM_SOBOL, max_parallelism=NUM_SOBOL
                 ),
                 GenerationStep(model=Models.GPEI, num_trials=-1),
             ]
@@ -54,15 +65,20 @@
         scheduler = Scheduler(
             experiment=self.branin_experiment,
             generation_strategy=self.generation_strategy,
             options=SchedulerOptions(),
         )
         # need to run some trials to initialize the ModelBridge
         scheduler.run_n_trials(max_trials=NUM_SOBOL + 1)
+
         model_bridge = get_fitted_model_bridge(scheduler)
+        self.assertEqual(len(model_bridge.get_training_data()), NUM_SOBOL)
+
+        model_bridge = get_fitted_model_bridge(scheduler, force_refit=True)
+        self.assertEqual(len(model_bridge.get_training_data()), NUM_SOBOL + 1)
 
         # testing compute_model_fit_metrics_from_modelbridge with default metrics
         fit_metrics = compute_model_fit_metrics_from_modelbridge(
             model_bridge=model_bridge,
             experiment=self.branin_experiment,
             untransform=False,
         )
@@ -76,15 +92,56 @@
         std = fit_metrics.get("std_of_the_standardized_error")
         self.assertIsInstance(std, dict)
         std = cast(Dict[str, float], std)
         self.assertTrue("branin" in std)
         std_branin = std["branin"]
         self.assertIsInstance(std_branin, float)
 
-        # testing with empty metrics
-        empty_metrics = compute_model_fit_metrics_from_modelbridge(
-            model_bridge=model_bridge,
-            experiment=self.branin_experiment,
-            fit_metrics_dict={},
-        )
-        self.assertIsInstance(empty_metrics, dict)
-        self.assertTrue(len(empty_metrics) == 0)
+        # checking non-default model-fit-metric
+        for untransform, generalization in product([True, False], [True, False]):
+            with self.subTest(untransform=untransform):
+                fit_metrics = compute_model_fit_metrics_from_modelbridge(
+                    model_bridge=model_bridge,
+                    experiment=scheduler.experiment,
+                    generalization=generalization,
+                    untransform=untransform,
+                    fit_metrics_dict={"Entropy": entropy_of_observations},
+                )
+                entropy = fit_metrics.get("Entropy")
+                self.assertIsInstance(entropy, dict)
+                entropy = cast(Dict[str, float], entropy)
+                self.assertTrue("branin" in entropy)
+                entropy_branin = entropy["branin"]
+                self.assertIsInstance(entropy_branin, float)
+
+                predict = (
+                    _predict_on_cross_validation_data
+                    if generalization
+                    else _predict_on_training_data
+                )
+                y_obs, _, _ = predict(
+                    model_bridge=model_bridge, untransform=untransform
+                )
+                y_obs_branin = np.array(y_obs["branin"])[:, np.newaxis]
+                entropy_truth = _entropy_via_kde(y_obs_branin)
+                self.assertAlmostEqual(entropy_branin, entropy_truth)
+
+                # testing with empty metrics
+                empty_metrics = compute_model_fit_metrics_from_modelbridge(
+                    model_bridge=model_bridge,
+                    experiment=self.branin_experiment,
+                    fit_metrics_dict={},
+                )
+                self.assertIsInstance(empty_metrics, dict)
+                self.assertTrue(len(empty_metrics) == 0)
+
+                # testing log filtering
+                with warnings.catch_warnings(record=True) as ws:
+                    fit_metrics = compute_model_fit_metrics_from_modelbridge(
+                        model_bridge=model_bridge,
+                        experiment=self.branin_experiment,
+                        untransform=untransform,
+                        generalization=generalization,
+                    )
+                self.assertFalse(
+                    any("Input data is not standardized" in str(w.message) for w in ws)
+                )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_model_spec.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_model_spec.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
 from unittest import mock
 from unittest.mock import MagicMock, Mock, patch
 
 from ax.core.observation import ObservationFeatures
 from ax.exceptions.core import UserInputError
 from ax.modelbridge.factory import get_sobol
@@ -17,14 +19,15 @@
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_experiment
 from ax.utils.testing.mock import fast_botorch_optimize
 
 
 class BaseModelSpecTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_branin_experiment()
         sobol = Models.SOBOL(search_space=self.experiment.search_space)
         sobol_run = sobol.gen(n=20)
         self.experiment.new_batch_trial().add_generator_run(
             sobol_run
         ).run().mark_completed()
         self.data = self.experiment.fetch_data()
@@ -52,15 +55,15 @@
         ms.fit(experiment=self.experiment, data=self.data)
         wrapped_extract_ssd.assert_called_once()
         self.assertIsNotNone(ms._last_fit_arg_ids)
         self.assertEqual(ms._last_fit_arg_ids["experiment"], id(self.experiment))
         # This should skip the model fit.
         with mock.patch("ax.modelbridge.torch.logger") as mock_logger:
             ms.fit(experiment=self.experiment, data=self.data)
-        mock_logger.info.assert_called_with(
+        mock_logger.debug.assert_called_with(
             "The observations are identical to the last set of observations "
             "used to fit the model. Skipping model fitting."
         )
         wrapped_extract_ssd.assert_called_once()
 
     def test_model_key(self) -> None:
         ms = ModelSpec(model_enum=Models.GPEI)
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_modelbridge_utils.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_modelbridge_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from dataclasses import dataclass
 from typing import List, Union
 
 import numpy as np
 import torch
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_pairwise_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_pairwise_modelbridge.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict
 
 import numpy as np
 import torch
 from ax.core import Arm, GeneratorRun
 from ax.core.observation import ObservationData, ObservationFeatures
 from ax.core.parameter import RangeParameter
 from ax.core.types import TEvaluationOutcome, TParameterization
-
 from ax.modelbridge.pairwise import (
     _binary_pref_to_comp_pair,
     _consolidate_comparisons,
     PairwiseModelBridge,
 )
 from ax.models.torch.botorch_modular.model import BoTorchModel
 from ax.models.torch.botorch_modular.surrogate import Surrogate
@@ -29,14 +30,16 @@
 from botorch.models.pairwise_gp import PairwiseGP, PairwiseLaplaceMarginalLogLikelihood
 from botorch.models.transforms.input import Normalize
 from botorch.utils.datasets import RankingDataset
 
 
 class PairwiseModelBridgeTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
+
         def evaluate(
             parameters: Dict[str, TParameterization]
         ) -> Dict[str, TEvaluationOutcome]:
             # A pair at a time
             assert len(parameters.keys()) == 2
             arm1, arm2 = list(parameters.keys())
             arm1_outcome_values = [
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_prediction_utils.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_prediction_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.core.types import TEvaluationOutcome, TParameterization
 from ax.modelbridge.prediction_utils import predict_at_point, predict_by_features
 from ax.service.ax_client import AxClient
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_random_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_random_modelbridge.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,46 +1,49 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import OrderedDict
+from typing import List
 from unittest import mock
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ParameterType, RangeParameter
-from ax.core.parameter_constraint import OrderConstraint, SumConstraint
+from ax.core.parameter_constraint import (
+    OrderConstraint,
+    ParameterConstraint,
+    SumConstraint,
+)
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import SearchSpaceExhausted
 from ax.modelbridge.random import RandomModelBridge
 from ax.modelbridge.registry import Cont_X_trans
 from ax.models.random.base import RandomModel
 from ax.models.random.sobol import SobolGenerator
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_small_discrete_search_space
 
 
 class RandomModelBridgeTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         x = RangeParameter("x", ParameterType.FLOAT, lower=0, upper=1)
         y = RangeParameter("y", ParameterType.FLOAT, lower=1, upper=2)
         z = RangeParameter("z", ParameterType.FLOAT, lower=0, upper=5)
         self.parameters = [x, y, z]
-        parameter_constraints = [
+        parameter_constraints: List[ParameterConstraint] = [
             OrderConstraint(x, y),
             SumConstraint([x, z], False, 3.5),
         ]
-
-        # pyre-fixme[6]: For 2nd param expected
-        #  `Optional[List[ParameterConstraint]]` but got `List[Union[OrderConstraint,
-        #  SumConstraint]]`.
         self.search_space = SearchSpace(self.parameters, parameter_constraints)
-
         self.model_gen_options = {"option": "yes"}
 
     @mock.patch("ax.modelbridge.random.RandomModelBridge.__init__", return_value=None)
     # pyre-fixme[3]: Return type must be annotated.
     # pyre-fixme[2]: Parameter must be annotated.
     def test_Fit(self, mock_init):
         # pyre-fixme[20]: Argument `model` expected.
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_registry.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_registry.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import OrderedDict
 
 import numpy as np
 import pandas as pd
 import torch
 from ax.core.data import Data
 from ax.core.observation import ObservationFeatures
@@ -147,15 +149,14 @@
                     "value": "ax.models.torch.utils.predict_from_model",
                 },
                 "best_point_recommender": {
                     "is_callable_as_path": True,
                     "value": f"{botorch_defaults}.recommend_best_observed_point",
                 },
                 "refit_on_cv": False,
-                "refit_on_update": True,
                 "warm_start_refitting": True,
                 "use_input_warping": False,
                 "use_loocv_pseudo_likelihood": False,
                 "prior": None,
             },
         )
         self.assertEqual(
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_rembo_strategy.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_rembo_strategy.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import torch
 from ax.core.experiment import Experiment
 from ax.core.objective import Objective
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.outcome_constraint import ComparisonOp, OutcomeConstraint
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
@@ -91,15 +93,14 @@
             if i < 2:
                 self.assertLess(len(gs.arms_by_proj[3]), 4)
 
             exp.new_batch_trial(generator_run=gs.gen(experiment=exp, n=2)).run()
             if i >= 2:
                 self.assertFalse(any(len(x) < 4 for x in gs.arms_by_proj.values()))
 
-        self.assertTrue(len(gs.model_transitions) > 0)
         gs2 = gs.clone_reset()
         self.assertEqual(gs2.D, 20)
         self.assertEqual(gs2.d, 6)
 
     def test_HeSBOStrategy(self) -> None:
         gs = HeSBOStrategy(D=10, d=4, init_per_proj=2)
         self.assertEqual(gs.name, "HeSBO")
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_robust_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_robust_modelbridge.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Optional
 
 from ax.core import Objective, OptimizationConfig
 from ax.core.objective import MultiObjective
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import (
     MultiObjectiveOptimizationConfig,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_torch_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_torch_modelbridge.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from contextlib import ExitStack
 from typing import Any, Dict, Optional
 from unittest import mock
 from unittest.mock import Mock
 
 import numpy as np
 import torch
@@ -18,32 +20,39 @@
 from ax.core.objective import Objective
 from ax.core.observation import (
     Observation,
     ObservationData,
     ObservationFeatures,
     recombine_observations,
 )
-from ax.core.optimization_config import OptimizationConfig
+from ax.core.optimization_config import (
+    MultiObjectiveOptimizationConfig,
+    OptimizationConfig,
+)
 from ax.core.outcome_constraint import ScalarizedOutcomeConstraint
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace, SearchSpaceDigest
 from ax.core.types import ComparisonOp
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.torch import TorchModelBridge
 from ax.modelbridge.transforms.base import Transform
+from ax.models.torch.botorch_modular.model import BoTorchModel
 from ax.models.torch_base import TorchGenResults, TorchModel
+from ax.utils.common.constants import Keys
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.testing.core_stubs import (
     get_branin_data,
     get_branin_experiment,
     get_branin_search_space,
     get_experiment_with_observations,
+    get_optimization_config_no_constraints,
     get_search_space_for_range_value,
 )
+from ax.utils.testing.mock import fast_botorch_optimize
 from ax.utils.testing.modeling_stubs import get_observation1, transform_1, transform_2
 from botorch.utils.datasets import (
     ContextualDataset,
     MultiTaskDataset,
     SupervisedDataset,
 )
 
@@ -143,15 +152,15 @@
             )
         model_fit_args = model.fit.mock_calls[0][2]
         self.assertEqual(model_fit_args["datasets"], list(datasets.values()))
         self.assertEqual(model_fit_args["search_space_digest"], ssd)
         self.assertIsNone(model_fit_args["candidate_metadata"])
         self.assertEqual(ma._last_observations, observations)
 
-        with mock.patch(f"{TorchModelBridge.__module__}.logger.info") as mock_logger:
+        with mock.patch(f"{TorchModelBridge.__module__}.logger.debug") as mock_logger:
             ma._fit(
                 model=model,
                 search_space=search_space,
                 observations=observations,
             )
         mock_logger.assert_called_once_with(
             "The observations are identical to the last set of observations "
@@ -334,14 +343,15 @@
         ma.parameters = ["x", "y"]
         model_eval_acqf = mock_torch_model.return_value.evaluate_acquisition_function
         model_eval_acqf.return_value = torch.tensor([5.0], dtype=torch.float64)
 
         ma._model_space = get_branin_search_space()
         ma._optimization_config = None
         ma.outcomes = ["test_metric"]
+        ma._fit_out_of_design = False
 
         with self.assertRaisesRegex(ValueError, "optimization_config"):
             ma.evaluate_acquisition_function(
                 observation_features=[
                     ObservationFeatures(parameters={"x": 1.0, "y": 2.0})
                 ],
             )
@@ -350,17 +360,15 @@
             "ax.modelbridge.torch.extract_search_space_digest",
             return_value=SearchSpaceDigest(feature_names=[], bounds=[]),
         ):
             acqf_vals = ma.evaluate_acquisition_function(
                 observation_features=[
                     ObservationFeatures(parameters={"x": 1.0, "y": 2.0})
                 ],
-                optimization_config=OptimizationConfig(
-                    objective=Objective(metric=Metric(name="test_metric"))
-                ),
+                optimization_config=get_optimization_config_no_constraints(),
             )
 
         self.assertEqual(acqf_vals, [5.0])
         t.transform_observation_features.assert_any_call(
             [ObservationFeatures(parameters={"x": 1.0, "y": 2.0})],
         )
         t.transform_observation_features.reset_mock()
@@ -379,17 +387,15 @@
             return_value=SearchSpaceDigest(feature_names=[], bounds=[]),
         ):
             acqf_vals = ma.evaluate_acquisition_function(
                 observation_features=[
                     ObservationFeatures(parameters={"x": 1.0, "y": 2.0}),
                     ObservationFeatures(parameters={"x": 1.0, "y": 2.0}),
                 ],
-                optimization_config=OptimizationConfig(
-                    objective=Objective(metric=Metric(name="test_metric"))
-                ),
+                optimization_config=get_optimization_config_no_constraints(),
             )
         t.transform_observation_features.assert_any_call(
             [ObservationFeatures(parameters={"x": 1.0, "y": 2.0})],
         )
         t.transform_observation_features.reset_mock()
         self.assertTrue(
             torch.equal(  # `call_args` is an (args, kwargs) tuple
@@ -405,17 +411,15 @@
             acqf_vals = ma.evaluate_acquisition_function(
                 observation_features=[
                     [
                         ObservationFeatures(parameters={"x": 1.0, "y": 2.0}),
                         ObservationFeatures(parameters={"x": 1.0, "y": 2.0}),
                     ]
                 ],
-                optimization_config=OptimizationConfig(
-                    objective=Objective(metric=Metric(name="test_metric"))
-                ),
+                optimization_config=get_optimization_config_no_constraints(),
             )
         t.transform_observation_features.assert_any_call(
             [
                 ObservationFeatures(parameters={"x": 1.0, "y": 2.0}),
                 ObservationFeatures(parameters={"x": 1.0, "y": 2.0}),
             ],
         )
@@ -887,7 +891,45 @@
         datasets, _, _ = mb._get_fit_args(
             search_space=search_space,
             observations=observations,
             parameters=feature_names,
             update_outcomes_and_parameters=False,
         )
         self.assertEqual(mb.outcomes, expected_outcomes)
+
+    @fast_botorch_optimize
+    def test_gen_metadata_untransform(self) -> None:
+        experiment = get_experiment_with_observations(
+            observations=[[0.0, 1.0], [2.0, 3.0]]
+        )
+        model = BoTorchModel()
+        mb = TorchModelBridge(
+            experiment=experiment,
+            search_space=experiment.search_space,
+            data=experiment.lookup_data(),
+            model=model,
+            transforms=[],
+        )
+        for additional_metadata in (
+            {},
+            {"objective_thresholds": None},
+            {
+                "objective_thresholds": checked_cast(
+                    MultiObjectiveOptimizationConfig, experiment.optimization_config
+                ).objective_thresholds,
+            },
+        ):
+            gen_return_value = TorchGenResults(
+                points=torch.tensor([[1.0, 2.0, 3.0]]),
+                weights=torch.tensor([1.0]),
+                gen_metadata={Keys.EXPECTED_ACQF_VAL: [1.0], **additional_metadata},
+            )
+            with mock.patch.object(
+                mb, "_untransform_objective_thresholds"
+            ) as mock_untransform, mock.patch.object(
+                model, "gen", return_value=gen_return_value
+            ):
+                mb.gen(n=1)
+            if additional_metadata.get("objective_thresholds", None) is None:
+                mock_untransform.assert_not_called()
+            else:
+                mock_untransform.assert_called_once()
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_torch_moo_modelbridge.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_torch_moo_modelbridge.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from contextlib import ExitStack
 from typing import List, Optional
 from unittest.mock import patch
 
 import numpy as np
 import torch
 from ax.core.metric import Metric
@@ -15,15 +17,14 @@
 from ax.core.optimization_config import MultiObjectiveOptimizationConfig
 from ax.core.outcome_constraint import (
     ComparisonOp,
     ObjectiveThreshold,
     OutcomeConstraint,
 )
 from ax.core.parameter_constraint import ParameterConstraint
-from ax.exceptions.core import UnsupportedError
 from ax.modelbridge.factory import get_sobol
 from ax.modelbridge.modelbridge_utils import (
     get_pareto_frontier_and_configs,
     observed_hypervolume,
     observed_pareto_frontier,
     pareto_frontier,
     predicted_hypervolume,
@@ -34,14 +35,15 @@
 from ax.models.torch.botorch_modular.model import BoTorchModel
 from ax.models.torch.botorch_moo import MultiObjectiveBotorchModel
 from ax.models.torch.botorch_moo_defaults import (
     infer_objective_thresholds,
     pareto_frontier_evaluator,
 )
 from ax.service.utils.report_utils import exp_to_df
+from ax.utils.common.random import set_rng_seed
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.testing.core_stubs import (
     get_branin_data_multi_objective,
     get_branin_experiment_with_multi_objective,
     get_hierarchical_search_space,
     get_hss_trials_with_fixed_parameter,
@@ -167,15 +169,20 @@
             )
         self.assertTrue(len(predicted_frontier) <= 2)
         self.assertIsNone(predicted_frontier[0].arm_name, None)
 
         with patch(
             PARETO_FRONTIER_EVALUATOR_PATH, wraps=pareto_frontier_evaluator
         ) as wrapped_frontier_evaluator:
-            (observed_frontier, f, obj_w, obj_t,) = get_pareto_frontier_and_configs(
+            (
+                observed_frontier,
+                f,
+                obj_w,
+                obj_t,
+            ) = get_pareto_frontier_and_configs(
                 modelbridge=modelbridge,
                 objective_thresholds=objective_thresholds,
                 observation_features=observation_features,
                 observation_data=observation_data,
                 use_model_predictions=False,
             )
 
@@ -198,15 +205,20 @@
         # Remove the thresholds for testing None handling.
         checked_cast(
             MultiObjectiveOptimizationConfig, modelbridge._optimization_config
         )._objective_thresholds = []
         with patch(
             PARETO_FRONTIER_EVALUATOR_PATH, wraps=pareto_frontier_evaluator
         ) as wrapped_frontier_evaluator:
-            (observed_frontier, f, obj_w, obj_t,) = get_pareto_frontier_and_configs(
+            (
+                observed_frontier,
+                f,
+                obj_w,
+                obj_t,
+            ) = get_pareto_frontier_and_configs(
                 modelbridge=modelbridge,
                 objective_thresholds=None,
                 observation_features=observation_features,
                 observation_data=observation_data,
                 use_model_predictions=False,
             )
         wrapped_frontier_evaluator.assert_called_once()
@@ -275,49 +287,14 @@
         observation_features = [
             ObservationFeatures(parameters={"x1": 0.0, "x2": 1.0}),
             ObservationFeatures(parameters={"x1": 1.0, "x2": 0.0}),
         ]
 
         with self.assertWarns(
             Warning,
-            msg="FYI: The default behavior of `get_pareto_frontier_and_configs` when "
-            "`transform_outcomes_and_configs` is not specified has changed. Previously,"
-            " the default was `transform_outcomes_and_configs=True`; now this argument "
-            "is deprecated and behavior is as if "
-            "`transform_outcomes_and_configs=False`. You did not specify "
-            "`transform_outcomes_and_configs`, so this warning requires no action.",
-        ):
-            res = get_pareto_frontier_and_configs(
-                modelbridge=modelbridge,
-                observation_features=observation_features,
-            )
-            self.assertEqual(len(res), 4)
-
-        with self.assertRaises(UnsupportedError):
-            get_pareto_frontier_and_configs(
-                modelbridge=modelbridge,
-                observation_features=observation_features,
-                transform_outcomes_and_configs=True,
-            )
-
-        with self.assertWarns(
-            DeprecationWarning,
-            msg="You passed `transform_outcomes_and_configs=False`. Specifying "
-            "`transform_outcomes_and_configs` at all is deprecated because `False` is "
-            "now the only allowed behavior. In the future, this will become an error.",
-        ):
-            res = get_pareto_frontier_and_configs(
-                modelbridge=modelbridge,
-                observation_features=observation_features,
-                transform_outcomes_and_configs=False,
-            )
-            self.assertEqual(len(res), 4)
-
-        with self.assertWarns(
-            Warning,
             msg="You provided `observation_data` when `use_model_predictions` is True; "
             "`observation_data` will not be used.",
         ):
             res = get_pareto_frontier_and_configs(
                 modelbridge,
                 observation_features=observation_features,
                 use_model_predictions=True,
@@ -592,15 +569,15 @@
             init_position=len(exp.arms_by_name) - 1,
         )
         sobol_run = sobol_generator.gen(n=2)
         trial = exp.new_batch_trial(optimize_for_power=True)
         trial.add_generator_run(sobol_run)
         trial.mark_running(no_runner_required=True).mark_completed()
         data = exp.fetch_data()
-        torch.manual_seed(0)  # make model fitting deterministic
+        set_rng_seed(0)  # make model fitting deterministic
         modelbridge = TorchModelBridge(
             search_space=exp.search_space,
             model=MultiObjectiveBotorchModel(),
             optimization_config=exp.optimization_config,
             transforms=ST_MTGP_trans,
             experiment=exp,
             data=data,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_transition_criterion.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_transition_criterion.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from logging import Logger
 from unittest.mock import patch
 
 import pandas as pd
 from ax.core.base_trial import TrialStatus
 from ax.core.data import Data
@@ -70,15 +72,17 @@
         with patch.object(experiment, "fetch_data", return_value=data):
             # We have seen three "yes" and three "no"
             self.assertTrue(
                 generation_strategy._maybe_move_to_next_step(
                     raise_data_required_error=False
                 )
             )
-            self.assertEqual(generation_strategy._curr.model_enum, Models.GPEI)
+            self.assertEqual(
+                generation_strategy._curr.model_spec_to_gen_from.model_enum, Models.GPEI
+            )
 
     def test_default_step_criterion_setup(self) -> None:
         """This test ensures that the default completion criterion for GenerationSteps
         is set as expected.
 
         The default completion criterion is to create two TransitionCriterion, one
         of type `MaximumTrialsInStatus` and one of type `MinTrials`.
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/tests/test_utils.py` & `ax-platform-0.4.0/ax/modelbridge/tests/test_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 from unittest.mock import patch
 
 import numpy as np
 from ax.core.arm import Arm
 from ax.core.data import Data
 from ax.core.generator_run import GeneratorRun
@@ -41,14 +43,15 @@
 
 
 TEST_PARAMETERIZATON_LIST = ["5", "foo", "True", "5"]
 
 
 class TestModelbridgeUtils(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_experiment()
         self.arm = Arm({"x": 5, "y": "foo", "z": True, "w": 5})
         self.trial = self.experiment.new_trial(GeneratorRun([self.arm]))
         self.experiment_2 = get_experiment()
         self.batch_trial = self.experiment_2.new_batch_trial(GeneratorRun([self.arm]))
         self.batch_trial.set_status_quo_with_weight(self.experiment_2.status_quo, 1)
         self.obs_feat = ObservationFeatures.from_arm(
@@ -111,15 +114,17 @@
         self.assertListEqual(list(res[0][1]), [0, -0.5, -0.5])
         self.assertEqual(res[1][0][0], 0)
         self.assertEqual(res[1][1][0], -1)
 
     def test_extract_objective_thresholds(self) -> None:
         outcomes = ["m1", "m2", "m3", "m4"]
         objective = MultiObjective(
-            objectives=[Objective(metric=Metric(name)) for name in outcomes[:3]]
+            objectives=[
+                Objective(metric=Metric(name), minimize=False) for name in outcomes[:3]
+            ]
         )
         objective_thresholds = [
             ObjectiveThreshold(
                 metric=Metric(name),
                 op=ComparisonOp.LEQ,
                 bound=float(i + 2),
                 relative=False,
@@ -153,15 +158,15 @@
             objective=objective,
             outcomes=outcomes,
         )
         self.assertTrue(np.array_equal(obj_t[:2], expected_obj_t_not_nan[:2]))
         self.assertTrue(np.isnan(obj_t[-2:]).all())
 
         # Fails if a threshold does not have a corresponding metric.
-        objective2 = Objective(Metric("m1"))
+        objective2 = Objective(Metric("m1"), minimize=False)
         with self.assertRaisesRegex(ValueError, "corresponding metrics"):
             extract_objective_thresholds(
                 objective_thresholds=objective_thresholds,
                 objective=objective2,
                 outcomes=outcomes,
             )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/torch.py` & `ax-platform-0.4.0/ax/modelbridge/torch.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from collections import defaultdict
 from copy import deepcopy
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, Union
 
@@ -627,15 +629,15 @@
         model: TorchModel,
         search_space: SearchSpace,
         observations: List[Observation],
         parameters: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> None:
         if self.model is not None and observations == self._last_observations:
-            logger.info(
+            logger.debug(
                 "The observations are identical to the last set of observations "
                 "used to fit the model. Skipping model fitting."
             )
             return
         datasets, candidate_metadata, search_space_digest = self._get_fit_args(
             search_space=search_space,
             observations=observations,
@@ -677,44 +679,41 @@
             pending_observations=pending_observations,
             fixed_features=fixed_features,
             model_gen_options=augmented_model_gen_options,
             optimization_config=optimization_config,
         )
 
         # Generate the candidates
-        # TODO(ehotaj): For some reason, we're getting models which do not support MOO
-        # even when optimization_config has multiple objectives, so we can't use
-        # self.is_moo_problem here.
-        is_moo_problem = self.is_moo_problem and isinstance(
-            self.model, (BoTorchModel, MultiObjectiveBotorchModel)
-        )
         gen_results = not_none(self.model).gen(
             n=n,
             search_space_digest=search_space_digest,
             torch_opt_config=torch_opt_config,
         )
 
         gen_metadata = gen_results.gen_metadata
-        if is_moo_problem and "objective_thresholds" in gen_metadata:
+        if (
+            isinstance(optimization_config, MultiObjectiveOptimizationConfig)
+            and gen_metadata.get("objective_thresholds", None) is not None
+        ):
             # If objective_thresholds are supplied by the user, then the transformed
             # user-specified objective thresholds are in gen_metadata. Otherwise,
             # if using a hypervolume based acquisition function, then
             # the inferred objective thresholds are in gen_metadata.
             opt_config_metrics = (
                 torch_opt_config.opt_config_metrics
                 or not_none(self._optimization_config).metrics
             )
-            gen_metadata[
-                "objective_thresholds"
-            ] = self._untransform_objective_thresholds(
-                objective_thresholds=gen_metadata["objective_thresholds"],
-                objective_weights=torch_opt_config.objective_weights,
-                bounds=search_space_digest.bounds,
-                opt_config_metrics=opt_config_metrics,
-                fixed_features=torch_opt_config.fixed_features,
+            gen_metadata["objective_thresholds"] = (
+                self._untransform_objective_thresholds(
+                    objective_thresholds=gen_metadata["objective_thresholds"],
+                    objective_weights=torch_opt_config.objective_weights,
+                    bounds=search_space_digest.bounds,
+                    opt_config_metrics=opt_config_metrics,
+                    fixed_features=torch_opt_config.fixed_features,
+                )
             )
 
         # Transform array to observations
         observation_features = self._array_to_observation_features(
             X=gen_results.points.detach().cpu().clone().numpy(),
             candidate_metadata=gen_results.candidate_metadata,
         )
@@ -865,14 +864,15 @@
             fixed_features=fixed_features_dict,
             pending_observations=pend_o,
             model_gen_options=model_gen_options or {},
             rounding_func=rounding_func,
             opt_config_metrics=opt_config_metrics,
             is_moo=optimization_config.is_moo_problem,
             risk_measure=risk_measure,
+            fit_out_of_design=self._fit_out_of_design,
         )
         return search_space_digest, torch_opt_config
 
     def _transform_observations(
         self, observations: List[Observation]
     ) -> Tuple[Tensor, Tensor, Tensor]:
         """Apply terminal transform to given observation data and return result.
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/base.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import List, Optional, TYPE_CHECKING
 
 from ax.core.observation import (
     Observation,
     ObservationData,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/cap_parameter.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/cap_parameter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional, TYPE_CHECKING
 
 from ax.core.observation import Observation
 from ax.core.parameter import RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
 from ax.models.types import TConfig
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/cast.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/cast.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.search_space import HierarchicalSearchSpace, SearchSpace
+from ax.exceptions.core import UserInputError
 from ax.modelbridge.transforms.base import Transform
 from ax.models.types import TConfig
 from ax.utils.common.typeutils import checked_cast, not_none
 
 if TYPE_CHECKING:
     # import as module to make sphinx-autodoc-typehints happy
     from ax import modelbridge as modelbridge_module  # noqa F401
@@ -41,17 +44,32 @@
         self,
         search_space: Optional[SearchSpace] = None,
         observations: Optional[List[Observation]] = None,
         modelbridge: Optional["modelbridge_module.base.ModelBridge"] = None,
         config: Optional[TConfig] = None,
     ) -> None:
         self.search_space: SearchSpace = not_none(search_space).clone()
-        self.flatten_hss: bool = (
-            config is None or checked_cast(bool, config.get("flatten_hss", True))
-        ) and isinstance(search_space, HierarchicalSearchSpace)
+        config = (config or {}).copy()
+        self.flatten_hss: bool = checked_cast(
+            bool,
+            config.pop(
+                "flatten_hss", isinstance(search_space, HierarchicalSearchSpace)
+            ),
+        )
+        self.inject_dummy_values_to_complete_flat_parameterization: bool = checked_cast(
+            bool,
+            config.pop("inject_dummy_values_to_complete_flat_parameterization", True),
+        )
+        self.use_random_dummy_values: bool = checked_cast(
+            bool, config.pop("use_random_dummy_values", False)
+        )
+        if config:
+            raise UserInputError(
+                f"Unexpected config parameters for `Cast` transform: {config}."
+            )
 
     def _transform_search_space(self, search_space: SearchSpace) -> SearchSpace:
         """Flattens the hierarchical search space and returns the flat
         ``SearchSpace`` if this transform is configured to flatten hierarchical
         search spaces. Does nothing if the search space is not hierarchical.
 
         NOTE: All calls to `Cast.transform_...` transform Ax objects defined in
@@ -84,15 +102,21 @@
             return observation_features
         # Inject the parameters model suggested in the flat search space, which then
         # got removed during casting to HSS as they were not applicable under the
         # hierarchical structure of the search space.
         return [
             checked_cast(
                 HierarchicalSearchSpace, self.search_space
-            ).flatten_observation_features(observation_features=obs_feats)
+            ).flatten_observation_features(
+                observation_features=obs_feats,
+                inject_dummy_values_to_complete_flat_parameterization=(
+                    self.inject_dummy_values_to_complete_flat_parameterization
+                ),
+                use_random_dummy_values=self.use_random_dummy_values,
+            )
             for obs_feats in observation_features
         ]
 
     def untransform_observation_features(
         self, observation_features: List[ObservationFeatures]
     ) -> List[ObservationFeatures]:
         """Untransform observation features by casting parameter values to their
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/choice_encode.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/choice_encode.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from typing import Dict, List, Optional, Tuple, TYPE_CHECKING
+# pyre-strict
+
+from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING
 
 import numpy as np
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, Parameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.core.types import TParamValue
 from ax.modelbridge.transforms.base import Transform
+from ax.modelbridge.transforms.deprecated_transform_mixin import (
+    DeprecatedTransformMixin,
+)
 from ax.modelbridge.transforms.utils import (
     ClosestLookupDict,
     construct_new_search_space,
 )
 from ax.models.types import TConfig
 
 if TYPE_CHECKING:
@@ -33,15 +38,15 @@
     The resulting choice parameter will be considered ordered iff the original
     parameter is.
 
     In the inverse transform, parameters will be mapped back onto the original domain.
 
     This transform does not transform task parameters (use TaskEncode for this).
 
-    Note that this behavior is different from that of OrderedChoiceEncode, which
+    Note that this behavior is different from that of OrderedChoiceToIntegerRange, which
     transforms (ordered) ChoiceParameters to integer RangeParameters (rather than
     ChoiceParameters).
 
     Transform is done in-place.
     """
 
     def __init__(
@@ -114,15 +119,15 @@
                     # pyre: pval is declared to have type `int` but is used as
                     # pyre-fixme[9]: type `Union[bool, float, str]`.
                     pval: int = obsf.parameters[p_name]
                     obsf.parameters[p_name] = reverse_transform[pval]
         return observation_features
 
 
-class OrderedChoiceEncode(ChoiceEncode):
+class OrderedChoiceToIntegerRange(ChoiceEncode):
     """Convert ordered ChoiceParameters to integer RangeParameters.
 
     Parameters will be transformed to an integer RangeParameters, mapped from the
     original choice domain to a contiguous range `0, 1, ..., n_choices - 1`
     of integers. Does not transform task parameters.
 
     In the inverse transform, parameters will be mapped back onto the original domain.
@@ -181,14 +186,21 @@
                     transformed_parameters=transformed_parameters
                 )
                 for pc in search_space.parameter_constraints
             ],
         )
 
 
+class OrderedChoiceEncode(DeprecatedTransformMixin, OrderedChoiceToIntegerRange):
+    """Deprecated alias for OrderedChoiceToIntegerRange."""
+
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        super().__init__(*args, **kwargs)
+
+
 def transform_choice_values(p: ChoiceParameter) -> Tuple[np.ndarray, ParameterType]:
     """Transforms the choice values and returns the new parameter type.
 
     If the choices were numeric (int or float) and ordered, then they're cast
     to float and rescaled to [0, 1]. Otherwise, they're cast to integers
     `0, 1, ..., n_choices - 1`.
     """
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/convert_metric_names.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/convert_metric_names.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, TYPE_CHECKING
 
 from ax.core.multi_type_experiment import MultiTypeExperiment
 from ax.core.observation import Observation, ObservationData
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
 from ax.models.types import TConfig
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/derelativize.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/derelativize.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import List, Optional, TYPE_CHECKING
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.outcome_constraint import OutcomeConstraint, ScalarizedOutcomeConstraint
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/int_range_to_choice.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/int_range_to_choice.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from typing import Dict, List, Optional, Set, TYPE_CHECKING
+# pyre-strict
+
+from numbers import Real
+from typing import cast, Dict, List, Optional, Set, TYPE_CHECKING
 
 from ax.core.observation import Observation
 from ax.core.parameter import ChoiceParameter, Parameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
 from ax.modelbridge.transforms.utils import construct_new_search_space
 from ax.models.types import TConfig
@@ -29,34 +32,36 @@
         search_space: Optional[SearchSpace] = None,
         observations: Optional[List[Observation]] = None,
         modelbridge: Optional["modelbridge_module.base.ModelBridge"] = None,
         config: Optional[TConfig] = None,
     ) -> None:
         assert search_space is not None, "IntRangeToChoice requires search space"
         config = config or {}
-        self.max_choices: float = config.get("max_choices", float("inf"))  # pyre-ignore
+        self.max_choices: float = float(
+            cast(Real, (config.get("max_choices", float("inf"))))
+        )
         # Identify parameters that should be transformed
         self.transform_parameters: Set[str] = {
             p_name
             for p_name, p in search_space.parameters.items()
             if isinstance(p, RangeParameter)
             and p.parameter_type == ParameterType.INT
-            and p.upper - p.lower + 1 <= self.max_choices
+            and p.cardinality() <= self.max_choices
         }
 
     def _transform_search_space(self, search_space: SearchSpace) -> SearchSpace:
         transformed_parameters: Dict[str, Parameter] = {}
         for p_name, p in search_space.parameters.items():
             if (
                 p_name in self.transform_parameters
                 and isinstance(p, RangeParameter)
                 and p.parameter_type == ParameterType.INT
-                and p.upper - p.lower + 1 <= self.max_choices
+                and p.cardinality() <= self.max_choices
             ):
-                values = list(range(p.lower, p.upper + 1))  # pyre-ignore
+                values = list(range(int(p.lower), int(p.upper) + 1))
                 target_value = (
                     None
                     if p.target_value is None
                     else next(i for i, v in enumerate(values) if v == p.target_value)
                 )
                 transformed_parameters[p_name] = ChoiceParameter(
                     name=p_name,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/int_to_float.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/int_to_float.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Dict, List, Optional, Set, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import Parameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
@@ -63,15 +65,15 @@
 
         # Identify parameters that should be transformed
         self.transform_parameters: Set[str] = {
             p_name
             for p_name, p in self.search_space.parameters.items()
             if isinstance(p, RangeParameter)
             and p.parameter_type == ParameterType.INT
-            and (p.upper - p.lower + 1 >= self.min_choices or p.log_scale)
+            and ((p.cardinality() >= self.min_choices) or p.log_scale)
         }
         if contains_constrained_integer(self.search_space, self.transform_parameters):
             self.rounding = "randomized"
             self.contains_constrained_integer: bool = True
         else:
             self.contains_constrained_integer: bool = False
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/inverse_gaussian_cdf_y.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/inverse_gaussian_cdf_y.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import List, Optional, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationData
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
 from ax.modelbridge.transforms.utils import match_ci_width_truncated
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/ivw.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/ivw.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Dict, List
 
 import numpy as np
 from ax.core.observation import ObservationData
 from ax.modelbridge.transforms.base import Transform
 from ax.utils.common.logger import get_logger
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/log.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/log.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from typing import List, Optional, Set, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/log_y.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/log_y.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from logging import Logger
 
 from typing import Callable, List, Optional, Tuple, TYPE_CHECKING
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/logit.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/logit.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional, Set, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
 from ax.models.types import TConfig
@@ -15,15 +17,15 @@
 
 if TYPE_CHECKING:
     # import as module to make sphinx-autodoc-typehints happy
     from ax import modelbridge as modelbridge_module  # noqa F401
 
 
 class Logit(Transform):
-    """Apply logit transfor to a float RangeParameter domain.
+    """Apply logit transform to a float RangeParameter domain.
 
     Transform is done in-place.
     """
 
     def __init__(
         self,
         search_space: Optional[SearchSpace] = None,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/map_unit_x.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/map_unit_x.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from collections import defaultdict
 
 from typing import Dict, List, Optional, Tuple, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationFeatures
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/merge_repeated_measurements.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/merge_repeated_measurements.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from collections import defaultdict
+from copy import deepcopy
 from typing import DefaultDict, Dict, List, Optional
 
 import numpy as np
 from ax.core.arm import Arm
 from ax.core.observation import Observation, ObservationData, separate_observations
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.base import ModelBridge
@@ -94,19 +97,20 @@
     def transform_observations(
         self,
         observations: List[Observation],
     ) -> List[Observation]:
         # Transform observations
         new_observations = []
         observation_features, observation_data = separate_observations(observations)
+        arm_to_merged = deepcopy(self.arm_to_merged)
         for j, obsd in enumerate(observation_data):
             key = Arm.md5hash(observation_features[j].parameters)
             # pop to ensure that the resulting observations list has one
             # observation per unique arm
-            metric_dict = self.arm_to_merged.pop(key, None)
+            metric_dict = arm_to_merged.pop(key, None)
             if metric_dict is None:
                 continue
             merged_means = np.zeros(len(obsd.metric_names))
             merged_covariance = np.zeros(
                 (len(obsd.metric_names), len(obsd.metric_names))
             )
             for i, m in enumerate(obsd.metric_names):
@@ -114,10 +118,14 @@
                 merged_means[i] = merged_metric["mean"]
                 merged_covariance[i, i] = merged_metric["var"]
             new_obsd = ObservationData(
                 metric_names=obsd.metric_names,
                 means=merged_means,
                 covariance=merged_covariance,
             )
-            new_obs = Observation(features=observation_features[j], data=new_obsd)
+            new_obs = Observation(
+                features=observation_features[j],
+                data=new_obsd,
+                arm_name=observations[j].arm_name,
+            )
             new_observations.append(new_obs)
         return new_observations
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/metrics_as_task.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/metrics_as_task.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, TYPE_CHECKING
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/one_hot.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/one_hot.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, TYPE_CHECKING
 
 import numpy as np
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, Parameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.core.types import TParameterization, TParamValue
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/percentile_y.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/percentile_y.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from logging import Logger
 from typing import List, Optional, TYPE_CHECKING
 
 from ax.core.observation import Observation, ObservationData
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import DataRequiredError
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/power_transform_y.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/power_transform_y.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from collections import defaultdict
 from logging import Logger
 from typing import Dict, List, Optional, Tuple, TYPE_CHECKING
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/relativize.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/relativize.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from abc import ABC, abstractmethod
 
 from math import sqrt
 from typing import Callable, Dict, List, Optional, Tuple, TYPE_CHECKING
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/remove_fixed.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/remove_fixed.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, TYPE_CHECKING, Union
 
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, FixedParameter, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
 from ax.modelbridge.transforms.utils import construct_new_search_space
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/rounding.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/rounding.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 import random
 from copy import copy
 from typing import Set
 
 import numpy as np
 from ax.core.parameter_constraint import OrderConstraint
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/search_space_to_choice.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/search_space_to_choice.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional, TYPE_CHECKING
 
 from ax.core.arm import Arm
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, FixedParameter, ParameterType
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.exceptions.core import UnsupportedError
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/search_space_to_float.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/search_space_to_float.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List
 
 from ax.core.arm import Arm
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.base import Transform
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/standardize_y.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/standardize_y.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import DefaultDict, Dict, List, Optional, Tuple, TYPE_CHECKING, Union
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.outcome_constraint import OutcomeConstraint, ScalarizedOutcomeConstraint
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/stratified_standardize_y.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/stratified_standardize_y.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import defaultdict
 from logging import Logger
 from typing import DefaultDict, Dict, List, Optional, Tuple, TYPE_CHECKING, Union
 
 import numpy as np
 from ax.core.observation import Observation, ObservationFeatures, separate_observations
 from ax.core.optimization_config import OptimizationConfig
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/task_encode.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/task_encode.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, TYPE_CHECKING
 
 from ax.core.observation import Observation
 from ax.core.parameter import ChoiceParameter, Parameter, ParameterType
 from ax.core.search_space import SearchSpace
 from ax.core.types import TParamValue
-from ax.modelbridge.transforms.choice_encode import OrderedChoiceEncode
+from ax.modelbridge.transforms.choice_encode import OrderedChoiceToIntegerRange
 from ax.modelbridge.transforms.utils import construct_new_search_space
 from ax.models.types import TConfig
 
 if TYPE_CHECKING:
     # import as module to make sphinx-autodoc-typehints happy
     from ax import modelbridge as modelbridge_module  # noqa F401
 
 
-class TaskEncode(OrderedChoiceEncode):
+class TaskEncode(OrderedChoiceToIntegerRange):
     """Convert task ChoiceParameters to integer-valued ChoiceParameters.
 
     Parameters will be transformed to an integer ChoiceParameter with
     property `is_task=True`, mapping values from the original choice domain to a
     contiguous range integers `0, 1, ..., n_choices-1`.
 
     In the inverse transform, parameters will be mapped back onto the original domain.
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_base_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_base_transform.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from unittest.mock import MagicMock
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.modelbridge.transforms.base import Transform
 from ax.utils.common.testutils import TestCase
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_cap_parameter_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_cap_parameter_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import UnsupportedError
 from ax.modelbridge.transforms.cap_parameter import CapParameter
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class CapParameterTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "a", lower=1, upper=3, parameter_type=ParameterType.FLOAT
                 ),
                 ChoiceParameter(
                     "b", parameter_type=ParameterType.STRING, values=["a", "b", "c"]
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_cast_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_cast_transform.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from unittest.mock import patch
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import (
     ChoiceParameter,
     FixedParameter,
     ParameterType,
     RangeParameter,
 )
 from ax.core.search_space import HierarchicalSearchSpace, SearchSpace
+from ax.exceptions.core import UserInputError
 from ax.modelbridge.transforms.cast import Cast
 from ax.utils.common.constants import Keys
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_hierarchical_search_space
 
 
 class CastTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "a", lower=1.0, upper=5.0, parameter_type=ParameterType.FLOAT
                 ),
                 RangeParameter(
                     "b",
@@ -62,14 +66,18 @@
                 "l2_reg_weight": 0.0001,
                 "num_boost_rounds": 12,
             },
             trial_index=10,
             metadata=None,
         )
 
+    def test_invalid_config(self) -> None:
+        with self.assertRaisesRegex(UserInputError, "Unexpected config"):
+            Cast(search_space=self.search_space, config={"flatten_hs": "foo"})
+
     def test_transform_observation_features(self) -> None:
         # Verify running the transform on already-casted features does nothing
         observation_features = [
             ObservationFeatures(parameters={"a": 1.2345, "b": 2.34, "c": "a", "d": 2})
         ]
         obs_ft2 = deepcopy(observation_features)
         obs_ft2 = self.t.transform_observation_features(obs_ft2)
@@ -152,14 +160,38 @@
                 self.assertIn(p_name, obsf.parameters)
             # Check that full parameterization is recorded in metadata
             self.assertEqual(
                 obsf.metadata.get(Keys.FULL_PARAMETERIZATION),
                 self.obs_feats_hss.parameters,
             )
 
+    def test_transform_observation_features_HSS_dummy_values_settings(self) -> None:
+        t = Cast(
+            search_space=self.hss,
+            config={
+                "inject_dummy_values_to_complete_flat_parameterization": True,
+                "use_random_dummy_values": True,
+            },
+            observations=[],
+        )
+        self.assertTrue(t.inject_dummy_values_to_complete_flat_parameterization)
+        with patch.object(
+            t.search_space,
+            "flatten_observation_features",
+            wraps=t.search_space.flatten_observation_features,  # pyre-ignore
+        ) as mock_flatten_obsf:
+            t.transform_observation_features(observation_features=[self.obs_feats_hss])
+        mock_flatten_obsf.assert_called_once()
+        self.assertTrue(
+            mock_flatten_obsf.call_args.kwargs[
+                "inject_dummy_values_to_complete_flat_parameterization"
+            ]
+        )
+        self.assertTrue(mock_flatten_obsf.call_args.kwargs["use_random_dummy_values"])
+
     def test_untransform_observation_features_HSS(self) -> None:
         # Test transformation in one subtree of HSS.
         with patch.object(
             self.t_hss.search_space,
             "cast_observation_features",
             wraps=self.t_hss.search_space.cast_observation_features,  # pyre-ignore
         ) as mock_cast_obsf:
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_centered_unit_x_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_centered_unit_x_transform.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.modelbridge.transforms.centered_unit_x import CenteredUnitX
 from ax.modelbridge.transforms.tests.test_unit_x_transform import UnitXTransformTest
 
 
 class CenteredUnitXTransformTest(UnitXTransformTest):
 
     transform_class = CenteredUnitX
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_choice_encode_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_choice_encode_transform.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,31 +1,38 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from typing import List, Sized
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.parameter_constraint import ParameterConstraint
 from ax.core.search_space import RobustSearchSpace, SearchSpace
-from ax.modelbridge.transforms.choice_encode import ChoiceEncode, OrderedChoiceEncode
+from ax.modelbridge.transforms.choice_encode import (
+    ChoiceEncode,
+    OrderedChoiceEncode,
+    OrderedChoiceToIntegerRange,
+)
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class ChoiceEncodeTransformTest(TestCase):
     t_class = ChoiceEncode
 
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x", lower=1, upper=3, parameter_type=ParameterType.FLOAT
                 ),
                 RangeParameter("a", lower=1, upper=2, parameter_type=ParameterType.INT),
                 ChoiceParameter(
@@ -160,15 +167,15 @@
                     values=[1.0, 10.0, 100.0],
                     is_ordered=True,
                     is_fidelity=True,
                     target_value=100.0,
                 )
             ]
         )
-        t = OrderedChoiceEncode(search_space=ss3, observations=[])
+        t = OrderedChoiceToIntegerRange(search_space=ss3, observations=[])
         with self.assertRaises(ValueError):
             t.transform_search_space(ss3)
 
     def test_w_parameter_distributions(self) -> None:
         rss = get_robust_search_space()
         # pyre-fixme[16]: `Parameter` has no attribute `_is_ordered`.
         rss.parameters["c"]._is_ordered = True
@@ -202,16 +209,16 @@
         self.assertEqual(set(rss.parameters.keys()), set(rss_new.parameters.keys()))
         self.assertEqual(rss.parameter_distributions, rss_new.parameter_distributions)
         # pyre-fixme[16]: `SearchSpace` has no attribute `_environmental_variables`.
         self.assertEqual(rss._environmental_variables, rss_new._environmental_variables)
         self.assertEqual(rss_new.parameters.get("c").parameter_type, ParameterType.INT)
 
 
-class OrderedChoiceEncodeTransformTest(ChoiceEncodeTransformTest):
-    t_class = OrderedChoiceEncode
+class OrderedChoiceToIntegerRangeTransformTest(ChoiceEncodeTransformTest):
+    t_class = OrderedChoiceToIntegerRange
 
     def setUp(self) -> None:
         super().setUp()
         # expected parameters after transform
         self.expected_transformed_params = {
             "x": 2.2,
             "a": 2,
@@ -252,18 +259,36 @@
                     values=[1.0, 10.0, 100.0],
                     is_ordered=True,
                     is_fidelity=True,
                     target_value=100.0,
                 )
             ]
         )
-        t = OrderedChoiceEncode(search_space=ss3, observations=[])
+        t = OrderedChoiceToIntegerRange(search_space=ss3, observations=[])
         with self.assertRaises(ValueError):
             t.transform_search_space(ss3)
 
+    def test_deprecated_OrderedChoiceEncode(self) -> None:
+        # Ensure we error if we try to transform a fidelity parameter
+        ss3 = SearchSpace(
+            parameters=[
+                ChoiceParameter(
+                    "b",
+                    parameter_type=ParameterType.FLOAT,
+                    values=[1.0, 10.0, 100.0],
+                    is_ordered=True,
+                    is_fidelity=True,
+                    target_value=100.0,
+                )
+            ]
+        )
+        t = OrderedChoiceToIntegerRange(search_space=ss3, observations=[])
+        t_deprecated = OrderedChoiceEncode(search_space=ss3, observations=[])
+        self.assertEqual(t.__dict__, t_deprecated.__dict__)
+
 
 def normalize_values(values: Sized) -> List[float]:
     values = np.array(values, dtype=float)
     vmin, vmax = values.min(), values.max()
     if len(values) > 1:
         values = (values - vmin) / (vmax - vmin)
     return values.tolist()
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_convert_metric_names_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_convert_metric_names_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import copy
 
 from ax.core.observation import observations_from_data
 from ax.modelbridge.transforms.convert_metric_names import (
     convert_mt_observations,
     ConvertMetricNames,
     tconfig_from_mt_experiment,
 )
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_multi_type_experiment
 
 
 class ConvertMetricNamesTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_multi_type_experiment(add_trials=True)
         self.data = self.experiment.fetch_data()
         self.observations = observations_from_data(self.experiment, self.data)
         self.observation_data = [o.data for o in self.observations]
         self.observation_features = [o.features for o in self.observations]
         self.tconfig = tconfig_from_mt_experiment(self.experiment)
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_derelativize_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_derelativize_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from unittest import mock
 from unittest.mock import Mock, patch
 
 import numpy as np
 from ax.core.data import Data
 from ax.core.experiment import Experiment
@@ -23,14 +25,15 @@
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.transforms.derelativize import Derelativize
 from ax.utils.common.testutils import TestCase
 
 
 class DerelativizeTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         m = mock.patch.object(ModelBridge, "__abstractmethods__", frozenset())
         self.addCleanup(m.stop)
         m.start()
 
     @mock.patch(
         "ax.modelbridge.base.observations_from_data",
         autospec=True,
@@ -96,15 +99,15 @@
             transforms=[],
             experiment=Experiment(search_space, "test"),
             data=Data(),
             status_quo_name="1_1",
         )
 
         # Test with no relative constraints
-        objective = Objective(Metric("c"))
+        objective = Objective(Metric("c"), minimize=True)
         oc = OptimizationConfig(
             objective=objective,
             outcome_constraints=[
                 OutcomeConstraint(
                     Metric("a"), ComparisonOp.LEQ, bound=2, relative=False
                 ),
                 ScalarizedOutcomeConstraint(
@@ -294,15 +297,15 @@
 
     def test_Errors(self) -> None:
         t = Derelativize(
             search_space=None,
             observations=[],
         )
         oc = OptimizationConfig(
-            objective=Objective(Metric("c")),
+            objective=Objective(Metric("c"), minimize=False),
             outcome_constraints=[
                 OutcomeConstraint(Metric("a"), ComparisonOp.LEQ, bound=2, relative=True)
             ],
         )
         search_space = SearchSpace(
             parameters=[RangeParameter("x", ParameterType.FLOAT, 0, 20)]
         )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_int_range_to_choice_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_int_range_to_choice_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.exceptions.core import UnsupportedError
 from ax.modelbridge.transforms.int_range_to_choice import IntRangeToChoice
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class IntRangeToChoiceTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter("a", lower=1, upper=5, parameter_type=ParameterType.INT),
                 ChoiceParameter(
                     "b", parameter_type=ParameterType.STRING, values=["a", "b", "c"]
                 ),
             ],
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_int_to_float_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_int_to_float_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,40 +1,42 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
+from typing import List
 from unittest import mock
 
 from ax.core.observation import ObservationFeatures
-from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
+from ax.core.parameter import ChoiceParameter, Parameter, ParameterType, RangeParameter
 from ax.core.parameter_constraint import OrderConstraint, SumConstraint
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.exceptions.core import UnsupportedError
 from ax.modelbridge.transforms.int_to_float import IntToFloat
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class IntToFloatTransformTest(TestCase):
     def setUp(self) -> None:
-        parameters = [
+        super().setUp()
+        parameters: List[Parameter] = [
             RangeParameter("x", lower=1, upper=3, parameter_type=ParameterType.FLOAT),
             RangeParameter("a", lower=1, upper=2, parameter_type=ParameterType.INT),
             RangeParameter("d", lower=1, upper=3, parameter_type=ParameterType.INT),
             ChoiceParameter(
                 "b", parameter_type=ParameterType.STRING, values=["a", "b", "c"]
             ),
         ]
         self.search_space = SearchSpace(
-            # pyre-fixme[6]: For 1st param expected `List[Parameter]` but got
-            #  `List[Union[ChoiceParameter, RangeParameter]]`.
             parameters=parameters,
             parameter_constraints=[
                 OrderConstraint(
                     lower_parameter=parameters[0], upper_parameter=parameters[1]
                 )
             ],
         )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_inverse_gaussian_cdf_y_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_inverse_gaussian_cdf_y_transform.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.observation import ObservationData
 from ax.modelbridge.transforms.inverse_gaussian_cdf_y import InverseGaussianCdfY
 from ax.utils.common.testutils import TestCase
 
 
 class InverseGaussianCdfYTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd_mid = ObservationData(
             metric_names=["m1", "m2"],
             means=np.array([0.5, 0.9]),
             covariance=np.array([[0.005, 0.0], [0.0, 0.005]]),
         )
         self.obsd_extreme = ObservationData(
             metric_names=["m1", "m2"],
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_ivw_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_ivw_transform.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.core.observation import ObservationData
 from ax.modelbridge.transforms.ivw import IVW, ivw_metric_merge
 from ax.utils.common.testutils import TestCase
 
 
 class IVWTransformTest(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_log_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_log_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import UnsupportedError
 from ax.modelbridge.transforms.log import Log
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class LogTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x",
                     lower=1,
                     upper=3,
                     parameter_type=ParameterType.FLOAT,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_log_y_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_log_y_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from copy import deepcopy
 
 import numpy as np
 from ax.core.metric import Metric
 from ax.core.objective import MultiObjective, Objective
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
@@ -24,14 +26,15 @@
     norm_to_lognorm,
 )
 from ax.utils.common.testutils import TestCase
 
 
 class LogYTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2", "m3"],
             means=np.array([0.5, 1.0, 1.0]),
             covariance=np.diag(np.array([1.0, 1.0, np.exp(1) - 1])),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m1", "m2", "m2"],
@@ -94,15 +97,15 @@
             metric_names=["m1", "m2", "m3"],
             means=np.array([0.5, 1.0, -0.5]),
             covariance=np.diag(np.array([1.0, 1.0, 1.0])),
         )
         tf = LogY(
             search_space=None,
             observations=[],
-            config={"metrics": ["m3"]},  # pyre-ignore
+            config={"metrics": ["m3"]},
         )
         obsd1 = deepcopy(self.obsd1)
         obsd1_ = tf._transform_observation_data([obsd1])
         self.assertTrue(np.allclose(obsd1_[0].means, obsd1_t.means))
         self.assertTrue(np.allclose(obsd1_[0].covariance, obsd1_t.covariance))
 
         obsd1 = tf._untransform_observation_data(obsd1_)
@@ -134,15 +137,15 @@
         # basic test
         m1 = Metric(name="m1")
         objective_m1 = Objective(metric=m1, minimize=False)
         oc = OptimizationConfig(objective=objective_m1, outcome_constraints=[])
         tf = LogY(
             search_space=None,
             observations=self.observations,
-            config={"metrics": ["m1"]},  # pyre-ignore
+            config={"metrics": ["m1"]},
         )
         oc_tf = tf.transform_optimization_config(deepcopy(oc), None, None)
         self.assertEqual(oc_tf, oc)
         # output constraint on a different metric should work
         m2 = Metric(name="m2")
         oc = OptimizationConfig(
             objective=objective_m1,
@@ -223,15 +226,15 @@
         oc = MultiObjectiveOptimizationConfig(
             objective=mo,
             objective_thresholds=objective_thresholds,
         )
         tf = LogY(
             search_space=None,
             observations=self.observations,
-            config={"metrics": ["m1"]},  # pyre-ignore
+            config={"metrics": ["m1"]},
         )
         oc_tf = tf.transform_optimization_config(deepcopy(oc), None, None)
         oc.objective_thresholds[0].bound = math.log(1.234)
         self.assertEqual(oc_tf, oc)
 
 
 class LogNormTest(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_logit_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_logit_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import UnsupportedError, UserInputError
 from ax.modelbridge.transforms.logit import Logit
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 from scipy.special import expit, logit
 
 
 class LogitTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x",
                     lower=0.9,
                     upper=0.999,
                     parameter_type=ParameterType.FLOAT,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_map_unit_x_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_map_unit_x_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.map_unit_x import MapUnitX
 from ax.utils.common.testutils import TestCase
 
 
 class MapUnitXTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.target_lb = MapUnitX.target_lb
         self.target_range = MapUnitX.target_range
         self.target_ub = self.target_lb + self.target_range
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x", lower=1, upper=3, parameter_type=ParameterType.FLOAT
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_merge_repeated_measurements_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_merge_repeated_measurements_transform.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+from copy import deepcopy
+
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.modelbridge.transforms.merge_repeated_measurements import (
     MergeRepeatedMeasurements,
 )
 from ax.utils.common.testutils import TestCase
 
@@ -131,50 +135,73 @@
                     [
                         [1.0, 0.0],
                         [0.0, 2.0],
                     ]
                 ),
             ),
             features=obs_feat1,
+            arm_name="0_0",
         )
         obs2 = Observation(
             data=ObservationData(
                 metric_names=["m1", "m2"],
                 means=np.array([1.0, 1.0]),
                 covariance=np.array(
                     [
                         [1.0, 0.0],
                         [0.0, 3.0],
                     ]
                 ),
             ),
             features=obs_feat1,
+            arm_name="0_0",
         )
         # different arm
         obs3 = Observation(
             data=ObservationData(
                 metric_names=["m1", "m2"],
                 means=np.array([3.0, 1.0]),
                 covariance=np.array(
                     [
                         [4.0, 0.0],
                         [0.0, 5.0],
                     ]
                 ),
             ),
             features=ObservationFeatures(parameters={"a": 1.0, "b": 0.0}),
+            arm_name="0_1",
         )
         expected_obs = Observation(
             data=ObservationData(
                 metric_names=["m1", "m2"],
                 means=np.array([1.0, 1.6]),
                 covariance=np.array([[0.5, 0.0], [0.0, 1.2]]),
             ),
             features=obs_feat1,
+            arm_name="0_0",
         )
         observations = [obs1, obs2, obs3]
+        observations_copy = deepcopy(observations)
         t = MergeRepeatedMeasurements(observations=observations)
         observations2 = t.transform_observations(observations)
         compare_obs(
             test=self, obs1=expected_obs, obs2=observations2[0], discrepancy_tol=1e-8
         )
         compare_obs(test=self, obs1=obs3, obs2=observations2[1], discrepancy_tol=0.0)
+        # test repeating the transform
+        observations2_copy = t.transform_observations(observations_copy)
+        compare_obs(
+            test=self,
+            obs1=observations2[0],
+            obs2=observations2_copy[0],
+            discrepancy_tol=0,
+        )
+        compare_obs(
+            test=self,
+            obs1=observations2[1],
+            obs2=observations2_copy[1],
+            discrepancy_tol=0,
+        )
+        # check arm names
+        arm_names = {obs.arm_name for obs in observations}
+        arm_names2 = {obs.arm_name for obs in observations2}
+        self.assertEqual(arm_names, arm_names2)
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_metrics_as_task_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_metrics_as_task_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,26 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
-
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.parameter import ChoiceParameter
 from ax.modelbridge.transforms.metrics_as_task import MetricsAsTask
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_search_space_for_range_values
 
 
 class MetricsAsTaskTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.metric_task_map = {
             "metric1": ["metric2", "metric3"],
             "metric2": ["metric3"],
         }
         self.observations = [
             Observation(
                 data=ObservationData(
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_one_hot_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_one_hot_transform.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.parameter_constraint import ParameterConstraint
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.modelbridge.transforms.one_hot import OH_PARAM_INFIX, OneHot
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class OneHotTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x",
                     lower=1,
                     upper=3,
                     parameter_type=ParameterType.FLOAT,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_percentile_y_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_percentile_y_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.exceptions.core import DataRequiredError
 from ax.modelbridge.transforms.percentile_y import PercentileY
 from ax.utils.common.testutils import TestCase
 
 
 class PercentileYTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2"],
             means=np.array([0.0, 0.0]),
             covariance=np.array([[1.0, 0.0], [0.0, 1.0]]),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m2"],
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_power_y_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_power_y_transform.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from copy import deepcopy
 from math import isfinite, isnan
 from typing import List
 
 import numpy as np
@@ -35,14 +37,15 @@
             metric=metric, op=ComparisonOp.GEQ, bound=bound, relative=relative
         )
     ]
 
 
 class PowerTransformYTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2"],
             means=np.array([0.5, 0.9]),
             covariance=np.array([[0.03, 0.0], [0.0, 0.001]]),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m2"],
@@ -73,25 +76,14 @@
         with self.assertRaises(ValueError):
             PowerTransformY(**shared_init_args)
         # Test error for not specifying at least one metric
         with self.assertRaises(ValueError):
             PowerTransformY(**shared_init_args, config={})
         # Test default init
         for m in ["m1", "m2"]:
-            # pyre-fixme[6]: For 1st param expected `List[ObservationData]` but got
-            #  `Optional[List[ObservationData]]`.
-            # pyre-fixme[6]: For 1st param expected `List[ObservationFeatures]` but
-            #  got `Optional[List[ObservationData]]`.
-            # pyre-fixme[6]: For 1st param expected `Optional[ModelBridge]` but got
-            #  `Optional[List[ObservationData]]`.
-            # pyre-fixme[6]: For 1st param expected `SearchSpace` but got
-            #  `Optional[List[ObservationData]]`.
-            # pyre-fixme[6]: For 2nd param expected `Optional[Dict[str, Union[None,
-            #  Dict[str, typing.Any], OptimizationConfig, AcquisitionFunction, float,
-            #  int, str]]]` but got `Dict[str, List[str]]`.
             tf = PowerTransformY(**shared_init_args, config={"metrics": [m]})
             # tf.power_transforms should only exist for m and be a PowerTransformer
             self.assertIsInstance(tf.power_transforms, dict)
             self.assertEqual([*tf.power_transforms], [m])  # Check keys
             self.assertIsInstance(tf.power_transforms[m], PowerTransformer)
             # tf.inv_bounds should only exist for m and be a tuple of length 2
             self.assertIsInstance(tf.inv_bounds, dict)
@@ -181,15 +173,15 @@
         self.assertTrue(isnan(new_mean_1) and isnan(new_var_1))
         self.assertTrue(isfinite(new_mean_2) and isfinite(new_var_2))
 
     def test_TransformAndUntransformOneMetric(self) -> None:
         pt = PowerTransformY(
             search_space=None,
             observations=deepcopy(self.observations[:2]),
-            config={"metrics": ["m1"]},  # pyre-ignore
+            config={"metrics": ["m1"]},
         )
 
         # Transform the data and make sure we don't touch m1
         observation_data_tf = pt._transform_observation_data(
             deepcopy([self.obsd1, self.obsd2])
         )
         for obsd, obsd_orig in zip(observation_data_tf, [self.obsd1, self.obsd2]):
@@ -211,15 +203,15 @@
         cov_results = np.array(transformed_obsd_nan.covariance)
         self.assertTrue(np.all(np.isnan(np.diag(cov_results))))
 
     def test_TransformAndUntransformAllMetrics(self) -> None:
         pt = PowerTransformY(
             search_space=None,
             observations=deepcopy(self.observations[:2]),
-            config={"metrics": ["m1", "m2"]},  # pyre-ignore
+            config={"metrics": ["m1", "m2"]},
         )
 
         observation_data_tf = pt._transform_observation_data(
             deepcopy([self.obsd1, self.obsd2])
         )
         for obsd, obsd_orig in zip(observation_data_tf, [self.obsd1, self.obsd2]):
             for i in range(2):  # Both metrics should be transformed
@@ -247,30 +239,30 @@
 
         y_orig = np.array([data.means[0] for data in observation_data])[:, None]
         y1 = PowerTransformer("yeo-johnson").fit(y_orig).transform(y_orig).ravel()
 
         pt = PowerTransformY(
             search_space=None,
             observations=deepcopy(self.observations[:3]),
-            config={"metrics": ["m1"]},  # pyre-ignore
+            config={"metrics": ["m1"]},
         )
         observation_data_tf = pt._transform_observation_data(observation_data)
         y2 = [data.means[0] for data in observation_data_tf]
         for y1_, y2_ in zip(y1, y2):
             self.assertAlmostEqual(y1_, y2_)
 
     def test_TransformOptimizationConfig(self) -> None:
         # basic test
         m1 = Metric(name="m1")
         objective_m1 = Objective(metric=m1, minimize=False)
         oc = OptimizationConfig(objective=objective_m1, outcome_constraints=[])
         tf = PowerTransformY(
             search_space=None,
             observations=self.observations[:2],
-            config={"metrics": ["m1"]},  # pyre-ignore
+            config={"metrics": ["m1"]},
         )
         oc_tf = tf.transform_optimization_config(deepcopy(oc), None, None)
         self.assertEqual(oc_tf, oc)
         # Output constraint on a different metric should not transform the bound
         m2 = Metric(name="m2")
         for bound in [-1.234, 0, 2.345]:
             oc = OptimizationConfig(
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_relativize_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_relativize_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from typing import List, Tuple
 from unittest.mock import Mock, patch, PropertyMock
 
 import numpy as np
 from ax.core import BatchTrial
 from ax.core.observation import (
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_remove_fixed_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_remove_fixed_transform.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import (
     ChoiceParameter,
     FixedParameter,
     ParameterType,
@@ -17,14 +19,15 @@
 from ax.modelbridge.transforms.remove_fixed import RemoveFixed
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class RemoveFixedTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "a", lower=1, upper=3, parameter_type=ParameterType.FLOAT
                 ),
                 ChoiceParameter(
                     "b", parameter_type=ParameterType.STRING, values=["a", "b", "c"]
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_rounding_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_rounding_transform.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.modelbridge.transforms.rounding import (
     randomized_onehot_round,
     strict_onehot_round,
 )
 from ax.utils.common.testutils import TestCase
 
 
 class RoundingTest(TestCase):
-    def setUp(self) -> None:
-        pass
-
     def test_OneHotRound(self) -> None:
         self.assertTrue(
             np.allclose(
                 strict_onehot_round(np.array([0.1, 0.5, 0.3])), np.array([0, 1, 0])
             )
         )
         # One item should be set to one at random.
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_search_space_to_choice_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_search_space_to_choice_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.arm import Arm
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.parameter import (
     ChoiceParameter,
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_search_space_to_float_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_search_space_to_float_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.transforms.search_space_to_float import SearchSpaceToFloat
 from ax.utils.common.testutils import TestCase
 
 
 class SearchSpaceToFloatTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "a", lower=1, upper=3, parameter_type=ParameterType.FLOAT
                 ),
                 ChoiceParameter(
                     "b", parameter_type=ParameterType.STRING, values=["a", "b", "c"]
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_standardize_y_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_standardize_y_transform.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from math import sqrt
 from typing import List
 
 import numpy as np
 from ax.core.metric import Metric
 from ax.core.objective import Objective
@@ -18,14 +20,15 @@
 from ax.exceptions.core import DataRequiredError
 from ax.modelbridge.transforms.standardize_y import StandardizeY
 from ax.utils.common.testutils import TestCase
 
 
 class StandardizeYTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2", "m2"],
             means=np.array([1.0, 2.0, 1.0]),
             covariance=np.array([[1.0, 0.2, 0.4], [0.2, 2.0, 0.8], [0.4, 0.8, 3.0]]),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m1", "m2", "m2"],
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_stratified_standardize_y_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_stratified_standardize_y_transform.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from math import sqrt
 
 import numpy as np
 from ax.core.metric import Metric
 from ax.core.objective import Objective
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
@@ -19,14 +21,15 @@
 from ax.modelbridge.transforms.stratified_standardize_y import StratifiedStandardizeY
 from ax.modelbridge.transforms.tests.test_standardize_y_transform import osd_allclose
 from ax.utils.common.testutils import TestCase
 
 
 class StratifiedStandardizeYTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2", "m2"],
             means=np.array([1.0, 2.0, 8.0]),
             covariance=np.array([[1.0, 0.2, 0.4], [0.2, 2.0, 0.8], [0.4, 0.8, 3.0]]),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m1", "m2", "m2"],
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_task_encode_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_task_encode_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.modelbridge.transforms.task_encode import TaskEncode
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class TaskEncodeTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x", lower=1, upper=3, parameter_type=ParameterType.FLOAT
                 ),
                 ChoiceParameter(
                     "b",
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_trial_as_task_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_trial_as_task_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.search_space import SearchSpace
 from ax.exceptions.core import UnsupportedError
 from ax.modelbridge.transforms.trial_as_task import TrialAsTask
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_robust_search_space
 
 
 class TrialAsTaskTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x", lower=1, upper=4, parameter_type=ParameterType.FLOAT
                 )
             ]
         )
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_unit_x_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_unit_x_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType, RangeParameter
 from ax.core.parameter_constraint import ParameterConstraint
 from ax.core.search_space import RobustSearchSpace, SearchSpace
@@ -21,14 +23,15 @@
 
     transform_class = UnitX
     # pyre-fixme[4]: Attribute must be annotated.
     expected_c_dicts = [{"x": -1.0, "y": 1.0}, {"x": -1.0, "a": 1.0}]
     expected_c_bounds = [0.0, 1.0]
 
     def setUp(self) -> None:
+        super().setUp()
         self.target_lb = self.transform_class.target_lb
         self.target_range = self.transform_class.target_range
         self.target_ub = self.target_lb + self.target_range
         self.search_space = SearchSpace(
             parameters=[
                 RangeParameter(
                     "x", lower=1, upper=3, parameter_type=ParameterType.FLOAT
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_winsorize_legacy_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_winsorize_legacy_transform.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-import warnings
+# pyre-strict
+
 from copy import deepcopy
 
 import numpy as np
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
 from ax.exceptions.core import DataRequiredError
 from ax.modelbridge.transforms.winsorize import Winsorize
 from ax.utils.common.testutils import TestCase
 
 
 class WinsorizeTransformTestLegacy(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2", "m2"],
             means=np.array([0.0, 0.0, 1.0]),
             covariance=np.array([[1.0, 0.2, 0.4], [0.2, 2.0, 0.8], [0.4, 0.8, 3.0]]),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m1", "m2", "m2"],
@@ -101,28 +103,26 @@
                     "m1": (None, None),
                     "m2": (0.0, None),  # This should leave m2 untouched
                 },
             },
         )
 
     def test_PrintDeprecationWarning(self) -> None:
-        warnings.simplefilter("always", DeprecationWarning)
-        with warnings.catch_warnings(record=True) as ws:
+        expected_warning = (
+            "Winsorization received an out-of-date `transform_config`, containing "
+            "the following deprecated keys: {'winsorization_upper'}. Please "
+            "update the config according to the docs of "
+            "`ax.modelbridge.transforms.winsorize.Winsorize`."
+        )
+        with self.assertWarnsRegex(DeprecationWarning, expected_warning):
             Winsorize(
                 search_space=None,
                 observations=deepcopy(self.observations),
                 config={"winsorization_upper": 0.2},
             )
-            self.assertTrue(
-                "Winsorization received an out-of-date `transform_config`, containing "
-                "the following deprecated keys: {'winsorization_upper'}. Please "
-                "update the config according to the docs of "
-                "`ax.modelbridge.transforms.winsorize.Winsorize`."
-                in [str(w.message) for w in ws]
-            )
 
     def test_Init(self) -> None:
         self.assertEqual(self.t.cutoffs["m1"], (-float("inf"), 2.0))
         self.assertEqual(self.t.cutoffs["m2"], (-float("inf"), 2.0))
         self.assertEqual(self.t1.cutoffs["m1"], (-float("inf"), 1.0))
         self.assertEqual(self.t1.cutoffs["m2"], (-float("inf"), 1.0))
         self.assertEqual(self.t2.cutoffs["m1"], (0.0, float("inf")))
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/tests/test_winsorize_transform.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/tests/test_winsorize_transform.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
 from copy import deepcopy
 from typing import Dict, Optional, Tuple
 from unittest import mock
 
 import numpy as np
 from ax.core.data import Data
@@ -53,14 +55,15 @@
         arm_name="1_1",
     )
 ]
 
 
 class WinsorizeTransformTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.obsd1 = ObservationData(
             metric_names=["m1", "m2", "m2"],
             means=np.array([0.0, 0.0, 1.0]),
             covariance=np.array([[1.0, 0.2, 0.4], [0.2, 2.0, 0.8], [0.4, 0.8, 3.0]]),
         )
         self.obsd2 = ObservationData(
             metric_names=["m1", "m1", "m2", "m2"],
@@ -150,27 +153,25 @@
                         lower_quantile_margin=0.4, lower_boundary=0.0
                     ),
                 }
             },
         )
 
     def test_PrintDeprecationWarning(self) -> None:
-        warnings.simplefilter("always", DeprecationWarning)
-        with warnings.catch_warnings(record=True) as ws:
+        expected_warning = (
+            "Winsorization received an out-of-date `transform_config`, containing "
+            'the key `"optimization_config"`. Please update the config according '
+            "to the docs of `ax.modelbridge.transforms.winsorize.Winsorize`."
+        )
+        with self.assertWarnsRegex(DeprecationWarning, expected_warning):
             Winsorize(
                 search_space=None,
                 observations=deepcopy(self.observations),
                 config={"optimization_config": "dummy_val"},
             )
-            self.assertTrue(
-                "Winsorization received an out-of-date `transform_config`, containing "
-                'the key `"optimization_config"`. Please update the config according '
-                "to the docs of `ax.modelbridge.transforms.winsorize.Winsorize`."
-                in [str(w.message) for w in ws]
-            )
 
     def test_Init(self) -> None:
         self.assertEqual(self.t.cutoffs["m1"], (-INF, 2.0))
         self.assertEqual(self.t.cutoffs["m2"], (-INF, 2.0))
         self.assertEqual(self.t1.cutoffs["m1"], (-INF, 1.0))
         self.assertEqual(self.t1.cutoffs["m2"], (-INF, 1.0))
         self.assertEqual(self.t2.cutoffs["m1"], (0.0, INF))
@@ -484,45 +485,43 @@
         self.assertEqual(transform.cutoffs["m3"], (-INF, INF))
         # Add a scalarized outcome constraint which should print a warning
         optimization_config.outcome_constraints = [
             ScalarizedOutcomeConstraint(
                 metrics=[m1, m3], op=ComparisonOp.GEQ, bound=3, relative=False
             )
         ]
-        warnings.simplefilter("always", append=True)
         with warnings.catch_warnings(record=True) as ws:
             transform = get_transform(
                 observation_data=deepcopy(all_obsd),
                 optimization_config=optimization_config,
             )
-            for i in range(2):
-                self.assertTrue(
-                    "Automatic winsorization isn't supported for a "
-                    "`ScalarizedOutcomeConstraint`. Specify the winsorization settings "
-                    f"manually if you want to winsorize metric m{['1', '3'][i]}."
-                    in [str(w.message) for w in ws]
-                )
+        for i in range(2):
+            self.assertTrue(
+                "Automatic winsorization isn't supported for a "
+                "`ScalarizedOutcomeConstraint`. Specify the winsorization settings "
+                f"manually if you want to winsorize metric m{['1', '3'][i]}."
+                in [str(w.message) for w in ws]
+            )
         # Multi-objective without objective thresholds should warn and winsorize
         moo_objective = MultiObjective(
             [Objective(m1, minimize=False), Objective(m2, minimize=True)]
         )
         optimization_config = MultiObjectiveOptimizationConfig(objective=moo_objective)
-        warnings.simplefilter("always", append=True)
         with warnings.catch_warnings(record=True) as ws:
             transform = get_transform(
                 observation_data=deepcopy(all_obsd),
                 optimization_config=optimization_config,
             )
-            for _ in range(2):
-                self.assertTrue(
-                    "Encountered a `MultiObjective` without objective thresholds. We "
-                    "will winsorize each objective separately. We strongly recommend "
-                    "specifying the objective thresholds when using multi-objective "
-                    "optimization." in [str(w.message) for w in ws]
-                )
+        for _ in range(2):
+            self.assertTrue(
+                "Encountered a `MultiObjective` without objective thresholds. We "
+                "will winsorize each objective separately. We strongly recommend "
+                "specifying the objective thresholds when using multi-objective "
+                "optimization." in [str(w.message) for w in ws]
+            )
         self.assertEqual(transform.cutoffs["m1"], (-6.5, INF))
         self.assertEqual(transform.cutoffs["m2"], (-INF, 10.0))
         self.assertEqual(transform.cutoffs["m3"], (-INF, INF))
         # Add relative objective thresholds (should raise an error)
         objective_thresholds = [
             ObjectiveThreshold(m1, 3, relative=True),
             ObjectiveThreshold(m2, 4, relative=True),
@@ -575,15 +574,15 @@
         # ModelBridge with in-design status quo
         search_space = SearchSpace(
             parameters=[
                 RangeParameter("x", ParameterType.FLOAT, 0, 20),
                 RangeParameter("y", ParameterType.FLOAT, 0, 20),
             ]
         )
-        objective = Objective(Metric("c"))
+        objective = Objective(Metric("c"), minimize=False)
 
         # Test with relative constraint, in-design status quo
         oc = OptimizationConfig(
             objective=objective,
             outcome_constraints=[
                 OutcomeConstraint(
                     Metric("a"), ComparisonOp.LEQ, bound=2, relative=False
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/trial_as_task.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/trial_as_task.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Dict, List, Optional, TYPE_CHECKING, Union
 
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ChoiceParameter, ParameterType
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.exceptions.core import UnsupportedError
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/unit_x.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/unit_x.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, Tuple, TYPE_CHECKING
 
 import numpy as np
 from ax.core.observation import Observation, ObservationFeatures
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.parameter_constraint import ParameterConstraint
 from ax.core.parameter_distribution import ParameterDistribution
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/utils.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from collections import defaultdict
 from math import isnan
 from numbers import Number
 from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union
```

### Comparing `ax-platform-0.3.7/ax/modelbridge/transforms/winsorize.py` & `ax-platform-0.4.0/ax/modelbridge/transforms/winsorize.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
 from logging import Logger
 from typing import Dict, List, Optional, Tuple, TYPE_CHECKING, Union
 
 import numpy as np
 from ax.core.objective import MultiObjective, ScalarizedObjective
 from ax.core.observation import Observation, ObservationData
@@ -343,17 +345,16 @@
 
 
 def _get_tukey_cutoffs(Y: np.ndarray, lower: bool) -> float:
     """Compute winsorization cutoffs similarly to Tukey boxplots.
 
     See https://mathworld.wolfram.com/Box-and-WhiskerPlot.html for more details.
     """
-    # TODO: replace interpolation->method once it becomes standard.
-    q1 = np.percentile(Y, q=25, interpolation="lower")
-    q3 = np.percentile(Y, q=75, interpolation="higher")
+    q1 = np.percentile(Y, q=25, method="lower")
+    q3 = np.percentile(Y, q=75, method="higher")
     iqr = q3 - q1
     return q1 - 1.5 * iqr if lower else q3 + 1.5 * iqr
 
 
 def _get_auto_winsorization_cutoffs_single_objective(
     metric_values: List[float], minimize: bool
 ) -> Tuple[float, float]:
```

### Comparing `ax-platform-0.3.7/ax/models/base.py` & `ax-platform-0.4.0/ax/models/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict
 
 
 class Model:
     """Base class for an Ax model.
 
     Note: the core methods each model has: `fit`, `predict`, `gen`,
```

### Comparing `ax-platform-0.3.7/ax/models/discrete/eb_thompson.py` & `ax-platform-0.4.0/ax/models/discrete/eb_thompson.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from typing import List, Tuple
 
 import numpy as np
 from ax.models.discrete.thompson import ThompsonSampler
 from ax.utils.common.logger import get_logger
 from ax.utils.stats.statstools import positive_part_james_stein
```

### Comparing `ax-platform-0.3.7/ax/models/discrete/full_factorial.py` & `ax-platform-0.4.0/ax/models/discrete/full_factorial.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import itertools
 import logging
 from functools import reduce
 from operator import mul
 from typing import Dict, List, Optional, Tuple
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/models/discrete/thompson.py` & `ax-platform-0.4.0/ax/models/discrete/thompson.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import hashlib
 import json
 from typing import Dict, List, Optional, Tuple
 
 import numpy as np
 from ax.core.types import TGenMetadata, TParamValue, TParamValueList
 from ax.exceptions.constants import TS_MIN_WEIGHT_ERROR, TS_NO_FEASIBLE_ARMS_ERROR
@@ -257,15 +259,15 @@
         self, X: List[TParamValueList], Ys: List[List[float]], Yvars: List[List[float]]
     ) -> List[Dict[TParamValueList, Tuple[float, float]]]:
         """Construct lists of mappings, one per outcome, of parameterizations
         to the a tuple of their mean and variance.
         """
         X_to_Ys_and_Yvars = []
         hashableX = [self._hash_TParamValueList(x) for x in X]
-        for (Y, Yvar) in zip(Ys, Yvars):
+        for Y, Yvar in zip(Ys, Yvars):
             X_to_Ys_and_Yvars.append(dict(zip(hashableX, zip(Y, Yvar))))
         return X_to_Ys_and_Yvars
 
     def _hash_TParamValueList(self, x: TParamValueList) -> str:
         """Hash a list of parameter values. This is safer than converting the
         list to a tuple because of int/floats.
         """
```

### Comparing `ax-platform-0.3.7/ax/models/discrete_base.py` & `ax-platform-0.4.0/ax/models/discrete_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, Tuple
 
 import numpy as np
 from ax.core.types import TGenMetadata, TParamValue, TParamValueList
 from ax.models.base import Model
 from ax.models.types import TConfig
```

### Comparing `ax-platform-0.3.7/ax/models/model_utils.py` & `ax-platform-0.4.0/ax/models/model_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import itertools
 import warnings
 from typing import Callable, Dict, List, Optional, Protocol, Set, Tuple, Union
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/models/random/alebo_initializer.py` & `ax-platform-0.4.0/ax/models/random/alebo_initializer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Callable, Dict, List, Optional, Tuple
 from warnings import warn
 
 import numpy as np
 from ax.models.random.uniform import UniformGenerator
 from ax.models.types import TConfig
 from ax.utils.common.docutils import copy_doc
```

### Comparing `ax-platform-0.3.7/ax/models/random/base.py` & `ax-platform-0.4.0/ax/models/random/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
 from ax.exceptions.core import SearchSpaceExhausted
 from ax.models.base import Model
```

### Comparing `ax-platform-0.3.7/ax/models/random/rembo_initializer.py` & `ax-platform-0.4.0/ax/models/random/rembo_initializer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Callable, Dict, List, Optional, Tuple
 
 import numpy as np
 from ax.models.random.uniform import UniformGenerator
 from ax.models.types import TConfig
 from ax.utils.common.docutils import copy_doc
```

### Comparing `ax-platform-0.3.7/ax/models/random/sobol.py` & `ax-platform-0.4.0/ax/models/random/sobol.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
 from ax.models.base import Model
 from ax.models.model_utils import tunable_feature_indices
 from ax.models.random.base import RandomModel
```

### Comparing `ax-platform-0.3.7/ax/models/random/uniform.py` & `ax-platform-0.4.0/ax/models/random/uniform.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Optional
 
 import numpy as np
 from ax.models.random.base import RandomModel
 from scipy.stats import uniform
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_alebo.py` & `ax-platform-0.4.0/ax/models/tests/test_alebo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest import mock
 
 import numpy as np
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.alebo import (
     ALEBO,
@@ -280,15 +282,14 @@
         B = torch.tensor(
             [[1.0, 2.0, 3.0, 4.0, 5.0], [2.0, 3.0, 4.0, 5.0, 6.0]], dtype=torch.double
         )
         m = ALEBO(B=B, laplace_nsamp=5, fit_restarts=1)
         self.assertTrue(torch.equal(B, m.B))
         self.assertEqual(m.laplace_nsamp, 5)
         self.assertEqual(m.fit_restarts, 1)
-        self.assertEqual(m.refit_on_update, True)
         self.assertEqual(m.refit_on_cv, False)
         self.assertEqual(m.warm_start_refitting, False)
 
         train_X = torch.tensor(
             [
                 [0.0, 0.0, 0.0, 0.0, 0.0],
                 [1.0, 1.0, 1.0, 1.0, 1.0],
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_alebo_initializer.py` & `ax-platform-0.4.0/ax/models/tests/test_alebo_initializer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.models.random.alebo_initializer import ALEBOInitializer
 from ax.utils.common.testutils import TestCase
 
 
 class ALEBOSobolTest(TestCase):
     def test_ALEBOSobolModel(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_base.py` & `ax-platform-0.4.0/ax/models/tests/test_base.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.models.base import Model
 from ax.utils.common.testutils import TestCase
 
 
 class BaseModelTest(TestCase):
     def test_base_model(self) -> None:
         model = Model()
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_botorch_defaults.py` & `ax-platform-0.4.0/ax/models/tests/test_botorch_defaults.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from unittest import mock
 from unittest.mock import Mock
 
 import torch
 from ax.models.torch.botorch_defaults import (
     _get_acquisition_func,
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_botorch_kg.py` & `ax-platform-0.4.0/ax/models/tests/test_botorch_kg.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from typing import Any, Dict
 from unittest import mock
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.botorch_kg import _instantiate_KG, KnowledgeGradient
@@ -30,14 +32,15 @@
 
 def dummy_func(X: torch.Tensor) -> torch.Tensor:
     return X
 
 
 class KnowledgeGradientTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.tkwargs: Dict[str, Any] = {
             "device": torch.device("cpu"),
             "dtype": torch.double,
         }
         self.feature_names = ["x1", "x2", "x3"]
         self.metric_names = ["y"]
         self.dataset = SupervisedDataset(
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_botorch_mes.py` & `ax-platform-0.4.0/ax/models/tests/test_botorch_mes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from typing import Any, Dict
 from unittest import mock
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.botorch_mes import _instantiate_MES, MaxValueEntropySearch
@@ -22,14 +24,15 @@
 from botorch.models.transforms.input import Warp
 from botorch.sampling.normal import SobolQMCNormalSampler
 from botorch.utils.datasets import SupervisedDataset
 
 
 class MaxValueEntropySearchTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.tkwargs: Dict[str, Any] = {
             "device": torch.device("cpu"),
             "dtype": torch.double,
         }
         self.feature_names = ["x1", "x2", "x3"]
         self.metric_names = ["y"]
         self.training_data = [
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_botorch_model.py` & `ax-platform-0.4.0/ax/models/tests/test_botorch_model.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from itertools import chain, product
 from typing import Any, cast, Dict
 from unittest import mock
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_botorch_moo_defaults.py` & `ax-platform-0.4.0/ax/models/tests/test_botorch_moo_defaults.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from contextlib import ExitStack
 from typing import Any, cast, Dict, Tuple
 from unittest import mock
 
 import numpy as np
 import torch
 from ax.core.search_space import SearchSpaceDigest
@@ -17,18 +19,19 @@
     get_qLogEHVI,
     get_qLogNEHVI,
     get_weighted_mc_objective_and_objective_thresholds,
     infer_objective_thresholds,
     pareto_frontier_evaluator,
 )
 from ax.models.torch.utils import _get_X_pending_and_observed
+from ax.utils.common.random import with_rng_seed
 from ax.utils.common.testutils import TestCase
+from botorch.models.gp_regression import SingleTaskGP
 from botorch.utils.datasets import SupervisedDataset
 from botorch.utils.multi_objective.hypervolume import infer_reference_point
-from botorch.utils.sampling import manual_seed
 from botorch.utils.testing import MockModel, MockPosterior
 from torch._tensor import Tensor
 
 
 MOO_DEFAULTS_PATH: str = "ax.models.torch.botorch_moo_defaults"
 GET_ACQF_PATH: str = MOO_DEFAULTS_PATH + ".get_acquisition_function"
 GET_CONSTRAINT_PATH: str = MOO_DEFAULTS_PATH + ".get_outcome_constraint_transforms"
@@ -44,14 +47,15 @@
     mean = torch.cat([X, torch.prod(X, dim=1).reshape(-1, 1)], dim=1)
     cov = torch.zeros(mean.shape[0], mean.shape[1], mean.shape[1])
     return mean, cov
 
 
 class FrontierEvaluatorTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.X = torch.tensor(
             [[1.0, 0.0], [1.0, 1.0], [1.0, 3.0], [2.0, 2.0], [3.0, 1.0]]
         )
         self.Y = torch.tensor(
             [
                 [1.0, 0.0, 0.0],
                 [1.0, 1.0, 1.0],
@@ -237,24 +241,24 @@
         objective_thresholds = torch.arange(3, dtype=torch.float)
         obj_and_obj_t = get_weighted_mc_objective_and_objective_thresholds(
             objective_weights=weights,
             objective_thresholds=objective_thresholds,
         )
         (weighted_obj, new_obj_thresholds) = obj_and_obj_t
         cons_tfs = get_outcome_constraint_transforms(constraints)
-        with manual_seed(0):
+        with with_rng_seed(0):
             seed = torch.randint(1, 10000, (1,)).item()
         with ExitStack() as es:
             mock_get_acqf = es.enter_context(mock.patch(GET_ACQF_PATH))
             es.enter_context(
                 mock.patch(MOO_DEFAULTS_PATH + ".checked_cast", wraps=cast)
             )
             es.enter_context(mock.patch(GET_CONSTRAINT_PATH, return_value=cons_tfs))
             es.enter_context(mock.patch(GET_OBJ_PATH, return_value=obj_and_obj_t))
-            es.enter_context(manual_seed(0))
+            es.enter_context(with_rng_seed(0))
             get_qLogEHVI(
                 model=mm,
                 objective_weights=weights,
                 outcome_constraints=constraints,
                 objective_thresholds=objective_thresholds,
                 X_observed=X_observed,
                 X_pending=X_pending,
@@ -306,33 +310,31 @@
                 )
                 _mock_infer_reference_point = es.enter_context(
                     mock.patch(
                         "ax.models.torch.botorch_moo_defaults.infer_reference_point",
                         wraps=infer_reference_point,
                     )
                 )
-                # after subsetting, the model will only have two outputs
-                model = MockModel(
-                    MockPosterior(
-                        mean=torch.tensor(
-                            [
-                                [11.0, 2.0],
-                                [9.0, 3.0],
-                            ],
-                            **tkwargs,
-                        )
-                    )
-                )
+                model = SingleTaskGP(train_X=Xs[0], train_Y=torch.rand(2, 3, **tkwargs))
                 es.enter_context(
                     mock.patch.object(
                         model,
-                        "subset_output",
-                        return_value=model,
+                        "posterior",
+                        return_value=MockPosterior(
+                            mean=torch.tensor(
+                                [
+                                    [11.0, 2.0],
+                                    [9.0, 3.0],
+                                ],
+                                **tkwargs,
+                            )
+                        ),
                     )
                 )
+
                 # test passing Xs
                 obj_thresholds = infer_objective_thresholds(
                     model,
                     bounds=bounds,
                     objective_weights=objective_weights,
                     outcome_constraints=outcome_constraints,
                     fixed_features={},
@@ -378,31 +380,27 @@
                 )
                 _mock_infer_reference_point = es.enter_context(
                     mock.patch(
                         "ax.models.torch.botorch_moo_defaults.infer_reference_point",
                         wraps=infer_reference_point,
                     )
                 )
-                model = MockModel(
-                    MockPosterior(
-                        mean=torch.tensor(
-                            # after subsetting, there should only be two outcomes
-                            [
-                                [11.0, 2.0],
-                                [9.0, 3.0],
-                            ],
-                            **tkwargs,
-                        )
-                    )
-                )
                 es.enter_context(
                     mock.patch.object(
                         model,
-                        "subset_output",
-                        return_value=model,
+                        "posterior",
+                        return_value=MockPosterior(
+                            mean=torch.tensor(
+                                [
+                                    [11.0, 2.0],
+                                    [9.0, 3.0],
+                                ],
+                                **tkwargs,
+                            )
+                        ),
                     )
                 )
 
                 # test passing X_observed
                 obj_thresholds = infer_objective_thresholds(
                     model,
                     objective_weights=objective_weights,
@@ -425,34 +423,32 @@
             with self.assertRaises(ValueError):
                 infer_objective_thresholds(
                     model,
                     bounds=bounds,
                     objective_weights=objective_weights,
                 )
             # test subset_model without subset_idcs
-            subset_model = MockModel(
-                MockPosterior(
-                    mean=torch.tensor(
-                        [
-                            [11.0, 2.0],
-                            [9.0, 3.0],
-                        ],
-                        **tkwargs,
-                    )
-                )
-            )
             es.enter_context(
                 mock.patch.object(
-                    subset_model,
-                    "subset_output",
-                    return_value=subset_model,
+                    model,
+                    "posterior",
+                    return_value=MockPosterior(
+                        mean=torch.tensor(
+                            [
+                                [11.0, 2.0],
+                                [9.0, 3.0],
+                            ],
+                            **tkwargs,
+                        )
+                    ),
                 )
             )
+
             obj_thresholds = infer_objective_thresholds(
-                subset_model,
+                model,
                 objective_weights=objective_weights,
                 outcome_constraints=outcome_constraints,
                 X_observed=Xs[0],
             )
             self.assertTrue(
                 torch.equal(obj_thresholds[:2], torch.tensor([9.9, 3.3], **tkwargs))
             )
@@ -460,15 +456,15 @@
             # test passing subset_idcs
             subset_idcs = torch.tensor(
                 [0, 1],
                 dtype=torch.long,
                 device=tkwargs["device"],
             )
             obj_thresholds = infer_objective_thresholds(
-                subset_model,
+                model,
                 objective_weights=objective_weights,
                 outcome_constraints=outcome_constraints,
                 X_observed=Xs[0],
                 subset_idcs=subset_idcs,
             )
             self.assertTrue(
                 torch.equal(obj_thresholds[:2], torch.tensor([9.9, 3.3], **tkwargs))
@@ -490,23 +486,27 @@
                 )
                 _mock_infer_reference_point = es.enter_context(
                     mock.patch(
                         "ax.models.torch.botorch_moo_defaults.infer_reference_point",
                         wraps=infer_reference_point,
                     )
                 )
-                model = MockModel(
-                    MockPosterior(
-                        mean=torch.tensor(
-                            [
-                                [11.0, 2.0, 6.0],
-                                [9.0, 3.0, 4.0],
-                            ],
-                            **tkwargs,
-                        )
+                es.enter_context(
+                    mock.patch.object(
+                        model,
+                        "posterior",
+                        return_value=MockPosterior(
+                            mean=torch.tensor(
+                                [
+                                    [11.0, 2.0, 6.0],
+                                    [9.0, 3.0, 4.0],
+                                ],
+                                **tkwargs,
+                            )
+                        ),
                     )
                 )
                 # test passing Xs
                 obj_thresholds = infer_objective_thresholds(
                     model,
                     bounds=bounds,
                     objective_weights=objective_weights,
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_botorch_moo_model.py` & `ax-platform-0.4.0/ax/models/tests/test_botorch_moo_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from contextlib import ExitStack
 from typing import Any, Dict
 from unittest import mock
 
 import ax.models.torch.botorch_moo_defaults as botorch_moo_defaults
 import botorch.utils.multi_objective.hypervolume as hypervolume
@@ -29,21 +31,22 @@
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.mock import fast_botorch_optimize
 from botorch.acquisition.multi_objective import (
     logei as moo_logei,
     monte_carlo as moo_monte_carlo,
 )
 from botorch.models import ModelListGP
+from botorch.models.gp_regression import SingleTaskGP
 from botorch.models.transforms.input import Warp
 from botorch.optim.optimize import optimize_acqf_list
 from botorch.sampling.normal import IIDNormalSampler
 from botorch.utils.datasets import SupervisedDataset
 from botorch.utils.multi_objective.hypervolume import infer_reference_point
 from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization
-from botorch.utils.testing import MockModel, MockPosterior
+from botorch.utils.testing import MockPosterior
 
 
 FIT_MODEL_MO_PATH = "ax.models.torch.botorch_defaults.fit_gpytorch_mll"
 SAMPLE_SIMPLEX_UTIL_PATH = "ax.models.torch.utils.sample_simplex"
 SAMPLE_HYPERSPHERE_UTIL_PATH = "ax.models.torch.utils.sample_hypersphere"
 CHEBYSHEV_SCALARIZATION_PATH = (
     "ax.models.torch.botorch_defaults.get_chebyshev_scalarization"
@@ -383,34 +386,44 @@
             tfs,
             feature_names,
             metric_names,
         ) = _get_torch_test_data(dtype=dtype, cuda=cuda, constant_noise=True)
         Xs2, Ys2, Yvars2, _, _, _, _ = _get_torch_test_data(
             dtype=dtype, cuda=cuda, constant_noise=True
         )
+        Xs3, Ys3, Yvars3, _, _, _, _ = _get_torch_test_data(
+            dtype=dtype, cuda=cuda, constant_noise=True
+        )
         training_data = [
             SupervisedDataset(
                 X=Xs1[0],
                 Y=Ys1[0],
                 Yvar=Yvars1[0],
                 feature_names=feature_names,
-                outcome_names=metric_names,
+                outcome_names=["m1"],
             ),
             SupervisedDataset(
                 X=Xs2[0],
                 Y=Ys2[0],
                 Yvar=Yvars2[0],
                 feature_names=feature_names,
-                outcome_names=metric_names,
+                outcome_names=["m2"],
+            ),
+            SupervisedDataset(
+                X=Xs3[0],
+                Y=Ys3[0],
+                Yvar=Yvars3[0],
+                feature_names=feature_names,
+                outcome_names=["m3"],
             ),
         ]
 
         n = 3
-        objective_weights = torch.tensor([1.0, 1.0], **tkwargs)
-        obj_t = torch.tensor([1.0, 1.0], **tkwargs)
+        objective_weights = torch.tensor([1.0, 1.0, 0.0], **tkwargs)
+        obj_t = torch.tensor([1.0, 1.0, float("nan")], **tkwargs)
         # pyre-fixme[6]: For 1st param expected `(Model, Tensor, Optional[Tuple[Tenso...
         model = MultiObjectiveBotorchModel(acqf_constructor=acqf_constructor)
 
         X_dummy = torch.tensor([[[1.0, 2.0, 3.0]]], **tkwargs)
         acqfv_dummy = torch.tensor([[[1.0, 2.0, 3.0]]], **tkwargs)
 
         search_space_digest = SearchSpaceDigest(
@@ -459,17 +472,21 @@
             self.assertEqual(1, _mock_acqf.call_count)
             # check partitioning strategy
             # NEHVI should call FastNondominatedPartitioning 1 time
             # since a batched partitioning is used for 2 objectives
             _mock_partitioning.assert_called_once()
             self.assertTrue(
                 torch.equal(
-                    gen_results.gen_metadata["objective_thresholds"], obj_t.cpu()
+                    gen_results.gen_metadata["objective_thresholds"][:2],
+                    obj_t[:2].cpu(),
                 )
             )
+            self.assertTrue(
+                torch.isnan(gen_results.gen_metadata["objective_thresholds"][-1])
+            )
             _mock_fit_model = es.enter_context(mock.patch(FIT_MODEL_MO_PATH))
             # Optimizer options correctly passed through.
             self.assertEqual(
                 mock_optimize.call_args.kwargs["options"]["init_batch_limit"], 32
             )
             self.assertEqual(
                 mock_optimize.call_args.kwargs["options"]["batch_limit"], 1
@@ -501,30 +518,39 @@
             )
 
             # test inferred objective thresholds in gen()
             # create several data points
             Xs1 = [torch.cat([Xs1[0], Xs1[0] - 0.1], dim=0)]
             Ys1 = [torch.cat([Ys1[0], Ys1[0] - 0.5], dim=0)]
             Ys2 = [torch.cat([Ys2[0], Ys2[0] + 0.5], dim=0)]
+            Ys3 = [torch.cat([Ys3[0], Ys3[0] - 1.0], dim=0)]
             Yvars1 = [torch.cat([Yvars1[0], Yvars1[0] + 0.2], dim=0)]
             Yvars2 = [torch.cat([Yvars2[0], Yvars2[0] + 0.1], dim=0)]
+            Yvars3 = [torch.cat([Yvars3[0], Yvars3[0] + 0.4], dim=0)]
             training_data_multiple = [
                 SupervisedDataset(
                     X=Xs1[0],
                     Y=Ys1[0],
                     Yvar=Yvars1[0],
                     feature_names=feature_names,
-                    outcome_names=metric_names,
+                    outcome_names=["m1"],
                 ),
                 SupervisedDataset(
                     X=Xs1[0],
                     Y=Ys2[0],
                     Yvar=Yvars2[0],
                     feature_names=feature_names,
-                    outcome_names=metric_names,
+                    outcome_names=["m2"],
+                ),
+                SupervisedDataset(
+                    X=Xs1[0],
+                    Y=Ys3[0],
+                    Yvar=Yvars3[0],
+                    feature_names=feature_names,
+                    outcome_names=["m3"],
                 ),
             ]
             model.fit(
                 datasets=training_data_multiple,
                 search_space_digest=search_space_digest,
             )
             es.enter_context(
@@ -541,46 +567,29 @@
             )
             _mock_infer_reference_point = es.enter_context(
                 mock.patch(
                     "ax.models.torch.botorch_moo_defaults.infer_reference_point",
                     wraps=infer_reference_point,
                 )
             )
-            # after subsetting, the model will only have two outputs
-            _mock_num_outputs = es.enter_context(
-                mock.patch(
-                    "botorch.utils.testing.MockModel.num_outputs",
-                    new_callable=mock.PropertyMock,
-                )
-            )
-            _mock_num_outputs.return_value = 3
             preds = torch.tensor(
                 [
                     [11.0, 2.0],
                     [9.0, 3.0],
                 ],
                 **tkwargs,
             )
-            model.model = MockModel(
-                MockPosterior(
-                    mean=preds,
-                    samples=preds,
-                ),
-            )
-            subset_mock_model = MockModel(
-                MockPosterior(
-                    mean=preds,
-                    samples=preds,
-                ),
-            )
             es.enter_context(
                 mock.patch.object(
                     model.model,
-                    "subset_output",
-                    return_value=subset_mock_model,
+                    "posterior",
+                    return_value=MockPosterior(
+                        mean=preds,
+                        samples=preds,
+                    ),
                 )
             )
             es.enter_context(
                 mock.patch(
                     "botorch.acquisition.factory.get_sampler",
                     return_value=IIDNormalSampler(sample_shape=torch.Size([2])),
                 )
@@ -591,20 +600,22 @@
             )
             torch_opt_config = TorchOptConfig(
                 objective_weights=torch.tensor([-1.0, -1.0, 0.0], **tkwargs),
                 outcome_constraints=outcome_constraints,
                 model_gen_options={
                     # do not used cached root decomposition since
                     # MockPosterior does not have an mvn attribute
-                    "acquisition_function_kwargs": {
-                        "cache_root": False,
-                        "prune_baseline": False,
-                    }
-                    if use_noisy
-                    else {},
+                    "acquisition_function_kwargs": (
+                        {
+                            "cache_root": False,
+                            "prune_baseline": False,
+                        }
+                        if use_noisy
+                        else {}
+                    ),
                 },
             )
             gen_results = model.gen(
                 n,
                 search_space_digest=search_space_digest,
                 torch_opt_config=torch_opt_config,
             )
@@ -628,15 +639,17 @@
                     ckwargs["objective_weights"],
                     torch.tensor([-1.0, -1.0, 0.0], **tkwargs),
                 )
             )
             oc = ckwargs["outcome_constraints"]
             self.assertTrue(torch.equal(oc[0], outcome_constraints[0]))
             self.assertTrue(torch.equal(oc[1], outcome_constraints[1]))
-            self.assertIs(ckwargs["model"], subset_mock_model)
+            subset_model = ckwargs["model"]
+            self.assertIsInstance(subset_model, SingleTaskGP)
+            self.assertEqual(subset_model.num_outputs, 2)
             self.assertTrue(
                 torch.equal(
                     ckwargs["subset_idcs"],
                     torch.tensor([0, 1], device=tkwargs["device"]),
                 )
             )
             _mock_infer_reference_point.assert_called_once()
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_cbo_lcea.py` & `ax-platform-0.4.0/ax/models/tests/test_cbo_lcea.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from typing import cast
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.cbo_lcea import LCEABO
 from ax.utils.common.testutils import TestCase
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_cbo_lcem.py` & `ax-platform-0.4.0/ax/models/tests/test_cbo_lcem.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 import torch
 from ax.models.torch.cbo_lcem import LCEMBO
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.mock import fast_botorch_optimize
 from botorch.models.contextual_multioutput import LCEMGP
 from botorch.models.model_list_gp_regression import ModelListGP
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_cbo_sac.py` & `ax-platform-0.4.0/ax/models/tests/test_cbo_sac.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from typing import cast
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.cbo_sac import SACBO, SACGP
 from ax.utils.common.testutils import TestCase
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_discrete.py` & `ax-platform-0.4.0/ax/models/tests/test_discrete.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.models.discrete_base import DiscreteModel
 from ax.utils.common.testutils import TestCase
 
 
 class DiscreteModelTest(TestCase):
-    def setUp(self) -> None:
-        pass
-
     def test_discrete_model_get_state(self) -> None:
         discrete_model = DiscreteModel()
         self.assertEqual(discrete_model._get_state(), {})
 
     def test_discrete_model_feature_importances(self) -> None:
         discrete_model = DiscreteModel()
         with self.assertRaises(NotImplementedError):
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_eb_thompson.py` & `ax-platform-0.4.0/ax/models/tests/test_eb_thompson.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest.mock import patch
 
 import numpy as np
 from ax.models.discrete.eb_thompson import EmpiricalBayesThompsonSampler
 from ax.utils.common.testutils import TestCase
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_full_factorial.py` & `ax-platform-0.4.0/ax/models/tests/test_full_factorial.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import numpy as np
 from ax.models.discrete.full_factorial import FullFactorialGenerator
 from ax.utils.common.testutils import TestCase
 
 
 class FullFactorialGeneratorTest(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_fully_bayesian.py` & `ax-platform-0.4.0/ax/models/tests/test_fully_bayesian.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,39 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
-import warnings
-from abc import ABC
 from contextlib import ExitStack
 from itertools import count, product
 from logging import Logger
 from math import sqrt
 from typing import Any, cast, Dict, Type
 from unittest import mock
 
 import pyro
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.exceptions.core import AxError
+from ax.models.torch.botorch import BotorchModel
 from ax.models.torch.fully_bayesian import (
     FullyBayesianBotorchModel,
     FullyBayesianMOOBotorchModel,
     matern_kernel,
     rbf_kernel,
     single_task_pyro_model,
 )
 from ax.models.torch_base import TorchOptConfig
 from ax.utils.common.constants import Keys
 from ax.utils.common.logger import get_logger
+from ax.utils.common.random import set_rng_seed, with_rng_seed
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.torch_stubs import get_torch_test_data
 from botorch.acquisition.utils import get_infeasible_cost
 from botorch.models import ModelListGP
 from botorch.models.gp_regression import MIN_INFERRED_NOISE_LEVEL
 from botorch.models.model import ModelList
 from botorch.models.transforms.input import Warp
@@ -87,900 +89,922 @@
     return dummy_sample_list
 
 
 def dummy_func(X: torch.Tensor) -> torch.Tensor:
     return X
 
 
-class BaseFullyBayesianBotorchModelTest(ABC):
-    model_cls: Type[FullyBayesianBotorchModel]
-
-    def test_FullyBayesianBotorchModel(
-        self, dtype: torch.dtype = torch.float, cuda: bool = False
-    ) -> None:
-        # test deprecation warning
-        warnings.resetwarnings()  # this is necessary for building in mode/opt
-        warnings.simplefilter("always", append=True)
-        with warnings.catch_warnings(record=True) as ws:
-            self.model_cls(use_saas=True)
-            # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-            #  attribute `assertTrue`.
-            self.assertTrue(any(issubclass(w.category, DeprecationWarning) for w in ws))
-            msg = "Passing `use_saas` is no longer supported"
-            self.assertTrue(any(msg in str(w.message) for w in ws))
-        Xs1, Ys1, Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
-            dtype=dtype, cuda=cuda, constant_noise=True
-        )
-        Xs2, Ys2, Yvars2, _, _, _, _ = get_torch_test_data(
-            dtype=dtype, cuda=cuda, constant_noise=True
-        )
-        Yvars_inferred_noise = [
-            torch.full_like(Yvars1[0], float("nan")),
-            torch.full_like(Yvars2[0], float("nan")),
-        ]
-        # make input different for each output
-        Xs2_diff = [Xs2[0] + 0.1]
-        Xs = Xs1 + Xs2_diff
-        Ys = Ys1 + Ys2
-
-        options = product([True, False], [True, False], ["matern", "rbf"])
-        for inferred_noise, use_input_warping, gp_kernel in options:
-            Yvars = Yvars_inferred_noise if inferred_noise else Yvars1 + Yvars2
-            model = self.model_cls(
-                use_input_warping=use_input_warping,
-                thinning=1,
-                num_samples=4,
-                disable_progbar=True,
-                max_tree_depth=1,
-                gp_kernel=gp_kernel,
-                verbose=True,
-            )
-            if use_input_warping:
-                self.assertTrue(model.use_input_warping)
-            # Test ModelListGP
-            # make training data different for each output
-            tkwargs: Dict[str, Any] = {"dtype": dtype, "device": Xs1[0].device}
-            dummy_samples_list = _get_dummy_mcmc_samples(
-                num_samples=4, num_outputs=2, **tkwargs
-            )
-            for dummy_samples in dummy_samples_list:
+class BaseFullyBayesianBotorchModelTestCases:
+    class FullyBayesianBotorchModelTest(TestCase):
+        model_cls: Type[BotorchModel] = FullyBayesianBotorchModel
+
+        def test_FullyBayesianBotorchModel(
+            self, dtype: torch.dtype = torch.float, cuda: bool = False
+        ) -> None:
+            # test deprecation warning
+            with self.assertWarnsRegex(
+                DeprecationWarning, "Passing `use_saas` is no longer supported"
+            ):
+                self.model_cls(use_saas=True)
+            Xs1, Ys1, Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
+                dtype=dtype, cuda=cuda, constant_noise=True
+            )
+            Xs2, Ys2, Yvars2, _, _, _, _ = get_torch_test_data(
+                dtype=dtype, cuda=cuda, constant_noise=True
+            )
+            Yvars_inferred_noise = [
+                torch.full_like(Yvars1[0], float("nan")),
+                torch.full_like(Yvars2[0], float("nan")),
+            ]
+            # make input different for each output
+            Xs2_diff = [Xs2[0] + 0.1]
+            Xs = Xs1 + Xs2_diff
+            Ys = Ys1 + Ys2
+
+            options = product([True, False], [True, False], ["matern", "rbf"])
+            for inferred_noise, use_input_warping, gp_kernel in options:
+                Yvars = Yvars_inferred_noise if inferred_noise else Yvars1 + Yvars2
+                model = self.model_cls(
+                    use_input_warping=use_input_warping,
+                    thinning=1,
+                    num_samples=4,
+                    disable_progbar=True,
+                    max_tree_depth=1,
+                    gp_kernel=gp_kernel,
+                    verbose=True,
+                )
                 if use_input_warping:
-                    # pyre-fixme[16]: `str` has no attribute `__setitem__`.
-                    dummy_samples["c0"] = (
-                        torch.rand(4, 1, Xs1[0].shape[-1], **tkwargs) * 0.5 + 0.1
-                    )
-                    dummy_samples["c1"] = (
-                        torch.rand(4, 1, Xs1[0].shape[-1], **tkwargs) * 0.5 + 0.1
-                    )
-                if inferred_noise:
-                    dummy_samples["noise"] = torch.rand(4, 1, **tkwargs).clamp_min(
-                        MIN_INFERRED_NOISE_LEVEL
-                    )
-
-            with mock.patch(
-                RUN_INFERENCE_PATH,
-                side_effect=dummy_samples_list,
-            ) as _mock_fit_model:
-                model.fit(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X,
-                            Y=Y,
-                            Yvar=Yvar,
-                            feature_names=fns,
-                            outcome_names=[f"y{i}"],
-                        )
-                        for X, Y, Yvar, i in zip(Xs, Ys, Yvars, count())
-                    ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns,
-                        bounds=bounds,
-                        task_features=tfs,
-                    ),
+                    self.assertTrue(model.use_input_warping)
+                # Test ModelListGP
+                # make training data different for each output
+                tkwargs: Dict[str, Any] = {"dtype": dtype, "device": Xs1[0].device}
+                dummy_samples_list = _get_dummy_mcmc_samples(
+                    num_samples=4, num_outputs=2, **tkwargs
                 )
-
-                # Check that there are no unexpected constraints on the hyperparameters
-                for _, m in enumerate(cast(ModelList, model.model).models):
-                    if inferred_noise:
-                        noise_covar = m.likelihood.noise_covar
-                        self.assertEqual(  # pyre-fixme[16]
-                            noise_covar.raw_noise_constraint.__class__, GreaterThan
+                for dummy_samples in dummy_samples_list:
+                    if use_input_warping:
+                        # pyre-fixme[16]: `str` has no attribute `__setitem__`.
+                        dummy_samples["c0"] = (
+                            torch.rand(4, 1, Xs1[0].shape[-1], **tkwargs) * 0.5 + 0.1
                         )
-                        self.assertTrue(
-                            torch.allclose(
-                                noise_covar.raw_noise_constraint.lower_bound,
-                                torch.tensor(1e-4, dtype=dtype),
-                            )
+                        dummy_samples["c1"] = (
+                            torch.rand(4, 1, Xs1[0].shape[-1], **tkwargs) * 0.5 + 0.1
                         )
-                    else:
-                        self.assertFalse(  # pyre-fixme[16]
-                            hasattr(m.likelihood.noise_covar, "raw_noise_constraint")
+                    if inferred_noise:
+                        dummy_samples["noise"] = torch.rand(4, 1, **tkwargs).clamp_min(
+                            MIN_INFERRED_NOISE_LEVEL
                         )
 
-                    self.assertEqual(m.covar_module.__class__, ScaleKernel)
-                    self.assertEqual(
-                        m.covar_module.raw_outputscale_constraint.__class__, Positive
-                    )
-                    self.assertEqual(
-                        m.covar_module.base_kernel.__class__,
-                        RBFKernel if gp_kernel == "rbf" else MaternKernel,
-                    )
-                    self.assertEqual(m.covar_module.base_kernel.ard_num_dims, 3)
-                    self.assertEqual(
-                        m.covar_module.base_kernel.raw_lengthscale_constraint.__class__,
-                        Positive,
+                with mock.patch(
+                    RUN_INFERENCE_PATH,
+                    side_effect=dummy_samples_list,
+                ) as _mock_fit_model:
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[f"y{i}"],
+                            )
+                            for X, Y, Yvar, i in zip(Xs, Ys, Yvars, count())
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns,
+                            bounds=bounds,
+                            task_features=tfs,
+                        ),
                     )
 
-                #  attribute `assertEqual`.
-                self.assertEqual(_mock_fit_model.call_count, 2)
-                for i, call in enumerate(_mock_fit_model.call_args_list):
-                    _, ckwargs = call
-                    X = Xs[i]
-                    Y = Ys[i]
-                    Yvar = Yvars[i]
-                    # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                    #  attribute `assertIs`.
-                    self.assertIs(ckwargs["pyro_model"], single_task_pyro_model)
-
-                    self.assertTrue(torch.equal(ckwargs["X"], X))
-                    self.assertTrue(torch.equal(ckwargs["Y"], Y))
-                    if inferred_noise:
-                        self.assertTrue(torch.isnan(ckwargs["Yvar"]).all())
-                    else:
-                        self.assertTrue(torch.equal(ckwargs["Yvar"], Yvar))
-                    self.assertEqual(ckwargs["num_samples"], 4)
-                    self.assertEqual(ckwargs["warmup_steps"], 512)
-                    self.assertEqual(ckwargs["max_tree_depth"], 1)
-                    self.assertTrue(ckwargs["disable_progbar"])
-                    self.assertFalse(ckwargs["jit_compile"])
-                    self.assertEqual(ckwargs["use_input_warping"], use_input_warping)
-                    self.assertEqual(ckwargs["gp_kernel"], gp_kernel)
-                    self.assertTrue(ckwargs["verbose"])
+                    # Check that there are no unexpected constraints on the
+                    # hyperparameters
+                    for _, m in enumerate(cast(ModelList, model.model).models):
+                        if inferred_noise:
+                            noise_covar = m.likelihood.noise_covar
+                            self.assertEqual(
+                                noise_covar.raw_noise_constraint.__class__, GreaterThan
+                            )
+                            self.assertTrue(
+                                torch.allclose(
+                                    noise_covar.raw_noise_constraint.lower_bound,
+                                    torch.tensor(1e-4, dtype=dtype),
+                                )
+                            )
+                        else:
+                            self.assertFalse(
+                                hasattr(
+                                    m.likelihood.noise_covar, "raw_noise_constraint"
+                                )
+                            )
 
-                    # Check attributes
-                    self.assertTrue(torch.equal(model.Xs[i], Xs[i]))
-                    self.assertEqual(model.dtype, Xs[i].dtype)
-                    self.assertEqual(model.device, Xs[i].device)
-                    # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                    #  attribute `assertIsInstance`.
-                    self.assertIsInstance(model.model, ModelListGP)
-
-                    # Check fitting
-                    # Note each model in the model list is a batched model, where
-                    # the batch dim corresponds to the MCMC samples
-                    model_list = cast(ModelList, model.model).models
-                    # Put model in `eval` mode to transform the train inputs.
-                    m = model_list[i].eval()
-                    # check mcmc samples
-                    # pyre-fixme[6]: For 1st param expected `str` but got `int`.
-                    dummy_samples = dummy_samples_list[i]
-                    expected_train_inputs = Xs[i].expand(4, *Xs[i].shape)
-                    if use_input_warping:
-                        # train inputs should be warped inputs
-                        expected_train_inputs = m.input_transform(expected_train_inputs)
-                    self.assertTrue(
-                        torch.equal(
-                            m.train_inputs[0],
-                            expected_train_inputs,
+                        self.assertEqual(m.covar_module.__class__, ScaleKernel)
+                        self.assertEqual(
+                            m.covar_module.raw_outputscale_constraint.__class__,
+                            Positive,
                         )
-                    )
-                    self.assertTrue(
-                        torch.equal(
-                            m.train_targets,
-                            Ys[i].view(1, -1).expand(4, Ys[i].numel()),
+                        self.assertEqual(
+                            m.covar_module.base_kernel.__class__,
+                            RBFKernel if gp_kernel == "rbf" else MaternKernel,
                         )
-                    )
-                    expected_noise = (
-                        # pyre-fixme[6]: For 1st param expected `Union[None,
-                        #  List[typing.Any], int, slice, Tensor,
-                        #  typing.Tuple[typing.Any, ...]]` but got `str`.
-                        dummy_samples["noise"].view(m.likelihood.noise.shape)
-                        if inferred_noise
-                        else Yvars[i].view(1, -1).expand(4, Yvars[i].numel())
-                    )
-                    self.assertTrue(
-                        torch.allclose(
-                            m.likelihood.noise.detach(),
-                            expected_noise,
+                        self.assertEqual(m.covar_module.base_kernel.ard_num_dims, 3)
+                        ls_constraint = (
+                            m.covar_module.base_kernel.raw_lengthscale_constraint
                         )
-                    )
-                    self.assertIsInstance(m.likelihood, _GaussianLikelihoodBase)
-                    self.assertTrue(
-                        torch.allclose(
-                            m.covar_module.base_kernel.lengthscale.detach(),
-                            # pyre-fixme[6]: For 1st param expected `Union[None,
-                            #  List[typing.Any], int, slice, Tensor,
-                            #  typing.Tuple[typing.Any, ...]]` but got `str`.
-                            dummy_samples["lengthscale"].view(
-                                m.covar_module.base_kernel.lengthscale.shape
-                            ),
-                            rtol=1e-4,
-                            atol=1e-6,
+                        self.assertEqual(
+                            ls_constraint.__class__,
+                            Positive,
                         )
-                    )
-                    self.assertTrue(
-                        torch.allclose(
-                            m.covar_module.outputscale.detach(),
-                            # pyre-fixme[6]: For 1st param expected `Union[None,
-                            #  List[typing.Any], int, slice, Tensor,
-                            #  typing.Tuple[typing.Any, ...]]` but got `str`.
-                            dummy_samples["outputscale"].view(
-                                m.covar_module.outputscale.shape
-                            ),
+
+                    #  attribute `assertEqual`.
+                    self.assertEqual(_mock_fit_model.call_count, 2)
+                    for i, call in enumerate(_mock_fit_model.call_args_list):
+                        _, ckwargs = call
+                        X = Xs[i]
+                        Y = Ys[i]
+                        Yvar = Yvars[i]
+                        self.assertIs(ckwargs["pyro_model"], single_task_pyro_model)
+
+                        self.assertTrue(torch.equal(ckwargs["X"], X))
+                        self.assertTrue(torch.equal(ckwargs["Y"], Y))
+                        if inferred_noise:
+                            self.assertTrue(torch.isnan(ckwargs["Yvar"]).all())
+                        else:
+                            self.assertTrue(torch.equal(ckwargs["Yvar"], Yvar))
+                        self.assertEqual(ckwargs["num_samples"], 4)
+                        self.assertEqual(ckwargs["warmup_steps"], 512)
+                        self.assertEqual(ckwargs["max_tree_depth"], 1)
+                        self.assertTrue(ckwargs["disable_progbar"])
+                        self.assertFalse(ckwargs["jit_compile"])
+                        self.assertEqual(
+                            ckwargs["use_input_warping"], use_input_warping
                         )
-                    )
-                    self.assertTrue(
-                        torch.allclose(
-                            m.mean_module.constant.detach(),
+                        self.assertEqual(ckwargs["gp_kernel"], gp_kernel)
+                        self.assertTrue(ckwargs["verbose"])
+
+                        # Check attributes
+                        self.assertTrue(torch.equal(model.Xs[i], Xs[i]))
+                        self.assertEqual(model.dtype, Xs[i].dtype)
+                        self.assertEqual(model.device, Xs[i].device)
+                        self.assertIsInstance(model.model, ModelListGP)
+
+                        # Check fitting
+                        # Note each model in the model list is a batched model, where
+                        # the batch dim corresponds to the MCMC samples
+                        model_list = cast(ModelList, model.model).models
+                        # Put model in `eval` mode to transform the train inputs.
+                        m = model_list[i].eval()
+                        # check mcmc samples
+                        # pyre-fixme[6]: For 1st param expected `str` but got `int`.
+                        dummy_samples = dummy_samples_list[i]
+                        expected_train_inputs = Xs[i].expand(4, *Xs[i].shape)
+                        if use_input_warping:
+                            # train inputs should be warped inputs
+                            expected_train_inputs = m.input_transform(
+                                expected_train_inputs
+                            )
+                        self.assertTrue(
+                            torch.equal(
+                                m.train_inputs[0],
+                                expected_train_inputs,
+                            )
+                        )
+                        self.assertTrue(
+                            torch.equal(
+                                m.train_targets,
+                                Ys[i].view(1, -1).expand(4, Ys[i].numel()),
+                            )
+                        )
+                        expected_noise = (
                             # pyre-fixme[6]: For 1st param expected `Union[None,
                             #  List[typing.Any], int, slice, Tensor,
                             #  typing.Tuple[typing.Any, ...]]` but got `str`.
-                            dummy_samples["mean"].view(m.mean_module.constant.shape),
+                            dummy_samples["noise"].view(m.likelihood.noise.shape)
+                            if inferred_noise
+                            else Yvars[i].view(1, -1).expand(4, Yvars[i].numel())
                         )
-                    )
-                    if use_input_warping:
-                        self.assertTrue(hasattr(m, "input_transform"))
-                        self.assertIsInstance(m.input_transform, Warp)
                         self.assertTrue(
-                            torch.equal(
-                                m.input_transform.concentration0,
-                                # pyre-fixme[6]: For 1st param expected `str`
-                                #  but got `int`.
-                                # pyre-fixme[6]: For 1st param expected
-                                #  `Union[None, List[typing.Any], int, slice,
-                                #  Tensor, typing.Tuple[typing.Any, ...]]` but got
-                                #  `str`.
-                                dummy_samples_list[i]["c0"],
+                            torch.allclose(
+                                m.likelihood.noise.detach(),
+                                expected_noise,
                             )
                         )
+                        self.assertIsInstance(m.likelihood, _GaussianLikelihoodBase)
                         self.assertTrue(
-                            torch.equal(
-                                m.input_transform.concentration1,
-                                # pyre-fixme[6]: For 1st param expected `str`
-                                #  but got `int`.
-                                # pyre-fixme[6]: For 1st param expected
-                                #  `Union[None, List[typing.Any], int, slice,
-                                #  Tensor, typing.Tuple[typing.Any, ...]]` but got
-                                #  `str`.
-                                dummy_samples_list[i]["c1"],
+                            torch.allclose(
+                                m.covar_module.base_kernel.lengthscale.detach(),
+                                # pyre-fixme[6]: For 1st param expected `Union[None,
+                                #  List[typing.Any], int, slice, Tensor,
+                                #  typing.Tuple[typing.Any, ...]]` but got `str`.
+                                dummy_samples["lengthscale"].view(
+                                    m.covar_module.base_kernel.lengthscale.shape
+                                ),
+                                rtol=1e-4,
+                                atol=1e-6,
                             )
                         )
-                    else:
-                        self.assertFalse(hasattr(m, "input_transform"))
-            # test that multi-task is not implemented
-            (
-                Xs_mt,
-                Ys_mt,
-                Yvars_mt,
-                bounds_mt,
-                tfs_mt,
-                fns_mt,
-                mns_mt,
-            ) = get_torch_test_data(
-                dtype=dtype, cuda=cuda, constant_noise=True, task_features=[2]
-            )
-            with mock.patch(
-                RUN_INFERENCE_PATH,
-                side_effect=dummy_samples_list,
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertRaises`.
-            ) as _mock_fit_model, self.assertRaises(NotImplementedError):
-                model.fit(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X,
-                            Y=Y,
-                            Yvar=Yvar,
-                            feature_names=fns_mt,
-                            outcome_names=[mn],
-                        )
-                        for X, Y, Yvar, mn in zip(Xs_mt, Ys_mt, Yvars_mt, mns_mt)
-                    ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns_mt,
-                        bounds=bounds_mt,
-                        task_features=tfs_mt,
-                    ),
-                )
-            with mock.patch(
-                RUN_INFERENCE_PATH,
-                side_effect=dummy_samples_list,
-            ) as _mock_fit_model, self.assertRaises(NotImplementedError):
-                model.fit(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
+                        self.assertTrue(
+                            torch.allclose(
+                                m.covar_module.outputscale.detach(),
+                                # pyre-fixme[6]: For 1st param expected `Union[None,
+                                #  List[typing.Any], int, slice, Tensor,
+                                #  typing.Tuple[typing.Any, ...]]` but got `str`.
+                                dummy_samples["outputscale"].view(
+                                    m.covar_module.outputscale.shape
+                                ),
+                            )
                         )
-                        for X, Y, Yvar, mn in zip(
-                            Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
+                        self.assertTrue(
+                            torch.allclose(
+                                m.mean_module.constant.detach(),
+                                # pyre-fixme[6]: For 1st param expected `Union[None,
+                                #  List[typing.Any], int, slice, Tensor,
+                                #  typing.Tuple[typing.Any, ...]]` but got `str`.
+                                dummy_samples["mean"].view(
+                                    m.mean_module.constant.shape
+                                ),
+                            )
                         )
-                    ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns,
-                        bounds=bounds,
-                        fidelity_features=[0],
-                    ),
+                        if use_input_warping:
+                            self.assertTrue(hasattr(m, "input_transform"))
+                            self.assertIsInstance(m.input_transform, Warp)
+                            self.assertTrue(
+                                torch.equal(
+                                    m.input_transform.concentration0,
+                                    # pyre-fixme[6]: For 1st param expected `str`
+                                    #  but got `int`.
+                                    # pyre-fixme[6]: For 1st param expected
+                                    #  `Union[None, List[typing.Any], int, slice,
+                                    #  Tensor, typing.Tuple[typing.Any, ...]]` but got
+                                    #  `str`.
+                                    dummy_samples_list[i]["c0"],
+                                )
+                            )
+                            self.assertTrue(
+                                torch.equal(
+                                    m.input_transform.concentration1,
+                                    # pyre-fixme[6]: For 1st param expected `str`
+                                    #  but got `int`.
+                                    # pyre-fixme[6]: For 1st param expected
+                                    #  `Union[None, List[typing.Any], int, slice,
+                                    #  Tensor, typing.Tuple[typing.Any, ...]]` but got
+                                    #  `str`.
+                                    dummy_samples_list[i]["c1"],
+                                )
+                            )
+                        else:
+                            self.assertFalse(hasattr(m, "input_transform"))
+                # test that multi-task is not implemented
+                (
+                    Xs_mt,
+                    Ys_mt,
+                    Yvars_mt,
+                    bounds_mt,
+                    tfs_mt,
+                    fns_mt,
+                    mns_mt,
+                ) = get_torch_test_data(
+                    dtype=dtype, cuda=cuda, constant_noise=True, task_features=[2]
                 )
-            # fit model with same inputs (otherwise X_observed will be None)
-            model = self.model_cls(
-                use_input_warping=use_input_warping,
-                thinning=1,
-                num_samples=4,
-                disable_progbar=True,
-                max_tree_depth=1,
-                gp_kernel=gp_kernel,
-            )
-            Yvars = Yvars1 + Yvars2
-            dummy_samples_list = _get_dummy_mcmc_samples(
-                num_samples=4, num_outputs=2, **tkwargs
-            )
-            with mock.patch(
-                RUN_INFERENCE_PATH,
-                side_effect=dummy_samples_list,
-            ) as _mock_fit_model:
-                model.fit(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
-                        )
-                        for X, Y, Yvar, mn in zip(Xs1 + Xs2, Ys1 + Ys2, Yvars, mns * 2)
-                    ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns,
-                        bounds=bounds,
-                        task_features=tfs,
-                    ),
+                with mock.patch(
+                    RUN_INFERENCE_PATH,
+                    side_effect=dummy_samples_list,
+                ) as _mock_fit_model, self.assertRaises(NotImplementedError):
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns_mt,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(Xs_mt, Ys_mt, Yvars_mt, mns_mt)
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns_mt,
+                            bounds=bounds_mt,
+                            task_features=tfs_mt,
+                        ),
+                    )
+                with mock.patch(
+                    RUN_INFERENCE_PATH,
+                    side_effect=dummy_samples_list,
+                ) as _mock_fit_model, self.assertRaises(NotImplementedError):
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
+                            )
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns,
+                            bounds=bounds,
+                            fidelity_features=[0],
+                        ),
+                    )
+                # fit model with same inputs (otherwise X_observed will be None)
+                model = self.model_cls(
+                    use_input_warping=use_input_warping,
+                    thinning=1,
+                    num_samples=4,
+                    disable_progbar=True,
+                    max_tree_depth=1,
+                    gp_kernel=gp_kernel,
                 )
-
-            # Check the hyperparameters and shapes
-            models = cast(ModelList, model.model).models
-            self.assertEqual(len(models), 2)
-            m1, m2 = models
-            # Mean
-            self.assertEqual(m1.mean_module.constant.shape, (4,))
-            self.assertFalse(
-                torch.isclose(m1.mean_module.constant, m2.mean_module.constant).any()
-            )
-            # Outputscales
-            self.assertEqual(m1.covar_module.outputscale.shape, (4,))
-            self.assertFalse(
-                torch.isclose(
-                    m1.covar_module.outputscale, m2.covar_module.outputscale
-                ).any()
-            )
-            # Lengthscales
-            self.assertEqual(m1.covar_module.base_kernel.lengthscale.shape, (4, 1, 3))
-            self.assertFalse(
-                torch.isclose(
-                    m1.covar_module.base_kernel.lengthscale,
-                    m2.covar_module.base_kernel.lengthscale,
-                ).any()
-            )
-
-            # Check infeasible cost can be computed on the model
-            device = torch.device("cuda") if cuda else torch.device("cpu")
-            objective_weights = torch.tensor([1.0, 0.0], dtype=dtype, device=device)
-            objective_transform = get_objective_weights_transform(objective_weights)
-            infeasible_cost = (
-                get_infeasible_cost(
-                    X=Xs1[0],
-                    model=model.model,
-                    objective=objective_transform,
+                Yvars = Yvars1 + Yvars2
+                dummy_samples_list = _get_dummy_mcmc_samples(
+                    num_samples=4, num_outputs=2, **tkwargs
                 )
-                .detach()
-                .clone()
-            )
-            posterior = cast(GPyTorchPosterior, model.model.posterior(Xs1[0]))
-            expected_infeasible_cost = -1 * torch.min(
-                # pyre-fixme[20]: Argument `1` expected.
-                objective_transform(
-                    posterior.mean - 6 * posterior.variance.sqrt()
-                ).min(),
-                torch.tensor(0.0, dtype=dtype, device=device),
-            )
-            self.assertTrue(
-                torch.abs(infeasible_cost - expected_infeasible_cost) < 1e-5
-            )
-
-            # Check prediction
-            X = torch.tensor([[6.0, 7.0, 8.0]], **tkwargs)
-            f_mean, f_cov = model.predict(X)
-            self.assertTrue(f_mean.shape == torch.Size([1, 2]))
-            self.assertTrue(f_cov.shape == torch.Size([1, 2, 2]))
+                with mock.patch(
+                    RUN_INFERENCE_PATH,
+                    side_effect=dummy_samples_list,
+                ) as _mock_fit_model:
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars, mns * 2
+                            )
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns,
+                            bounds=bounds,
+                            task_features=tfs,
+                        ),
+                    )
 
-            # Check generation
-            objective_weights = torch.tensor(
-                [1.0, 0.0]
-                if self.model_cls is FullyBayesianBotorchModel
-                else [1.0, 1.0],
-                **tkwargs,
-            )
-            outcome_constraints = (
-                torch.tensor([[0.0, 1.0]], **tkwargs),
-                torch.tensor([[5.0]], **tkwargs),
-            )
-            linear_constraints = (
-                torch.tensor([[0.0, 1.0, 1.0]]),
-                torch.tensor([[100.0]]),
-            )
-            fixed_features = None
-            pending_observations = [
-                torch.tensor([[1.0, 3.0, 4.0]], **tkwargs),
-                torch.tensor([[2.0, 6.0, 8.0]], **tkwargs),
-            ]
-            n = 3
+                # Check the hyperparameters and shapes
+                models = cast(ModelList, model.model).models
+                self.assertEqual(len(models), 2)
+                m1, m2 = models
+                # Mean
+                self.assertEqual(m1.mean_module.constant.shape, (4,))
+                self.assertFalse(
+                    torch.isclose(
+                        m1.mean_module.constant, m2.mean_module.constant
+                    ).any()
+                )
+                # Outputscales
+                self.assertEqual(m1.covar_module.outputscale.shape, (4,))
+                self.assertFalse(
+                    torch.isclose(
+                        m1.covar_module.outputscale, m2.covar_module.outputscale
+                    ).any()
+                )
+                # Lengthscales
+                self.assertEqual(
+                    m1.covar_module.base_kernel.lengthscale.shape, (4, 1, 3)
+                )
+                self.assertFalse(
+                    torch.isclose(
+                        m1.covar_module.base_kernel.lengthscale,
+                        m2.covar_module.base_kernel.lengthscale,
+                    ).any()
+                )
 
-            X_dummy = torch.tensor([[[1.0, 2.0, 3.0]]], **tkwargs)
-            acqfv_dummy = torch.tensor([[[1.0, 2.0, 3.0]]], **tkwargs)
-            model_gen_options = {
-                Keys.OPTIMIZER_KWARGS: {"options": {"maxiter": 1}},
-                Keys.ACQF_KWARGS: {"mc_samples": 3},
-            }
-            search_space_digest = SearchSpaceDigest(
-                feature_names=fns,
-                bounds=bounds,
-            )
-            torch_opt_config = TorchOptConfig(
-                objective_weights=objective_weights,
-                objective_thresholds=torch.zeros(2, **tkwargs)
-                if self.model_cls is FullyBayesianMOOBotorchModel
-                else None,
-                outcome_constraints=outcome_constraints,
-                linear_constraints=linear_constraints,
-                fixed_features=fixed_features,
-                pending_observations=pending_observations,
-                # pyre-fixme[6]: For 7th param expected `Dict[str, Union[None,
-                #  Dict[str, typing.Any], OptimizationConfig, AcquisitionFunction,
-                #  float, int, str]]` but got `Dict[Keys, Dict[str, int]]`.
-                model_gen_options=model_gen_options,
-                rounding_func=dummy_func,
-                is_moo=self.model_cls is FullyBayesianMOOBotorchModel,
-            )
-            # test sequential optimize with constraints
-            with mock.patch(
-                "ax.models.torch.botorch_defaults.optimize_acqf",
-                return_value=(X_dummy, acqfv_dummy),
-            ) as _:
-                # Xgen, wgen, gen_metadata, cand_metadata = model.gen(
-                gen_results = model.gen(
-                    n=n,
-                    search_space_digest=search_space_digest,
-                    torch_opt_config=torch_opt_config,
+                # Check infeasible cost can be computed on the model
+                device = torch.device("cuda") if cuda else torch.device("cpu")
+                objective_weights = torch.tensor([1.0, 0.0], dtype=dtype, device=device)
+                objective_transform = get_objective_weights_transform(objective_weights)
+                infeasible_cost = (
+                    get_infeasible_cost(
+                        X=Xs1[0],
+                        model=model.model,
+                        objective=objective_transform,
+                    )
+                    .detach()
+                    .clone()
+                )
+                posterior = cast(GPyTorchPosterior, model.model.posterior(Xs1[0]))
+                expected_infeasible_cost = -1 * torch.min(
+                    # pyre-fixme[20]: Argument `1` expected.
+                    objective_transform(
+                        posterior.mean - 6 * posterior.variance.sqrt()
+                    ).min(),
+                    torch.tensor(0.0, dtype=dtype, device=device),
                 )
-                # note: gen() always returns CPU tensors
-                self.assertTrue(torch.equal(gen_results.points, X_dummy.cpu()))
                 self.assertTrue(
-                    torch.equal(gen_results.weights, torch.ones(n, dtype=dtype))
+                    torch.abs(infeasible_cost - expected_infeasible_cost) < 1e-5
                 )
 
-            # actually test optimization for 1 step without constraints
-            with mock.patch(
-                "ax.models.torch.botorch_defaults.optimize_acqf",
-                wraps=optimize_acqf,
-                return_value=(X_dummy, acqfv_dummy),
-            ) as _:
-                gen_results = model.gen(
-                    n=n,
-                    search_space_digest=search_space_digest,
-                    torch_opt_config=dataclasses.replace(
-                        torch_opt_config, linear_constraints=None
+                # Check prediction
+                X = torch.tensor([[6.0, 7.0, 8.0]], **tkwargs)
+                f_mean, f_cov = model.predict(X)
+                self.assertTrue(f_mean.shape == torch.Size([1, 2]))
+                self.assertTrue(f_cov.shape == torch.Size([1, 2, 2]))
+
+                # Check generation
+                objective_weights = torch.tensor(
+                    (
+                        [1.0, 0.0]
+                        if self.model_cls is FullyBayesianBotorchModel
+                        else [1.0, 1.0]
                     ),
+                    **tkwargs,
                 )
-                # note: gen() always returns CPU tensors
-                self.assertTrue(torch.equal(gen_results.points, X_dummy.cpu()))
-                self.assertTrue(
-                    torch.equal(gen_results.weights, torch.ones(n, dtype=dtype))
+                outcome_constraints = (
+                    torch.tensor([[0.0, 1.0]], **tkwargs),
+                    torch.tensor([[5.0]], **tkwargs),
                 )
-
-            # Check best point selection
-            if self.model_cls is FullyBayesianMOOBotorchModel:
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertRaisesRegex`.
-                with self.assertRaisesRegex(NotImplementedError, "Best observed"):
-                    model.best_point(
+                linear_constraints = (
+                    torch.tensor([[0.0, 1.0, 1.0]]),
+                    torch.tensor([[100.0]]),
+                )
+                fixed_features = None
+                pending_observations = [
+                    torch.tensor([[1.0, 3.0, 4.0]], **tkwargs),
+                    torch.tensor([[2.0, 6.0, 8.0]], **tkwargs),
+                ]
+                n = 3
+
+                X_dummy = torch.tensor([[[1.0, 2.0, 3.0]]], **tkwargs)
+                acqfv_dummy = torch.tensor([[[1.0, 2.0, 3.0]]], **tkwargs)
+                model_gen_options = {
+                    Keys.OPTIMIZER_KWARGS: {"options": {"maxiter": 1}},
+                    Keys.ACQF_KWARGS: {"mc_samples": 3},
+                }
+                search_space_digest = SearchSpaceDigest(
+                    feature_names=fns,
+                    bounds=bounds,
+                )
+                torch_opt_config = TorchOptConfig(
+                    objective_weights=objective_weights,
+                    objective_thresholds=(
+                        torch.zeros(2, **tkwargs)
+                        if self.model_cls is FullyBayesianMOOBotorchModel
+                        else None
+                    ),
+                    outcome_constraints=outcome_constraints,
+                    linear_constraints=linear_constraints,
+                    fixed_features=fixed_features,
+                    pending_observations=pending_observations,
+                    # pyre-fixme[6]: For 7th param expected `Dict[str, Union[None,
+                    #  Dict[str, typing.Any], OptimizationConfig, AcquisitionFunction,
+                    #  float, int, str]]` but got `Dict[Keys, Dict[str, int]]`.
+                    model_gen_options=model_gen_options,
+                    rounding_func=dummy_func,
+                    is_moo=self.model_cls is FullyBayesianMOOBotorchModel,
+                )
+                # test sequential optimize with constraints
+                with mock.patch(
+                    "ax.models.torch.botorch_defaults.optimize_acqf",
+                    return_value=(X_dummy, acqfv_dummy),
+                ) as _:
+                    # Xgen, wgen, gen_metadata, cand_metadata = model.gen(
+                    gen_results = model.gen(
+                        n=n,
                         search_space_digest=search_space_digest,
                         torch_opt_config=torch_opt_config,
                     )
-            else:
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertIsNotNone`.
-                self.assertIsNotNone(
-                    model.best_point(
-                        search_space_digest=search_space_digest,
-                        torch_opt_config=torch_opt_config,
+                    # note: gen() always returns CPU tensors
+                    self.assertTrue(torch.equal(gen_results.points, X_dummy.cpu()))
+                    self.assertTrue(
+                        torch.equal(gen_results.weights, torch.ones(n, dtype=dtype))
                     )
-                )
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertIsNone`.
-                self.assertIsNone(
-                    model.best_point(
+
+                # actually test optimization for 1 step without constraints
+                with mock.patch(
+                    "ax.models.torch.botorch_defaults.optimize_acqf",
+                    wraps=optimize_acqf,
+                    return_value=(X_dummy, acqfv_dummy),
+                ) as _:
+                    gen_results = model.gen(
+                        n=n,
                         search_space_digest=search_space_digest,
                         torch_opt_config=dataclasses.replace(
-                            torch_opt_config, fixed_features={0: 100.0}
+                            torch_opt_config, linear_constraints=None
                         ),
                     )
-                )
+                    # note: gen() always returns CPU tensors
+                    self.assertTrue(torch.equal(gen_results.points, X_dummy.cpu()))
+                    self.assertTrue(
+                        torch.equal(gen_results.weights, torch.ones(n, dtype=dtype))
+                    )
 
-            # Test cross-validation
-            mean, variance = model.cross_validate(
-                datasets=[
-                    SupervisedDataset(
-                        X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
+                # Check best point selection
+                if self.model_cls is FullyBayesianMOOBotorchModel:
+                    with self.assertRaisesRegex(NotImplementedError, "Best observed"):
+                        model.best_point(
+                            search_space_digest=search_space_digest,
+                            torch_opt_config=torch_opt_config,
+                        )
+                else:
+                    self.assertIsNotNone(
+                        model.best_point(
+                            search_space_digest=search_space_digest,
+                            torch_opt_config=torch_opt_config,
+                        )
+                    )
+                    self.assertIsNone(
+                        model.best_point(
+                            search_space_digest=search_space_digest,
+                            torch_opt_config=dataclasses.replace(
+                                torch_opt_config, fixed_features={0: 100.0}
+                            ),
+                        )
                     )
-                    for X, Y, Yvar, mn in zip(Xs1 + Xs2, Ys1 + Ys2, Yvars, mns * 2)
-                ],
-                X_test=torch.tensor(
-                    [[1.2, 3.2, 4.2], [2.4, 5.2, 3.2]], dtype=dtype, device=device
-                ),
-            )
-            self.assertTrue(mean.shape == torch.Size([2, 2]))
-            self.assertTrue(variance.shape == torch.Size([2, 2, 2]))
 
-            # Test cross-validation with refit_on_cv
-            model.refit_on_cv = True
-            with mock.patch(
-                RUN_INFERENCE_PATH,
-                side_effect=dummy_samples_list,
-            ) as _mock_fit_model:
+                # Test cross-validation
                 mean, variance = model.cross_validate(
                     datasets=[
                         SupervisedDataset(
                             X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
                         )
                         for X, Y, Yvar, mn in zip(Xs1 + Xs2, Ys1 + Ys2, Yvars, mns * 2)
                     ],
                     X_test=torch.tensor(
-                        [[1.2, 3.2, 4.2], [2.4, 5.2, 3.2]],
-                        dtype=dtype,
-                        device=device,
+                        [[1.2, 3.2, 4.2], [2.4, 5.2, 3.2]], dtype=dtype, device=device
                     ),
                 )
                 self.assertTrue(mean.shape == torch.Size([2, 2]))
                 self.assertTrue(variance.shape == torch.Size([2, 2, 2]))
 
-            # Test feature_importances
-            importances = model.feature_importances()
-            self.assertEqual(importances.shape, torch.Size([2, 1, 3]))
-
-            # test unfit model CV and feature_importances
-            unfit_model = self.model_cls()
-            with self.assertRaises(RuntimeError):
-                unfit_model.cross_validate(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
-                        )
-                        for X, Y, Yvar, mn in zip(
-                            Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
-                        )
-                    ],
-                    X_test=Xs1[0],
-                )
-            with self.assertRaises(RuntimeError):
-                unfit_model.feature_importances()
+                # Test cross-validation with refit_on_cv
+                model.refit_on_cv = True
+                with mock.patch(
+                    RUN_INFERENCE_PATH,
+                    side_effect=dummy_samples_list,
+                ) as _mock_fit_model:
+                    mean, variance = model.cross_validate(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars, mns * 2
+                            )
+                        ],
+                        X_test=torch.tensor(
+                            [[1.2, 3.2, 4.2], [2.4, 5.2, 3.2]],
+                            dtype=dtype,
+                            device=device,
+                        ),
+                    )
+                    self.assertTrue(mean.shape == torch.Size([2, 2]))
+                    self.assertTrue(variance.shape == torch.Size([2, 2, 2]))
+
+                # Test feature_importances
+                importances = model.feature_importances()
+                self.assertEqual(importances.shape, torch.Size([2, 1, 3]))
+
+                # test unfit model CV and feature_importances
+                unfit_model = self.model_cls()
+                with self.assertRaises(RuntimeError):
+                    unfit_model.cross_validate(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
+                            )
+                        ],
+                        X_test=Xs1[0],
+                    )
+                with self.assertRaises(RuntimeError):
+                    unfit_model.feature_importances()
+
+        def test_saasbo_sample(self) -> None:
+            for use_input_warping, gp_kernel in product(
+                [False, True], ["rbf", "matern"]
+            ):
+                with with_rng_seed(0):
+                    X = torch.randn(3, 2)
+                    Y = torch.randn(3, 1)
+                    Yvar = torch.randn(3, 1)
+                    kernel = NUTS(single_task_pyro_model, max_tree_depth=1)
+                    mcmc = MCMC(kernel, warmup_steps=0, num_samples=1)
+                    mcmc.run(
+                        X,
+                        Y,
+                        Yvar,
+                        use_input_warping=use_input_warping,
+                        gp_kernel=gp_kernel,
+                    )
+                    samples = mcmc.get_samples()
+                    self.assertTrue("kernel_tausq" in samples)
+                    self.assertTrue("_kernel_inv_length_sq" in samples)
+                    self.assertTrue("lengthscale" not in samples)
+                    if use_input_warping:
+                        self.assertIn("c0", samples)
+                        self.assertIn("c1", samples)
+                    else:
+                        self.assertNotIn("c0", samples)
+                        self.assertNotIn("c1", samples)
 
-    def test_saasbo_sample(self) -> None:
-        for use_input_warping, gp_kernel in product([False, True], ["rbf", "matern"]):
-            with torch.random.fork_rng():
-                torch.manual_seed(0)
-                X = torch.randn(3, 2)
-                Y = torch.randn(3, 1)
-                Yvar = torch.randn(3, 1)
-                kernel = NUTS(single_task_pyro_model, max_tree_depth=1)
+        def test_gp_kernels(self) -> None:
+            set_rng_seed(0)
+            X = torch.randn(3, 2)
+            Y = torch.randn(3, 1)
+            Yvar = torch.randn(3, 1)
+            kernel = NUTS(single_task_pyro_model, max_tree_depth=1)
+            with self.assertRaises(ValueError):
                 mcmc = MCMC(kernel, warmup_steps=0, num_samples=1)
                 mcmc.run(
                     X,
                     Y,
                     Yvar,
-                    use_input_warping=use_input_warping,
-                    gp_kernel=gp_kernel,
+                    gp_kernel="some_kernel_we_dont_support",
                 )
-                samples = mcmc.get_samples()
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertTrue`.
-                self.assertTrue("kernel_tausq" in samples)
-                self.assertTrue("_kernel_inv_length_sq" in samples)
-                self.assertTrue("lengthscale" not in samples)
-                if use_input_warping:
-                    # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                    #  attribute `assertIn`.
-                    self.assertIn("c0", samples)
-                    self.assertIn("c1", samples)
-                else:
-                    # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                    #  attribute `assertNotIn`.
-                    self.assertNotIn("c0", samples)
-                    self.assertNotIn("c1", samples)
-
-    def test_gp_kernels(self) -> None:
-        torch.manual_seed(0)
-        X = torch.randn(3, 2)
-        Y = torch.randn(3, 1)
-        Yvar = torch.randn(3, 1)
-        kernel = NUTS(single_task_pyro_model, max_tree_depth=1)
-        # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no attribute
-        #  `assertRaises`.
-        with self.assertRaises(ValueError):
-            mcmc = MCMC(kernel, warmup_steps=0, num_samples=1)
-            mcmc.run(
-                X,
-                Y,
-                Yvar,
-                gp_kernel="some_kernel_we_dont_support",
-            )
 
-    def test_FullyBayesianBotorchModel_cuda(self) -> None:
-        if torch.cuda.is_available():
-            self.test_FullyBayesianBotorchModel(cuda=True)
-
-    def test_FullyBayesianBotorchModel_double(self) -> None:
-        self.test_FullyBayesianBotorchModel(dtype=torch.double)
-
-    def test_FullyBayesianBotorchModel_double_cuda(self) -> None:
-        if torch.cuda.is_available():
-            self.test_FullyBayesianBotorchModel(dtype=torch.double, cuda=True)
-
-    def test_FullyBayesianBotorchModelConstraints(self) -> None:
-        Xs1, Ys1, Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
-            dtype=torch.float, cuda=False, constant_noise=True
-        )
-        Xs2, Ys2, Yvars2, _, _, _, _ = get_torch_test_data(
-            dtype=torch.float, cuda=False, constant_noise=True
-        )
-        # make infeasible
-        Xs2[0] = -1 * Xs2[0]
-        objective_weights = torch.tensor(
-            [-1.0, 1.0], dtype=torch.float, device=torch.device("cpu")
-        )
-        n = 3
-        model = self.model_cls(
-            num_samples=4,
-            thinning=1,
-            disable_progbar=True,
-            max_tree_depth=1,
-        )
-        dummy_samples = _get_dummy_mcmc_samples(
-            num_samples=4, num_outputs=2, dtype=torch.float, device=Xs1[0].device
-        )
-        search_space_digest = SearchSpaceDigest(
-            feature_names=fns,
-            bounds=bounds,
-            task_features=tfs,
-        )
-        with mock.patch(
-            RUN_INFERENCE_PATH, side_effect=dummy_samples
-        ) as _mock_fit_model:
-            model.fit(
-                datasets=[
-                    SupervisedDataset(
-                        X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
-                    )
-                    for X, Y, Yvar, mn in zip(
-                        Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
-                    )
-                ],
-                search_space_digest=search_space_digest,
+        def test_FullyBayesianBotorchModel_cuda(self) -> None:
+            if torch.cuda.is_available():
+                self.test_FullyBayesianBotorchModel(cuda=True)
+
+        def test_FullyBayesianBotorchModel_double(self) -> None:
+            self.test_FullyBayesianBotorchModel(dtype=torch.double)
+
+        def test_FullyBayesianBotorchModel_double_cuda(self) -> None:
+            if torch.cuda.is_available():
+                self.test_FullyBayesianBotorchModel(dtype=torch.double, cuda=True)
+
+        def test_FullyBayesianBotorchModelConstraints(self) -> None:
+            Xs1, Ys1, Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
+                dtype=torch.float, cuda=False, constant_noise=True
             )
-            # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-            #  attribute `assertEqual`.
-            self.assertEqual(_mock_fit_model.call_count, 2)
-
-        # because there are no feasible points:
-        # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no attribute
-        #  `assertRaises`.
-        with self.assertRaises(ValueError):
-            model.gen(
-                n,
-                search_space_digest=search_space_digest,
-                torch_opt_config=TorchOptConfig(objective_weights=objective_weights),
+            Xs2, Ys2, Yvars2, _, _, _, _ = get_torch_test_data(
+                dtype=torch.float, cuda=False, constant_noise=True
             )
-
-    # pyre-fixme[3]: Return type must be annotated.
-    # pyre-fixme[2]: Parameter must be annotated.
-    def test_FullyBayesianBotorchModelPyro(self, dtype=torch.double, cuda=False):
-        Xs1, Ys1, raw_Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
-            dtype=dtype, cuda=cuda, constant_noise=True
-        )
-        Xs2, Ys2, raw_Yvars2, _, _, _, _ = get_torch_test_data(
-            dtype=dtype, cuda=cuda, constant_noise=True
-        )
-        options = [(False, True, "rbf"), (True, False, "matern")]
-        for inferred_noise, use_input_warping, gp_kernel in options:
+            # make infeasible
+            Xs2[0] = -1 * Xs2[0]
+            objective_weights = torch.tensor(
+                [-1.0, 1.0], dtype=torch.float, device=torch.device("cpu")
+            )
+            n = 3
             model = self.model_cls(
                 num_samples=4,
-                warmup_steps=0,
                 thinning=1,
-                use_input_warping=use_input_warping,
                 disable_progbar=True,
                 max_tree_depth=1,
-                gp_kernel=gp_kernel,
-                verbose=True,
             )
-            if inferred_noise:
-                Yvars1 = [torch.full_like(raw_Yvars1[0], float("nan"))]
-                Yvars2 = [torch.full_like(raw_Yvars2[0], float("nan"))]
-            else:
-                Yvars1 = raw_Yvars1
-                Yvars2 = raw_Yvars2
-
             dummy_samples = _get_dummy_mcmc_samples(
-                num_samples=4,
-                num_outputs=2,
-                dtype=dtype,
-                device=Xs1[0].device,
+                num_samples=4, num_outputs=2, dtype=torch.float, device=Xs1[0].device
             )
-            with ExitStack() as es:
-                _mock_fit_model = es.enter_context(
-                    mock.patch(RUN_INFERENCE_PATH, side_effect=dummy_samples)
-                )
+            search_space_digest = SearchSpaceDigest(
+                feature_names=fns,
+                bounds=bounds,
+                task_features=tfs,
+            )
+            with mock.patch(
+                RUN_INFERENCE_PATH, side_effect=dummy_samples
+            ) as _mock_fit_model:
                 model.fit(
                     datasets=[
                         SupervisedDataset(
                             X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
                         )
                         for X, Y, Yvar, mn in zip(
                             Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
                         )
                     ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns,
-                        bounds=bounds,
-                        task_features=tfs,
-                    ),
+                    search_space_digest=search_space_digest,
                 )
-                # check run_inference arguments
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertEqual`.
                 self.assertEqual(_mock_fit_model.call_count, 2)
-                _, ckwargs = _mock_fit_model.call_args
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertIs`.
-                self.assertIs(ckwargs["pyro_model"], single_task_pyro_model)
-                self.assertFalse(ckwargs["jit_compile"])  # pyre-fixme[16]
-                # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest` has no
-                #  attribute `assertTrue`.
-                self.assertTrue(torch.equal(ckwargs["X"], Xs1[0]))
-                self.assertTrue(torch.equal(ckwargs["Y"], Ys1[0]))
-                if inferred_noise:
-                    self.assertTrue(torch.isnan(ckwargs["Yvar"]).all())
-                else:
-                    self.assertTrue(torch.equal(ckwargs["Yvar"], Yvars1[0]))
-                self.assertEqual(ckwargs["num_samples"], 4)
-                self.assertEqual(ckwargs["warmup_steps"], 0)
-                self.assertEqual(ckwargs["max_tree_depth"], 1)
-                self.assertTrue(ckwargs["disable_progbar"])
-                self.assertFalse(ckwargs["jit_compile"])
-                self.assertEqual(ckwargs["use_input_warping"], use_input_warping)
-                self.assertEqual(ckwargs["gp_kernel"], gp_kernel)
-                self.assertTrue(ckwargs["verbose"])
-
-            with ExitStack() as es:
-                _mock_mcmc = es.enter_context(mock.patch(MCMC_PATH))
-                _mock_mcmc.return_value.get_samples.side_effect = dummy_samples
-                _mock_nuts = es.enter_context(mock.patch(NUTS_PATH))
-                model.fit(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
-                        )
-                        for X, Y, Yvar, mn in zip(
-                            Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
-                        )
-                    ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns,
-                        bounds=bounds,
-                        task_features=tfs,
+
+            # because there are no feasible points:
+            with self.assertRaises(ValueError):
+                model.gen(
+                    n,
+                    search_space_digest=search_space_digest,
+                    torch_opt_config=TorchOptConfig(
+                        objective_weights=objective_weights
                     ),
                 )
-                # check MCMC.__init__ arguments
-                self.assertEqual(_mock_mcmc.call_count, 2)
-                _, ckwargs = _mock_mcmc.call_args
-                self.assertEqual(ckwargs["num_samples"], 4)
-                self.assertEqual(ckwargs["warmup_steps"], 0)
-                self.assertTrue(ckwargs["disable_progbar"])
-                # check NUTS.__init__ arguments
-                _mock_nuts.assert_called_with(
-                    single_task_pyro_model,
-                    jit_compile=False,
-                    full_mass=True,
-                    ignore_jit_warnings=True,
+
+        # pyre-fixme[3]: Return type must be annotated.
+        # pyre-fixme[2]: Parameter must be annotated.
+        def test_FullyBayesianBotorchModelPyro(self, dtype=torch.double, cuda=False):
+            Xs1, Ys1, raw_Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
+                dtype=dtype, cuda=cuda, constant_noise=True
+            )
+            Xs2, Ys2, raw_Yvars2, _, _, _, _ = get_torch_test_data(
+                dtype=dtype, cuda=cuda, constant_noise=True
+            )
+            options = [(False, True, "rbf"), (True, False, "matern")]
+            for inferred_noise, use_input_warping, gp_kernel in options:
+                model = self.model_cls(
+                    num_samples=4,
+                    warmup_steps=0,
+                    thinning=1,
+                    use_input_warping=use_input_warping,
+                    disable_progbar=True,
                     max_tree_depth=1,
+                    gp_kernel=gp_kernel,
+                    verbose=True,
                 )
-            # now actually run pyro
-            if not use_input_warping:
-                # input warping is quite slow, so we omit it for
-                # testing purposes
-                model.fit(
-                    datasets=[
-                        SupervisedDataset(
-                            X=X, Y=Y, Yvar=Yvar, feature_names=fns, outcome_names=[mn]
-                        )
-                        for X, Y, Yvar, mn in zip(
-                            Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
-                        )
-                    ],
-                    search_space_digest=SearchSpaceDigest(
-                        feature_names=fns,
-                        bounds=bounds,
-                        task_features=tfs,
-                    ),
+                if inferred_noise:
+                    Yvars1 = [torch.full_like(raw_Yvars1[0], float("nan"))]
+                    Yvars2 = [torch.full_like(raw_Yvars2[0], float("nan"))]
+                else:
+                    Yvars1 = raw_Yvars1
+                    Yvars2 = raw_Yvars2
+
+                dummy_samples = _get_dummy_mcmc_samples(
+                    num_samples=4,
+                    num_outputs=2,
+                    dtype=dtype,
+                    device=Xs1[0].device,
                 )
+                with ExitStack() as es:
+                    _mock_fit_model = es.enter_context(
+                        mock.patch(RUN_INFERENCE_PATH, side_effect=dummy_samples)
+                    )
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
+                            )
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns,
+                            bounds=bounds,
+                            task_features=tfs,
+                        ),
+                    )
+                    # check run_inference arguments
+                    self.assertEqual(_mock_fit_model.call_count, 2)
+                    _, ckwargs = _mock_fit_model.call_args
+                    self.assertIs(ckwargs["pyro_model"], single_task_pyro_model)
+                    self.assertFalse(ckwargs["jit_compile"])
+                    self.assertTrue(torch.equal(ckwargs["X"], Xs1[0]))
+                    self.assertTrue(torch.equal(ckwargs["Y"], Ys1[0]))
+                    if inferred_noise:
+                        self.assertTrue(torch.isnan(ckwargs["Yvar"]).all())
+                    else:
+                        self.assertTrue(torch.equal(ckwargs["Yvar"], Yvars1[0]))
+                    self.assertEqual(ckwargs["num_samples"], 4)
+                    self.assertEqual(ckwargs["warmup_steps"], 0)
+                    self.assertEqual(ckwargs["max_tree_depth"], 1)
+                    self.assertTrue(ckwargs["disable_progbar"])
+                    self.assertFalse(ckwargs["jit_compile"])
+                    self.assertEqual(ckwargs["use_input_warping"], use_input_warping)
+                    self.assertEqual(ckwargs["gp_kernel"], gp_kernel)
+                    self.assertTrue(ckwargs["verbose"])
 
-                for m, X, Y, Yvar in zip(
-                    cast(ModelList, model.model).models,
-                    Xs1 + Xs2,
-                    Ys1 + Ys2,
-                    Yvars1 + Yvars2,
-                ):
-                    self.assertTrue(
-                        torch.equal(
-                            m.train_inputs[0],
-                            X.expand(4, *X.shape),
-                        )
+                with ExitStack() as es:
+                    _mock_mcmc = es.enter_context(mock.patch(MCMC_PATH))
+                    _mock_mcmc.return_value.get_samples.side_effect = dummy_samples
+                    _mock_nuts = es.enter_context(mock.patch(NUTS_PATH))
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
+                            )
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns,
+                            bounds=bounds,
+                            task_features=tfs,
+                        ),
                     )
-                    self.assertTrue(
-                        torch.equal(
-                            m.train_targets,
-                            Y.view(1, -1).expand(4, Y.numel()),
-                        )
+                    # check MCMC.__init__ arguments
+                    self.assertEqual(_mock_mcmc.call_count, 2)
+                    _, ckwargs = _mock_mcmc.call_args
+                    self.assertEqual(ckwargs["num_samples"], 4)
+                    self.assertEqual(ckwargs["warmup_steps"], 0)
+                    self.assertTrue(ckwargs["disable_progbar"])
+                    # check NUTS.__init__ arguments
+                    _mock_nuts.assert_called_with(
+                        single_task_pyro_model,
+                        jit_compile=False,
+                        full_mass=True,
+                        ignore_jit_warnings=True,
+                        max_tree_depth=1,
+                    )
+                # now actually run pyro
+                if not use_input_warping:
+                    # input warping is quite slow, so we omit it for
+                    # testing purposes
+                    model.fit(
+                        datasets=[
+                            SupervisedDataset(
+                                X=X,
+                                Y=Y,
+                                Yvar=Yvar,
+                                feature_names=fns,
+                                outcome_names=[mn],
+                            )
+                            for X, Y, Yvar, mn in zip(
+                                Xs1 + Xs2, Ys1 + Ys2, Yvars1 + Yvars2, mns * 2
+                            )
+                        ],
+                        search_space_digest=SearchSpaceDigest(
+                            feature_names=fns,
+                            bounds=bounds,
+                            task_features=tfs,
+                        ),
                     )
-                    # check shapes of sampled parameters
-                    if not inferred_noise:
+
+                    for m, X, Y, Yvar in zip(
+                        cast(ModelList, model.model).models,
+                        Xs1 + Xs2,
+                        Ys1 + Ys2,
+                        Yvars1 + Yvars2,
+                    ):
                         self.assertTrue(
-                            torch.allclose(
-                                m.likelihood.noise.detach(),
-                                Yvar.view(1, -1).expand(4, Yvar.numel()),
+                            torch.equal(
+                                m.train_inputs[0],
+                                X.expand(4, *X.shape),
                             )
                         )
-                    else:
-                        self.assertEqual(m.likelihood.noise.shape, torch.Size([4, 1]))
+                        self.assertTrue(
+                            torch.equal(
+                                m.train_targets,
+                                Y.view(1, -1).expand(4, Y.numel()),
+                            )
+                        )
+                        # check shapes of sampled parameters
+                        if not inferred_noise:
+                            self.assertTrue(
+                                torch.allclose(
+                                    m.likelihood.noise.detach(),
+                                    Yvar.view(1, -1).expand(4, Yvar.numel()),
+                                )
+                            )
+                        else:
+                            self.assertEqual(
+                                m.likelihood.noise.shape, torch.Size([4, 1])
+                            )
 
-                    self.assertEqual(
-                        m.covar_module.base_kernel.lengthscale.shape,
-                        torch.Size([4, 1, X.shape[-1]]),
-                    )
-                    self.assertEqual(m.covar_module.outputscale.shape, torch.Size([4]))
-                    self.assertEqual(
-                        m.mean_module.constant.shape,
-                        torch.Size([4]),
-                    )
-                    if use_input_warping:
-                        self.assertTrue(hasattr(m, "input_transform"))
-                        # pyre-fixme[16]: `BaseFullyBayesianBotorchModelTest`
-                        #  has no attribute `assertIsInstance`.
-                        self.assertIsInstance(m.input_transform, Warp)
                         self.assertEqual(
-                            m.input_transform.concentration0.shape,
-                            torch.Size([4, 1, 3]),
+                            m.covar_module.base_kernel.lengthscale.shape,
+                            torch.Size([4, 1, X.shape[-1]]),
                         )
                         self.assertEqual(
-                            m.input_transform.concentration1.shape,
-                            torch.Size([4, 1, 3]),
+                            m.covar_module.outputscale.shape, torch.Size([4])
                         )
-                    else:
-                        self.assertFalse(hasattr(m, "input_transform"))
+                        self.assertEqual(
+                            m.mean_module.constant.shape,
+                            torch.Size([4]),
+                        )
+                        if use_input_warping:
+                            self.assertTrue(hasattr(m, "input_transform"))
+                            self.assertIsInstance(m.input_transform, Warp)
+                            self.assertEqual(
+                                m.input_transform.concentration0.shape,
+                                torch.Size([4, 1, 3]),
+                            )
+                            self.assertEqual(
+                                m.input_transform.concentration1.shape,
+                                torch.Size([4, 1, 3]),
+                            )
+                        else:
+                            self.assertFalse(hasattr(m, "input_transform"))
 
-    def test_FullyBayesianBotorchModelPyro_float(self) -> None:
-        self.test_FullyBayesianBotorchModelPyro(dtype=torch.float, cuda=False)
+        def test_FullyBayesianBotorchModelPyro_float(self) -> None:
+            self.test_FullyBayesianBotorchModelPyro(dtype=torch.float, cuda=False)
 
-    def test_FullyBayesianBotorchModelPyro_cuda_double(self) -> None:
-        if torch.cuda.is_available():
-            self.test_FullyBayesianBotorchModelPyro(dtype=torch.double, cuda=True)
+        def test_FullyBayesianBotorchModelPyro_cuda_double(self) -> None:
+            if torch.cuda.is_available():
+                self.test_FullyBayesianBotorchModelPyro(dtype=torch.double, cuda=True)
 
-    def test_FullyBayesianBotorchModelPyro_cuda_float(self) -> None:
-        if torch.cuda.is_available():
-            self.test_FullyBayesianBotorchModelPyro(dtype=torch.float, cuda=True)
+        def test_FullyBayesianBotorchModelPyro_cuda_float(self) -> None:
+            if torch.cuda.is_available():
+                self.test_FullyBayesianBotorchModelPyro(dtype=torch.float, cuda=True)
 
 
-class SingleObjectiveFullyBayesianBotorchModelTest(
-    TestCase, BaseFullyBayesianBotorchModelTest
+class FullyBayesianBotorchModelTest(
+    BaseFullyBayesianBotorchModelTestCases.FullyBayesianBotorchModelTest
 ):
-    model_cls = FullyBayesianBotorchModel
-
     def test_FullyBayesianBotorchModelOneOutcome(self) -> None:
         Xs1, Ys1, Yvars1, bounds, tfs, fns, mns = get_torch_test_data(
             dtype=torch.float, cuda=False, constant_noise=True
         )
         for use_input_warping, gp_kernel in product([True, False], ["rbf", "matern"]):
             model = self.model_cls(
                 use_input_warping=use_input_warping,
@@ -1022,17 +1046,17 @@
             if use_input_warping:
                 self.assertTrue(hasattr(model_list[0], "input_transform"))
                 self.assertIsInstance(model_list[0].input_transform, Warp)
             else:
                 self.assertFalse(hasattr(model_list[0], "input_transform"))
 
 
-class FullyBayesianMOOBotorchModelTest(TestCase, BaseFullyBayesianBotorchModelTest):
-    # pyre-fixme[15]: `model_cls` overrides attribute defined in
-    #  `BaseFullyBayesianBotorchModelTest` inconsistently.
+class FullyBayesianMOOBotorchModelTest(
+    BaseFullyBayesianBotorchModelTestCases.FullyBayesianBotorchModelTest
+):
     model_cls = FullyBayesianMOOBotorchModel
 
 
 class TestKernels(TestCase):
     def test_matern_kernel(self) -> None:
         a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)
         b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_model_utils.py` & `ax-platform-0.4.0/ax/models/tests/test_model_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from itertools import product
 from unittest.mock import MagicMock
 
 import numpy as np
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.model_utils import (
     best_observed_point,
@@ -15,17 +17,14 @@
     enumerate_discrete_combinations,
     mk_discrete_choices,
 )
 from ax.utils.common.testutils import TestCase
 
 
 class ModelUtilsTest(TestCase):
-    def setUp(self) -> None:
-        pass
-
     def test_BestObservedPoint(self) -> None:
         model = MagicMock()
 
         X1 = np.array(list(product(np.arange(0.0, 10.0), np.arange(0.0, 10.0))))
         X2 = np.array(list(product(np.arange(5.0, 15.0), np.arange(5.0, 15.0))))
         # Overlap of 5x5=25 points
         X3 = np.array(list(product(np.arange(20.0, 30.0), np.arange(20.0, 30.0))))
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_posterior_mean.py` & `ax-platform-0.4.0/ax/models/tests/test_posterior_mean.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.botorch import BotorchModel
 from ax.models.torch.botorch_moo import MultiObjectiveBotorchModel
 from ax.models.torch.posterior_mean import get_PosteriorMean
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_random.py` & `ax-platform-0.4.0/ax/models/tests/test_random.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 import torch
 from ax.models.random.base import RandomModel
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import not_none
 
 
 class RandomModelTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.random_model = RandomModel()
 
     def test_seed(self) -> None:
         # With manual seed.
         random_model = RandomModel(seed=5)
         self.assertEqual(random_model.seed, 5)
         # With no seed.
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_randomforest.py` & `ax-platform-0.4.0/ax/models/tests/test_randomforest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.randomforest import RandomForest
 from ax.utils.common.testutils import TestCase
 from botorch.utils.datasets import SupervisedDataset
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_rembo.py` & `ax-platform-0.4.0/ax/models/tests/test_rembo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.rembo import REMBO
 from ax.models.torch_base import TorchOptConfig
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import not_none
 from ax.utils.testing.mock import fast_botorch_optimize
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_rembo_initializer.py` & `ax-platform-0.4.0/ax/models/tests/test_rembo_initializer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.models.random.rembo_initializer import REMBOInitializer
 from ax.utils.common.testutils import TestCase
 
 
 class REMBOInitializerTest(TestCase):
     def test_REMBOInitializerModel(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_sobol.py` & `ax-platform-0.4.0/ax/models/tests/test_sobol.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Tuple
 from unittest import mock
 
 import numpy as np
 from ax.exceptions.core import SearchSpaceExhausted
 from ax.models.random.sobol import SobolGenerator
 from ax.utils.common.testutils import TestCase
 from botorch.utils.sampling import sample_polytope
 
 
 class SobolGeneratorTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.tunable_param_bounds = (0.0, 1.0)
         self.fixed_param_bounds = (1.0, 100.0)
 
     def _create_bounds(self, n_tunable: int, n_fixed: int) -> List[Tuple[float, float]]:
         tunable_bounds = [self.tunable_param_bounds] * n_tunable
         fixed_bounds = [self.fixed_param_bounds] * n_fixed
         return tunable_bounds + fixed_bounds
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_thompson.py` & `ax-platform-0.4.0/ax/models/tests/test_thompson.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import numpy as np
 from ax.exceptions.model import ModelError
 from ax.models.discrete.thompson import ThompsonSampler
 from ax.utils.common.testutils import TestCase
 
 
 class ThompsonSamplerTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.Xs = [[[1, 1], [2, 2], [3, 3], [4, 4]]]  # 4 arms, each of dimensionality 2
         self.Ys = [[1, 2, 3, 4]]
         self.Yvars = [[1, 1, 1, 1]]
         self.parameter_values = [[1, 2, 3, 4], [1, 2, 3, 4]]
         self.outcome_names = ["x", "y"]  # not used for regular TS
 
         self.multiple_metrics_Xs = [
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_torch.py` & `ax-platform-0.4.0/ax/models/tests/test_torch.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch_base import TorchModel, TorchOptConfig
 from ax.utils.common.testutils import TestCase
 from botorch.utils.datasets import SupervisedDataset
 
 
 class TorchModelTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.dataset = SupervisedDataset(
             X=torch.zeros(1, 1),
             Y=torch.zeros(1, 1),
             Yvar=torch.ones(1, 1),
             feature_names=["x1"],
             outcome_names=["y"],
         )
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_torch_model_utils.py` & `ax-platform-0.4.0/ax/models/tests/test_torch_model_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,26 +1,30 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 import torch
 from ax.exceptions.model import ModelError
 from ax.models.torch.utils import (
     _generate_sobol_points,
     is_noiseless,
     normalize_indices,
     subset_model,
     tensor_callable_to_array_callable,
 )
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import not_none
 from botorch.models import HeteroskedasticSingleTaskGP, SingleTaskGP
+from botorch.models.deterministic import GenericDeterministicModel
+from botorch.models.model import ModelList
 from botorch.models.model_list_gp_regression import ModelListGP
 from botorch.models.multitask import MultiTaskGP
 from torch import Tensor
 
 
 class TorchUtilsTest(TestCase):
     def test_is_noiseless(self) -> None:
@@ -77,14 +81,15 @@
         )
         x = np.array([5.0, 2.0])
         self.assertTrue(np.array_equal(new_func(x), np.array([25.0, 4.0])))
 
 
 class SubsetModelTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.x = torch.zeros(1, 1)
         self.y = torch.rand(1, 2)
         self.obj_t = torch.rand(2)
         self.model = SingleTaskGP(self.x, self.y)
         self.obj_weights = torch.tensor([1.0, 0.0])
 
     def test_can_subset(self) -> None:
@@ -187,35 +192,84 @@
         self.assertTrue(torch.equal(subset_model_results.indices, torch.tensor([0, 1])))
         # test error on size inconsistency
         obj_weights = torch.ones(4)
         obj_weights[0] = 0
         with self.assertRaises(RuntimeError):
             subset_model(model, obj_weights)
 
+
+class SubsetModelTestMultiTask(TestCase):
+    def setUp(self) -> None:
+        super().setUp()
+        self.x1 = torch.tensor([[1.0, 2.0, 1.0], [2.0, 3.0, 0.0]])
+        self.y1 = torch.tensor([[0.0], [1.0]])
+        self.x2 = torch.tensor([[0.0, 3.0, 1.0], [1.0, 4.0, 0.0]])
+        self.y2 = torch.tensor([[2.0], [3.0]])
+
     def test_multitask_modellist(self) -> None:
-        x1 = torch.tensor([[1.0, 2.0, 1.0], [2.0, 3.0, 0.0]])
-        y1 = torch.tensor([[0.0, 1.0]])
-        x2 = torch.tensor([[0.0, 3.0, 1.0], [1.0, 4.0, 0.0]])
-        y2 = torch.tensor([[2.0, 3.0]])
-        m1 = MultiTaskGP(x1, y1, task_feature=2)
-        m2 = MultiTaskGP(x2, y2, task_feature=2)
+        m1 = MultiTaskGP(self.x1, self.y1, task_feature=2, output_tasks=[0])
+        m2 = MultiTaskGP(self.x2, self.y2, task_feature=2, output_tasks=[0])
         model = ModelListGP(m1, m2)
         # test model is not subset when model.num_outputs >
         # len(obj_weights), but all outcomes are relevant.
         # This test is explicitly tests that model is not
         # subset when subset_model is called because all
         # outcomes are relevant.
         obj_weights = torch.ones(2)
         subset_model_results = subset_model(model, obj_weights)
         self.assertIs(subset_model_results.model, model)
         # test subset
-        subset_model_results = subset_model(model, self.obj_weights)
-        self.assertIsInstance(subset_model_results.model, ModelListGP)
-        self.assertEqual(len(subset_model_results.model.models), 1)
-        self.assertIsInstance(subset_model_results.model.models[0], MultiTaskGP)
+        obj_weights = torch.tensor([1.0, 0.0])
+        subset_model_results = subset_model(model, obj_weights)
         # check that the model is m1
-        self.assertTrue(
-            torch.equal(subset_model_results.model.models[0].train_inputs[0], x1)
+        self.assertIs(subset_model_results.model, m1)
+
+    def test_model_list(self) -> None:
+        # three output model
+        m1 = GenericDeterministicModel(lambda x: x, num_outputs=3)
+        # two output model
+        m2 = SingleTaskGP(
+            train_X=self.x1,
+            train_Y=torch.cat([self.y1, self.y2], dim=-1),
+        )
+        model = ModelList(m1, m2)
+        obj_weights = torch.zeros(5)
+        obj_weights[:3] = 1
+        subset_model_results = subset_model(model, obj_weights)
+        self.assertIs(subset_model_results.model, m1)
+        # set subset where m2 is subset
+        m1 = GenericDeterministicModel(lambda x: x, num_outputs=1)
+        model = ModelList(m1, m2)
+        obj_weights = torch.ones(3)
+        obj_weights[1] = 0
+        subset_model_results = subset_model(model, obj_weights)
+        models = subset_model_results.model.models
+        self.assertEqual(len(models), 2)
+        self.assertIs(models[0], m1)
+        self.assertIsInstance(models[1], SingleTaskGP)
+        # check that second model is the second output of m2
+        self.assertTrue(torch.equal(models[1].train_targets, m2.train_targets[1]))
+
+    def test_nested_model_list_gp(self) -> None:
+        m1 = MultiTaskGP(
+            train_X=torch.cat([self.x1, self.x2], dim=0),
+            train_Y=torch.cat([self.y1, self.y2], dim=0),
+            task_feature=2,
+            output_tasks=[0],
+        )
+        m2a = SingleTaskGP(
+            train_X=self.x1,
+            train_Y=self.y1,
         )
-        self.assertTrue(
-            torch.equal(subset_model_results.model.models[0].train_targets, y1)
+        m2b = SingleTaskGP(
+            train_X=self.x2,
+            train_Y=self.y2,
         )
+        model = ModelListGP(m1, ModelListGP(m2a, m2b))
+        obj_weights = torch.zeros(4)
+        obj_weights[0] = 1
+        obj_weights[2] = 1
+        subset_model_results = subset_model(model, obj_weights)
+        models = subset_model_results.model.models
+        self.assertEqual(len(models), 2)
+        self.assertIs(models[0], m1)
+        self.assertIs(models[1], m2b)
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_torch_utils.py` & `ax-platform-0.4.0/ax/models/tests/test_torch_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Set, Tuple
 from unittest.mock import MagicMock, Mock, patch
 
 import torch
 from ax.exceptions.core import UnsupportedError
 from ax.models.torch.utils import (
     _get_X_pending_and_observed,
@@ -33,14 +35,15 @@
 from botorch.acquisition.risk_measures import Expectation
 from botorch.exceptions.errors import BotorchTensorDimensionError
 from botorch.models.model import Model
 
 
 class TorchUtilsTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.device = torch.device("cpu")
         self.dtype = torch.double
         self.mock_botorch_model = MagicMock(Model)
         tkwargs = {"dtype": self.dtype, "device": self.device}
         self.X_dummy = torch.ones(1, 3, **tkwargs)
         self.outcome_constraints = (
             torch.tensor([[1.0]], **tkwargs),
@@ -75,14 +78,36 @@
             objective_weights=objective_weights,
             bounds=bounds,
             fixed_features=fixed_features,
         )
         expected = Xs[0]
         self.assertEqual(_to_obs_set(expected), _to_obs_set(not_none(X_observed)))
 
+        # Out of design observations are filtered out
+        Xs = [torch.tensor([[2.0, 3.0], [3.0, 4.0]])]
+        _, X_observed = _get_X_pending_and_observed(
+            Xs=Xs,
+            objective_weights=objective_weights,
+            bounds=bounds,
+            fixed_features=fixed_features,
+            fit_out_of_design=False,
+        )
+        self.assertIsNone(X_observed)
+
+        # Keep out of design observations
+        _, X_observed = _get_X_pending_and_observed(
+            Xs=Xs,
+            objective_weights=objective_weights,
+            bounds=bounds,
+            fixed_features=fixed_features,
+            fit_out_of_design=True,
+        )
+        expected = Xs[0]
+        self.assertEqual(_to_obs_set(expected), _to_obs_set(not_none(X_observed)))
+
     @patch(
         f"{get_botorch_objective_and_transform.__module__}.get_infeasible_cost",
         return_value=1.0,
     )
     def test_get_botorch_objective(self, _) -> None:
         # For KG with outcome constraints, use a ConstrainedMCObjective
         obj, tf = get_botorch_objective_and_transform(
```

### Comparing `ax-platform-0.3.7/ax/models/tests/test_uniform.py` & `ax-platform-0.4.0/ax/models/tests/test_uniform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import numpy as np
 from ax.exceptions.core import SearchSpaceExhausted
 from ax.models.random.uniform import UniformGenerator
 from ax.utils.common.testutils import TestCase
 
 
 class UniformGeneratorTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.tunable_param_bounds = (0, 1)
         self.fixed_param_bounds = (1, 100)
 
     # pyre-fixme[3]: Return type must be annotated.
     # pyre-fixme[2]: Parameter must be annotated.
     def _create_bounds(self, n_tunable, n_fixed):
         tunable_bounds = [self.tunable_param_bounds] * n_tunable
```

### Comparing `ax-platform-0.3.7/ax/models/torch/alebo.py` & `ax-platform-0.4.0/ax/models/torch/alebo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import dataclasses
 import re
 from collections import OrderedDict
 from logging import Logger
 from math import inf
@@ -258,15 +260,15 @@
     """
 
     def __init__(self, B: Tensor, batch_shape: torch.Size) -> None:
         super().__init__(
             has_lengthscale=False, ard_num_dims=None, eps=0.0, batch_shape=batch_shape
         )
         warn(
-            "ALEBOKernel is deprecated and should be removed in Ax 0.5.0.",
+            "ALEBOKernel is deprecated and should be removed in Ax 0.3.9.",
             DeprecationWarning,
         )
         # pyre-fixme[4]: Attribute must be annotated.
         self.d, D = B.shape
         if not self.d < D:
             raise ValueError(f"Expected B.shape[0] < B.shape[1], but got {B.shape=}.")
         self.B = B
@@ -331,15 +333,15 @@
         train_Yvar: (n x 1) Noise variances of each training Y.
     """
 
     def __init__(
         self, B: Tensor, train_X: Tensor, train_Y: Tensor, train_Yvar: Tensor
     ) -> None:
         warn(
-            "ALEBOGP is deprecated and should be removed in Ax 0.5.0. SAASBO "
+            "ALEBOGP is deprecated and should be removed in Ax 0.3.9. SAASBO "
             "(Models.SAASBO from ax.modelbridge.registry) likely provides better "
             "performance.",
             DeprecationWarning,
         )
         super().__init__(train_X=train_X, train_Y=train_Y, train_Yvar=train_Yvar)
         self.covar_module = ScaleKernel(
             base_kernel=ALEBOKernel(B=B, batch_shape=self._aug_batch_shape),
@@ -392,15 +394,15 @@
         observation_noise: Union[bool, Tensor] = False,
         posterior_transform: Optional[PosteriorTransform] = None,
         **kwargs: Any,
     ) -> GPyTorchPosterior:
         assert output_indices is None
         assert not observation_noise
         mvn = self(X)
-        posterior = GPyTorchPosterior(mvn=mvn)
+        posterior = GPyTorchPosterior(distribution=mvn)
         if posterior_transform is not None:
             return posterior_transform(posterior)
         return posterior
 
 
 def get_fitted_model(
     B: Tensor,
@@ -427,15 +429,15 @@
         nsamp: Number of samples to draw from kernel hyperparameter posterior.
         init_state_dict: Optionally begin MAP estimation with this state dict.
 
     Returns: Batch-mode (nsamp batches) fitted ALEBO GP.
     """
     warn(
         "`get_fitted_model` from ax.models.torch.alebo.py is deprecated and "
-        "should be removed in Ax 0.5.0.",
+        "should be removed in Ax 0.3.9.",
         DeprecationWarning,
     )
     # Get MAP estimate.
     mll = get_map_model(
         B=B,
         train_X=train_X,
         train_Y=train_Y,
@@ -478,15 +480,15 @@
         restarts: Number of restarts for MAP estimation.
         init_state_dict: Optionally begin MAP estimation with this state dict.
 
     Returns: non-batch ALEBO GP with MAP kernel hyperparameters.
     """
     warn(
         "`get_map_model` from ax.models.torch.alebo.py is deprecated and should "
-        "be removed in Ax 0.5.0.",
+        "be removed in Ax 0.3.9.",
         DeprecationWarning,
     )
     f_best = 1e8
     sd_best = {}
     # Fit with random restarts
     for _ in range(restarts):
         m = ALEBOGP(B=B, train_X=train_X, train_Y=train_Y, train_Yvar=train_Yvar)
@@ -524,15 +526,15 @@
         mll: MLL object of MAP ALEBO GP.
         nsamp: Number of samples to return.
 
     Returns: Batch tensors of the kernel hyperparameters Uvec, mean constant,
         and output scale.
     """
     warn(
-        "laplace_sample_U is deprecated and should be removed in Ax 0.5.0.",
+        "laplace_sample_U is deprecated and should be removed in Ax 0.3.9.",
         DeprecationWarning,
     )
     # Estimate diagonal of the Hessian
     mll.train()
     x0, property_dict, bounds = module_to_array(module=mll)
     x0 = x0.astype(np.float64)  # This is the MAP parameters
     H = np.zeros((len(x0), len(x0)))
@@ -594,15 +596,15 @@
         mean_constant_batch: Batch tensor of mean constant hyperparameter.
         output_scale_batch: Batch tensor of output scale hyperparameter.
 
     Returns: Batch-mode ALEBO GP.
     """
     warn(
         "`get_batch_model` from ax.models.torch.alebo.py is deprecated and "
-        "should be removed in Ax 0.5.0.",
+        "should be removed in Ax 0.3.9.",
         DeprecationWarning,
     )
     b = Uvec_batch.size(0)
     m_b = ALEBOGP(
         B=B,
         train_X=train_X.repeat(b, 1, 1),
         train_Y=train_Y.repeat(b, 1, 1),
@@ -635,15 +637,15 @@
 
     Args:
         m_b: Batch-mode GP.
         num_outputs: Number of outputs being modeled.
     """
     warn(
         "`extract_map_statedict` from ax.models.torch.alebo.py is deprecated and "
-        "should be removed in Ax 0.5.0.",
+        "should be removed in Ax 0.3.9.",
         DeprecationWarning,
     )
     is_modellist = num_outputs > 1
     map_sds: List[MutableMapping[str, Tensor]] = [
         OrderedDict() for i in range(num_outputs)
     ]
     sd = m_b.state_dict()
@@ -735,15 +737,15 @@
     Optimize the acquisition function for ALEBO.
 
     We are optimizing over a polytope within the subspace, and so begin each
     random restart of the acquisition function optimization with points that
     lie within that polytope.
     """
     warn(
-        "`alebo_acqf_optimizer` is deprecated and should be removed in Ax 0.5.0.",
+        "`alebo_acqf_optimizer` is deprecated and should be removed in Ax 0.3.9.",
         DeprecationWarning,
     )
     candidate_list, acq_value_list = [], []
     candidates = torch.tensor([], device=B.device, dtype=B.dtype)
     try:
         base_X_pending = acq_function.X_pending
         acq_has_X_pend = True
@@ -786,15 +788,15 @@
             candidates = torch.cat(candidate_list, dim=-2)
             if acq_has_X_pend:
                 acq_function.set_X_pending(
                     torch.cat([base_X_pending, candidates], dim=-2)
                     if base_X_pending is not None
                     else candidates
                 )
-        logger.info(f"Generated sequential candidate {i+1} of {n}")
+        logger.info(f"Generated sequential candidate {i + 1} of {n}")
     if acq_has_X_pend:
         acq_function.set_X_pending(base_X_pending)
     return candidates, torch.stack(acq_value_list)
 
 
 class ALEBO(BotorchModel):
     """Does Bayesian optimization in a linear subspace with ALEBO.
@@ -813,24 +815,23 @@
         fit_restarts: Number of random restarts for MAP estimation.
     """
 
     def __init__(
         self, B: Tensor, laplace_nsamp: int = 25, fit_restarts: int = 10
     ) -> None:
         warn(
-            "ALEBO is deprecated and should be removed in Ax 0.5.0.",
+            "ALEBO is deprecated and should be removed in Ax 0.3.9.",
             DeprecationWarning,
         )
         self.B = B
         # pyre-fixme[4]: Attribute must be annotated.
         self.Binv = torch.pinverse(B)
         self.laplace_nsamp = laplace_nsamp
         self.fit_restarts = fit_restarts
         super().__init__(
-            refit_on_update=True,  # Important to not get stuck in local opt.
             refit_on_cv=False,
             warm_start_refitting=False,
             acqf_constructor=ei_or_nei,  # pyre-ignore
             # pyre-fixme[6]: Expected `(AcquisitionFunction, Tensor, int, Optional[Li...
             acqf_optimizer=alebo_acqf_optimizer,
         )
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch.py` & `ax-platform-0.4.0/ax/models/torch/botorch.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import warnings
 from copy import deepcopy
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
@@ -119,16 +121,14 @@
             fitted model, with signature as described below.
         acqf_optimizer: A callable that optimizes the acquisition function, with
             signature as described below.
         best_point_recommender: A callable that recommends the best point, with
             signature as described below.
         refit_on_cv: If True, refit the model for each fold when performing
             cross-validation.
-        refit_on_update: If True, refit the model after updating the training
-            data using the `update` method.
         warm_start_refitting: If True, start model refitting from previous
             model parameters in order to speed up the fitting process.
         prior: An optional dictionary that contains the specification of GP model prior.
             Currently, the keys include:
             - covar_module_prior: prior on covariance matrix e.g.
                 {"lengthscale_prior": GammaPrior(3.0, 6.0)}.
             - type: type of prior on task covariance matrix e.g.`LKJCovariancePrior`.
@@ -245,15 +245,14 @@
         model_constructor: TModelConstructor = get_and_fit_model,
         model_predictor: TModelPredictor = predict_from_model,
         acqf_constructor: TAcqfConstructor = get_qLogNEI,
         # pyre-fixme[9]: acqf_optimizer declared/used type mismatch
         acqf_optimizer: TOptimizer = scipy_optimizer,
         best_point_recommender: TBestPointRecommender = recommend_best_observed_point,
         refit_on_cv: bool = False,
-        refit_on_update: bool = True,
         warm_start_refitting: bool = True,
         use_input_warping: bool = False,
         use_loocv_pseudo_likelihood: bool = False,
         prior: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> None:
         warnings.warn(
@@ -270,15 +269,14 @@
         self.model_predictor = model_predictor
         self.acqf_constructor = acqf_constructor
         self.acqf_optimizer = acqf_optimizer
         self.best_point_recommender = best_point_recommender
         # pyre-fixme[4]: Attribute must be annotated.
         self._kwargs = kwargs
         self.refit_on_cv = refit_on_cv
-        self.refit_on_update = refit_on_update
         self.warm_start_refitting = warm_start_refitting
         self.use_input_warping = use_input_warping
         self.use_loocv_pseudo_likelihood = use_loocv_pseudo_likelihood
         self.prior = prior
         self._model: Optional[Model] = None
         self.Xs = []
         self.Ys = []
@@ -347,14 +345,15 @@
             Xs=self.Xs,
             objective_weights=torch_opt_config.objective_weights,
             bounds=search_space_digest.bounds,
             pending_observations=torch_opt_config.pending_observations,
             outcome_constraints=torch_opt_config.outcome_constraints,
             linear_constraints=torch_opt_config.linear_constraints,
             fixed_features=torch_opt_config.fixed_features,
+            fit_out_of_design=torch_opt_config.fit_out_of_design,
         )
         model = self.model
         # subset model only to the outcomes we need for the optimization	357
         if options.get(Keys.SUBSET_MODEL, True):
             subset_model_results = subset_model(
                 model=model,
                 objective_weights=torch_opt_config.objective_weights,
@@ -422,17 +421,17 @@
                     override_qmc=True
                 )
             else:
                 raise e
 
         gen_metadata = {}
         if expected_acquisition_value.numel() > 0:
-            gen_metadata[
-                "expected_acquisition_value"
-            ] = expected_acquisition_value.tolist()
+            gen_metadata["expected_acquisition_value"] = (
+                expected_acquisition_value.tolist()
+            )
 
         return TorchGenResults(
             points=candidates.detach().cpu(),
             weights=torch.ones(n, dtype=self.dtype),
             gen_metadata=gen_metadata,
         )
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_defaults.py` & `ax-platform-0.4.0/ax/models/torch/botorch_defaults.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import functools
 from copy import deepcopy
 from random import randint
 from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple, Type, Union
 
 import torch
 from ax.models.model_utils import best_observed_point, get_observed
@@ -240,16 +242,15 @@
         self,  # making this a static method makes Pyre unhappy, better to keep `self`
         model: Model,
         objective_weights: Tensor,
         outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,
         X_observed: Optional[Tensor] = None,
         X_pending: Optional[Tensor] = None,
         **kwargs: Any,
-    ) -> AcquisitionFunction:
-        ...  # pragma: no cover
+    ) -> AcquisitionFunction: ...  # pragma: no cover
 
 
 def get_acqf(
     acquisition_function_name: str,
 ) -> Callable[[Callable[[], None]], TAcqfConstructor]:
     """Returns a decorator whose wrapper function instantiates an acquisition function.
 
@@ -799,14 +800,18 @@
             train_Y=Y,
             train_Yvar=None if all_nan_Yvar else Yvar,
             task_feature=task_feature,
             covar_module=covar_module,
             rank=kwargs.get("rank"),
             task_covar_prior=task_covar_prior,
             input_transform=warp_tf,
+            # specify output_tasks so that model.num_outputs
+            # is 1, since the model is only modeling
+            # a since metric.
+            output_tasks=all_tasks[:1],
         )
     return gp
 
 
 def _get_customized_covar_module(
     covar_module_prior_dict: Dict[str, Prior],
     ard_num_dims: int,
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_kg.py` & `ax-platform-0.4.0/ax/models/torch/botorch_kg.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.botorch import BotorchModel, get_rounding_func
 from ax.models.torch.botorch_defaults import recommend_best_out_of_sample_point
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_mes.py` & `ax-platform-0.4.0/ax/models/torch/botorch_mes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.botorch import BotorchModel, get_rounding_func
 from ax.models.torch.botorch_defaults import recommend_best_out_of_sample_point
 from ax.models.torch.utils import (
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/acquisition.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/acquisition.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import dataclasses
 import functools
 import operator
 import warnings
 from functools import partial, reduce
@@ -150,50 +152,52 @@
         )
 
         # If a point is pending on any model do not count it as observed.
         # Do this by stacking pending on top of observed, filtering repeats, then
         # removing pending points.
         # TODO[sdaulton] Is this a sound approach? Should we be doing something more
         # sophisticated here?
-        if unique_Xs_pending is None and unique_Xs_observed_maybe_pending is None:
-            unique_Xs_observed = None
-        elif unique_Xs_pending is None:
+        if unique_Xs_pending is None:
             unique_Xs_observed = unique_Xs_observed_maybe_pending
+        elif unique_Xs_observed_maybe_pending is None:
+            unique_Xs_observed = None
         else:
             unique_Xs_observed = _tensor_difference(
                 A=unique_Xs_pending, B=unique_Xs_observed_maybe_pending
             )
 
             if torch.numel(unique_Xs_observed_maybe_pending) != torch.numel(
                 unique_Xs_observed
             ):
                 logger.warning(
                     "Encountered Xs pending for some Surrogates but observed for "
                     "others. Considering these points to be pending."
                 )
 
         # Store objective thresholds for all outcomes (including non-objectives).
-        self._objective_thresholds: Optional[
-            Tensor
-        ] = torch_opt_config.objective_thresholds
+        self._objective_thresholds: Optional[Tensor] = (
+            torch_opt_config.objective_thresholds
+        )
         self._full_objective_weights: Tensor = torch_opt_config.objective_weights
         full_outcome_constraints = torch_opt_config.outcome_constraints
 
         # TODO[mpolson64] Handle more elegantly in the future. Since right now we
         # only use one objective and posterior_transform this should be fine.
         primary_surrogate = (
             self.surrogates[Keys.PRIMARY_SURROGATE]
             if len(self.surrogates) > 1
             else next(iter(self.surrogates.values()))
         )
 
         primary_Xs_pending, primary_Xs_observed = Xs_pending_and_observed[
-            Keys.PRIMARY_SURROGATE
-            if len(self.surrogates) > 1
-            else next(iter(Xs_pending_and_observed.keys()))
+            (
+                Keys.PRIMARY_SURROGATE
+                if len(self.surrogates) > 1
+                else next(iter(Xs_pending_and_observed.keys()))
+            )
         ]
 
         # Subset model only to the outcomes we need for the optimization.
         if self.options.pop(Keys.SUBSET_MODEL, True):
             subset_model_results = subset_model(
                 model=primary_surrogate.model,
                 objective_weights=torch_opt_config.objective_weights,
@@ -211,19 +215,23 @@
             outcome_constraints = torch_opt_config.outcome_constraints
             objective_thresholds = torch_opt_config.objective_thresholds
             subset_idcs = None
         # If objective weights suggest multiple objectives but objective
         # thresholds are not specified, infer them using the model that
         # has already been subset to avoid re-subsetting it within
         # `inter_objective_thresholds`.
-        if objective_weights.nonzero().numel() > 1 and (
-            self._objective_thresholds is None
-            or self._objective_thresholds[torch_opt_config.objective_weights != 0]
-            .isnan()
-            .any()
+        if (
+            objective_weights.nonzero().numel() > 1
+            and (
+                self._objective_thresholds is None
+                or self._objective_thresholds[torch_opt_config.objective_weights != 0]
+                .isnan()
+                .any()
+            )
+            and primary_Xs_observed is not None
         ):
             if torch_opt_config.risk_measure is not None:
                 # TODO[T131759263]: modify the heuristic to support risk measures.
                 raise NotImplementedError(
                     "Objective thresholds must be provided when using risk measures."
                 )
             self._objective_thresholds = infer_objective_thresholds(
@@ -311,15 +319,15 @@
             training_data=training_data,
             objective=objective,
             posterior_transform=posterior_transform,
             **input_constructor_kwargs,
         )
         self.acqf = botorch_acqf_class(**acqf_inputs)  # pyre-ignore [45]
         self.X_pending: Optional[Tensor] = unique_Xs_pending
-        self.X_observed: Tensor = not_none(unique_Xs_observed)
+        self.X_observed: Optional[Tensor] = unique_Xs_observed
 
     @property
     def botorch_acqf_class(self) -> Type[AcquisitionFunction]:
         """BoTorch ``AcquisitionFunction`` class underlying this ``Acquisition``."""
         return self.acqf.__class__
 
     @property
@@ -374,15 +382,15 @@
         self,
         n: int,
         search_space_digest: SearchSpaceDigest,
         inequality_constraints: Optional[List[Tuple[Tensor, Tensor, float]]] = None,
         fixed_features: Optional[Dict[int, float]] = None,
         rounding_func: Optional[Callable[[Tensor], Tensor]] = None,
         optimizer_options: Optional[Dict[str, Any]] = None,
-    ) -> Tuple[Tensor, Tensor]:
+    ) -> Tuple[Tensor, Tensor, Tensor]:
         """Generate a set of candidates via multi-start optimization. Obtains
         candidates and their associated acquisition function values.
 
         Args:
             n: The number of candidates to generate.
             search_space_digest: A ``SearchSpaceDigest`` object containing search space
                 properties, e.g. ``bounds`` for optimization.
@@ -401,16 +409,17 @@
                 which is applied to the candidates before the `rounding_func`.
                 `post_processing_func` can be used to support more customized options
                 that typically only exist in MBM, such as BoTorch transforms.
                 See the docstring of `TorchOptConfig` for more information on passing
                 down these options while constructing a generation strategy.
 
         Returns:
-            A two-element tuple containing an `n x d`-dim tensor of generated candidates
-            and a tensor with the associated acquisition value.
+            A three-element tuple containing an `n x d`-dim tensor of generated
+            candidates, a tensor with the associated acquisition values, and a tensor
+            with the weight for each candidate.
         """
         # NOTE: Could make use of `optimizer_class` when it's added to BoTorch
         # instead of calling `optimizer_acqf` or `optimize_acqf_discrete` etc.
         _tensorize = partial(torch.tensor, dtype=self.dtype, device=self.device)
         ssd = search_space_digest
         bounds = _tensorize(ssd.bounds).t()
 
@@ -426,75 +435,83 @@
             optimizer_options=optimizer_options_with_defaults,
         )
         discrete_features = sorted(ssd.ordinal_features + ssd.categorical_features)
         if fixed_features is not None:
             for i in fixed_features:
                 if not 0 <= i < len(ssd.feature_names):
                     raise ValueError(f"Invalid fixed_feature index: {i}")
-
+        # Return a weight of 1 for each arm by default. This can be
+        # customized in subclasses if necessary.
+        arm_weights = torch.ones(n, dtype=self.dtype)
         # 1. Handle the fully continuous search space.
         if (
             optimizer_options_with_defaults.pop("force_use_optimize_acqf", False)
             or not discrete_features
         ):
-            return optimize_acqf(
+            candidates, acqf_values = optimize_acqf(
                 acq_function=self.acqf,
                 bounds=bounds,
                 q=n,
                 inequality_constraints=inequality_constraints,
                 fixed_features=fixed_features,
                 post_processing_func=post_processing_func,
                 **optimizer_options_with_defaults,
             )
+            return candidates, acqf_values, arm_weights
 
         # 2. Handle search spaces with discrete features.
         discrete_choices = mk_discrete_choices(ssd=ssd, fixed_features=fixed_features)
 
         # 2a. Handle the fully discrete search space.
         if len(discrete_choices) == len(ssd.feature_names):
             X_observed = self.X_observed
             if self.X_pending is not None:
-                X_observed = torch.cat([X_observed, self.X_pending], dim=0)
+                if X_observed is None:
+                    X_observed = self.X_pending
+                else:
+                    X_observed = torch.cat([X_observed, self.X_pending], dim=0)
 
             # Special handling for search spaces with a large number of choices
             total_choices = reduce(
                 operator.mul, [float(len(c)) for c in discrete_choices.values()]
             )
             if total_choices > MAX_CHOICES_ENUMERATE:
                 discrete_choices = [
                     torch.tensor(c, device=self.device, dtype=self.dtype)
                     for c in discrete_choices.values()
                 ]
-                return optimize_acqf_discrete_local_search(
+                candidates, acqf_values = optimize_acqf_discrete_local_search(
                     acq_function=self.acqf,
                     q=n,
                     discrete_choices=discrete_choices,
                     inequality_constraints=inequality_constraints,
                     X_avoid=X_observed,
                     **optimizer_options_with_defaults,
                 )
+                return candidates, acqf_values, arm_weights
 
             # Enumerate all possible choices
             all_choices = (discrete_choices[i] for i in range(len(discrete_choices)))
             all_choices = _tensorize(tuple(product(*all_choices)))
 
             # This can be vectorized, but using a for-loop to avoid memory issues
-            for x in X_observed:
-                all_choices = all_choices[
-                    (all_choices - x).abs().max(dim=-1).values > DUPLICATE_TOL
-                ]
+            if X_observed is not None:
+                for x in X_observed:
+                    all_choices = all_choices[
+                        (all_choices - x).abs().max(dim=-1).values > DUPLICATE_TOL
+                    ]
 
             # Filter out candidates that violate the constraints
             # TODO: It will be more memory-efficient to do this filtering before
             # converting the generator into a tensor. However, if we run into memory
             # issues we are likely better off being smarter in how we optimize the
             # acquisition function.
             inequality_constraints = inequality_constraints or []
             is_feasible = torch.ones(all_choices.shape[0], dtype=torch.bool)
-            for (inds, weights, bound) in inequality_constraints:
+            for inds, weights, bound in inequality_constraints:
                 is_feasible &= (all_choices[..., inds] * weights).sum(dim=-1) >= bound
             all_choices = all_choices[is_feasible]
 
             num_choices = all_choices.size(dim=0)
             if num_choices == 0:
                 raise SearchSpaceExhausted(
                     "No more feasible choices in a fully discrete search space."
@@ -502,43 +519,46 @@
             if num_choices < n:
                 warnings.warn(
                     (
                         f"Requested n={n} candidates from fully discrete search "
                         f"space, but only {num_choices} possible choices remain."
                     ),
                     AxWarning,
+                    stacklevel=2,
                 )
                 n = num_choices
 
             discrete_opt_options = optimizer_argparse(
                 self.acqf,
                 bounds=bounds,
                 q=n,
                 optimizer_options=optimizer_options,
                 optimizer_is_discrete=True,
             )
-            return optimize_acqf_discrete(
+            candidates, acqf_values = optimize_acqf_discrete(
                 acq_function=self.acqf, q=n, choices=all_choices, **discrete_opt_options
             )
+            return candidates, acqf_values, arm_weights
 
         # 2b. Handle mixed search spaces that have discrete and continuous features.
-        return optimize_acqf_mixed(
+        candidates, acqf_values = optimize_acqf_mixed(
             acq_function=self.acqf,
             bounds=bounds,
             q=n,
             # For now we just enumerate all possible discrete combinations. This is not
             # scalable and and only works for a reasonably small number of choices. A
             # slowdown warning is logged in `enumerate_discrete_combinations` if needed.
             fixed_features_list=enumerate_discrete_combinations(
                 discrete_choices=discrete_choices
             ),
             inequality_constraints=inequality_constraints,
             post_processing_func=post_processing_func,
             **optimizer_options_with_defaults,
         )
+        return candidates, acqf_values, arm_weights
 
     def evaluate(self, X: Tensor) -> Tensor:
         """Evaluate the acquisition function on the candidate set `X`.
 
         Args:
             X: A `batch_shape x q x d`-dim Tensor of t-batches with `q` `d`-dim design
                 points each.
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/covar_modules.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/covar_modules.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Type, Union
 
 import torch
 from ax.models.torch.botorch_modular.kernels import ScaleMaternKernel
 from ax.utils.common.typeutils import _argparse_type_encoder
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/input_transforms.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/input_transforms.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Type
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.utils import normalize_indices
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/input_constructors/outcome_transform.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/input_constructors/outcome_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Type
 
 from ax.utils.common.typeutils import _argparse_type_encoder
 from botorch.models.transforms.outcome import OutcomeTransform, Standardize
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/kernels.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/kernels.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 import torch
 from ax.exceptions.core import AxError
 from gpytorch.constraints import Interval
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/model.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/model.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
+from collections.abc import Sequence
 from copy import deepcopy
 from dataclasses import dataclass, field
 from functools import wraps
 from itertools import chain
 from typing import (
     Any,
     Callable,
@@ -133,15 +136,14 @@
             and data if not specified.
         surrogate_specs: Optional Mapping of names onto SurrogateSpecs, which specify
             how to initialize specific Surrogates to model specific outcomes. If None
             is provided a single Surrogate will be created and set up automatically
             based on the data provided.
         surrogate: In liu of SurrogateSpecs, an instance of `Surrogate` may be
             provided to be used as the sole Surrogate for all outcomes
-        refit_on_update: Unused.
         refit_on_cv: Whether to reoptimize model parameters during call to
             `BoTorchmodel.cross_validate`.
         warm_start_refit: Whether to load parameters from either the provided
             state dict or the state dict of the current BoTorch `Model` during
             refitting. If False, model parameters will be reoptimized from
             scratch on refit. NOTE: This setting is ignored during
             `cross_validate` if the corresponding `refit_on_...` is False.
@@ -162,15 +164,14 @@
         self,
         surrogate_specs: Optional[Mapping[str, SurrogateSpec]] = None,
         surrogate: Optional[Surrogate] = None,
         acquisition_class: Optional[Type[Acquisition]] = None,
         acquisition_options: Optional[Dict[str, Any]] = None,
         botorch_acqf_class: Optional[Type[AcquisitionFunction]] = None,
         # TODO: [T168715924] Revisit these "refit" arguments.
-        refit_on_update: bool = True,
         refit_on_cv: bool = False,
         warm_start_refit: bool = True,
     ) -> None:
         # Ensure only surrogate_specs or surrogate is provided
         if surrogate_specs and surrogate:
             raise UserInputError(
                 "Only one of `surrogate_specs` and `surrogate` arguments is expected."
@@ -209,15 +210,14 @@
         else:
             self._surrogates = {}
 
         self.acquisition_class = acquisition_class or Acquisition
         self.acquisition_options = acquisition_options or {}
         self._botorch_acqf_class = botorch_acqf_class
 
-        self.refit_on_update = refit_on_update
         self.refit_on_cv = refit_on_cv
         self.warm_start_refit = warm_start_refit
 
     @property
     def surrogates(self) -> Dict[str, Surrogate]:
         """Surrogates by label"""
         return self._surrogates
@@ -246,15 +246,15 @@
         """
         if not self._botorch_acqf_class:
             raise ValueError("BoTorch `AcquisitionFunction` has not yet been set.")
         return self._botorch_acqf_class
 
     def fit(
         self,
-        datasets: List[SupervisedDataset],
+        datasets: Sequence[SupervisedDataset],
         search_space_digest: SearchSpaceDigest,
         candidate_metadata: Optional[List[List[TCandidateMetadata]]] = None,
         # state dict by surrogate label
         state_dicts: Optional[Mapping[str, OrderedDict[str, Tensor]]] = None,
         refit: bool = True,
         **additional_model_inputs: Any,
     ) -> None:
@@ -287,17 +287,17 @@
         if Keys.ONLY_SURROGATE in self._surrogates.keys():
             surrogate = self._surrogates[Keys.ONLY_SURROGATE]
             surrogate.model_options.update(additional_model_inputs)
             surrogate.fit(
                 datasets=datasets,
                 search_space_digest=search_space_digest,
                 candidate_metadata=candidate_metadata,
-                state_dict=state_dicts.get(Keys.ONLY_SURROGATE)
-                if state_dicts
-                else None,
+                state_dict=(
+                    state_dicts.get(Keys.ONLY_SURROGATE) if state_dicts else None
+                ),
                 refit=refit,
             )
             self._output_order = list(range(len(outcome_names)))
             return
 
         # Step 1. Initialize a Surrogate for every SurrogateSpec
         self._surrogates = {
@@ -421,15 +421,15 @@
 
         acqf = self._instantiate_acquisition(
             search_space_digest=search_space_digest,
             torch_opt_config=torch_opt_config,
             acq_options=acq_options,
         )
         botorch_rounding_func = get_rounding_func(torch_opt_config.rounding_func)
-        candidates, expected_acquisition_value = acqf.optimize(
+        candidates, expected_acquisition_value, weights = acqf.optimize(
             n=n,
             search_space_digest=search_space_digest,
             inequality_constraints=_to_inequality_constraints(
                 linear_constraints=torch_opt_config.linear_constraints
             ),
             fixed_features=torch_opt_config.fixed_features,
             rounding_func=botorch_rounding_func,
@@ -438,15 +438,15 @@
         gen_metadata = self._get_gen_metadata_from_acqf(
             acqf=acqf,
             torch_opt_config=torch_opt_config,
             expected_acquisition_value=expected_acquisition_value,
         )
         return TorchGenResults(
             points=candidates.detach().cpu(),
-            weights=torch.ones(n, dtype=self.dtype),
+            weights=weights,
             gen_metadata=gen_metadata,
         )
 
     def _get_gen_metadata_from_acqf(
         self,
         acqf: Acquisition,
         torch_opt_config: TorchOptConfig,
@@ -497,15 +497,15 @@
             acq_options=acq_options,
         )
         return acqf.evaluate(X=X)
 
     @copy_doc(TorchModel.cross_validate)
     def cross_validate(
         self,
-        datasets: List[SupervisedDataset],
+        datasets: Sequence[SupervisedDataset],
         X_test: Tensor,
         search_space_digest: SearchSpaceDigest,
         **additional_model_inputs: Any,
     ) -> Tuple[Tensor, Tensor]:
         # Will fail if metric_names exist across multiple models
         metric_names = sum((ds.outcome_names for ds in datasets), [])
         surrogate_labels = (
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/multi_fidelity.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/multi_fidelity.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Mapping, Optional
 
 from ax.core.search_space import SearchSpaceDigest
 from ax.exceptions.core import UnsupportedError
 from ax.models.torch.botorch_modular.acquisition import Acquisition
 from ax.models.torch.botorch_modular.surrogate import Surrogate
 from ax.models.torch_base import TorchOptConfig
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/optimizer_argparse.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/optimizer_argparse.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Type, TypeVar, Union
 
 import torch
 from ax.utils.common.constants import Keys
 from ax.utils.common.typeutils import _argparse_type_encoder
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/sebo.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/sebo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import functools
 from copy import deepcopy
 from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
@@ -208,15 +210,15 @@
         self,
         n: int,
         search_space_digest: SearchSpaceDigest,
         inequality_constraints: Optional[List[Tuple[Tensor, Tensor, float]]] = None,
         fixed_features: Optional[Dict[int, float]] = None,
         rounding_func: Optional[Callable[[Tensor], Tensor]] = None,
         optimizer_options: Optional[Dict[str, Any]] = None,
-    ) -> Tuple[Tensor, Tensor]:
+    ) -> Tuple[Tensor, Tensor, Tensor]:
         """Generate a set of candidates via multi-start optimization. Obtains
         candidates and their associated acquisition function values.
 
         Args:
             n: The number of candidates to generate.
             search_space_digest: A ``SearchSpaceDigest`` object containing search space
                 properties, e.g. ``bounds`` for optimization.
@@ -226,31 +228,38 @@
             fixed_features: A map `{feature_index: value}` for features that
                 should be fixed to a particular value during generation.
             rounding_func: A function that post-processes an optimization
                 result appropriately (i.e., according to `round-trip`
                 transformations).
             optimizer_options: Options for the optimizer function, e.g. ``sequential``
                 or ``raw_samples``.
+
+        Returns:
+            A three-element tuple containing an `n x d`-dim tensor of generated
+            candidates, a tensor with the associated acquisition values, and a tensor
+            with the weight for each candidate.
         """
         if self.penalty_name == "L0_norm":
             if inequality_constraints is not None:
                 raise NotImplementedError(
                     "Homotopy does not support optimization with inequality "
                     + "constraints. Use L1 penalty norm instead."
                 )
-            candidates, expected_acquisition_value = self._optimize_with_homotopy(
-                n=n,
-                search_space_digest=search_space_digest,
-                fixed_features=fixed_features,
-                rounding_func=rounding_func,
-                optimizer_options=optimizer_options,
+            candidates, expected_acquisition_value, weights = (
+                self._optimize_with_homotopy(
+                    n=n,
+                    search_space_digest=search_space_digest,
+                    fixed_features=fixed_features,
+                    rounding_func=rounding_func,
+                    optimizer_options=optimizer_options,
+                )
             )
         else:
             # if L1 norm use standard moo-opt
-            candidates, expected_acquisition_value = super().optimize(
+            candidates, expected_acquisition_value, weights = super().optimize(
                 n=n,
                 search_space_digest=search_space_digest,
                 inequality_constraints=inequality_constraints,
                 fixed_features=fixed_features,
                 rounding_func=rounding_func,
                 optimizer_options=optimizer_options,
             )
@@ -259,24 +268,24 @@
         candidates = clamp_candidates(
             X=candidates,
             target_point=self.target_point,
             clamp_tol=CLAMP_TOL,
             device=self.device,
             dtype=self.dtype,
         )
-        return candidates, expected_acquisition_value
+        return candidates, expected_acquisition_value, weights
 
     def _optimize_with_homotopy(
         self,
         n: int,
         search_space_digest: SearchSpaceDigest,
         fixed_features: Optional[Dict[int, float]] = None,
         rounding_func: Optional[Callable[[Tensor], Tensor]] = None,
         optimizer_options: Optional[Dict[str, Any]] = None,
-    ) -> Tuple[Tensor, Tensor]:
+    ) -> Tuple[Tensor, Tensor, Tensor]:
         """Optimize SEBO ACQF with L0 norm using homotopy."""
         # extend to fixed a no homotopy_schedule schedule
         _tensorize = partial(torch.tensor, dtype=self.dtype, device=self.device)
         ssd = search_space_digest
         bounds = _tensorize(ssd.bounds).t()
 
         homotopy_schedule = LogLinearHomotopySchedule(start=0.1, end=1e-3, num_steps=30)
@@ -340,15 +349,19 @@
             num_restarts=optimizer_options_with_defaults["num_restarts"],
             raw_samples=optimizer_options_with_defaults["raw_samples"],
             post_processing_func=rounding_func,
             fixed_features=fixed_features,
             batch_initial_conditions=batch_initial_conditions,
         )
 
-        return candidates, expected_acquisition_value
+        return (
+            candidates,
+            expected_acquisition_value,
+            torch.ones(n, dtype=candidates.dtype),
+        )
 
 
 def L1_norm_func(X: Tensor, init_point: Tensor) -> Tensor:
     r"""L1_norm takes in a a `batch_shape x n x d`-dim input tensor `X`
     to a `batch_shape x n x 1`-dimensional L1 norm tensor. To be used
     for constructing a GenericDeterministicModel.
     """
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/surrogate.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/surrogate.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,23 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import inspect
+from collections import OrderedDict
+from collections.abc import Sequence
 from copy import deepcopy
 from logging import Logger
-from typing import Any, Dict, List, Optional, OrderedDict, Tuple, Type, Union
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.core.types import TCandidateMetadata
 from ax.exceptions.core import UserInputError
 from ax.models.model_utils import best_in_sample_point
 from ax.models.torch.botorch_modular.input_constructors.covar_modules import (
@@ -45,14 +49,15 @@
 from ax.utils.common.typeutils import (
     _argparse_type_encoder,
     checked_cast,
     checked_cast_optional,
 )
 from botorch.models.model import Model
 from botorch.models.model_list_gp_regression import ModelListGP
+from botorch.models.multitask import MultiTaskGP
 from botorch.models.pairwise_gp import PairwiseGP
 from botorch.models.transforms.input import (
     ChainedInputTransform,
     InputPerturbation,
     InputTransform,
 )
 from botorch.models.transforms.outcome import ChainedOutcomeTransform, OutcomeTransform
@@ -382,20 +387,20 @@
                     input_classes=input_class,
                     input_options=input_options,
                     dataset=dataset,
                     search_space_digest=search_space_digest,
                 )
 
             elif input_name == "outcome_transform":
-                formatted_model_inputs[
-                    input_name
-                ] = self._make_botorch_outcome_transform(
-                    input_classes=input_class,
-                    input_options=input_options,
-                    dataset=dataset,
+                formatted_model_inputs[input_name] = (
+                    self._make_botorch_outcome_transform(
+                        input_classes=input_class,
+                        input_options=input_options,
+                        dataset=dataset,
+                    )
                 )
             else:
                 formatted_model_inputs[input_name] = input_class(**input_options)
 
     def _make_botorch_input_transform(
         self,
         input_classes: List[Type[InputTransform]],
@@ -485,15 +490,15 @@
             if len(outcome_transforms) > 1
             else outcome_transforms[0]
         )
         return outcome_transform_instance
 
     def fit(
         self,
-        datasets: List[SupervisedDataset],
+        datasets: Sequence[SupervisedDataset],
         search_space_digest: SearchSpaceDigest,
         candidate_metadata: Optional[List[List[TCandidateMetadata]]] = None,
         state_dict: Optional[OrderedDict[str, Tensor]] = None,
         refit: bool = True,
     ) -> None:
         """Fits the underlying BoTorch ``Model`` to ``m`` outcomes.
 
@@ -537,15 +542,15 @@
             datasets=datasets,
             botorch_model_class=botorch_model_class,
             allow_batched_models=self.allow_batched_models,
         )
 
         if not should_use_model_list and len(datasets) > 1:
             datasets = convert_to_block_design(datasets=datasets, force=True)
-        self._training_data = datasets
+        self._training_data = list(datasets)  # So that it can be modified if needed.
 
         models = []
         outcome_names = []
         for i, dataset in enumerate(datasets):
             submodel_state_dict = None
             if state_dict is not None:
                 if should_use_model_list:
@@ -663,15 +668,15 @@
         acqf = Acquisition(  # TODO: For multi-fidelity, might need diff. class.
             surrogates={"self": self},
             botorch_acqf_class=acqf_class,
             search_space_digest=search_space_digest,
             torch_opt_config=torch_opt_config,
             options=acqf_options,
         )
-        candidates, acqf_values = acqf.optimize(
+        candidates, acqf_values, _ = acqf.optimize(
             n=1,
             search_space_digest=search_space_digest,
             inequality_constraints=_to_inequality_constraints(
                 linear_constraints=torch_opt_config.linear_constraints
             ),
             fixed_features=torch_opt_config.fixed_features,
         )
@@ -837,7 +842,31 @@
         dataset=dataset,
         # This is used when constructing the input transforms.
         search_space_digest=search_space_digest,
         # This is used to check if the arguments are supported.
         botorch_model_class_args=botorch_model_class_args,
     )
     return formatted_model_inputs
+
+
+@submodel_input_constructor.register(MultiTaskGP)
+def _submodel_input_constructor_mtgp(
+    botorch_model_class: Type[Model],
+    dataset: SupervisedDataset,
+    search_space_digest: SearchSpaceDigest,
+    surrogate: Surrogate,
+) -> Dict[str, Any]:
+    if len(dataset.outcome_names) > 1:
+        raise NotImplementedError("Multi-output Multi-task GPs are not yet supported.")
+    formatted_model_inputs = _submodel_input_constructor_base(
+        botorch_model_class=botorch_model_class,
+        dataset=dataset,
+        search_space_digest=search_space_digest,
+        surrogate=surrogate,
+    )
+    task_feature = formatted_model_inputs.get("task_feature")
+    if task_feature is None:
+        return formatted_model_inputs
+    # specify output tasks so that model.num_outputs = 1
+    # since the model only models a single outcome
+    formatted_model_inputs["output_tasks"] = dataset.X[:1, task_feature].tolist()
+    return formatted_model_inputs
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_modular/utils.py` & `ax-platform-0.4.0/ax/models/torch/botorch_modular/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
+from collections import OrderedDict
+from collections.abc import Sequence
 from logging import Logger
-from typing import Any, Callable, Dict, List, Optional, OrderedDict, Tuple, Type
+from typing import Any, Callable, Dict, List, Optional, Tuple, Type
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.exceptions.core import AxError, AxWarning, UnsupportedError
 from ax.models.types import TConfig
 from ax.utils.common.constants import Keys
 from ax.utils.common.logger import get_logger
@@ -35,15 +39,15 @@
 from torch import Tensor
 
 MIN_OBSERVED_NOISE_LEVEL = 1e-7
 logger: Logger = get_logger(__name__)
 
 
 def use_model_list(
-    datasets: List[SupervisedDataset],
+    datasets: Sequence[SupervisedDataset],
     botorch_model_class: Type[Model],
     allow_batched_models: bool = True,
 ) -> bool:
 
     if issubclass(botorch_model_class, MultiTaskGP):
         # We currently always wrap multi-task models into `ModelListGP`.
         return True
@@ -61,15 +65,15 @@
         return not allow_batched_models
     # If there are multiple Xs and they are not all equal, we
     # use `ListSurrogate` and `ModelListGP`.
     return True
 
 
 def choose_model_class(
-    datasets: List[SupervisedDataset],
+    datasets: Sequence[SupervisedDataset],
     search_space_digest: SearchSpaceDigest,
 ) -> Type[Model]:
     """Chooses a BoTorch `Model` using the given data (currently just Yvars)
     and its properties (information about task and fidelity features).
 
     Args:
         Yvars: List of tensors, each representing observation noise for a
@@ -167,25 +171,25 @@
         opt_options = checked_cast(
             dict, model_gen_options.get(Keys.OPTIMIZER_KWARGS, {})
         ).copy()
     return acq_options, opt_options
 
 
 def convert_to_block_design(
-    datasets: List[SupervisedDataset],
+    datasets: Sequence[SupervisedDataset],
     force: bool = False,
 ) -> List[SupervisedDataset]:
     # Convert data to "block design". TODO: Figure out a better
     # solution for this using the data containers (pass outcome
     # names as properties of the data containers)
     is_fixed = [ds.Yvar is not None for ds in datasets]
     if any(is_fixed) and not all(is_fixed):
         raise UnsupportedError(
             "Cannot convert mixed data with and without variance "
-            "observaitons to `block design`."
+            "observations to `block design`."
         )
     is_fixed = all(is_fixed)
     Xs = [dataset.X for dataset in datasets]
     for dset in datasets[1:]:
         if dset.feature_names != datasets[0].feature_names:
             raise ValueError(
                 "Feature names must be the same across all datasets, "
@@ -205,14 +209,15 @@
                 "outcomes use `force=True`."
             )
         warnings.warn(
             "Forcing converion of data not complying to a block design "
             "to block design by dropping observations that are not shared "
             "between outcomes.",
             AxWarning,
+            stacklevel=3,
         )
         X_shared, idcs_shared = _get_shared_rows(Xs=Xs)
         Y = torch.cat([ds.Y[i] for ds, i in zip(datasets, idcs_shared)], dim=-1)
         if is_fixed:
             Yvar = torch.cat(
                 # pyre-fixme[16]: `Optional` has no attribute `__getitem__`.
                 [ds.Yvar[i] for ds, i in zip(datasets, idcs_shared)],
@@ -341,16 +346,16 @@
             return combined_func
 
     else:
         return rounding_func
 
 
 def check_outcome_dataset_match(
-    outcome_names: List[str],
-    datasets: List[SupervisedDataset],
+    outcome_names: Sequence[str],
+    datasets: Sequence[SupervisedDataset],
     exact_match: bool,
 ) -> None:
     """Check that the given outcome names match those of datasets.
 
     Based on `exact_match` we either require that outcome names are
     a subset of all outcomes or require the them to be the same.
 
@@ -384,16 +389,16 @@
         raise AxError(
             "Each outcome name must correspond to an outcome in the datasets. "
             f"Got {outcome_names=} but the datasets model {set_all_outcomes}."
         )
 
 
 def get_subset_datasets(
-    datasets: List[SupervisedDataset],
-    subset_outcome_names: List[str],
+    datasets: Sequence[SupervisedDataset],
+    subset_outcome_names: Sequence[str],
 ) -> List[SupervisedDataset]:
     """Get the list of datasets corresponding to the given subset of
     outcome names. This is used to separate out datasets that are
     used by one surrogate.
 
     Args:
         datasets: A list of `SupervisedDataset` objects.
@@ -455,8 +460,8 @@
     expected_substring = f"models.{submodel_index}."
     len_substring = len(expected_substring)
     new_items = [
         (k[len_substring:], v)
         for k, v in state_dict.items()
         if k.startswith(expected_substring)
     ]
-    return OrderedDict(new_items)  # pyre-ignore [29]: T168826187
+    return OrderedDict(new_items)
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_moo.py` & `ax-platform-0.4.0/ax/models/torch/botorch_moo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.exceptions.core import AxError
 from ax.models.torch.botorch import (
@@ -200,15 +202,14 @@
         #  int, Optional[Dict[int, float]], Optional[Callable[[Tensor], Tensor]],
         #  **(Any)], Tensor]`.
         acqf_optimizer: TOptimizer = scipy_optimizer,
         # TODO: Remove best_point_recommender for botorch_moo. Used in modelbridge._gen.
         best_point_recommender: TBestPointRecommender = recommend_best_observed_point,
         frontier_evaluator: TFrontierEvaluator = pareto_frontier_evaluator,
         refit_on_cv: bool = False,
-        refit_on_update: bool = True,
         warm_start_refitting: bool = False,
         use_input_warping: bool = False,
         use_loocv_pseudo_likelihood: bool = False,
         prior: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> None:
         self.model_constructor = model_constructor
@@ -216,15 +217,14 @@
         self.acqf_constructor = acqf_constructor
         self.acqf_optimizer = acqf_optimizer
         self.best_point_recommender = best_point_recommender
         self.frontier_evaluator = frontier_evaluator
         # pyre-fixme[4]: Attribute must be annotated.
         self._kwargs = kwargs
         self.refit_on_cv = refit_on_cv
-        self.refit_on_update = refit_on_update
         self.warm_start_refitting = warm_start_refitting
         self.use_input_warping = use_input_warping
         self.use_loocv_pseudo_likelihood = use_loocv_pseudo_likelihood
         self.prior = prior
         self.model: Optional[Model] = None
         self.Xs = []
         self.Ys = []
```

### Comparing `ax-platform-0.3.7/ax/models/torch/botorch_moo_defaults.py` & `ax-platform-0.4.0/ax/models/torch/botorch_moo_defaults.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 """
 References
 
 .. [Daulton2020qehvi]
     S. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume
     Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural
     Information Processing Systems 33, 2020.
@@ -293,17 +295,19 @@
             objective=objective,
             X_observed=X_observed,
             X_pending=X_pending,
             constraints=cons_tfs,
             prune_baseline=prune_baseline,
             mc_samples=mc_samples,
             alpha=alpha,
-            seed=seed
-            if seed is not None
-            else cast(int, torch.randint(1, 10000, (1,)).item()),
+            seed=(
+                seed
+                if seed is not None
+                else cast(int, torch.randint(1, 10000, (1,)).item())
+            ),
             ref_point=objective_thresholds.tolist(),
             marginalize_dim=marginalize_dim,
             cache_root=cache_root,
         ),
     )
 
 
@@ -463,20 +467,24 @@
             acquisition_function_name=acqf_name,
             model=model,
             objective=objective,
             X_observed=X_observed,
             X_pending=X_pending,
             constraints=cons_tfs,
             mc_samples=mc_samples,
-            alpha=get_default_partitioning_alpha(num_objectives=num_objectives)
-            if alpha is None
-            else alpha,
-            seed=seed
-            if seed is not None
-            else cast(int, torch.randint(1, 10000, (1,)).item()),
+            alpha=(
+                get_default_partitioning_alpha(num_objectives=num_objectives)
+                if alpha is None
+                else alpha
+            ),
+            seed=(
+                seed
+                if seed is not None
+                else cast(int, torch.randint(1, 10000, (1,)).item())
+            ),
             ref_point=objective_thresholds.tolist(),
             Y=Y,
         ),
     )
 
 
 # TODO (jej): rewrite optimize_acqf wrappers to avoid duplicate code.
```

### Comparing `ax-platform-0.3.7/ax/models/torch/cbo_lcea.py` & `ax-platform-0.4.0/ax/models/torch/cbo_lcea.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, cast, Dict, List, Optional, Tuple, Union
 
 from ax.core.search_space import SearchSpaceDigest
 from ax.core.types import TCandidateMetadata
-from ax.models.torch.alebo import ei_or_nei
 from ax.models.torch.botorch import BotorchModel
+from ax.models.torch.botorch_defaults import get_qLogNEI
 from ax.models.torch.cbo_sac import generate_model_space_decomposition
 from ax.models.torch_base import TorchModel, TorchOptConfig
 from ax.utils.common.docutils import copy_doc
 from ax.utils.common.logger import get_logger
 from botorch.fit import fit_gpytorch_mll
 from botorch.models.contextual import LCEAGP
 from botorch.models.gpytorch import GPyTorchModel
@@ -105,16 +107,15 @@
         self.embs_dim_list = embs_dim_list
         # pyre-fixme[4]: Attribute must be annotated.
         self.gp_model_args = gp_model_args if gp_model_args is not None else {}
         self.feature_names: List[str] = []
         # pyre-fixme[4]: Attribute must be annotated.
         self.train_embedding = self.gp_model_args.get("train_embedding", True)
         super().__init__(
-            model_constructor=self.get_and_fit_model,
-            acqf_constructor=ei_or_nei,  # pyre-ignore
+            model_constructor=self.get_and_fit_model, acqf_constructor=get_qLogNEI
         )
 
     @copy_doc(TorchModel.fit)
     def fit(
         self,
         datasets: List[SupervisedDataset],
         search_space_digest: SearchSpaceDigest,
```

### Comparing `ax-platform-0.3.7/ax/models/torch/cbo_lcem.py` & `ax-platform-0.4.0/ax/models/torch/cbo_lcem.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, List, Optional
 
 import torch
 from ax.models.torch.botorch import BotorchModel
 from botorch.fit import fit_gpytorch_mll
 from botorch.models.contextual_multioutput import LCEMGP
 from botorch.models.model_list_gp_regression import ModelListGP
@@ -64,33 +66,28 @@
 
         models = []
         for i, X in enumerate(Xs):
             # validate input Yvars
             Yvar = Yvars[i].clamp_min_(MIN_OBSERVED_NOISE_LEVEL)
             is_nan = torch.isnan(Yvar)
             all_nan_Yvar = torch.all(is_nan)
-            if all_nan_Yvar:
-                gp_m = LCEMGP(
-                    train_X=X,
-                    train_Y=Ys[i],
-                    task_feature=task_feature,
-                    context_cat_feature=self.context_cat_feature,
-                    context_emb_feature=self.context_emb_feature,
-                    embs_dim_list=self.embs_dim_list,
-                )
-            else:
-                gp_m = LCEMGP(
-                    train_X=X,
-                    train_Y=Ys[i],
-                    train_Yvar=Yvar,
-                    task_feature=task_feature,
-                    context_cat_feature=self.context_cat_feature,
-                    context_emb_feature=self.context_emb_feature,
-                    embs_dim_list=self.embs_dim_list,
-                )
+            all_tasks, _, _ = LCEMGP.get_all_tasks(train_X=X, task_feature=task_feature)
+            gp_m = LCEMGP(
+                train_X=X,
+                train_Y=Ys[i],
+                train_Yvar=None if all_nan_Yvar else Yvar,
+                task_feature=task_feature,
+                context_cat_feature=self.context_cat_feature,
+                context_emb_feature=self.context_emb_feature,
+                embs_dim_list=self.embs_dim_list,
+                # specify output tasks so that model.num_outputs = 1
+                # since the model only models a single outcome.
+                output_tasks=all_tasks[:1],
+            )
+
             models.append(gp_m)
         # Use a ModelListGP
         model = ModelListGP(*models)
         model.to(Xs[0])
         mll = SumMarginalLogLikelihood(model.likelihood, model)
         fit_gpytorch_mll(mll)
         return model
```

### Comparing `ax-platform-0.3.7/ax/models/torch/cbo_sac.py` & `ax-platform-0.4.0/ax/models/torch/cbo_sac.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Dict, List, Optional
 
 from ax.core.search_space import SearchSpaceDigest
 from ax.core.types import TCandidateMetadata
 from ax.models.torch.botorch import BotorchModel
 from ax.models.torch_base import TorchModel
```

### Comparing `ax-platform-0.3.7/ax/models/torch/fully_bayesian.py` & `ax-platform-0.4.0/ax/models/torch/fully_bayesian.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 """
 Models and utilities for fully bayesian inference.
 
 TODO: move some of this into botorch.
 
 References
@@ -517,15 +519,14 @@
         model_constructor: TModelConstructor = get_and_fit_model_mcmc,
         model_predictor: TModelPredictor = predict_from_model_mcmc,
         acqf_constructor: TAcqfConstructor = get_fully_bayesian_acqf,
         # pyre-fixme[9]: acqf_optimizer declared/used type mismatch
         acqf_optimizer: TOptimizer = scipy_optimizer,
         best_point_recommender: TBestPointRecommender = recommend_best_observed_point,
         refit_on_cv: bool = False,
-        refit_on_update: bool = True,
         warm_start_refitting: bool = True,
         use_input_warping: bool = False,
         # use_saas is deprecated. TODO: remove
         use_saas: Optional[bool] = None,
         num_samples: int = 256,
         warmup_steps: int = 512,
         thinning: int = 16,
@@ -547,16 +548,14 @@
                 fitted model, with signature as described below.
             acqf_optimizer: A callable that optimizes the acquisition function, with
                 signature as described below.
             best_point_recommender: A callable that recommends the best point, with
                 signature as described below.
             refit_on_cv: If True, refit the model for each fold when performing
                 cross-validation.
-            refit_on_update: If True, refit the model after updating the training
-                data using the `update` method.
             warm_start_refitting: If True, start model refitting from previous
                 model parameters in order to speed up the fitting process.
             use_input_warping: A boolean indicating whether to use input warping
             use_saas: [deprecated] A boolean indicating whether to use the SAAS model
             num_samples: The number of MCMC samples. Note that with thinning,
                 num_samples/thinning samples are retained.
             warmup_steps: The number of burn-in steps for NUTS.
@@ -575,15 +574,14 @@
             self,
             model_constructor=model_constructor,
             model_predictor=model_predictor,
             acqf_constructor=acqf_constructor,
             acqf_optimizer=acqf_optimizer,
             best_point_recommender=best_point_recommender,
             refit_on_cv=refit_on_cv,
-            refit_on_update=refit_on_update,
             warm_start_refitting=warm_start_refitting,
             use_input_warping=use_input_warping,
             num_samples=num_samples,
             warmup_steps=warmup_steps,
             thinning=thinning,
             max_tree_depth=max_tree_depth,
             disable_progbar=disable_progbar,
@@ -613,15 +611,14 @@
         #  int, Optional[Dict[int, float]], Optional[Callable[[Tensor], Tensor]],
         #  **(Any)], Tensor]`.
         acqf_optimizer: TOptimizer = scipy_optimizer,
         # TODO: Remove best_point_recommender for botorch_moo. Used in modelbridge._gen.
         best_point_recommender: TBestPointRecommender = recommend_best_observed_point,
         frontier_evaluator: TFrontierEvaluator = pareto_frontier_evaluator,
         refit_on_cv: bool = False,
-        refit_on_update: bool = True,
         warm_start_refitting: bool = False,
         use_input_warping: bool = False,
         num_samples: int = 256,
         warmup_steps: int = 512,
         thinning: int = 16,
         max_tree_depth: int = 6,
         # use_saas is deprecated. TODO: remove
@@ -640,15 +637,14 @@
             model_constructor=model_constructor,
             model_predictor=model_predictor,
             acqf_constructor=acqf_constructor,
             acqf_optimizer=acqf_optimizer,
             best_point_recommender=best_point_recommender,
             frontier_evaluator=frontier_evaluator,
             refit_on_cv=refit_on_cv,
-            refit_on_update=refit_on_update,
             warm_start_refitting=warm_start_refitting,
             use_input_warping=use_input_warping,
             num_samples=num_samples,
             warmup_steps=warmup_steps,
             thinning=thinning,
             max_tree_depth=max_tree_depth,
             disable_progbar=disable_progbar,
```

### Comparing `ax-platform-0.3.7/ax/models/torch/fully_bayesian_model_utils.py` & `ax-platform-0.4.0/ax/models/torch/fully_bayesian_model_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, List, Optional, Tuple
 
 import pyro
 import torch
 from ax.models.torch.botorch_defaults import _get_model
 from botorch.models.gp_regression import MIN_INFERRED_NOISE_LEVEL
 from botorch.models.gpytorch import GPyTorchModel
@@ -56,17 +58,19 @@
     if len(fidelity_features) > 0:
         raise NotImplementedError(
             "Fidelity MF-GP models are not currently supported with MCMC!"
         )
 
     num_mcmc_samples = num_samples // thinning
     covar_modules = [
-        _get_rbf_kernel(num_samples=num_mcmc_samples, dim=Xs[0].shape[-1])
-        if gp_kernel == "rbf"
-        else _get_matern_kernel(num_samples=num_mcmc_samples, dim=Xs[0].shape[-1])
+        (
+            _get_rbf_kernel(num_samples=num_mcmc_samples, dim=Xs[0].shape[-1])
+            if gp_kernel == "rbf"
+            else _get_matern_kernel(num_samples=num_mcmc_samples, dim=Xs[0].shape[-1])
+        )
         for _ in range(len(Xs))
     ]
 
     models = [
         _get_model(
             X=X.unsqueeze(0).expand(num_mcmc_samples, X.shape[0], -1),
             Y=Y.unsqueeze(0).expand(num_mcmc_samples, Y.shape[0], -1),
```

### Comparing `ax-platform-0.3.7/ax/models/torch/posterior_mean.py` & `ax-platform-0.4.0/ax/models/torch/posterior_mean.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from typing import Any, Optional, Tuple
 
 import torch
 from ax.models.torch.botorch_defaults import NO_FEASIBLE_POINTS_MESSAGE
 from ax.utils.common.typeutils import checked_cast
 from botorch.acquisition.acquisition import AcquisitionFunction
```

### Comparing `ax-platform-0.3.7/ax/models/torch/randomforest.py` & `ax-platform-0.4.0/ax/models/torch/randomforest.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Tuple
 
 import numpy as np
 import torch
 from ax.core.search_space import SearchSpaceDigest
```

### Comparing `ax-platform-0.3.7/ax/models/torch/rembo.py` & `ax-platform-0.4.0/ax/models/torch/rembo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from typing import Any, List, Optional, Tuple
 from warnings import warn
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.core.types import TCandidateMetadata
@@ -43,15 +45,15 @@
         A: Tensor,
         initial_X_d: Tensor,
         bounds_d: List[Tuple[float, float]],
         **kwargs: Any,
     ) -> None:
         warn(
             "REMBO is deprecated and does not guarantee correctness. "
-            "It will be removed in Ax 0.5.0.",
+            "It will be removed in Ax 0.3.9.",
             DeprecationWarning,
         )
         self.A = A
         # compute pseudo inverse once and cache it
         self._pinvA: Tensor = torch.pinverse(A)
         # Projected points in low-d space generated in the optimization
         self.X_d: List[Tensor] = list(initial_X_d)
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_acquisition.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_acquisition.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import dataclasses
 import itertools
 from contextlib import ExitStack
 from itertools import chain
 from typing import Any, Dict, Optional
@@ -82,14 +84,15 @@
     def evaluate(self, X: Tensor, **kwargs: Any) -> Tensor:
         return X.sum(dim=-1)
 
 
 class AcquisitionTest(TestCase):
     @fast_botorch_optimize
     def setUp(self) -> None:
+        super().setUp()
         qNEI_input_constructor = get_acqf_input_constructor(qNoisyExpectedImprovement)
         self.mock_input_constructor = mock.MagicMock(
             qNEI_input_constructor, side_effect=qNEI_input_constructor
         )
         # Adding wrapping here to be able to count calls and inspect arguments.
         _register_acqf_input_constructor(
             acqf_cls=DummyAcquisitionFunction,
@@ -159,17 +162,17 @@
             fixed_features=self.fixed_features,
         )
 
     def get_acquisition_function(
         self, fixed_features: Optional[Dict[int, float]] = None, one_shot: bool = False
     ) -> Acquisition:
         return Acquisition(
-            botorch_acqf_class=DummyOneShotAcquisitionFunction
-            if one_shot
-            else self.botorch_acqf_class,
+            botorch_acqf_class=(
+                DummyOneShotAcquisitionFunction if one_shot else self.botorch_acqf_class
+            ),
             surrogates={"surrogate": self.surrogate},
             search_space_digest=self.search_space_digest,
             torch_opt_config=dataclasses.replace(
                 self.torch_opt_config, fixed_features=fixed_features or {}
             ),
             options=self.options,
         )
@@ -293,15 +296,15 @@
                 ckwargs["constraints"],
                 self.constraints,
             )
             mock_get_outcome_constraint_transforms.assert_called_once_with(
                 outcome_constraints=self.outcome_constraints
             )
 
-    @mock.patch(f"{ACQUISITION_PATH}.optimize_acqf")
+    @mock.patch(f"{ACQUISITION_PATH}.optimize_acqf", return_value=(Mock(), Mock()))
     def test_optimize(self, mock_optimize_acqf: Mock) -> None:
         acquisition = self.get_acquisition_function(fixed_features=self.fixed_features)
         acquisition.optimize(
             n=3,
             search_space_digest=self.search_space_digest,
             inequality_constraints=self.inequality_constraints,
             fixed_features=self.fixed_features,
@@ -415,74 +418,95 @@
                 rounding_func=self.rounding_func,
             )
 
         # check this works without any fixed_feature specified
         # 2 candidates have acqf value 8, but [1, 3, 4] is pending and thus should
         # not be selected. [2, 3, 4] is the best point, but has already been picked
         acquisition = self.get_acquisition_function()
-        X_selected, _ = acquisition.optimize(
+        X_selected, _, weights = acquisition.optimize(
             n=2,
             search_space_digest=ssd1,
             rounding_func=self.rounding_func,
         )
         expected = torch.tensor([[2, 2, 4], [2, 3, 3]]).to(self.X)
         self.assertTrue(X_selected.shape == (2, 3))
         self.assertTrue(
             all((x.unsqueeze(0) == expected).all(dim=-1).any() for x in X_selected)
         )
+        self.assertTrue(torch.equal(weights, torch.ones(2)))
         # check with fixed feature
         # Since parameter 1 is fixed to 2, the best 3 candidates are
         # [4, 2, 4], [3, 2, 4], [4, 2, 3]
         ssd2 = SearchSpaceDigest(
             feature_names=["a", "b", "c"],
             # pyre-fixme[6]: For 2nd param expected `List[Tuple[Union[float, int],
             #  Union[float, int]]]` but got `List[Tuple[int, int, int]]`.
             bounds=[(0, 0, 0), (4, 4, 4)],
             categorical_features=[0, 1, 2],
             # pyre-fixme[6]: For 4th param expected `Dict[int, List[Union[float,
             #  int]]]` but got `Dict[int, List[int]]`.
             discrete_choices={k: [0, 1, 2, 3, 4] for k in range(3)},
         )
-        X_selected, _ = acquisition.optimize(
+        X_selected, _, weights = acquisition.optimize(
             n=3,
             search_space_digest=ssd2,
             fixed_features=self.fixed_features,
             rounding_func=self.rounding_func,
         )
         expected = torch.tensor([[4, 2, 4], [3, 2, 4], [4, 2, 3]]).to(self.X)
         self.assertTrue(X_selected.shape == (3, 3))
         self.assertTrue(
             all((x.unsqueeze(0) == expected).all(dim=-1).any() for x in X_selected)
         )
+        self.assertTrue(torch.equal(weights, torch.ones(3)))
         # check with a constraint that -1 * x[0]  -1 * x[1] >= 0 which should make
         # [0, 0, 4] the best candidate.
-        X_selected, _ = acquisition.optimize(
+        X_selected, _, weights = acquisition.optimize(
             n=1,
             search_space_digest=ssd2,
             rounding_func=self.rounding_func,
             inequality_constraints=[
                 (torch.tensor([0, 1], dtype=torch.int64), -torch.ones(2), 0)
             ],
         )
         expected = torch.tensor([[0, 0, 4]]).to(self.X)
         self.assertTrue(torch.equal(expected, X_selected))
+        self.assertTrue(torch.equal(weights, torch.tensor([1.0], dtype=self.X.dtype)))
         # Same thing but use two constraints instead
-        X_selected, _ = acquisition.optimize(
+        X_selected, _, weights = acquisition.optimize(
             n=1,
             search_space_digest=ssd2,
             rounding_func=self.rounding_func,
             inequality_constraints=[
                 (torch.tensor([0], dtype=torch.int64), -torch.ones(1), 0),
                 (torch.tensor([1], dtype=torch.int64), -torch.ones(1), 0),
             ],
         )
         expected = torch.tensor([[0, 0, 4]]).to(self.X)
         self.assertTrue(torch.equal(expected, X_selected))
+        self.assertTrue(torch.equal(weights, torch.tensor([1.0])))
+        # With no X_observed or X_pending.
+        acquisition = self.get_acquisition_function()
+        acquisition.X_observed, acquisition.X_pending = None, None
+        X_selected, _, weights = acquisition.optimize(
+            n=2,
+            search_space_digest=ssd1,
+            rounding_func=self.rounding_func,
+        )
+        self.assertTrue(torch.equal(weights, torch.ones(2)))
+        expected = torch.tensor([[1, 3, 4], [2, 3, 4]]).to(self.X)
+        self.assertTrue(X_selected.shape == (2, 3))
+        self.assertTrue(
+            all((x.unsqueeze(0) == expected).all(dim=-1).any() for x in X_selected)
+        )
 
-    @mock.patch(f"{ACQUISITION_PATH}.optimize_acqf_discrete_local_search")
+    @mock.patch(
+        f"{ACQUISITION_PATH}.optimize_acqf_discrete_local_search",
+        return_value=(Mock(), Mock()),
+    )
     def test_optimize_acqf_discrete_local_search(
         self,
         mock_optimize_acqf_discrete_local_search: Mock,
     ) -> None:
         tkwargs = {"dtype": self.X.dtype, "device": self.X.device}
         ssd = SearchSpaceDigest(
             feature_names=["a", "b", "c"],
@@ -518,15 +542,17 @@
         )
         X_avoid_true = torch.cat((self.X, self.pending_observations[0]), dim=0)
         self.assertEqual(kwargs["X_avoid"].shape, X_avoid_true.shape)
         self.assertTrue(  # The order of the rows may not match
             all((X_avoid_true == x).all(dim=-1).any().item() for x in kwargs["X_avoid"])
         )
 
-    @mock.patch(f"{ACQUISITION_PATH}.optimize_acqf_mixed")
+    @mock.patch(
+        f"{ACQUISITION_PATH}.optimize_acqf_mixed", return_value=(Mock(), Mock())
+    )
     def test_optimize_mixed(self, mock_optimize_acqf_mixed: Mock) -> None:
         tkwargs = {"dtype": self.X.dtype, "device": self.X.device}
         ssd = SearchSpaceDigest(
             feature_names=["a", "b"],
             bounds=[(0, 1), (0, 2)],
             categorical_features=[1],
             discrete_choices={1: [0, 1, 2]},
@@ -558,15 +584,17 @@
                 mock_optimize_acqf_mixed.call_args[1]["bounds"], expected_bounds
             )
         )
         # Check that we don't use mixed optimizer if force_use_optimize_acqf is True.
         mock_optimize_acqf_mixed.reset_mock()
         optimizer_options = self.optimizer_options.copy()
         optimizer_options["force_use_optimize_acqf"] = True
-        with mock.patch(f"{ACQUISITION_PATH}.optimize_acqf") as mock_optimize_acqf:
+        with mock.patch(
+            f"{ACQUISITION_PATH}.optimize_acqf", return_value=(Mock(), Mock())
+        ) as mock_optimize_acqf:
             acquisition.optimize(
                 n=3,
                 search_space_digest=ssd,
                 inequality_constraints=self.inequality_constraints,
                 fixed_features=None,
                 rounding_func=self.rounding_func,
                 optimizer_options=optimizer_options,
@@ -610,15 +638,22 @@
         wraps=lambda y: y,
     )
     @mock.patch(f"{ACQUISITION_PATH}._get_X_pending_and_observed")
     def test_init_moo(
         self,
         mock_get_X: Mock,
         _,
+        with_no_X_observed: bool = False,
+        with_outcome_constraints: bool = True,
     ) -> None:
+        acqf_class = (
+            DummyAcquisitionFunction
+            if with_no_X_observed
+            else qNoisyExpectedHypervolumeImprovement
+        )
         moo_training_data = [
             SupervisedDataset(
                 X=self.X,
                 Y=self.Y.repeat(1, 3),
                 feature_names=self.feature_names,
                 outcome_names=["m1", "m2", "m3"],
             )
@@ -627,29 +662,36 @@
         moo_objective_thresholds = torch.tensor(
             [0.5, 1.5, float("nan")], **self.tkwargs
         )
         self.surrogate.fit(
             datasets=moo_training_data,
             search_space_digest=self.search_space_digest,
         )
-        mock_get_X.return_value = (self.pending_observations[0], self.X[:1])
+        if with_no_X_observed:
+            mock_get_X.return_value = (self.pending_observations[0], None)
+        else:
+            mock_get_X.return_value = (self.pending_observations[0], self.X[:1])
         outcome_constraints = (
-            torch.tensor([[1.0, 0.0, 0.0]], **self.tkwargs),
-            torch.tensor([[10.0]], **self.tkwargs),
+            (
+                torch.tensor([[1.0, 0.0, 0.0]], **self.tkwargs),
+                torch.tensor([[10.0]], **self.tkwargs),
+            )
+            if with_outcome_constraints
+            else None
         )
 
         torch_opt_config = dataclasses.replace(
             self.torch_opt_config,
             objective_weights=moo_objective_weights,
             outcome_constraints=outcome_constraints,
             objective_thresholds=moo_objective_thresholds,
         )
         acquisition = Acquisition(
             surrogates={"surrogate": self.surrogate},
-            botorch_acqf_class=qNoisyExpectedHypervolumeImprovement,
+            botorch_acqf_class=acqf_class,
             search_space_digest=self.search_space_digest,
             torch_opt_config=torch_opt_config,
             options=self.options,
         )
         self.assertTrue(
             torch.equal(
                 moo_objective_thresholds[:2],
@@ -676,41 +718,53 @@
                         samples=preds,
                     ),
                 )
             )
             acquisition = Acquisition(
                 surrogates={"surrogate": self.surrogate},
                 search_space_digest=self.search_space_digest,
-                botorch_acqf_class=qNoisyExpectedHypervolumeImprovement,
+                botorch_acqf_class=acqf_class,
                 torch_opt_config=dataclasses.replace(
                     torch_opt_config,
                     objective_thresholds=None,
                 ),
                 options=self.options,
             )
-            self.assertTrue(
-                torch.equal(
-                    acquisition.objective_thresholds[:2],
-                    torch.tensor([9.9, 3.3], **self.tkwargs),
+            if with_no_X_observed:
+                self.assertIsNone(acquisition.objective_thresholds)
+            else:
+                self.assertTrue(
+                    torch.equal(
+                        acquisition.objective_thresholds[:2],
+                        torch.tensor([9.9, 3.3], **self.tkwargs),
+                    )
                 )
-            )
-            self.assertTrue(np.isnan(acquisition.objective_thresholds[2].item()))
+                self.assertTrue(np.isnan(acquisition.objective_thresholds[2].item()))
             # With partial thresholds.
             acquisition = Acquisition(
                 surrogates={"surrogate": self.surrogate},
                 search_space_digest=self.search_space_digest,
-                botorch_acqf_class=qNoisyExpectedHypervolumeImprovement,
+                botorch_acqf_class=acqf_class,
                 torch_opt_config=dataclasses.replace(
                     torch_opt_config,
                     objective_thresholds=torch.tensor(
                         [float("nan"), 5.5, float("nan")], **self.tkwargs
                     ),
                 ),
                 options=self.options,
             )
-            self.assertTrue(
-                torch.equal(
-                    acquisition.objective_thresholds[:2],
-                    torch.tensor([9.9, 5.5], **self.tkwargs),
+            if with_no_X_observed:
+                # Thresholds are not updated.
+                self.assertEqual(acquisition.objective_thresholds[1].item(), 5.5)
+                self.assertTrue(np.isnan(acquisition.objective_thresholds[0].item()))
+                self.assertTrue(np.isnan(acquisition.objective_thresholds[2].item()))
+            else:
+                self.assertTrue(
+                    torch.equal(
+                        acquisition.objective_thresholds[:2],
+                        torch.tensor([9.9, 5.5], **self.tkwargs),
+                    )
                 )
-            )
-            self.assertTrue(np.isnan(acquisition.objective_thresholds[2].item()))
+                self.assertTrue(np.isnan(acquisition.objective_thresholds[2].item()))
+
+    def test_init_no_X_observed(self) -> None:
+        self.test_init_moo(with_no_X_observed=True, with_outcome_constraints=False)
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_covar_modules_argparse.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_covar_modules_argparse.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from unittest.mock import patch
 
 import torch
 from ax.models.torch.botorch_modular.input_constructors.covar_modules import (
     covar_module_argparse,
@@ -108,24 +110,24 @@
                 ScaleMaternKernel,
                 botorch_model_class=botorch_model_class,
                 dataset=self.dataset,
                 lengthscale_prior=GammaPrior(6.0, 3.0),
                 outputscale_prior=GammaPrior(2, 0.15),
             )
 
-            covar_module_kwargs[
-                "lengthscale_prior_concentration"
-            ] = covar_module_kwargs["lengthscale_prior"].concentration.item()
+            covar_module_kwargs["lengthscale_prior_concentration"] = (
+                covar_module_kwargs["lengthscale_prior"].concentration.item()
+            )
             covar_module_kwargs["lengthscale_prior_rate"] = covar_module_kwargs[
                 "lengthscale_prior"
             ].rate.item()
 
-            covar_module_kwargs[
-                "outputscale_prior_concentration"
-            ] = covar_module_kwargs["outputscale_prior"].concentration.item()
+            covar_module_kwargs["outputscale_prior_concentration"] = (
+                covar_module_kwargs["outputscale_prior"].concentration.item()
+            )
             covar_module_kwargs["outputscale_prior_rate"] = covar_module_kwargs[
                 "outputscale_prior"
             ].rate.item()
 
             covar_module_kwargs.pop("lengthscale_prior")
             covar_module_kwargs.pop("outputscale_prior")
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_input_transform_argparse.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_input_transform_argparse.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from unittest.mock import patch
 
 import numpy as np
 
 import torch
@@ -28,14 +30,15 @@
 
 class DummyInputTransform(InputTransform):  # pyre-ignore [13]
     pass
 
 
 class InputTransformArgparseTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         X = torch.randn((10, 4))
         Y = torch.randn((10, 2))
         self.dataset = SupervisedDataset(
             X=X,
             Y=Y,
             feature_names=[f"x{i}" for i in range(4)],
             outcome_names=[f"y{i}" for i in range(2)],
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_kernels.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_kernels.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from itertools import product
 
 import torch
 from ax.exceptions.core import AxError
 from ax.models.torch.botorch_modular.kernels import ScaleMaternKernel, TemporalKernel
@@ -55,17 +57,17 @@
                 lengthscale_prior=ls_prior,
                 outputscale_prior=os_prior,
                 temporal_lengthscale_prior=tls_prior,
                 period_length_prior=pl_prior if fixed_period_length is None else None,
                 fixed_period_length=fixed_period_length,
                 lengthscale_constraint=ls_constraint,
                 outputscale_constraint=os_constraint,
-                period_length_constraint=pl_constraint
-                if fixed_period_length is None
-                else None,
+                period_length_constraint=(
+                    pl_constraint if fixed_period_length is None else None
+                ),
                 temporal_lengthscale_constraint=tls_constraint,
             )
             self.assertTrue(isinstance(covar.base_kernel.kernels[0], MaternKernel))
             self.assertTrue(isinstance(covar.base_kernel.kernels[1], PeriodicKernel))
             matern, periodic = covar.base_kernel.kernels
 
             self.assertEqual(matern.ard_num_dims, matern_ard_num_dims)
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_model.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
+from collections import OrderedDict
 from contextlib import ExitStack
 from copy import deepcopy
-from typing import Dict, OrderedDict, Type
+from typing import Dict, Type
 from unittest import mock
 from unittest.mock import Mock
 
 import numpy as np
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.exceptions.core import AxWarning, UnsupportedError
@@ -63,14 +66,15 @@
 ACQ_OPTIONS: Dict[str, SobolQMCNormalSampler] = {
     Keys.SAMPLER: SobolQMCNormalSampler(sample_shape=torch.Size([1024]))
 }
 
 
 class BoTorchModelTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.botorch_model_class = SingleTaskGP
         self.surrogate = Surrogate(botorch_model_class=self.botorch_model_class)
         self.acquisition_class = Acquisition
         self.botorch_acqf_class = qExpectedImprovement
         self.acquisition_options = ACQ_OPTIONS
         self.model = BoTorchModel(
             surrogate=self.surrogate,
@@ -180,28 +184,25 @@
         self.assertEqual(model.acquisition_class, Acquisition)
         # Model that specifies `botorch_acqf_class`.
         model = BoTorchModel(botorch_acqf_class=qExpectedImprovement)
         self.assertEqual(model.acquisition_class, Acquisition)
         self.assertEqual(model.botorch_acqf_class, qExpectedImprovement)
 
         # Check defaults for refitting settings.
-        self.assertTrue(model.refit_on_update)
         self.assertFalse(model.refit_on_cv)
         self.assertTrue(model.warm_start_refit)
 
         # Check setting non-default refitting settings
         mdl2 = BoTorchModel(
             surrogate=self.surrogate,
             acquisition_class=self.acquisition_class,
             acquisition_options=self.acquisition_options,
-            refit_on_update=False,
             refit_on_cv=True,
             warm_start_refit=False,
         )
-        self.assertFalse(mdl2.refit_on_update)
         self.assertTrue(mdl2.refit_on_cv)
         self.assertFalse(mdl2.warm_start_refit)
 
     def test_surrogates_property(self) -> None:
         self.assertEqual(self.surrogate, list(self.model.surrogates.values())[0])
 
     def test_Xs_property(self) -> None:
@@ -390,15 +391,14 @@
             datasets=self.block_design_training_data,
             search_space_digest=self.mf_search_space_digest,
             candidate_metadata=self.candidate_metadata,
         )
 
         old_surrogate = self.model.surrogates[Keys.ONLY_SURROGATE]
         old_surrogate._model = mock.MagicMock()
-        # pyre-ignore [29]: T168826187
         old_surrogate._model.state_dict.return_value = OrderedDict({"key": "val"})
 
         for refit_on_cv, warm_start_refit in [
             (True, True),
             (True, False),
             (False, True),
         ]:
@@ -476,16 +476,17 @@
             qLogNEI_input_constructor, side_effect=qLogNEI_input_constructor
         )
         _register_acqf_input_constructor(
             acqf_cls=qLogNoisyExpectedImprovement,
             input_constructor=mock_input_constructor,
         )
         mock_optimize.return_value = (
-            torch.tensor([1.0]),
+            torch.tensor([[1.0]]),
             torch.tensor([2.0]),
+            torch.tensor([1.0]),
         )
         surrogate = Surrogate(botorch_model_class=botorch_model_class)
         model = BoTorchModel(
             surrogate=surrogate,
             acquisition_class=Acquisition,
             acquisition_options=self.acquisition_options,
         )
@@ -818,16 +819,16 @@
         for submodel in model_list.models:
             # There are fidelity features and nonempty Yvars, so
             # MFGP should be chosen.
             self.assertIsInstance(submodel, SingleTaskMultiFidelityGP)
 
     @mock.patch(
         f"{ACQUISITION_PATH}.Acquisition.optimize",
-        # Dummy candidates and acquisition function value.
-        return_value=(torch.tensor([[2.0]]), torch.tensor([1.0])),
+        # Dummy candidates, acquisition value, and weights
+        return_value=(torch.tensor([[2.0]]), torch.tensor([1.0]), torch.tensor([1.0])),
     )
     def test_MOO(self, _) -> None:
         # Add mock for qLogNEHVI input constructor to catch arguments passed to it.
         qLogNEHVI_input_constructor = get_acqf_input_constructor(
             qLogNoisyExpectedHypervolumeImprovement
         )
         mock_input_constructor = mock.MagicMock(
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_multi_fidelity.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_multi_fidelity.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 from unittest.mock import Mock, patch
 
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.models.torch.botorch_modular.acquisition import Acquisition
 from ax.models.torch.botorch_modular.multi_fidelity import MultiFidelityAcquisition
@@ -27,14 +29,15 @@
     f"{qMultiFidelityKnowledgeGradient.__module__}.qMultiFidelityKnowledgeGradient"
 )
 
 
 class MultiFidelityAcquisitionTest(TestCase):
     @fast_botorch_optimize
     def setUp(self) -> None:
+        super().setUp()
         self.botorch_model_class = SingleTaskMultiFidelityGP
         self.surrogate = Surrogate(botorch_model_class=self.botorch_model_class)
         self.X = torch.tensor([[1.0, 2.0, 3.0], [2.0, 3.0, 4.0]])
         self.Y = torch.tensor([[3.0], [4.0]])
         self.Yvar = torch.tensor([[0.0], [2.0]])
         self.feature_names = ["a", "b", "c"]
         self.metric_names = ["metric"]
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_optimizer_argparse.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_optimizer_argparse.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from importlib import reload
 from unittest.mock import patch
 
 from ax.models.torch.botorch_modular import optimizer_argparse as Argparse
 from ax.models.torch.botorch_modular.optimizer_argparse import (
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_outcome_transform_argparse.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_outcome_transform_argparse.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import torch
 from ax.models.torch.botorch_modular.input_constructors.outcome_transform import (
     outcome_transform_argparse,
 )
 from ax.utils.common.testutils import TestCase
 from botorch.models.transforms.outcome import OutcomeTransform, Standardize
 from botorch.utils.datasets import SupervisedDataset
@@ -14,14 +16,15 @@
 
 class DummyOutcomeTransform(OutcomeTransform):
     pass
 
 
 class OutcomeTransformArgparseTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         X = torch.randn((10, 4))
         Y = torch.randn((10, 1))
         self.dataset = SupervisedDataset(
             X=X,
             Y=Y,
             feature_names=["chicken", "eggs", "pigeons", "bunnies"],
             outcome_names=["farm"],
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_sebo.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_sebo.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import dataclasses
 import functools
 from typing import Any, Dict, Optional, Union
 from unittest import mock
 from unittest.mock import Mock
@@ -41,14 +43,15 @@
 CURRENT_PATH: str = __name__
 SURROGATE_PATH: str = Surrogate.__module__
 
 
 class TestSebo(TestCase):
     @fast_botorch_optimize
     def setUp(self) -> None:
+        super().setUp()
         tkwargs: Dict[str, Any] = {"dtype": torch.double}
         self.botorch_model_class = SingleTaskGP
         self.surrogates = Surrogate(botorch_model_class=self.botorch_model_class)
         self.X = torch.tensor([[1.0, 2.0, 3.0], [2.0, 3.0, 4.0]], **tkwargs)
         self.target_point = torch.tensor([1.0, 1.0, 1.0], **tkwargs)
         self.Y = torch.tensor([[3.0], [4.0]], **tkwargs)
         self.Yvar = torch.tensor([[0.0], [2.0]], **tkwargs)
@@ -238,25 +241,26 @@
         )
         mock_homotopy.return_value = Homotopy(homotopy_parameters=[hp])
 
         search_space_digest = SearchSpaceDigest(
             feature_names=["a"],
             bounds=[(-10.0, 5.0)],
         )
-        candidate, acqf_val = acquisition._optimize_with_homotopy(
+        candidate, acqf_val, weights = acquisition._optimize_with_homotopy(
             n=1,
             search_space_digest=search_space_digest,
             optimizer_options={
                 "num_restarts": 2,
                 "sequential": True,
                 "raw_samples": 16,
             },
         )
         self.assertEqual(candidate, torch.zeros(1, **tkwargs))
         self.assertEqual(acqf_val, 5 * torch.ones(1, **tkwargs))
+        self.assertEqual(weights, torch.ones(1, **tkwargs))
 
     @mock.patch(f"{SEBOACQUISITION_PATH}.optimize_acqf_homotopy")
     def test_optimize_l0(self, mock_optimize_acqf_homotopy: Mock) -> None:
         mock_optimize_acqf_homotopy.return_value = (
             torch.tensor([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]], dtype=torch.double),
             torch.tensor([1.0, 2.0], dtype=torch.double),
         )
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_surrogate.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_surrogate.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 import math
-from typing import Any, Dict, OrderedDict, Tuple, Type
+from collections import OrderedDict
+from typing import Any, Dict, Tuple, Type
 from unittest.mock import MagicMock, Mock, patch
 
 import numpy as np
 import torch
 from ax.core.search_space import RobustSearchSpaceDigest, SearchSpaceDigest
 from ax.exceptions.core import UserInputError
 from ax.models.torch.botorch_modular.acquisition import Acquisition
@@ -125,14 +128,15 @@
             )
             self.assertEqual(model_kwargs["fidelity_features"], [0])
             self.assertEqual(model_kwargs["categorical_features"], [1])
 
 
 class SurrogateTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.device = torch.device("cpu")
         self.dtype = torch.float
         self.tkwargs = {"device": self.device, "dtype": self.dtype}
         (
             self.Xs,
             self.Ys,
             self.Yvars,
@@ -568,15 +572,19 @@
                 ):
                     self.assertTrue(generic_equals(ckwargs[attr], getattr(self, attr)))
 
     @fast_botorch_optimize
     @patch(f"{ACQUISITION_PATH}.Acquisition.__init__", return_value=None)
     @patch(
         f"{ACQUISITION_PATH}.Acquisition.optimize",
-        return_value=([torch.tensor([0.0])], [torch.tensor([1.0])]),
+        return_value=(
+            torch.tensor([[0.0]]),
+            torch.tensor([1.0]),
+            torch.tensor([1.0]),
+        ),
     )
     @patch(
         f"{SURROGATE_PATH}.pick_best_out_of_sample_point_acqf_class",
         return_value=(qSimpleRegret, {Keys.SAMPLER: SobolQMCNormalSampler}),
     )
     def test_best_out_of_sample_point(
         self,
@@ -609,15 +617,15 @@
                 surrogates={"self": surrogate},
                 botorch_acqf_class=qSimpleRegret,
                 search_space_digest=self.search_space_digest,
                 torch_opt_config=torch_opt_config,
                 options={Keys.SAMPLER: SobolQMCNormalSampler},
             )
             self.assertTrue(torch.equal(candidate, torch.tensor([0.0])))
-            self.assertTrue(torch.equal(acqf_value, torch.tensor([1.0])))
+            self.assertTrue(torch.equal(acqf_value, torch.tensor(1.0)))
 
     def test_serialize_attributes_as_kwargs(self) -> None:
         for botorch_model_class in [SaasFullyBayesianSingleTaskGP, SingleTaskGP]:
             surrogate, _ = self._get_surrogate(botorch_model_class=botorch_model_class)
             expected = {
                 k: v for k, v in surrogate.__dict__.items() if not k.startswith("_")
             }
@@ -722,14 +730,15 @@
                 for m in checked_cast(ModelListGP, surrogate.model).models
             )
         )
 
 
 class SurrogateWithModelListTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.outcomes = ["outcome_1", "outcome_2"]
         self.mll_class = ExactMarginalLogLikelihood
         self.dtype = torch.double
         self.task_features = [0]
         Xs1, Ys1, Yvars1, self.bounds, _, self.feature_names, _ = get_torch_test_data(
             dtype=self.dtype, task_features=self.task_features, offset=1.0
         )
@@ -821,17 +830,19 @@
         self, mock_MTGP_construct_inputs: Mock, mock_fit: Mock
     ) -> None:
         self.surrogate.model_options.update({"output_tasks": [2]})
         for fixed_noise in (False, True):
             mock_fit.reset_mock()
             mock_MTGP_construct_inputs.reset_mock()
             self.surrogate.fit(
-                datasets=self.fixed_noise_training_data
-                if fixed_noise
-                else self.supervised_training_data,
+                datasets=(
+                    self.fixed_noise_training_data
+                    if fixed_noise
+                    else self.supervised_training_data
+                ),
                 search_space_digest=dataclasses.replace(
                     self.multi_task_search_space_digest,
                     task_features=self.task_features,
                 ),
             )
             # Should construct inputs for MTGP twice.
             self.assertEqual(len(mock_MTGP_construct_inputs.call_args_list), 2)
@@ -940,15 +951,14 @@
                 self.assertTrue(isinstance(surrogate.model, ModelListGP))
             mock_MLL.reset_mock()
             mock_fit_gpytorch.reset_mock()
             mock_fit_nuts.reset_mock()
 
             # Should `load_state_dict` when `state_dict` is not `None`
             # and `refit` is `False`.
-            # pyre-ignore [29]: T168826187
             state_dict = OrderedDict({"state_attribute": torch.ones(2)})
             surrogate._submodels = {}  # Prevent re-use of fitted model.
             surrogate.fit(
                 datasets=[self.ds1, self.ds3],
                 search_space_digest=search_space_digest,
                 refit=False,
                 state_dict=state_dict,
```

### Comparing `ax-platform-0.3.7/ax/models/torch/tests/test_utils.py` & `ax-platform-0.4.0/ax/models/torch/tests/test_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
-from typing import OrderedDict
+from collections import OrderedDict
 
 import numpy as np
 import torch
 from ax.core.search_space import SearchSpaceDigest
 from ax.exceptions.core import AxError, AxWarning, UnsupportedError
 from ax.models.torch.botorch_modular.utils import (
     _get_shared_rows,
@@ -39,14 +41,15 @@
 from botorch.models.model_list_gp_regression import ModelListGP
 from botorch.models.multitask import MultiTaskGP
 from botorch.utils.datasets import SupervisedDataset
 
 
 class BoTorchModelUtilsTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.dtype = torch.float
         (
             self.Xs,
             self.Ys,
             self.Yvars,
             _,
             _,
@@ -608,15 +611,14 @@
             ),
         )
 
     def test_subset_state_dict(self) -> None:
         m0 = SingleTaskGP(train_X=torch.rand(5, 2), train_Y=torch.rand(5, 1))
         m1 = SingleTaskGP(train_X=torch.rand(5, 2), train_Y=torch.rand(5, 1))
         model_list = ModelListGP(m0, m1)
-        # pyre-ignore [6]: T168826187
         model_list_state_dict = checked_cast(OrderedDict, model_list.state_dict())
         # Subset the model dict from model list and check that it is correct.
         m0_state_dict = model_list.models[0].state_dict()
         subsetted_m0_state_dict = subset_state_dict(
             state_dict=model_list_state_dict, submodel_index=0
         )
         self.assertEqual(m0_state_dict.keys(), subsetted_m0_state_dict.keys())
```

### Comparing `ax-platform-0.3.7/ax/models/torch/utils.py` & `ax-platform-0.4.0/ax/models/torch/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from dataclasses import dataclass
 from logging import Logger
 from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Type
 
 import numpy as np
 import torch
 from ax.exceptions.core import UnsupportedError
@@ -93,14 +95,15 @@
 def _filter_X_observed(
     Xs: List[Tensor],
     objective_weights: Tensor,
     bounds: List[Tuple[float, float]],
     outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,
     linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,
     fixed_features: Optional[Dict[int, float]] = None,
+    fit_out_of_design: bool = False,
 ) -> Optional[Tensor]:
     r"""Filter input points to those appearing in objective or constraints.
 
     Args:
         Xs: The input tensors of a model.
         objective_weights: The objective is to maximize a weighted sum of
             the columns of f(x). These are the weights.
@@ -109,44 +112,48 @@
             and m outputs at f(x), A is (k x m) and b is (k x 1) such that
             A f(x) <= b. (Not used by single task models)
         linear_constraints: A tuple of (A, b). For k linear constraints on
             d-dimensional x, A is (k x d) and b is (k x 1) such that
             A x <= b. (Not used by single task models)
         fixed_features: A map {feature_index: value} for features that
             should be fixed to a particular value during generation.
+        fit_out_of_design: If specified, all training data is returned.
+            Otherwise, only in design points are returned.
 
     Returns:
         Tensor: All points that are feasible and appear in the objective or
             the constraints. None if there are no such points.
     """
     # Get points observed for all objective and constraint outcomes
     X_obs = get_observed(
         Xs=Xs,
         objective_weights=objective_weights,
         outcome_constraints=outcome_constraints,
     )
-    # Filter to those that satisfy constraints.
-    X_obs = filter_constraints_and_fixed_features(
-        X=X_obs,
-        bounds=bounds,
-        linear_constraints=linear_constraints,
-        fixed_features=fixed_features,
-    )
+    if not fit_out_of_design:
+        # Filter to those that satisfy constraints.
+        X_obs = filter_constraints_and_fixed_features(
+            X=X_obs,
+            bounds=bounds,
+            linear_constraints=linear_constraints,
+            fixed_features=fixed_features,
+        )
     if len(X_obs) > 0:
         return torch.as_tensor(X_obs)  # please the linter
 
 
 def _get_X_pending_and_observed(
     Xs: List[Tensor],
     objective_weights: Tensor,
     bounds: List[Tuple[float, float]],
     pending_observations: Optional[List[Tensor]] = None,
     outcome_constraints: Optional[Tuple[Tensor, Tensor]] = None,
     linear_constraints: Optional[Tuple[Tensor, Tensor]] = None,
     fixed_features: Optional[Dict[int, float]] = None,
+    fit_out_of_design: bool = False,
 ) -> Tuple[Optional[Tensor], Optional[Tensor]]:
     r"""Get pending and observed points.
 
     If all points would otherwise be filtered, remove `linear_constraints`
     and `fixed_features` from filter and retry.
 
     Args:
@@ -161,14 +168,16 @@
             and m outputs at f(x), A is (k x m) and b is (k x 1) such that
             A f(x) <= b. (Not used by single task models)
         linear_constraints: A tuple of (A, b). For k linear constraints on
             d-dimensional x, A is (k x d) and b is (k x 1) such that
             A x <= b. (Not used by single task models)
         fixed_features: A map {feature_index: value} for features that
             should be fixed to a particular value during generation.
+        fit_out_of_design: If specified, all training data is returned.
+            Otherwise, only in design points are returned.
 
     Returns:
         Tensor: Pending points that are feasible and appear in the objective or
             the constraints. None if there are no such points.
         Tensor: Observed points that are feasible and appear in the objective or
             the constraints. None if there are no such points.
     """
@@ -186,23 +195,25 @@
     filtered_X_observed = _filter_X_observed(
         Xs=Xs,
         objective_weights=objective_weights,
         outcome_constraints=outcome_constraints,
         bounds=bounds,
         linear_constraints=linear_constraints,
         fixed_features=fixed_features,
+        fit_out_of_design=fit_out_of_design,
     )
     if filtered_X_observed is not None and len(filtered_X_observed) > 0:
         return X_pending, filtered_X_observed
     else:
         unfiltered_X_observed = _filter_X_observed(
             Xs=Xs,
             objective_weights=objective_weights,
             bounds=bounds,
             outcome_constraints=outcome_constraints,
+            fit_out_of_design=fit_out_of_design,
         )
         return X_pending, unfiltered_X_observed
 
 
 def _generate_sobol_points(
     n_sobol: int,
     bounds: List[Tuple[float, float]],
@@ -468,17 +479,17 @@
         return _get_weighted_mo_objective(objective_weights=objective_weights), None
     if outcome_constraints:
         if X_observed is None:
             raise UnsupportedError(
                 "X_observed is required to construct a constrained BoTorch objective."
             )
         # If there are outcome constraints, we use MC Acquisition functions.
-        obj_tf: Callable[
-            [Tensor, Optional[Tensor]], Tensor
-        ] = get_objective_weights_transform(objective_weights)
+        obj_tf: Callable[[Tensor, Optional[Tensor]], Tensor] = (
+            get_objective_weights_transform(objective_weights)
+        )
 
         def objective(samples: Tensor, X: Optional[Tensor] = None) -> Tensor:
             return obj_tf(samples, X)
 
         # SampleReducingMCAcquisitionFunctions take care of the constraint handling
         # directly, and the constraints get passed in the constructor of an MBM
         # Acquisition object.
```

### Comparing `ax-platform-0.3.7/ax/models/torch_base.py` & `ax-platform-0.4.0/ax/models/torch_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import dataclass, field
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from ax.core.metric import Metric
@@ -82,14 +84,15 @@
     fixed_features: Optional[Dict[int, float]] = None
     pending_observations: Optional[List[Tensor]] = None
     model_gen_options: TConfig = field(default_factory=dict)
     rounding_func: Optional[Callable[[Tensor], Tensor]] = None
     opt_config_metrics: Optional[Dict[str, Metric]] = None
     is_moo: bool = False
     risk_measure: Optional[RiskMeasureMCObjective] = None
+    fit_out_of_design: bool = False
 
 
 @dataclass(frozen=True)
 class TorchGenResults:
     """
     points: (n x d) Tensor of generated points.
     weights: n-tensor of weights for each point.
```

### Comparing `ax-platform-0.3.7/ax/models/types.py` & `ax-platform-0.4.0/ax/models/types.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from typing import Any, Dict, Union
+# pyre-strict
+
+from typing import Any, Dict, List, Union
 
 from ax.core.optimization_config import OptimizationConfig
 from ax.models.winsorization_config import WinsorizationConfig
 from botorch.acquisition import AcquisitionFunction
 
 # pyre-ignore [33]: `TConfig` cannot alias to a type containing `Any`.
 TConfig = Dict[
     str,
     Union[
         int,
         float,
         str,
         AcquisitionFunction,
+        List[str],
         Dict[int, Any],
         Dict[str, Any],
         OptimizationConfig,
         WinsorizationConfig,
         None,
     ],
 ]
```

### Comparing `ax-platform-0.3.7/ax/models/winsorization_config.py` & `ax-platform-0.4.0/ax/models/winsorization_config.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from dataclasses import dataclass
 from typing import Optional
 
 
 @dataclass
 class WinsorizationConfig:
     """Dataclass for storing Winsorization configuration parameters
```

### Comparing `ax-platform-0.3.7/ax/plot/bandit_rollout.py` & `ax-platform-0.4.0/ax/plot/bandit_rollout.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, List
 
 import plotly.graph_objs as go
 from ax.core.batch_trial import BatchTrial
 from ax.core.experiment import Experiment
 from ax.plot.base import AxPlotConfig, AxPlotTypes
 from ax.plot.color import MIXED_SCALE, rgba
```

### Comparing `ax-platform-0.3.7/ax/plot/base.py` & `ax-platform-0.4.0/ax/plot/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 import json
 from typing import Any, Dict, List, NamedTuple, Optional, Union
 
 from ax.core.types import TParameterization
 from ax.utils.common.serialization import named_tuple_to_dict
 from plotly import utils
```

### Comparing `ax-platform-0.3.7/ax/plot/color.py` & `ax-platform-0.4.0/ax/plot/color.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 from numbers import Real
 from typing import List, Tuple
 
 # type aliases
 TRGB = Tuple[Real, ...]
```

### Comparing `ax-platform-0.3.7/ax/plot/contour.py` & `ax-platform-0.4.0/ax/plot/contour.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import re
 from copy import deepcopy
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, List, Optional, Tuple
 
 import numpy as np
 import plotly.graph_objs as go
 from ax.core.observation import ObservationFeatures
 from ax.modelbridge.base import ModelBridge
 from ax.plot.base import AxPlotConfig, AxPlotTypes, PlotData
 from ax.plot.color import BLUE_SCALE, GREEN_PINK_SCALE, GREEN_SCALE
@@ -337,14 +339,15 @@
     generator_runs_dict: TNullableGeneratorRunsDict = None,
     relative: bool = False,
     density: int = 50,
     slice_values: Optional[Dict[str, Any]] = None,
     lower_is_better: bool = False,
     fixed_features: Optional[ObservationFeatures] = None,
     trial_index: Optional[int] = None,
+    parameters_to_use: Optional[List[str]] = None,
 ) -> go.Figure:
     """Create interactive plot with predictions for a 2-d slice of the parameter
     space.
 
     Args:
         model: ModelBridge that contains model for predictions
         metric_name: Name of metric to plot
@@ -356,14 +359,16 @@
             other parameters. If not provided, then the status quo values will
             be used if there is a status quo, otherwise the mean of numeric
             parameters or the mode of choice parameters.
         lower_is_better: Lower values for metric are better.
         fixed_features: An ObservationFeatures object containing the values of
             features (including non-parameter features like context) to be set
             in the slice.
+        parameters_to_use: List of parameters to use in the plot, in the order they
+            should appear. If None or empty list, use all parameters.
 
     Returns:
         go.Figure: interactive plot of objective vs. parameters
     """
 
     # NOTE: This implements a hack to allow Plotly to specify two parameters
     # simultaneously. It is not possible within Plotly to specify a third,
@@ -372,14 +377,23 @@
 
     if trial_index is not None:
         if slice_values is None:
             slice_values = {}
         slice_values["TRIAL_PARAM"] = str(trial_index)
 
     range_parameters = get_range_parameters(model, min_num_values=5)
+    if parameters_to_use is not None:
+        if len(parameters_to_use) <= 1:
+            raise ValueError(
+                "Contour plots require two or more parameters. "
+                f"Got {parameters_to_use=}."
+            )
+        # Subset range parameters and put them in the same order as parameters_to_use.
+        range_param_name_dict = {p.name: p for p in range_parameters}
+        range_parameters = [range_param_name_dict[pname] for pname in parameters_to_use]
     plot_data, _, _ = get_plot_data(
         model, generator_runs_dict or {}, {metric_name}, fixed_features=fixed_features
     )
 
     # TODO T38563759: Sort parameters by feature importances
     param_names = [parameter.name for parameter in range_parameters]
 
@@ -880,14 +894,15 @@
     generator_runs_dict: TNullableGeneratorRunsDict = None,
     relative: bool = False,
     density: int = 50,
     slice_values: Optional[Dict[str, Any]] = None,
     lower_is_better: bool = False,
     fixed_features: Optional[ObservationFeatures] = None,
     trial_index: Optional[int] = None,
+    parameters_to_use: Optional[List[str]] = None,
 ) -> AxPlotConfig:
     """Create interactive plot with predictions for a 2-d slice of the parameter
     space.
 
     Args:
         model: ModelBridge that contains model for predictions
         metric_name: Name of metric to plot
@@ -899,14 +914,16 @@
             other parameters. If not provided, then the status quo values will
             be used if there is a status quo, otherwise the mean of numeric
             parameters or the mode of choice parameters.
         lower_is_better: Lower values for metric are better.
         fixed_features: An ObservationFeatures object containing the values of
             features (including non-parameter features like context) to be set
             in the slice.
+        parameters_to_use: List of parameters to use in the plot, in the order they
+            should appear. If None or empty list, use all parameters.
 
     Returns:
         AxPlotConfig: interactive plot of objective vs. parameters
     """
     return AxPlotConfig(
         data=interact_contour_plotly(
             model=model,
@@ -914,10 +931,11 @@
             generator_runs_dict=generator_runs_dict,
             relative=relative,
             density=density,
             slice_values=slice_values,
             lower_is_better=lower_is_better,
             fixed_features=fixed_features,
             trial_index=trial_index,
+            parameters_to_use=parameters_to_use,
         ),
         plot_type=AxPlotTypes.GENERIC,
     )
```

### Comparing `ax-platform-0.3.7/ax/plot/css/base.css` & `ax-platform-0.4.0/ax/plot/css/base.css`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/plot/diagnostic.py` & `ax-platform-0.4.0/ax/plot/diagnostic.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from typing import Any, Dict, List, Optional, Tuple
 
 import numpy as np
 import plotly.graph_objs as go
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
@@ -125,17 +127,16 @@
             if se_raw is not None
             else [0.0] * len(y_raw)
         )
 
         min_, max_ = _get_min_max_with_errors(y_raw, y_hat, se_raw, se_hat)
         if autoset_axis_limits:
             y_raw_np = np.array(y_raw)
-            # TODO: replace interpolation->method once it becomes standard.
-            q1 = np.nanpercentile(y_raw_np, q=25, interpolation="lower").min()
-            q3 = np.nanpercentile(y_raw_np, q=75, interpolation="higher").max()
+            q1 = np.nanpercentile(y_raw_np, q=25, method="lower").min()
+            q3 = np.nanpercentile(y_raw_np, q=75, method="higher").max()
             y_lower = q1 - 1.5 * (q3 - q1)
             y_upper = q3 + 1.5 * (q3 - q1)
             y_raw_np = y_raw_np.clip(y_lower, y_upper).tolist()
             min_robust, max_robust = _get_min_max_with_errors(
                 y_raw_np, y_hat, se_raw, se_hat
             )
             y_padding = 0.05 * (max_robust - min_robust)
```

### Comparing `ax-platform-0.3.7/ax/plot/feature_importances.py` & `ax-platform-0.4.0/ax/plot/feature_importances.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Dict, Optional, Union
 
 import numpy as np
 import pandas as pd
 import plotly.graph_objs as go
+from ax.core.parameter import ChoiceParameter
 from ax.exceptions.core import NoDataError
 from ax.modelbridge import ModelBridge
 from ax.plot.base import AxPlotConfig, AxPlotTypes
 from ax.plot.helper import compose_annotation
 from ax.utils.common.logger import get_logger
 from plotly import subplots
 
@@ -137,57 +140,77 @@
     if label_dict is not None:
         sensitivity_values = {  # pyre-ignore
             label_dict.get(metric_name, metric_name): v
             for metric_name, v in sensitivity_values.items()
         }
     traces = []
     dropdown = []
+    categorical_features = []
+    if model is not None:
+        categorical_features = [
+            name
+            for name, par in model.model_space.parameters.items()
+            if isinstance(par, ChoiceParameter) and not par.is_ordered
+        ]
+
     for i, metric_name in enumerate(sorted(sensitivity_values.keys())):
         importances = sensitivity_values[metric_name]
         factor_col = "Factor"
         importance_col = "Importance"
         sign_col = "Sign"
         error_plot = np.asarray(next(iter(importances.values()))).size > 1
         if error_plot:
             importance_col_se = "SE"
             df = pd.DataFrame(
                 [
                     {
                         factor_col: factor,
                         importance_col: np.asarray(importance)[0],
                         importance_col_se: np.asarray(importance)[2],
-                        sign_col: np.sign(np.asarray(importance)[0]).astype(int),
+                        sign_col: (
+                            0
+                            if factor in categorical_features
+                            else 2 * (np.asarray(importance)[0] >= 0).astype(int) - 1
+                        ),
                     }
                     for factor, importance in importances.items()
                 ]
             )
             df[importance_col] = df[importance_col].abs()
             df = df.sort_values(importance_col)
             error_x = {"type": "data", "array": df[importance_col_se], "visible": True}
 
         else:
             df = pd.DataFrame(
                 [
                     {
                         factor_col: factor,
                         importance_col: importance,
-                        sign_col: np.sign(importance).astype(int),
+                        sign_col: (
+                            0
+                            if factor in categorical_features
+                            else 2 * (importance >= 0).astype(int) - 1
+                        ),
                     }
                     for factor, importance in importances.items()
                 ]
             )
             df[importance_col] = df[importance_col].abs()
             df = df.sort_values(importance_col)
             error_x = None
         if relative:
             df[importance_col] = df[importance_col].div(df[importance_col].sum())
 
-        colors = {-1: "darkorange", 1: "steelblue"}
-        names = {-1: "Decreases metric", 1: "Increases metric"}
-        legend_counter = {-1: 0, 1: 0}
+        colors = {-1: "darkorange", 0: "gray", 1: "steelblue"}
+        names = {
+            -1: "Decreases metric",
+            0: "Affects metric (categorical choice)",
+            1: "Increases metric",
+        }
+        legend_counter = {-1: 0, 0: 0, 1: 0}
         all_positive = all(df[sign_col] >= 0)
         for _, row in df.iterrows():
             traces.append(
                 go.Bar(
                     name=names[row[sign_col]],
                     orientation="h",
                     visible=i == 0,
```

### Comparing `ax-platform-0.3.7/ax/plot/helper.py` & `ax-platform-0.4.0/ax/plot/helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,29 +1,25 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from collections import Counter
 
 from logging import Logger
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
 import numpy as np
 from ax.core.generator_run import GeneratorRun
 from ax.core.observation import Observation, ObservationFeatures
-from ax.core.parameter import (
-    ChoiceParameter,
-    FixedParameter,
-    Parameter,
-    ParameterType,
-    RangeParameter,
-)
+from ax.core.parameter import ChoiceParameter, FixedParameter, Parameter, RangeParameter
 from ax.core.types import TParameterization
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.prediction_utils import (
     _compute_scalarized_outcome,
     predict_at_point,
 )
 from ax.modelbridge.transforms.ivw import IVW
@@ -438,18 +434,15 @@
 
     Returns: List of RangeParameters.
     """
     return [
         parameter
         for parameter in parameters
         if isinstance(parameter, RangeParameter)
-        and (
-            parameter.parameter_type == ParameterType.FLOAT
-            or parameter.upper - parameter.lower + 1 >= min_num_values
-        )
+        and parameter.cardinality() >= min_num_values  # float has inf cardinality
     ]
 
 
 def get_range_parameters(
     model: ModelBridge, min_num_values: int = 0
 ) -> List[RangeParameter]:
     """
```

### Comparing `ax-platform-0.3.7/ax/plot/js/common/helpers.js` & `ax-platform-0.4.0/ax/plot/js/common/helpers.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/plot/marginal_effects.py` & `ax-platform-0.4.0/ax/plot/marginal_effects.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, List
 
 import pandas as pd
 import plotly.graph_objs as go
 from ax.modelbridge.base import ModelBridge
 from ax.plot.base import AxPlotConfig, AxPlotTypes, DECIMALS
 from ax.plot.helper import get_plot_data
```

### Comparing `ax-platform-0.3.7/ax/plot/parallel_coordinates.py` & `ax-platform-0.4.0/ax/plot/parallel_coordinates.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional
 
 import pandas as pd
 from ax.core.experiment import Experiment
 from ax.plot.base import AxPlotConfig, AxPlotTypes
 from ax.service.utils.report_utils import _get_shortest_unique_suffix_dict, exp_to_df
 from plotly import express as px, graph_objs as go
```

### Comparing `ax-platform-0.3.7/ax/plot/pareto_frontier.py` & `ax-platform-0.4.0/ax/plot/pareto_frontier.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
 from typing import Dict, Iterable, List, Optional, Tuple, Union
 
 import numpy as np
 import pandas as pd
 import plotly.graph_objs as go
 from ax.core.experiment import Experiment
```

### Comparing `ax-platform-0.3.7/ax/plot/pareto_utils.py` & `ax-platform-0.4.0/ax/plot/pareto_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-import copy
+# pyre-strict
+
 from copy import deepcopy
 from itertools import combinations
 from logging import Logger
-from typing import cast, Dict, List, NamedTuple, Optional, Tuple, Union
+from typing import Dict, List, NamedTuple, Optional, Tuple, Union
 
 import numpy as np
 import torch
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.metric import Metric
-from ax.core.objective import ScalarizedObjective
+from ax.core.objective import MultiObjective, ScalarizedObjective
 from ax.core.observation import ObservationFeatures
-from ax.core.optimization_config import (
-    MultiObjectiveOptimizationConfig,
-    OptimizationConfig,
-)
+from ax.core.optimization_config import MultiObjectiveOptimizationConfig
 from ax.core.outcome_constraint import (
     ComparisonOp,
     ObjectiveThreshold,
     OutcomeConstraint,
 )
 from ax.core.search_space import RobustSearchSpace, SearchSpace
 from ax.core.types import TParameterization
@@ -37,14 +35,15 @@
 )
 from ax.modelbridge.registry import Models
 from ax.modelbridge.torch import TorchModelBridge
 from ax.modelbridge.transforms.search_space_to_float import SearchSpaceToFloat
 from ax.models.torch.posterior_mean import get_PosteriorMean
 from ax.models.torch_base import TorchModel
 from ax.utils.common.logger import get_logger
+from ax.utils.common.typeutils import checked_cast
 from ax.utils.stats.statstools import relativize
 from botorch.utils.multi_objective import is_non_dominated
 from botorch.utils.multi_objective.hypervolume import infer_reference_point
 
 # type aliases
 Mu = Dict[str, List[float]]
 Cov = Dict[str, Dict[str, List[float]]]
@@ -573,15 +572,15 @@
     optimization_config = MultiObjectiveOptimizationConfig(
         objective=obj, outcome_constraints=outcome_constraints
     )
     return optimization_config
 
 
 def infer_reference_point_from_experiment(
-    experiment: Experiment,
+    experiment: Experiment, data: Data
 ) -> List[ObjectiveThreshold]:
     """This functions is a wrapper around ``infer_reference_point`` to find the nadir
     point from the pareto front of an experiment. Aside from converting experiment
     to tensors, this wrapper transforms back and forth the objectives of the experiment
     so that they are appropriately used by ``infer_reference_point``.
 
     Args:
@@ -594,33 +593,47 @@
         raise ValueError(
             "This function works for MOO experiments only."
             f" Experiment {experiment.name} is single objective."
         )
 
     # Reading experiment data.
     mb_reference = get_tensor_converter_model(
-        experiment=experiment, data=experiment.fetch_data()
+        experiment=experiment,
+        data=data,
     )
     obs_feats, obs_data, _ = _get_modelbridge_training_data(modelbridge=mb_reference)
 
     # Since objectives could have arbitrary orders in objective_thresholds and
     # further down the road `get_pareto_frontier_and_configs` arbitrarily changes the
     # orders of the objectives, we fix the objective orders here based on the
     # observation_data and maintain it throughout the flow.
     objective_orders = obs_data[0].metric_names
 
     # Defining a dummy reference point so that all observed points are considered
     # when calculating the Pareto front. Also, defining a multiplier to turn all
     # the objectives to be maximized. Note that the multiplier at this point
     # contains 0 for outcome_constraint metrics, but this will be dropped later.
-    dummy_rp = copy.deepcopy(
-        experiment.optimization_config.objective_thresholds  # pyre-ignore
+    opt_config = checked_cast(
+        MultiObjectiveOptimizationConfig, experiment.optimization_config
     )
+    inferred_rp = _get_objective_thresholds(optimization_config=opt_config)
     multiplier = [0] * len(objective_orders)
-    for ot in dummy_rp:
+    if len(opt_config.objective_thresholds) > 0:
+        inferred_rp = deepcopy(opt_config.objective_thresholds)
+    else:
+        inferred_rp = []
+        for objective in checked_cast(MultiObjective, opt_config.objective).objectives:
+            ot = ObjectiveThreshold(
+                metric=objective.metric,
+                bound=0.0,  # dummy value
+                op=ComparisonOp.LEQ if objective.minimize else ComparisonOp.GEQ,
+                relative=False,
+            )
+            inferred_rp.append(ot)
+    for ot in inferred_rp:
         # In the following, we find the index of the objective in
         # `objective_orders`. If there is an objective that does not exist
         # in `obs_data`, a ValueError is raised.
         try:
             objective_index = objective_orders.index(ot.metric.name)
         except ValueError:
             raise ValueError(f"Metric {ot.metric.name} does not exist in `obs_data`.")
@@ -633,20 +646,18 @@
             multiplier[objective_index] = 1
 
     # Finding the pareto frontier
     frontier_observations, f, obj_w, _ = get_pareto_frontier_and_configs(
         modelbridge=mb_reference,
         observation_features=obs_feats,
         observation_data=obs_data,
-        objective_thresholds=dummy_rp,
+        objective_thresholds=inferred_rp,
         use_model_predictions=False,
     )
-
     if len(frontier_observations) == 0:
-        opt_config = cast(OptimizationConfig, mb_reference._optimization_config)
         outcome_constraints = opt_config._outcome_constraints
         if len(outcome_constraints) == 0:
             raise RuntimeError(
                 "No frontier observations found in the experiment and no constraints "
                 "are present. Please check the data of the experiment."
             )
 
@@ -658,18 +669,19 @@
 
         opt_config._outcome_constraints = []  # removing the constraints
         # getting the unconstrained pareto frontier
         frontier_observations, f, obj_w, _ = get_pareto_frontier_and_configs(
             modelbridge=mb_reference,
             observation_features=obs_feats,
             observation_data=obs_data,
-            objective_thresholds=dummy_rp,
+            objective_thresholds=inferred_rp,
             use_model_predictions=False,
         )
-        opt_config._outcome_constraints = outcome_constraints  # restoring constraints
+        # restoring constraints
+        opt_config._outcome_constraints = outcome_constraints
 
     # Need to reshuffle columns of `f` and `obj_w` to be consistent
     # with objective_orders.
     order = [
         objective_orders.index(metric_name)
         for metric_name in frontier_observations[0].data.metric_names
     ]
@@ -691,19 +703,42 @@
     rp = multiplier_nonzero * rp_raw
 
     # Removing the non-objective metrics form the order.
     objective_orders_reduced = [
         x for (i, x) in enumerate(objective_orders) if multiplier[i] != 0
     ]
 
-    # Constructing the objective thresholds.
-    # NOTE: This assumes that objective_thresholds is already initialized.
-    nadir_objective_thresholds = copy.deepcopy(
-        experiment.optimization_config.objective_thresholds
-    )
-
-    for obj_threshold in nadir_objective_thresholds:
+    for obj_threshold in inferred_rp:
         obj_threshold.bound = rp[
             objective_orders_reduced.index(obj_threshold.metric.name)
         ].item()
+    return inferred_rp
+
+
+def _get_objective_thresholds(
+    optimization_config: MultiObjectiveOptimizationConfig,
+) -> List[ObjectiveThreshold]:
+    """Get objective thresholds for an optimization config.
+
+    This will return objective thresholds with dummy values if there are
+    no objective thresholds on the optimization config.
 
-    return nadir_objective_thresholds
+    Args:
+        optimization_config: Optimization config.
+
+    Returns:
+        List of objective thresholds.
+    """
+    if optimization_config.objective_thresholds is not None:
+        return deepcopy(optimization_config.objective_thresholds)
+    objective_thresholds = []
+    for objective in checked_cast(
+        MultiObjective, optimization_config.objective
+    ).objectives:
+        ot = ObjectiveThreshold(
+            metric=objective.metric,
+            bound=0.0,  # dummy value
+            op=ComparisonOp.LEQ if objective.minimize else ComparisonOp.GEQ,
+            relative=False,
+        )
+        objective_thresholds.append(ot)
+    return objective_thresholds
```

### Comparing `ax-platform-0.3.7/ax/plot/render.py` & `ax-platform-0.4.0/ax/plot/render.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 import json
 import os
 import pkgutil
 import uuid
 from typing import Dict
```

### Comparing `ax-platform-0.3.7/ax/plot/scatter.py` & `ax-platform-0.4.0/ax/plot/scatter.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numbers
 import warnings
 from collections import OrderedDict
 
 from logging import Logger
 from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union
```

### Comparing `ax-platform-0.3.7/ax/plot/slice.py` & `ax-platform-0.4.0/ax/plot/slice.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 from ax.core.observation import ObservationFeatures
 from ax.modelbridge.base import ModelBridge
 from ax.plot.base import AxPlotConfig, AxPlotTypes, PlotData
@@ -422,22 +424,24 @@
                 sd[metric],
                 is_log[metric],
                 cur_visible,
             )
             pbutton_data_args["x"] += [trace["x"] for trace in traces]
             pbutton_data_args["y"] += [trace["y"] for trace in traces]
             pbutton_data_args["error_y"] += [
-                {
-                    "type": "data",
-                    "array": trace["error_y"]["array"],
-                    "visible": True,
-                    "color": "black",
-                }
-                if "error_y" in trace and "array" in trace["error_y"]
-                else []
+                (
+                    {
+                        "type": "data",
+                        "array": trace["error_y"]["array"],
+                        "visible": True,
+                        "color": "black",
+                    }
+                    if "error_y" in trace and "array" in trace["error_y"]
+                    else []
+                )
                 for trace in traces
             ]
             if first_param_bool:
                 init_traces.extend(traces)
         pbutton_args = [
             pbutton_data_args,
             {
```

### Comparing `ax-platform-0.3.7/ax/plot/table_view.py` & `ax-platform-0.4.0/ax/plot/table_view.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 from typing import Tuple
 
 import pandas as pd
 import plotly.graph_objs as go
 from ax.core.data import Data
 from ax.core.experiment import Experiment
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/long_running/test_pareto_utils.py` & `ax-platform-0.4.0/ax/plot/tests/long_running/test_pareto_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.exceptions.core import UnsupportedError
 from ax.metrics.branin import BraninMetric
 from ax.modelbridge.registry import Models
 from ax.plot.pareto_utils import compute_posterior_pareto_frontier
-
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_experiment
 
 
 # These tests are long-running tests (please see the TARGETS file
 # for details).
 # Please do not add any tests here before making sure that
 # they need to be allowed to run longer (i.e. >600 sec when
 # run in streesRun mode).
 class ComputePosteriorParetoFrontierTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         experiment = get_branin_experiment()
         experiment.add_tracking_metric(
             BraninMetric(name="m2", param_names=["x1", "x2"])
         )
         sobol = Models.SOBOL(experiment.search_space)
         a = sobol.gen(5)
         experiment.new_batch_trial(generator_run=a).run()
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_contours.py` & `ax-platform-0.4.0/ax/plot/tests/test_contours.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import plotly.graph_objects as go
 from ax.modelbridge.registry import Models
 from ax.plot.base import AxPlotConfig
 from ax.plot.contour import (
     interact_contour,
     interact_contour_plotly,
     plot_contour,
     plot_contour_plotly,
 )
 from ax.utils.common.testutils import TestCase
-from ax.utils.testing.core_stubs import get_branin_experiment
+from ax.utils.testing.core_stubs import (
+    get_branin_experiment,
+    get_high_dimensional_branin_experiment,
+)
 from ax.utils.testing.mock import fast_botorch_optimize
 
 
 class ContoursTest(TestCase):
     @fast_botorch_optimize
     def test_Contours(self) -> None:
         exp = get_branin_experiment(with_str_choice_param=True, with_batch=True)
@@ -37,21 +42,42 @@
             list(model.metric_names)[0],
         )
         self.assertIsInstance(plot, go.Figure)
         plot = interact_contour_plotly(model, list(model.metric_names)[0])
         self.assertIsInstance(plot, go.Figure)
         plot = interact_contour(model, list(model.metric_names)[0])
         self.assertIsInstance(plot, AxPlotConfig)
-        plot = plot = plot_contour(
+        plot = plot_contour(
             model, model.parameters[0], model.parameters[1], list(model.metric_names)[0]
         )
         self.assertIsInstance(plot, AxPlotConfig)
 
         # Make sure all parameters and metrics are displayed in tooltips
         tooltips = list(exp.parameters.keys()) + list(exp.metrics.keys())
         for d in plot.data["data"]:
             # Only check scatter plots hoverovers
             if d["type"] != "scatter":
                 continue
             for text in d["text"]:
                 for tt in tooltips:
                     self.assertTrue(tt in text)
+
+        exp = get_high_dimensional_branin_experiment(with_batch=True)
+        exp.trials[0].run()
+        model = Models.BOTORCH_MODULAR(
+            experiment=exp,
+            data=exp.fetch_data(),
+        )
+        with self.assertRaisesRegex(
+            ValueError, "Contour plots require two or more parameters"
+        ):
+            interact_contour_plotly(
+                model, list(model.metric_names)[0], parameters_to_use=["foo"]
+            )
+        for i in [2, 3]:
+            parameters_to_use = model.parameters[:i]
+            plot = interact_contour_plotly(
+                model, list(model.metric_names)[0], parameters_to_use=parameters_to_use
+            )
+            # pyre-ignore[16]: `plotly.graph_objs.graph_objs.Figure`
+            # has no attribute `layout`.
+            self.assertEqual(len(plot.layout.updatemenus[0].buttons), i)
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_diagnostic.py` & `ax-platform-0.4.0/ax/plot/tests/test_diagnostic.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import plotly.graph_objects as go
 from ax.modelbridge.cross_validation import cross_validate
 from ax.modelbridge.registry import Models
 from ax.plot.base import AxPlotConfig
 from ax.plot.diagnostic import (
     interact_cross_validation,
     interact_cross_validation_plotly,
@@ -16,14 +18,15 @@
 from ax.utils.testing.core_stubs import get_branin_experiment
 from ax.utils.testing.mock import fast_botorch_optimize
 
 
 class DiagnosticTest(TestCase):
     @fast_botorch_optimize
     def setUp(self) -> None:
+        super().setUp()
         exp = get_branin_experiment(with_batch=True)
         exp.trials[0].run()
         self.model = Models.BOTORCH_MODULAR(
             # Model bridge kwargs
             experiment=exp,
             data=exp.fetch_data(),
         )
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_feature_importances.py` & `ax-platform-0.4.0/ax/plot/tests/test_feature_importances.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from typing import Dict
 
 import torch
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.registry import Models
 from ax.plot.base import AxPlotConfig
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_fitted_scatter.py` & `ax-platform-0.4.0/ax/plot/tests/test_fitted_scatter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 import plotly.graph_objects as go
 from ax.core.data import Data
 from ax.modelbridge.registry import Models
 from ax.plot.base import AxPlotConfig
 from ax.plot.scatter import interact_fitted, interact_fitted_plotly
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_helper.py` & `ax-platform-0.4.0/ax/plot/tests/test_helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from ax.plot.helper import arm_name_to_sort_key, extend_range
 from ax.utils.common.testutils import TestCase
 
 
 class HelperTest(TestCase):
     def test_extend_range(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_parallel_coordinates.py` & `ax-platform-0.4.0/ax/plot/tests/test_parallel_coordinates.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from ax.plot.base import AxPlotConfig, AxPlotTypes
 from ax.plot.parallel_coordinates import plot_parallel_coordinates
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_experiment
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_pareto_utils.py` & `ax-platform-0.4.0/ax/plot/tests/test_pareto_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import copy
 from unittest import mock
 from unittest.mock import patch
 
 import numpy as np
 import torch
 from ax.core.data import Data
@@ -40,14 +42,15 @@
     get_robust_search_space_environmental,
     get_search_space,
 )
 
 
 class ParetoUtilsTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         experiment = get_branin_experiment()
         experiment.add_tracking_metric(
             BraninMetric(name="m2", param_names=["x1", "x2"])
         )
         sobol = Models.SOBOL(experiment.search_space)
         a = sobol.gen(5)
         experiment.new_batch_trial(generator_run=a).run()
@@ -242,15 +245,18 @@
         # Getting an experiment with 2 objectives by the above observations.
         experiment = get_experiment_with_observations(
             observations=observations,
             minimize=True,
             scalarized=False,
             constrained=False,
         )
-        inferred_reference_point = infer_reference_point_from_experiment(experiment)
+        data = experiment.fetch_data()
+        inferred_reference_point = infer_reference_point_from_experiment(
+            experiment, data=data
+        )
         # The nadir point for this experiment is [-0.5, 0.5]. The function actually
         # deducts 0.1*Y_range from each of the objectives. Since the range for each
         # of the objectives is +/-1.5, the inferred reference point would
         # be [-0.35, 0.35].
         self.assertEqual(inferred_reference_point[0].op, ComparisonOp.LEQ)
         self.assertEqual(inferred_reference_point[0].bound, -0.35)
         self.assertEqual(inferred_reference_point[0].metric.name, "m1")
@@ -259,15 +265,15 @@
         self.assertEqual(inferred_reference_point[1].metric.name, "m2")
 
         with mock.patch(
             "ax.plot.pareto_utils.get_pareto_frontier_and_configs",
             return_value=([], [], [], []),
         ):
             with self.assertRaisesRegex(RuntimeError, "No frontier observations found"):
-                infer_reference_point_from_experiment(experiment)
+                infer_reference_point_from_experiment(experiment, data=data)
 
     def test_constrained_infer_reference_point_from_experiment(self) -> None:
         experiments = []
         observations = [[-1.0, 1.0], [-0.5, 2.0], [-2.0, 0.5], [-0.1, 0.1]]
         # adding constraint observations
         observations = [o + [c] for o, c in zip(observations, [1.0, 0.5, 1.0, 1.0])]
         # Getting an experiment with 2 objectives by the above observations.
@@ -284,22 +290,23 @@
         experiment = copy.deepcopy(experiment)
         # Ensure that no observation is feasible.
         experiment.optimization_config.outcome_constraints[0].bound = 1000.0
         experiments.append(experiment)
 
         for experiment in experiments:
             # special case logs a warning message.
+            data = experiment.fetch_data()
             if experiment.optimization_config.outcome_constraints[0].bound == 1000.0:
                 with self.assertLogs(logger, "WARNING"):
                     inferred_reference_point = infer_reference_point_from_experiment(
-                        experiment
+                        experiment, data=data
                     )
             else:
                 inferred_reference_point = infer_reference_point_from_experiment(
-                    experiment
+                    experiment, data=data
                 )
             # The nadir point for this experiment is [-0.5, 0.5]. The function actually
             # deducts 0.1*Y_range from each of the objectives. Since the range for each
             # of the objectives is +/-1.5, the inferred reference point would
             # be [-0.35, 0.35].
             self.assertEqual(inferred_reference_point[0].op, ComparisonOp.LEQ)
             self.assertEqual(inferred_reference_point[0].bound, -0.35)
@@ -371,15 +378,17 @@
             return_value=(
                 frontier_observations_shuffled,
                 f_shuffled,
                 obj_w_shuffled,
                 obj_t_shuffled,
             ),
         ):
-            inferred_reference_point = infer_reference_point_from_experiment(experiment)
+            inferred_reference_point = infer_reference_point_from_experiment(
+                experiment, data=experiment.fetch_data()
+            )
 
             self.assertEqual(inferred_reference_point[0].op, ComparisonOp.LEQ)
             self.assertEqual(inferred_reference_point[0].bound, -0.35)
             self.assertEqual(inferred_reference_point[0].metric.name, "m1")
             self.assertEqual(inferred_reference_point[1].op, ComparisonOp.GEQ)
             self.assertEqual(inferred_reference_point[1].bound, 0.35)
             self.assertEqual(inferred_reference_point[1].metric.name, "m2")
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_slices.py` & `ax-platform-0.4.0/ax/plot/tests/test_slices.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import plotly.graph_objects as go
 from ax.modelbridge.registry import Models
 from ax.plot.base import AxPlotConfig
 from ax.plot.slice import (
     interact_slice,
     interact_slice_plotly,
     plot_slice,
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_tile_fitted.py` & `ax-platform-0.4.0/ax/plot/tests/test_tile_fitted.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Optional
 from unittest import mock
 
 from ax.core.arm import Arm
 from ax.core.metric import Metric
 from ax.core.search_space import SearchSpace
 from ax.modelbridge.base import ModelBridge
```

### Comparing `ax-platform-0.3.7/ax/plot/tests/test_traces.py` & `ax-platform-0.4.0/ax/plot/tests/test_traces.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 import plotly.graph_objects as go
 from ax.modelbridge.registry import Models
 from ax.plot.base import AxPlotConfig
 from ax.plot.trace import (
     optimization_trace_single_method,
     optimization_trace_single_method_plotly,
@@ -18,14 +20,15 @@
 from ax.utils.testing.core_stubs import get_branin_experiment
 from ax.utils.testing.mock import fast_botorch_optimize
 
 
 class TracesTest(TestCase):
     @fast_botorch_optimize
     def setUp(self) -> None:
+        super().setUp()
         self.exp = get_branin_experiment(with_batch=True)
         self.exp.trials[0].run()
         self.model = Models.BOTORCH_MODULAR(
             # Model bridge kwargs
             experiment=self.exp,
             data=self.exp.fetch_data(),
         )
```

### Comparing `ax-platform-0.3.7/ax/plot/trace.py` & `ax-platform-0.4.0/ax/plot/trace.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import pandas as pd
 import plotly.graph_objs as go
 from ax.core.experiment import Experiment
@@ -339,48 +341,14 @@
         y=[optimum] * 2,
         mode="lines",
         line={"dash": "dash", "color": rgba(optimum_color)},
         name="Optimum",
     )
 
 
-def model_transitions_scatter(
-    model_transitions: List[int],
-    y_range: List[float],
-    generator_change_color: Tuple[int] = COLORS.TEAL.value,
-) -> List[go.Scatter]:
-    """Creates a graph object for the line(s) representing generator changes.
-
-    Args:
-        model_transitions: iterations, before which generators
-            changed
-        y_range: upper and lower values of the y-range of the plot
-        generator_change_color: tuple of 3 int values representing
-            an RGB color. Defaults to orange.
-
-    Returns:
-        go.Scatter: plotly graph objects for the lines representing generator
-            changes
-    """
-    if len(y_range) != 2:
-        raise ValueError("y_range should have two values, lower and upper.")
-    data: List[go.Scatter] = []
-    for change in model_transitions:
-        data.append(
-            go.Scatter(
-                x=[change] * 2,
-                y=y_range,
-                mode="lines",
-                line={"dash": "dash", "color": rgba(generator_change_color)},
-                name="model change",
-            )
-        )
-    return data
-
-
 def optimization_trace_single_method_plotly(
     y: np.ndarray,
     optimum: Optional[float] = None,
     model_transitions: Optional[List[int]] = None,
     title: str = "",
     ylabel: str = "",
     hover_labels: Optional[List[str]] = None,
@@ -462,35 +430,14 @@
     if optimum is not None:
         data.append(
             optimum_objective_scatter(
                 optimum=optimum, num_iterations=y.shape[1], optimum_color=optimum_color
             )
         )
 
-    if model_transitions is not None:
-        if plot_trial_points:
-            y_lower = np.percentile(y, 25, axis=0).min()
-            y_upper = np.percentile(y, 75, axis=0).max()
-        else:
-            # pyre-fixme[61]: `y_running_optimum` is undefined, or not always defined.
-            y_lower = np.percentile(y_running_optimum, 25, axis=0).min()
-            # pyre-fixme[61]: `y_running_optimum` is undefined, or not always defined.
-            y_upper = np.percentile(y_running_optimum, 75, axis=0).max()
-        if optimum is not None and optimum < y_lower:
-            y_lower = optimum
-        if optimum is not None and optimum > y_upper:
-            y_upper = optimum
-        data.extend(
-            model_transitions_scatter(
-                model_transitions=model_transitions,
-                y_range=[y_lower, y_upper],
-                generator_change_color=generator_change_color,
-            )
-        )
-
     layout = go.Layout(
         title=title,
         showlegend=True,
         yaxis={"title": ylabel},
         xaxis={"title": "Iteration"},
     )
     layout_yaxis_range = None
@@ -510,19 +457,18 @@
     All best points are included in this range, and by default the worst points are
     truncated at some distance below the median, where that distance is given by
     1.5 * (the distance between the median and the best quartile).
 
     If `force_include_value` is provided, the worst points will be truncated at this
     value if it is worse than the truncation point described above.
     """
-    # TODO: replace interpolation->method once it becomes standard.
-    q1 = np.percentile(y, q=25, interpolation="lower").min()
-    q2_min = np.percentile(y, q=50, interpolation="linear").min()
-    q2_max = np.percentile(y, q=50, interpolation="linear").max()
-    q3 = np.percentile(y, q=75, interpolation="higher").max()
+    q1 = np.percentile(y, q=25, method="lower").min()
+    q2_min = np.percentile(y, q=50, method="linear").min()
+    q2_max = np.percentile(y, q=50, method="linear").max()
+    q3 = np.percentile(y, q=75, method="higher").max()
     if optimization_direction == "minimize":
         y_lower = y.min()
         y_upper = q2_max + 1.5 * (q2_max - q1)
         if force_include_value is not None:
             y_upper = max(y_upper, force_include_value)
     else:
         y_lower = q2_min - 1.5 * (q3 - q2_min)
```

### Comparing `ax-platform-0.3.7/ax/runners/simulated_backend.py` & `ax-platform-0.4.0/ax/runners/simulated_backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from collections import defaultdict
 from typing import Any, Callable, Dict, Iterable, Optional, Set
 
 import numpy as np
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.runner import Runner
```

### Comparing `ax-platform-0.3.7/ax/runners/single_running_trial_mixin.py` & `ax-platform-0.4.0/ax/runners/single_running_trial_mixin.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import defaultdict
 from typing import Dict, Iterable, Set
 
 from ax.core.base_trial import BaseTrial, TrialStatus
 
 
 class SingleRunningTrialMixin:
```

### Comparing `ax-platform-0.3.7/ax/runners/synthetic.py` & `ax-platform-0.4.0/ax/runners/synthetic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Iterable, List, Optional, Set
 
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.runner import Runner
 
 
 class SyntheticRunner(Runner):
```

### Comparing `ax-platform-0.3.7/ax/runners/tests/test_botorch_test_problem.py` & `ax-platform-0.4.0/ax/benchmark/tests/metrics/test_jennaton.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,99 +1,113 @@
-#!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
 
-from itertools import product
-from unittest.mock import Mock
+import math
+from random import random
+from unittest import mock
 
-import torch
+from ax.benchmark.metrics.jenatton import jenatton_test_function, JenattonMetric
 from ax.core.arm import Arm
-from ax.core.base_trial import TrialStatus
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
+from ax.core.trial import Trial
 from ax.utils.common.testutils import TestCase
-from botorch.test_functions.base import ConstrainedBaseTestProblem
-from botorch.test_functions.synthetic import ConstrainedHartmann, Hartmann
-from botorch.utils.transforms import normalize
-
-
-class TestBotorchTestProblemRunner(TestCase):
-    def test_botorch_test_problem_runner(self) -> None:
-        for test_problem_class, modified_bounds in product(
-            (Hartmann, ConstrainedHartmann), (None, [(0.0, 2.0)] * 6)
-        ):
-            test_problem = test_problem_class(dim=6).to(dtype=torch.double)
-            test_problem_kwargs = {"dim": 6}
-            runner = BotorchTestProblemRunner(
-                test_problem_class=test_problem_class,
-                test_problem_kwargs=test_problem_kwargs,
-                modified_bounds=modified_bounds,
-            )
-            self.assertIsInstance(runner.test_problem, test_problem_class)
-            self.assertEqual(runner.test_problem.dim, test_problem_kwargs["dim"])
-            self.assertEqual(runner.test_problem.bounds.dtype, torch.double)
-            self.assertEqual(
-                runner._is_constrained,
-                isinstance(test_problem, ConstrainedBaseTestProblem),
-            )
-            self.assertEqual(runner._modified_bounds, modified_bounds)
-            # check equality with different class
-            self.assertNotEqual(runner, Hartmann(dim=6))
-            self.assertEqual(runner, runner)
-            # test evaluate with original bounds
-            X = torch.rand(1, 6, dtype=torch.double)
-            res = runner.evaluate_with_original_bounds(X)
-            if modified_bounds is not None:
-                X_tf = normalize(X, torch.tensor(modified_bounds, dtype=torch.double).T)
-            else:
-                X_tf = X
-            obj = test_problem(X_tf)
-            if runner._is_constrained:
-                expected_res = torch.cat(
-                    [obj.view(-1), test_problem.evaluate_slack(X_tf).view(-1)], dim=-1
-                )
-            else:
-                expected_res = obj
-            self.assertTrue(torch.equal(res, expected_res))
-
-            # test run
-            trial = Mock()
-            trial.arms = [
-                Arm(
-                    name="0_0",
-                    parameters={f"x{i}": X[:, i].item() for i in range(6)},
-                )
-            ]
-            trial.index = 0
-            res = runner.run(trial=trial)
-            self.assertEqual(res, {"Ys": {"0_0": expected_res.tolist()}})
-            # test poll trial status
-            self.assertEqual(
-                {TrialStatus.COMPLETED: {0}}, runner.poll_trial_status([trial])
-            )
-            # test serialize args
-            serialize_init_args = BotorchTestProblemRunner.serialize_init_args(
-                obj=runner
-            )
-            self.assertEqual(
-                serialize_init_args,
-                {
-                    "test_problem_module": runner._test_problem_class.__module__,
-                    "test_problem_class_name": runner._test_problem_class.__name__,
-                    "test_problem_kwargs": runner._test_problem_kwargs,
-                    "modified_bounds": runner._modified_bounds,
-                },
-            )
-            # test deserialize args
-            deserialize_init_args = BotorchTestProblemRunner.deserialize_init_args(
-                serialize_init_args
-            )
-            self.assertEqual(
-                deserialize_init_args,
-                {
-                    "test_problem_class": test_problem_class,
-                    "test_problem_kwargs": test_problem_kwargs,
-                    "modified_bounds": modified_bounds,
-                },
-            )
+
+
+class JenattonMetricTest(TestCase):
+
+    def test_jenatton_test_function(self) -> None:
+        rand_params = {f"x{i}": random() for i in range(4, 8)}
+        rand_params["r8"] = random()
+        rand_params["r9"] = random()
+
+        for x3 in (0, 1):
+            self.assertAlmostEqual(
+                jenatton_test_function(
+                    x1=0,
+                    x2=0,
+                    x3=x3,
+                    **{**rand_params, "x4": 2.0, "r8": 0.05},
+                ),
+                4.15,
+            )
+            self.assertAlmostEqual(
+                jenatton_test_function(
+                    x1=0,
+                    x2=1,
+                    x3=x3,
+                    **{**rand_params, "x5": 2.0, "r8": 0.05},
+                ),
+                4.25,
+            )
+        for x2 in (0, 1):
+            self.assertAlmostEqual(
+                jenatton_test_function(
+                    x1=1,
+                    x2=x2,
+                    x3=0,
+                    **{**rand_params, "x6": 2.0, "r9": 0.05},
+                ),
+                4.35,
+            )
+            self.assertAlmostEqual(
+                jenatton_test_function(
+                    x1=1,
+                    x2=x2,
+                    x3=1,
+                    **{**rand_params, "x7": 2.0, "r9": 0.05},
+                ),
+                4.45,
+            )
+
+    def test_init(self) -> None:
+        metric = JenattonMetric()
+        self.assertEqual(metric.name, "jenatton")
+        self.assertTrue(metric.lower_is_better)
+        self.assertEqual(metric.noise_std, 0.0)
+        self.assertFalse(metric.observe_noise_sd)
+        metric = JenattonMetric(name="nottanej", noise_std=0.1, observe_noise_sd=True)
+        self.assertEqual(metric.name, "nottanej")
+        self.assertTrue(metric.lower_is_better)
+        self.assertEqual(metric.noise_std, 0.1)
+        self.assertTrue(metric.observe_noise_sd)
+
+    def test_fetch_trial_data(self) -> None:
+        arm = mock.Mock(spec=Arm)
+        arm.parameters = {"x1": 0, "x2": 1, "x5": 2.0, "r8": 0.05}
+        trial = mock.Mock(spec=Trial)
+        trial.arms_by_name = {"0_0": arm}
+        trial.index = 0
+
+        metric = JenattonMetric()
+        df = metric.fetch_trial_data(trial=trial).value.df  # pyre-ignore [16]
+        self.assertEqual(len(df), 1)
+        res_dict = df.iloc[0].to_dict()
+        self.assertEqual(res_dict["arm_name"], "0_0")
+        self.assertEqual(res_dict["metric_name"], "jenatton")
+        self.assertEqual(res_dict["mean"], 4.25)
+        self.assertTrue(math.isnan(res_dict["sem"]))
+        self.assertEqual(res_dict["trial_index"], 0)
+
+        metric = JenattonMetric(name="nottanej", noise_std=0.1, observe_noise_sd=True)
+        df = metric.fetch_trial_data(trial=trial).value.df  # pyre-ignore [16]
+        self.assertEqual(len(df), 1)
+        res_dict = df.iloc[0].to_dict()
+        self.assertEqual(res_dict["arm_name"], "0_0")
+        self.assertEqual(res_dict["metric_name"], "nottanej")
+        self.assertNotEqual(res_dict["mean"], 4.25)
+        self.assertEqual(res_dict["sem"], 0.1)
+        self.assertEqual(res_dict["trial_index"], 0)
+
+    def test_make_ground_truth_metric(self) -> None:
+        metric = JenattonMetric()
+        gt_metric = metric.make_ground_truth_metric()
+        self.assertIsInstance(gt_metric, JenattonMetric)
+        self.assertEqual(gt_metric.noise_std, 0.0)
+        self.assertFalse(gt_metric.observe_noise_sd)
+        metric = JenattonMetric(noise_std=0.1, observe_noise_sd=True)
+        gt_metric = metric.make_ground_truth_metric()
+        self.assertIsInstance(gt_metric, JenattonMetric)
+        self.assertEqual(gt_metric.noise_std, 0.0)
+        self.assertFalse(gt_metric.observe_noise_sd)
```

### Comparing `ax-platform-0.3.7/ax/runners/tests/test_single_running_trial_mixin.py` & `ax-platform-0.4.0/ax/runners/tests/test_single_running_trial_mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from ax.core.base_trial import TrialStatus
 from ax.runners.single_running_trial_mixin import SingleRunningTrialMixin
 from ax.runners.synthetic import SyntheticRunner
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_branin_experiment
 
 
 class SyntheticRunnerWithSingleRunningTrial(SingleRunningTrialMixin, SyntheticRunner):
-    ...
+    pass
 
 
 class SingleRunningTrialMixinTest(TestCase):
     def test_single_running_trial_mixin(self) -> None:
         runner = SyntheticRunnerWithSingleRunningTrial()
         exp = get_branin_experiment(with_trial=True, with_batch=True)
         exp.runner = runner
```

### Comparing `ax-platform-0.3.7/ax/runners/tests/test_torchx.py` & `ax-platform-0.4.0/ax/runners/tests/test_torchx.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 import shutil
 import tempfile
 from typing import List
 
 from ax.core import (
     BatchTrial,
@@ -26,14 +28,15 @@
 from ax.utils.common.constants import Keys
 from ax.utils.common.testutils import TestCase
 from torchx.components import utils
 
 
 class TorchXRunnerTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.test_dir = tempfile.mkdtemp("torchx_runtime_hpo_ax_test")
 
         self.old_cwd = os.getcwd()
         os.chdir(os.path.dirname(os.path.dirname(__file__)))
 
         self._parameters: List[Parameter] = [
             RangeParameter(
```

### Comparing `ax-platform-0.3.7/ax/runners/torchx.py` & `ax-platform-0.4.0/ax/runners/torchx.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import inspect
 
 from logging import Logger
 from typing import Any, Callable, Dict, Iterable, Mapping, Optional, Set
 
 from ax.core import Trial
 from ax.core.base_trial import BaseTrial, TrialStatus
```

### Comparing `ax-platform-0.3.7/ax/service/ax_client.py` & `ax-platform-0.4.0/ax/service/ax_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 import logging
 import warnings
 from functools import partial
 
 from logging import Logger
 from typing import (
@@ -89,16 +91,16 @@
     CORE_CLASS_ENCODER_REGISTRY,
     CORE_DECODER_REGISTRY,
     CORE_ENCODER_REGISTRY,
 )
 from ax.utils.common.docutils import copy_doc
 from ax.utils.common.executils import retry_on_exception
 from ax.utils.common.logger import _round_floats_for_logging, get_logger
+from ax.utils.common.random import with_rng_seed
 from ax.utils.common.typeutils import checked_cast, not_none
-from botorch.utils.sampling import manual_seed
 from pyre_extensions import assert_is_instance
 
 logger: Logger = get_logger(__name__)
 
 
 AxClientSubclass = TypeVar("AxClientSubclass", bound="AxClient")
 
@@ -327,17 +329,17 @@
         objective_kwargs = {}
         if objectives is not None:
             objective_kwargs["objectives"] = {
                 objective: ("minimize" if properties.minimize else "maximize")
                 for objective, properties in objectives.items()
             }
             if len(objectives.keys()) > 1:
-                objective_kwargs[
-                    "objective_thresholds"
-                ] = self.build_objective_thresholds(objectives)
+                objective_kwargs["objective_thresholds"] = (
+                    self.build_objective_thresholds(objectives)
+                )
 
         experiment = self.make_experiment(
             name=name,
             description=description,
             owners=owners,
             parameters=parameters,
             parameter_constraints=parameter_constraints,
@@ -489,27 +491,33 @@
         logger=logger,
         exception_types=(RuntimeError,),
         check_message_contains=["Cholesky", "cholesky"],
         suppress_all_errors=False,
         wrap_error_message_in=CHOLESKY_ERROR_ANNOTATION,
     )
     def get_next_trial(
-        self, ttl_seconds: Optional[int] = None, force: bool = False
+        self,
+        ttl_seconds: Optional[int] = None,
+        force: bool = False,
+        fixed_features: Optional[FixedFeatures] = None,
     ) -> Tuple[TParameterization, int]:
         """
         Generate trial with the next set of parameters to try in the iteration process.
 
         Note: Service API currently supports only 1-arm trials.
 
         Args:
             ttl_seconds: If specified, will consider the trial failed after this
                 many seconds. Used to detect dead trials that were not marked
                 failed properly.
             force: If set to True, this function will bypass the global stopping
                 strategy's decision and generate a new trial anyway.
+            fixed_features: A FixedFeatures object containing any
+                features that should be fixed at specified values during
+                generation.
 
         Returns:
             Tuple of trial parameterization, trial index
         """
 
         # Check if the global stopping strategy suggests to stop the optimization.
         # This is needed only if there is actually a stopping strategy specified,
@@ -524,15 +532,18 @@
                 experiment=self.experiment
             )
             if stop_optimization:
                 raise OptimizationShouldStop(message=global_stopping_message)
 
         try:
             trial = self.experiment.new_trial(
-                generator_run=self._gen_new_generator_run(), ttl_seconds=ttl_seconds
+                generator_run=self._gen_new_generator_run(
+                    fixed_features=fixed_features
+                ),
+                ttl_seconds=ttl_seconds,
             )
         except MaxParallelismReachedException as e:
             if self._early_stopping_strategy is not None:
                 e.message += (  # noqa: B306
                     " When stopping trials early, make sure to call `stop_trial_early` "
                     "on the stopped trial."
                 )
@@ -540,16 +551,15 @@
         logger.info(
             f"Generated new trial {trial.index} with parameters "
             f"{round_floats_for_logging(item=not_none(trial.arm).parameters)} "
             f"using model {not_none(trial.generator_run)._model_key}."
         )
         trial.mark_running(no_runner_required=True)
         self._save_or_update_trial_in_db_if_possible(
-            experiment=self.experiment,
-            trial=trial,
+            experiment=self.experiment, trial=trial
         )
         # TODO[T79183560]: Ensure correct handling of generator run when using
         # foreign keys.
         self._update_generation_strategy_in_db_if_possible(
             generation_strategy=self.generation_strategy,
             new_generator_runs=[self.generation_strategy._generator_runs[-1]],
         )
@@ -575,15 +585,18 @@
         # Ensure that experiment is set on the generation strategy.
         if self.generation_strategy._experiment is None:
             self.generation_strategy.experiment = self.experiment
 
         return self.generation_strategy.current_generator_run_limit()
 
     def get_next_trials(
-        self, max_trials: int, ttl_seconds: Optional[int] = None
+        self,
+        max_trials: int,
+        ttl_seconds: Optional[int] = None,
+        fixed_features: Optional[FixedFeatures] = None,
     ) -> Tuple[Dict[int, TParameterization], bool]:
         """Generate as many trials as currently possible.
 
         NOTE: Useful for running multiple trials in parallel: produces multiple trials,
         with their number limited by:
           - parallelism limit on current generation step,
           - number of trials in current generation step,
@@ -592,14 +605,17 @@
           - and ``max_trials`` argument to this method.
 
         Args:
             max_trials: Limit on how many trials the call to this method should produce.
             ttl_seconds: If specified, will consider the trial failed after this
                 many seconds. Used to detect dead trials that were not marked
                 failed properly.
+            fixed_features: A FixedFeatures object containing any
+                features that should be fixed at specified values during
+                generation.
 
         Returns: two-item tuple of:
               - mapping from trial indices to parameterizations in those trials,
               - boolean indicator of whether optimization is completed and no more
                 trials can be generated going forward.
         """
         gen_limit, optimization_complete = self.get_current_trial_generation_limit()
@@ -611,15 +627,17 @@
         # limit` is non-negative.
         if gen_limit >= 0:
             max_trials = min(gen_limit, max_trials)
 
         trials_dict = {}
         for _ in range(max_trials):
             try:
-                params, trial_index = self.get_next_trial(ttl_seconds=ttl_seconds)
+                params, trial_index = self.get_next_trial(
+                    ttl_seconds=ttl_seconds, fixed_features=fixed_features
+                )
                 trials_dict[trial_index] = params
             except OptimizationComplete as err:
                 logger.info(
                     f"Encountered exception indicating optimization completion: {err}"
                 )
                 return trials_dict, True
 
@@ -945,18 +963,17 @@
         return optimization_trace_single_method(
             y=(
                 np.minimum.accumulate(best_objectives, axis=1)
                 if objective.minimize
                 else np.maximum.accumulate(best_objectives, axis=1)
             ),
             optimum=objective_optimum,
-            title="Model performance vs. # of iterations",
+            title="Best objective found vs. # of iterations",
             ylabel=objective_name.capitalize(),
             hover_labels=hover_labels,
-            model_transitions=self.generation_strategy.model_transitions,
         )
 
     def get_contour_plot(
         self,
         param_x: Optional[str] = None,
         param_y: Optional[str] = None,
         metric_name: Optional[str] = None,
@@ -1289,14 +1306,17 @@
             experiment=self.experiment,
         )
 
     def stop_trial_early(self, trial_index: int) -> None:
         trial = self.get_trial(trial_index)
         trial.mark_early_stopped()
         logger.info(f"Early stopped trial {trial_index}.")
+        self._save_or_update_trial_in_db_if_possible(
+            experiment=self.experiment, trial=trial
+        )
 
     def estimate_early_stopping_savings(self, map_key: Optional[str] = None) -> float:
         """Estimate early stopping savings using progressions of the MapMetric present
         on the EarlyStoppingConfig as a proxy for resource usage.
 
         Args:
             map_key: The name of the map_key by which to estimate early stopping
@@ -1420,22 +1440,24 @@
         experiment = object_from_json(
             serialized.pop("experiment"),
             decoder_registry=decoder_registry,
             class_decoder_registry=class_decoder_registry,
         )
         serialized_generation_strategy = serialized.pop("generation_strategy")
         ax_client = cls(
-            generation_strategy=generation_strategy_from_json(
-                generation_strategy_json=serialized_generation_strategy,
-                experiment=experiment,
-                decoder_registry=decoder_registry,
-                class_decoder_registry=class_decoder_registry,
-            )
-            if serialized_generation_strategy is not None
-            else None,
+            generation_strategy=(
+                generation_strategy_from_json(
+                    generation_strategy_json=serialized_generation_strategy,
+                    experiment=experiment,
+                    decoder_registry=decoder_registry,
+                    class_decoder_registry=class_decoder_registry,
+                )
+                if serialized_generation_strategy is not None
+                else None
+            ),
             enforce_sequential_optimization=serialized.pop(
                 "_enforce_sequential_optimization"
             ),
             **kwargs,
         )
         ax_client._experiment = experiment
         return ax_client
@@ -1618,16 +1640,15 @@
                     "or more required metrics."
                 )
                 trial.mark_failed()
             else:
                 trial.mark_completed()
 
         self._save_or_update_trial_in_db_if_possible(
-            experiment=self.experiment,
-            trial=trial,
+            experiment=self.experiment, trial=trial
         )
 
         return update_info
 
     def _set_experiment(
         self,
         experiment: Experiment,
@@ -1699,14 +1720,22 @@
     def _set_generation_strategy(
         self, choose_generation_strategy_kwargs: Optional[Dict[str, Any]] = None
     ) -> None:
         """Selects the generation strategy and applies specified dispatch kwargs,
         if any.
         """
         choose_generation_strategy_kwargs = choose_generation_strategy_kwargs or {}
+        if (
+            "use_batch_trials" in choose_generation_strategy_kwargs
+            and type(self) is AxClient
+        ):
+            raise UnsupportedError(
+                "AxClient API does not support batch trials yet."
+                " We plan to add this support in coming versions."
+            )
         random_seed = choose_generation_strategy_kwargs.pop(
             "random_seed", self._random_seed
         )
         enforce_sequential_optimization = choose_generation_strategy_kwargs.pop(
             "enforce_sequential_optimization", self._enforce_sequential_optimization
         )
         if self._generation_strategy is None:
@@ -1728,42 +1757,40 @@
         suppress_all_errors: bool = False,
     ) -> bool:
         return super()._save_generation_strategy_to_db_if_possible(
             generation_strategy=generation_strategy or self.generation_strategy,
             suppress_all_errors=suppress_all_errors,
         )
 
-    def _get_last_completed_trial_index(self) -> int:
-        # infer last completed trial as the trial_index to use
-        # TODO: use Experiment.completed_trials once D46484953 lands.
-        completed_indices = [
-            t.index for t in self.experiment.trials_by_status[TrialStatus.COMPLETED]
-        ]
-        completed_indices.append(0)  # handle case of no completed trials
-        return max(completed_indices)
-
-    def _gen_new_generator_run(self, n: int = 1) -> GeneratorRun:
+    def _gen_new_generator_run(
+        self, n: int = 1, fixed_features: Optional[FixedFeatures] = None
+    ) -> GeneratorRun:
         """Generate new generator run for this experiment.
 
         Args:
             n: Number of arms to generate.
+            fixed_features: A FixedFeatures object containing any
+                features that should be fixed at specified values during
+                generation.
         """
         # If random seed is not set for this optimization, context manager does
         # nothing; otherwise, it sets the random seed for torch, but only for the
         # scope of this call. This is important because torch seed is set globally,
         # so if we just set the seed without the context manager, it can have
         # serious negative impact on the performance of the models that employ
         # stochasticity.
 
-        fixed_feats = InstantiationBase.make_fixed_observation_features(
-            fixed_features=FixedFeatures(
-                parameters={}, trial_index=self._get_last_completed_trial_index()
+        fixed_feats = (
+            InstantiationBase.make_fixed_observation_features(
+                fixed_features=fixed_features
             )
+            if fixed_features
+            else None
         )
-        with manual_seed(seed=self._random_seed):
+        with with_rng_seed(seed=self._random_seed):
             return not_none(self.generation_strategy).gen(
                 experiment=self.experiment,
                 n=n,
                 pending_observations=self._get_pending_observation_features(
                     experiment=self.experiment
                 ),
                 fixed_features=fixed_feats,
```

### Comparing `ax-platform-0.3.7/ax/service/interactive_loop.py` & `ax-platform-0.4.0/ax/service/interactive_loop.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import time
 from logging import Logger
 from queue import Queue
 from threading import Event, Lock, Thread
 from typing import Any, Callable, Dict, Optional, Tuple
 
 from ax.core.types import TEvaluationOutcome, TParameterization
```

### Comparing `ax-platform-0.3.7/ax/service/managed_loop.py` & `ax-platform-0.4.0/ax/service/managed_loop.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import inspect
 import logging
 from typing import Iterable, List, Optional, Tuple
 
 from ax.core.arm import Arm
@@ -29,15 +31,19 @@
 from ax.modelbridge.dispatch_utils import choose_generation_strategy
 from ax.modelbridge.generation_strategy import GenerationStrategy
 from ax.modelbridge.registry import Models
 from ax.service.utils.best_point import (
     get_best_parameters_from_model_predictions,
     get_best_raw_objective_point,
 )
-from ax.service.utils.instantiation import InstantiationBase, TParameterRepresentation
+from ax.service.utils.instantiation import (
+    DEFAULT_OBJECTIVE_NAME,
+    InstantiationBase,
+    TParameterRepresentation,
+)
 from ax.utils.common.executils import retry_on_exception
 from ax.utils.common.logger import get_logger
 from ax.utils.common.typeutils import not_none
 
 
 logger: logging.Logger = get_logger(__name__)
 
@@ -93,20 +99,20 @@
         arms_per_trial: int = 1,
         wait_time: int = 0,
         random_seed: Optional[int] = None,
         generation_strategy: Optional[GenerationStrategy] = None,
     ) -> "OptimizationLoop":
         """Constructs a synchronous `OptimizationLoop` using an evaluation
         function."""
+        if objective_name is None:
+            objective_name = DEFAULT_OBJECTIVE_NAME
         experiment = InstantiationBase.make_experiment(
             name=experiment_name,
             parameters=parameters,
-            objectives={objective_name: "minimize" if minimize else "maximize"}
-            if objective_name
-            else None,
+            objectives={objective_name: "minimize" if minimize else "maximize"},
             parameter_constraints=parameter_constraints,
             outcome_constraints=outcome_constraints,
         )
         return OptimizationLoop(
             experiment=experiment,
             total_trials=total_trials,
             arms_per_trial=arms_per_trial,
```

### Comparing `ax-platform-0.3.7/ax/service/scheduler.py` & `ax-platform-0.4.0/ax/service/scheduler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from collections import defaultdict
 from copy import deepcopy
 from datetime import datetime
 from enum import Enum
 from logging import LoggerAdapter
@@ -35,30 +37,31 @@
 from ax.core.map_data import MapData
 from ax.core.map_metric import MapMetric
 from ax.core.metric import Metric, MetricFetchE, MetricFetchResult
 from ax.core.optimization_config import (
     MultiObjectiveOptimizationConfig,
     OptimizationConfig,
 )
-from ax.core.outcome_constraint import ObjectiveThreshold
 from ax.core.runner import Runner
 from ax.core.types import TModelPredictArm, TParameterization
 
 from ax.early_stopping.utils import estimate_early_stopping_savings
 from ax.exceptions.core import (
     AxError,
     DataRequiredError,
     OptimizationComplete,
     UnsupportedError,
     UserInputError,
 )
-from ax.exceptions.generation_strategy import MaxParallelismReachedException
+from ax.exceptions.generation_strategy import (
+    AxGenerationException,
+    MaxParallelismReachedException,
+)
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.generation_strategy import GenerationStrategy
-from ax.plot.pareto_utils import infer_reference_point_from_experiment
 from ax.service.utils.best_point_mixin import BestPointMixin
 from ax.service.utils.scheduler_options import SchedulerOptions, TrialType
 from ax.service.utils.with_db_settings_base import DBSettings, WithDBSettingsBase
 from ax.utils.common.constants import Keys
 from ax.utils.common.docutils import copy_doc
 from ax.utils.common.executils import retry_on_exception
 from ax.utils.common.logger import (
@@ -193,33 +196,31 @@
     # Will be set to `True` if generation strategy signals that the optimization
     # is complete, in which case the optimization should gracefully exit early.
     _optimization_complete: bool = False
     # This will disable the global stopping strategy. It is useful in some
     # applications where the user wants to run the optimization loop to exhaust
     # the declared number of trials.
     __ignore_global_stopping_strategy: bool = False
-    # In MOO cases, the following will be populated by an inferred reference point
-    # for pareto front after a certain number of completed trials.
-    __inferred_reference_point: Optional[List[ObjectiveThreshold]] = None
     # Default kwargs passed when fetching data if not overridden on `SchedulerOptions`
     DEFAULT_FETCH_KWARGS = {
         "overwrite_existing_data": True,
     }
 
     def __init__(
         self,
         experiment: Experiment,
         generation_strategy: GenerationStrategyInterface,
         options: SchedulerOptions,
         db_settings: Optional[DBSettings] = None,
         _skip_experiment_save: bool = False,
     ) -> None:
         self.experiment = experiment
-        # Initialize options used in `__repr__` upfront, before any errors
-        # might be enncountered, reporting of which would call `__repr__`.
+        # Set up logger with an optional filepath handler. Note: we set the
+        # logger before setting options since that can trigger errors.
+        self._set_logger(options=options)
         self.options = options
         # NOTE: Parallelism schedule is embedded in the generation
         # strategy, as `GenerationStep.max_parallelism`.
         self.generation_strategy = generation_strategy
 
         if not isinstance(experiment, Experiment):
             raise TypeError("{experiment} is not an Ax experiment.")
@@ -229,22 +230,18 @@
         # Initialize storage layer for the scheduler.
         super().__init__(
             db_settings=db_settings,
             logging_level=self.options.logging_level,
             suppress_all_errors=self.options.suppress_storage_errors_after_retries,
         )
 
-        # Set up logger with an optional filepath handler
-        self._set_logger()
-
         # Validate experiment and GS; ensure that experiment has immutable
         # search space and opt. config to avoid storing their  copies on each
         # generator run.
         self._validate_remaining_trials(experiment=experiment)
-        self._validate_runner_and_implemented_metrics(experiment=experiment)
         if self.options.enforce_immutable_search_space_and_opt_config:
             self._enforce_immutable_search_space_and_opt_config()
         self._initialize_experiment_status_properties()
 
         if self.db_settings_set and not _skip_experiment_save:
             self._maybe_save_experiment_and_generation_strategy(
                 experiment=experiment, generation_strategy=generation_strategy
@@ -355,14 +352,16 @@
         return self._options  # pyre-ignore [16]
 
     @options.setter
     def options(self, options: SchedulerOptions) -> None:
         """Set scheduler options."""
         self._validate_options(options=options)
         self._options = options
+        # validate runners and metrics since validate_metrics is an option
+        self._validate_runner_and_implemented_metrics(experiment=self.experiment)
 
     @property
     def running_trials(self) -> List[BaseTrial]:
         """Currently running trials.
 
         Returns:
             List of trials that are currently running.
@@ -436,28 +435,16 @@
             and a string describing the reason for stopping.
         """
         if (
             not self.__ignore_global_stopping_strategy
             and self.options.global_stopping_strategy is not None
         ):
             gss = not_none(self.options.global_stopping_strategy)
-            if (
-                self.experiment.is_moo_problem
-                and self.__inferred_reference_point is None
-                and len(self.experiment.trials_by_status[TrialStatus.COMPLETED])
-                >= gss.min_trials
-            ):
-                # We infer the nadir reference point to be used by the GSS.
-                self.__inferred_reference_point = infer_reference_point_from_experiment(
-                    self.experiment
-                )
-
             stop_optimization, global_stopping_msg = gss.should_stop_optimization(
-                experiment=self.experiment,
-                objective_thresholds=self.__inferred_reference_point,
+                experiment=self.experiment
             )
             if stop_optimization:
                 return True, global_stopping_msg
 
         if self.options.total_trials is None:
             # We validate that `total_trials` is set in `run_all_trials`,
             # so it will not run indefinitely.
@@ -512,15 +499,15 @@
             trial_indices=trial_indices,
             use_model_predictions=use_model_predictions,
         )
 
     @copy_doc(BestPointMixin.get_trace)
     def get_trace(
         self,
-        optimization_config: Optional[MultiObjectiveOptimizationConfig] = None,
+        optimization_config: Optional[OptimizationConfig] = None,
     ) -> List[float]:
         return BestPointMixin._get_trace(
             experiment=self.experiment,
             optimization_config=optimization_config,
         )
 
     @copy_doc(BestPointMixin.get_trace_by_progression)
@@ -1708,19 +1695,19 @@
             if self._log_next_no_trials_reason:
                 self.logger.info(
                     "Generated all trials that can be generated currently. "
                     "Max parallelism currently reached."
                 )
             self.logger.debug(f"Message from generation strategy: {err}")
             return []
-        except Exception as err:
+        except AxGenerationException as err:
             if self._log_next_no_trials_reason:
                 self.logger.info(
                     "Generated all trials that can be generated currently. "
-                    "`generation_strategy` encountered an unknown error "
+                    "`generation_strategy` encountered an error "
                     f"{err}."
                 )
             self.logger.debug(f"Message from generation strategy: {err}")
             return []
 
         if self.options.trial_type == TrialType.TRIAL and any(
             len(generator_run_list[0].arms) > 1 or len(generator_run_list) > 1
@@ -1833,23 +1820,23 @@
             seconds_since_run_trial = (
                 current_timestamp_in_millis()
                 - not_none(self._latest_trial_start_timestamp)
             ) * 1000
             if seconds_since_run_trial < self.options.min_seconds_before_poll:
                 sleep(self.options.min_seconds_before_poll - seconds_since_run_trial)
 
-    def _set_logger(self) -> None:
+    def _set_logger(self, options: SchedulerOptions) -> None:
         """Set up the logger with appropriate logging levels."""
         cls_name = self.__class__.__name__
         logger = get_logger(name=f"{__name__}.{cls_name}@{hex(id(self))}")
-        set_stderr_log_level(self.options.logging_level)
-        if self.options.log_filepath is not None:
+        set_stderr_log_level(options.logging_level)
+        if options.log_filepath is not None:
             handler = build_file_handler(
-                filepath=not_none(self.options.log_filepath),
-                level=self.options.logging_level,
+                filepath=not_none(options.log_filepath),
+                level=options.logging_level,
             )
             logger.addHandler(handler)
         self.logger = LoggerAdapter(logger, extra={"output_name": cls_name})
 
     def _validate_remaining_trials(self, experiment: Experiment) -> None:
         """Check how many trials are remaining in `total_trials` given the trials
         already on experiment and make sure that there will be trials for the
@@ -1917,17 +1904,17 @@
 
         self.logger.info(
             f"`Scheduler` requires experiment to have immutable search "
             "space and optimization config. Setting property "
             f"{Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF.value} "
             "to `True` on experiment."
         )
-        self.experiment._properties[
-            Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF.value
-        ] = True
+        self.experiment._properties[Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF.value] = (
+            True
+        )
 
     def _initialize_experiment_status_properties(self) -> None:
         """Initializes status-tracking properties of the experiment, which will
         be appended to in ``run_trials_and_yield_results``."""
         for status_prop_enum_member in ExperimentStatusProperties:
             if status_prop_enum_member not in self.experiment._properties:
                 self.experiment._properties[status_prop_enum_member.value] = []
@@ -1939,28 +1926,28 @@
         in properties of this experiment for monitoring of experiment success.
         """
         to_append: Dict[str, Any] = {
             ExperimentStatusProperties.RUN_TRIALS_STATUS.value: status.value
         }
         if num_preexisting_trials is not None:
             new_trials = len(self.experiment.trials) - num_preexisting_trials
-            to_append[
-                ExperimentStatusProperties.NUM_TRIALS_RUN_PER_CALL.value
-            ] = new_trials
+            to_append[ExperimentStatusProperties.NUM_TRIALS_RUN_PER_CALL.value] = (
+                new_trials
+            )
         self._append_to_experiment_properties(to_append=to_append)
 
     def _record_optimization_complete_message(self) -> None:
         """Adds a simple optimization completion message to this scheduler's markdown
         messages.
         """
         completion_msg = OPTIMIZATION_COMPLETION_MSG.format(
             num_trials=len(self.experiment.trials),
-            experiment_name=self.experiment.name
-            if self.experiment._name is not None
-            else "unnamed",
+            experiment_name=(
+                self.experiment.name if self.experiment._name is not None else "unnamed"
+            ),
         )
         if "Optimization complete" in self.markdown_messages:
             self.markdown_messages["Optimization complete"] += "\n\n" + completion_msg
         else:
             self.markdown_messages["Optimization complete"] = completion_msg
 
     def _append_to_experiment_properties(self, to_append: Dict[str, Any]) -> None:
@@ -2098,23 +2085,26 @@
                 n_failed=num_bad_in_scheduler,
                 n_ran=num_ran_in_scheduler,
                 min_failed=self.options.min_failed_trials_for_failure_rate_check,
             )
         )
 
 
-def get_fitted_model_bridge(scheduler: Scheduler) -> ModelBridge:
+def get_fitted_model_bridge(
+    scheduler: Scheduler, force_refit: bool = False
+) -> ModelBridge:
     """Returns a fitted ModelBridge object. If the model is fit already, directly
     returns the already fitted model. Otherwise, fits and returns a new one.
 
     Args:
         scheduler: The scheduler object from which to get the fitted model.
+        force_refit: If True, will force a data lookup and a refit of the model.
 
     Returns:
         A ModelBridge object fitted to the observations of the scheduler's experiment.
     """
     gs = scheduler.standard_generation_strategy
     model_bridge = gs.model  # Optional[ModelBridge]
-    if model_bridge is None:  # Need to re-fit the model.
-        gs._fit_current_model(data=None)  # Will lookup_data if it none is provided.
+    if model_bridge is None or force_refit:  # Need to re-fit the model.
+        gs._fit_current_model(data=None)  # Will lookup_data if none is provided.
         model_bridge = cast(ModelBridge, gs.model)
     return model_bridge
```

### Comparing `ax-platform-0.3.7/ax/service/tests/scheduler_test_utils.py` & `ax-platform-0.4.0/ax/service/tests/scheduler_test_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
-from datetime import timedelta
+from datetime import datetime, timedelta
 from logging import WARNING
 from math import ceil
 from random import randint
 from tempfile import NamedTemporaryFile
 from typing import Any, Callable, cast, Dict, Iterable, Optional, Set, Type
 from unittest.mock import call, Mock, patch, PropertyMock
 
@@ -25,14 +27,15 @@
 from ax.core.metric import Metric
 from ax.core.objective import Objective
 from ax.core.optimization_config import OptimizationConfig
 from ax.core.runner import Runner
 from ax.core.utils import get_pending_observation_features_based_on_trial_status
 from ax.early_stopping.strategies import BaseEarlyStoppingStrategy
 from ax.exceptions.core import OptimizationComplete, UnsupportedError, UserInputError
+from ax.exceptions.generation_strategy import AxGenerationException
 from ax.metrics.branin import BraninMetric
 from ax.metrics.branin_map import BraninTimestampMapMetric
 from ax.modelbridge.cross_validation import compute_model_fit_metrics_from_modelbridge
 from ax.modelbridge.dispatch_utils import choose_generation_strategy
 from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
 from ax.modelbridge.registry import Models, ST_MTGP_trans
 from ax.runners.single_running_trial_mixin import SingleRunningTrialMixin
@@ -69,14 +72,15 @@
     get_branin_multi_objective_optimization_config,
     get_branin_search_space,
     get_generator_run,
     get_sobol,
     SpecialGenerationStrategy,
 )
 from ax.utils.testing.mock import fast_botorch_optimize
+from pyre_extensions import none_throws
 
 from sqlalchemy.orm.exc import StaleDataError
 
 DUMMY_EXCEPTION = "test_exception"
 
 
 class SyntheticRunnerWithStatusPolling(SyntheticRunner):
@@ -91,15 +95,15 @@
         if randint(0, 3) > 0:
             running = [t.index for t in trials]
             return {TrialStatus.COMPLETED: {running[randint(0, len(running) - 1)]}}
         return {}
 
 
 class SyntheticRunnerWithSingleRunningTrial(SingleRunningTrialMixin, SyntheticRunner):
-    ...
+    pass
 
 
 class SyntheticRunnerWithPredictableStatusPolling(SyntheticRunner):
     """Test runner that implements `poll_trial_status`, required for compatibility
     with the ``Scheduler``, which polls completed."""
 
     def poll_trial_status(
@@ -236,26 +240,53 @@
     run_trial_call_count = 0
 
     def run_multiple(self, trials: Iterable[BaseTrial]) -> Dict[int, Dict[str, Any]]:
         self.run_trial_call_count += 1
         raise RuntimeError("Failing for testing purposes.")
 
 
+class RunnerToAllowMultipleMapMetricFetches(SyntheticRunnerWithStatusPolling):
+    """``Runner`` that gives a trial 3 seconds to run before considering
+    the trial completed, which gives us some time to fetch the ``MapMetric``
+    a few times, if there is one on the experiment. Useful for testing behavior
+    with repeated ``MapMetric`` fetches.
+    """
+
+    def poll_trial_status(
+        self, trials: Iterable[BaseTrial]
+    ) -> Dict[TrialStatus, Set[int]]:
+        running_trials = next(iter(trials)).experiment.trials_by_status[
+            TrialStatus.RUNNING
+        ]
+        completed, still_running = set(), set()
+        for t in running_trials:
+            # pyre-ignore[58]: Operand is actually supported between these
+            if datetime.now() - t.time_run_started > timedelta(seconds=3):
+                completed.add(t.index)
+            else:
+                still_running.add(t.index)
+
+        return {
+            TrialStatus.COMPLETED: completed,
+            TrialStatus.RUNNING: still_running,
+        }
+
+
 class AxSchedulerTestCase(TestCase):
     """Tests base `Scheduler` functionality.  This test case is meant to
     test Scheduler using `GenerationStrategy`, but be extensible so
     it can be applied to any type of `GenerationStrategyInterface`
     by overriding `GENERATION_STRATEGY_INTERFACE_CLASS` and
     `_get_generation_strategy_strategy_for_test()`. You may also need
     to subclass and change some specific tests that don't apply to
     your specific `GenerationStrategyInterface`."""
 
-    GENERATION_STRATEGY_INTERFACE_CLASS: Type[
-        GenerationStrategyInterface
-    ] = GenerationStrategy
+    GENERATION_STRATEGY_INTERFACE_CLASS: Type[GenerationStrategyInterface] = (
+        GenerationStrategy
+    )
     PENDING_FEATURES_CALL_LOCATION: str = str(GenerationStrategy.__module__)
     ALWAYS_USE_DB = False
     EXPECTED_SCHEDULER_REPR: str = (
         "Scheduler(experiment=Experiment(branin_test_experiment), "
         "generation_strategy=GenerationStrategy(name='Sobol+BoTorch', "
         "steps=[Sobol for 5 trials, BoTorch for subsequent trials]), "
         "options=SchedulerOptions(max_pending_trials=10, "
@@ -274,24 +305,27 @@
 
     def setUp(self) -> None:
         super().setUp()
         self.branin_experiment = get_branin_experiment()
         self.branin_timestamp_map_metric_experiment = (
             get_branin_experiment_with_timestamp_map_metric()
         )
+        self.branin_timestamp_map_metric_experiment.runner = (
+            RunnerToAllowMultipleMapMetricFetches()
+        )
 
         self.runner = SyntheticRunnerWithStatusPolling()
         self.branin_experiment.runner = self.runner
-        self.branin_experiment._properties[
-            Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF
-        ] = True
+        self.branin_experiment._properties[Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF] = (
+            True
+        )
         self.branin_experiment_no_impl_runner_or_metrics = Experiment(
             search_space=get_branin_search_space(),
             optimization_config=OptimizationConfig(
-                objective=Objective(metric=Metric(name="branin"))
+                objective=Objective(metric=Metric(name="branin"), minimize=False)
             ),
             name="branin_experiment_no_impl_runner_or_metrics",
         )
         self.sobol_GPEI_GS = choose_generation_strategy(
             search_space=get_branin_search_space()
         )
         self.two_sobol_steps_GS = GenerationStrategy(  # Contrived GS to ensure
@@ -327,14 +361,15 @@
             RunnerWithFrequentFailedTrials: 2002,
             NoReportResultsRunner: 2003,
             BrokenRunnerValueError: 2004,
             RunnerWithAllFailedTrials: 2005,
             BrokenRunnerRuntimeError: 2006,
             SyntheticRunnerWithSingleRunningTrial: 2007,
             SyntheticRunnerWithPredictableStatusPolling: 2008,
+            RunnerToAllowMultipleMapMetricFetches: 2009,
             **CORE_RUNNER_REGISTRY,
         }
 
     @property
     def db_config(self) -> SQAConfig:
         encoder_registry = {
             SyntheticRunnerWithStatusPolling: runner_to_dict,
@@ -754,41 +789,14 @@
             scheduler.experiment.trials[0].arm.parameters,
             parameter_dict,
         )
         self.assertTrue(  # Make sure all trials got to complete.
             all(t.completed_successfully for t in scheduler.experiment.trials.values())
         )
 
-    def test_inferring_reference_point(self) -> None:
-        init_test_engine_and_session_factory(force_init=True)
-        experiment = get_branin_experiment_with_multi_objective()
-        experiment.runner = self.runner
-        gs = self._get_generation_strategy_strategy_for_test(
-            experiment=experiment,
-            generation_strategy=self.sobol_GS_no_parallelism,
-        )
-
-        scheduler = Scheduler(
-            experiment=experiment,
-            generation_strategy=gs,
-            options=SchedulerOptions(
-                # Stops the optimization after 5 trials.
-                global_stopping_strategy=DummyGlobalStoppingStrategy(
-                    min_trials=2, trial_to_stop=5
-                ),
-            ),
-            db_settings=self.db_settings,
-        )
-
-        with patch(
-            "ax.service.scheduler.infer_reference_point_from_experiment"
-        ) as mock_infer_rp:
-            scheduler.run_n_trials(max_trials=10)
-            mock_infer_rp.assert_called_once()
-
     def test_global_stopping(self) -> None:
         gs = self._get_generation_strategy_strategy_for_test(
             experiment=self.branin_experiment,
             generation_strategy=self.sobol_GS_no_parallelism,
         )
         scheduler = Scheduler(
             experiment=self.branin_experiment,  # Has runner and metrics.
@@ -1054,14 +1062,75 @@
             Scheduler(
                 experiment=self.branin_experiment,
                 generation_strategy=gs,
                 options=SchedulerOptions(total_trials=1),
                 db_settings=self.db_settings,
             )
 
+    def test_sqa_storage_map_metric_experiment(self) -> None:
+        init_test_engine_and_session_factory(force_init=True)
+        gs = self._get_generation_strategy_strategy_for_test(
+            experiment=self.branin_timestamp_map_metric_experiment,
+            generation_strategy=self.two_sobol_steps_GS,
+        )
+        self.assertIsNotNone(self.branin_timestamp_map_metric_experiment)
+        NUM_TRIALS = 5
+        scheduler = Scheduler(
+            experiment=self.branin_timestamp_map_metric_experiment,
+            generation_strategy=gs,
+            options=SchedulerOptions(
+                total_trials=NUM_TRIALS,
+                init_seconds_between_polls=0,  # No wait between polls so test is fast.
+            ),
+            db_settings=self.db_settings,
+        )
+        with patch.object(
+            scheduler.experiment,
+            "attach_data",
+            Mock(wraps=scheduler.experiment.attach_data),
+        ) as mock_experiment_attach_data:
+            # Artificial timestamp logic so we can later check that it's the
+            # last-timestamp data that was preserved after multiple `attach_
+            # data` calls.
+            with patch(
+                f"{Experiment.__module__}.current_timestamp_in_millis",
+                side_effect=lambda: len(
+                    scheduler.experiment.trials_by_status[TrialStatus.COMPLETED]
+                )
+                * 1000
+                + mock_experiment_attach_data.call_count,
+            ):
+                scheduler.run_all_trials()
+        # Check that experiment and GS were saved and test reloading with reduced state.
+        exp, loaded_gs = scheduler._load_experiment_and_generation_strategy(
+            self.branin_timestamp_map_metric_experiment.name, reduced_state=True
+        )
+        exp = none_throws(exp)
+        self.assertEqual(len(exp.trials), NUM_TRIALS)
+
+        # There should only be one data object for each trial, since by default the
+        # `Scheduler` should override previous data objects when it gets new ones in
+        # a subsequent `fetch` call.
+        for _, datas in exp.data_by_trial.items():
+            self.assertEqual(len(datas), 1)
+
+        # We also should have attempted the fetch more times
+        # than there are trials because we have a `MapMetric` (many more since we are
+        # waiting 3 seconds for each trial).
+        self.assertGreater(mock_experiment_attach_data.call_count, NUM_TRIALS)
+
+        # Check that it's the last-attached data that was kept, using
+        # expected value based on logic in mocked "current_timestamp_in_millis"
+        num_attach_calls = mock_experiment_attach_data.call_count
+        expected_ts_last_trial = len(exp.trials) * 1000 + num_attach_calls
+        self.assertEqual(
+            next(iter(exp.data_by_trial[len(exp.trials) - 1])),
+            expected_ts_last_trial,
+        )
+
     def test_sqa_storage_with_experiment_name(self) -> None:
         init_test_engine_and_session_factory(force_init=True)
         gs = self._get_generation_strategy_strategy_for_test(
             experiment=self.branin_experiment,
             generation_strategy=self.two_sobol_steps_GS,
         )
         self.assertIsNotNone(self.branin_experiment)
@@ -1076,42 +1145,46 @@
             db_settings=self.db_settings,
         )
         # Check that experiment and GS were saved.
         exp, loaded_gs = scheduler._load_experiment_and_generation_strategy(
             self.branin_experiment.name
         )
         self.assertEqual(exp, self.branin_experiment)
+        exp = none_throws(exp)
         self.assertEqual(
-            len(gs._generator_runs), len(not_none(loaded_gs)._generator_runs)
+            # pyre-fixme[16]: Add `_generator_runs` back to GSI interface or move
+            # interface to node-level from strategy-level (the latter is likely the
+            # better option) TODO
+            len(gs._generator_runs),
+            len(not_none(loaded_gs)._generator_runs),
         )
         scheduler.run_all_trials()
         # Check that experiment and GS were saved and test reloading with reduced state.
         exp, loaded_gs = scheduler._load_experiment_and_generation_strategy(
             self.branin_experiment.name, reduced_state=True
         )
-        # pyre-fixme[16]: `Optional` has no attribute `trials`.
+        exp = none_throws(exp)
         self.assertEqual(len(exp.trials), NUM_TRIALS)
         # Because of RGS, gs has queued additional unused candidates
         self.assertGreaterEqual(len(gs._generator_runs), NUM_TRIALS)
         new_scheduler = Scheduler.from_stored_experiment(
             experiment_name=self.branin_experiment.name,
             options=SchedulerOptions(
                 total_trials=NUM_TRIALS + 1,
                 init_seconds_between_polls=0,  # No wait between polls so test is fast.
             ),
             db_settings=self.db_settings,
         )
         # Hack "resumed from storage timestamp" into `exp` to make sure all other fields
         # are equal, since difference in resumed from storage timestamps is expected.
-        # pyre-fixme[16]: `Optional` has no attribute `_properties`.
-        exp._properties[
-            ExperimentStatusProperties.RESUMED_FROM_STORAGE_TIMESTAMPS
-        ] = new_scheduler.experiment._properties[
-            ExperimentStatusProperties.RESUMED_FROM_STORAGE_TIMESTAMPS
-        ]
+        exp._properties[ExperimentStatusProperties.RESUMED_FROM_STORAGE_TIMESTAMPS] = (
+            new_scheduler.experiment._properties[
+                ExperimentStatusProperties.RESUMED_FROM_STORAGE_TIMESTAMPS
+            ]
+        )
         self.assertEqual(new_scheduler.experiment, exp)
         self.assertLessEqual(
             len(gs._generator_runs),
             len(new_scheduler.generation_strategy._generator_runs),
         )
         self.assertEqual(
             len(
@@ -1146,15 +1219,15 @@
             ),
             db_settings=self.db_settings_if_always_needed,
         )
         scheduler.run_n_trials(max_trials=1)
         with patch.object(
             GenerationStrategy,
             "gen_for_multiple_trials_with_multiple_models",
-            side_effect=RuntimeError("model error"),
+            side_effect=AxGenerationException("model error"),
         ):
             with self.assertRaises(SchedulerInternalError):
                 scheduler.run_n_trials(max_trials=3)
 
     def test_run_trials_and_yield_results(self) -> None:
         total_trials = 3
         gs = self._get_generation_strategy_strategy_for_test(
@@ -1223,17 +1296,19 @@
                 scheduler.run_trials_and_yield_results(max_trials=total_trials)
             )
             expected_num_polls = 2 if respect_parellelism else 1
             self.assertEqual(len(res_list), expected_num_polls + 1)
             # Both trials in first batch of parallelism will be early stopped
             self.assertEqual(
                 len(res_list[0]["trials_early_stopped_so_far"]),
-                self.two_sobol_steps_GS._steps[0].max_parallelism
-                if respect_parellelism
-                else total_trials,
+                (
+                    self.two_sobol_steps_GS._steps[0].max_parallelism
+                    if respect_parellelism
+                    else total_trials
+                ),
             )
             # Third trial in second batch of parallelism will be early stopped
             self.assertEqual(len(res_list[1]["trials_early_stopped_so_far"]), 3)
             self.assertEqual(
                 mock_should_stop_trials_early.call_count, expected_num_polls
             )
             self.assertEqual(
@@ -1797,17 +1872,17 @@
             self.branin_experiment.new_trial(generator_run=sobol_run)
         self.assertEqual(len(self.branin_experiment.trials), 5)
         should_stop, message = scheduler.completion_criterion()
         self.assertTrue(should_stop)
         self.assertEqual(message, "Exceeding the total number of trials.")
 
     def test_get_fitted_model_bridge(self) -> None:
-        self.branin_experiment._properties[
-            Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF
-        ] = True
+        self.branin_experiment._properties[Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF] = (
+            True
+        )
         # generation strategy
         NUM_SOBOL = 5
         generation_strategy = GenerationStrategy(
             steps=[
                 GenerationStep(
                     model=Models.SOBOL, num_trials=NUM_SOBOL, max_parallelism=NUM_SOBOL
                 ),
@@ -2113,14 +2188,23 @@
                     ),
                     GenerationStep(
                         model=Models.BOTORCH_MODULAR,
                         model_kwargs={
                             # this will cause and error if the model
                             # doesn't get fixed features
                             "transforms": ST_MTGP_trans,
+                            "transform_configs": {
+                                "TrialAsTask": {
+                                    "trial_level_map": {
+                                        "trial_index": {
+                                            str(i): str(i) for i in range(3)
+                                        }
+                                    }
+                                }
+                            },
                         },
                         num_trials=1,
                     ),
                 ]
             ),
         )
 
@@ -2135,7 +2219,25 @@
             db_settings=self.db_settings_if_always_needed,
         )
         scheduler.run_n_trials(max_trials=3)
 
         # This is to ensure it generated from all nodes
         self.assertTrue(scheduler.standard_generation_strategy.optimization_complete)
         self.assertEqual(len(self.branin_experiment.trials), 3)
+
+    def test_update_options_with_validate_metrics(self) -> None:
+        experiment = self.branin_experiment_no_impl_runner_or_metrics
+        experiment.runner = self.runner
+        scheduler = Scheduler(
+            experiment=experiment,
+            generation_strategy=self._get_generation_strategy_strategy_for_test(
+                experiment=self.branin_experiment_no_impl_runner_or_metrics,
+                generation_strategy=self.sobol_GPEI_GS,
+            ),
+            options=SchedulerOptions(total_trials=10, validate_metrics=False),
+            db_settings=self.db_settings_if_always_needed,
+        )
+        with self.assertRaisesRegex(
+            UnsupportedError,
+            ".*Metrics {'branin'} do not implement fetching logic.",
+        ):
+            scheduler.options = SchedulerOptions(total_trials=10, validate_metrics=True)
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_ax_client.py` & `ax-platform-0.4.0/ax/service/tests/test_ax_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 import sys
 import time
 from itertools import product
 from math import ceil
 from typing import Dict, List, Optional, Set, Tuple, TYPE_CHECKING
 from unittest import mock
@@ -25,14 +27,15 @@
     ChoiceParameter,
     FixedParameter,
     ParameterType,
     RangeParameter,
 )
 from ax.core.parameter_constraint import OrderConstraint
 from ax.core.search_space import HierarchicalSearchSpace
+from ax.core.trial import Trial
 from ax.core.types import (
     ComparisonOp,
     TEvaluationOutcome,
     TModelPredictArm,
     TParameterization,
     TParamValue,
 )
@@ -42,38 +45,44 @@
     UnsupportedError,
     UnsupportedPlotError,
     UserInputError,
 )
 from ax.exceptions.generation_strategy import MaxParallelismReachedException
 from ax.metrics.branin import branin
 from ax.modelbridge.dispatch_utils import DEFAULT_BAYESIAN_PARALLELISM
-from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
+from ax.modelbridge.generation_strategy import (
+    GenerationNode,
+    GenerationStep,
+    GenerationStrategy,
+)
+from ax.modelbridge.model_spec import ModelSpec
 from ax.modelbridge.random import RandomModelBridge
 from ax.modelbridge.registry import Models
 
 from ax.service.ax_client import AxClient, ObjectiveProperties
 from ax.service.utils.best_point import (
     get_best_parameters_from_model_predictions_with_trial_index,
     get_pareto_optimal_parameters,
     observed_pareto,
     predicted_pareto,
 )
+from ax.service.utils.instantiation import FixedFeatures
 from ax.storage.sqa_store.db import init_test_engine_and_session_factory
 from ax.storage.sqa_store.decoder import Decoder
 from ax.storage.sqa_store.encoder import Encoder
 from ax.storage.sqa_store.sqa_config import SQAConfig
 from ax.storage.sqa_store.structs import DBSettings
+from ax.utils.common.random import with_rng_seed
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.measurement.synthetic_functions import Branin
 from ax.utils.testing.core_stubs import DummyEarlyStoppingStrategy
 from ax.utils.testing.mock import fast_botorch_optimize
 from ax.utils.testing.modeling_stubs import get_observation1, get_observation1trans
 from botorch.test_functions.multi_objective import BraninCurrin
-from botorch.utils.sampling import manual_seed
 
 if TYPE_CHECKING:
     from ax.core.types import TTrialEvaluation
 
 
 RANDOM_SEED = 239
 DUMMY_RUN_METADATA = {
@@ -135,25 +144,25 @@
             {"name": "y", "type": "range", "bounds": [0.0, 1.0]},
         ],
         objectives={
             "branin": ObjectiveProperties(
                 minimize=minimize,
                 # pyre-fixme[6]: For 2nd param expected `Optional[float]` but got
                 #  `Optional[Tensor]`.
-                threshold=branin_currin.ref_point[0]
-                if include_objective_thresholds
-                else None,
+                threshold=(
+                    branin_currin.ref_point[0] if include_objective_thresholds else None
+                ),
             ),
             "currin": ObjectiveProperties(
                 minimize=minimize,
                 # pyre-fixme[6]: For 2nd param expected `Optional[float]` but got
                 #  `Optional[Tensor]`.
-                threshold=branin_currin.ref_point[1]
-                if include_objective_thresholds
-                else None,
+                threshold=(
+                    branin_currin.ref_point[1] if include_objective_thresholds else None
+                ),
             ),
         },
         outcome_constraints=outcome_constraints,
         choose_generation_strategy_kwargs={
             "num_initialization_trials": num_trials,
             "random_seed": random_seed,
         },
@@ -1951,14 +1960,26 @@
                 name="test_experiment",
                 parameters=[{"name": "x", "type": "range", "bounds": [-5.0, 10.0]}],
                 overwrite_existing_experiment=True,
             )
         # Original experiment should still be in DB and not have been overwritten.
         self.assertEqual(len(ax_client.experiment.trials), 5)
 
+        # Attach an early stopped trial.
+        parameters, trial_index = ax_client.get_next_trial()
+        ax_client.stop_trial_early(trial_index=trial_index)
+
+        # Reload experiment and check that trial status is accurate.
+        ax_client_new = AxClient(db_settings=db_settings)
+        ax_client_new.load_experiment_from_database("test_experiment")
+        self.assertEqual(
+            ax_client.experiment.trials_by_status,
+            ax_client_new.experiment.trials_by_status,
+        )
+
     def test_overwrite(self) -> None:
         init_test_engine_and_session_factory(force_init=True)
         ax_client = AxClient()
         ax_client.create_experiment(
             name="test_experiment",
             parameters=[
                 {"name": "x", "type": "range", "bounds": [-5.0, 10.0]},
@@ -2361,15 +2382,15 @@
             "Sobol",
         )
 
         cfg = not_none(ax_client.experiment.optimization_config)
         assert isinstance(cfg, MultiObjectiveOptimizationConfig)
         thresholds = np.array([t.bound for t in cfg.objective_thresholds])
 
-        with manual_seed(seed=RANDOM_SEED):
+        with with_rng_seed(seed=RANDOM_SEED):
             predicted_pareto = ax_client.get_pareto_optimal_parameters()
 
         # For the predicted frontier, we don't know the solution a priori, so let's
         # check it
         for _, (point, _) in predicted_pareto.values():
             values = np.array([point["branin"], point["currin"]])
             within_threshold = (
@@ -2469,27 +2490,27 @@
             num_trials=20, include_objective_thresholds=False
         )
         ax_client.generation_strategy._maybe_move_to_next_step()
         ax_client.generation_strategy._fit_current_model(
             data=ax_client.experiment.lookup_data()
         )
 
-        with manual_seed(seed=RANDOM_SEED):
+        with with_rng_seed(seed=RANDOM_SEED):
             predicted_pareto = ax_client.get_pareto_optimal_parameters()
 
         # Check that we specified objective threshold overrides (because we
         # inferred them)
         self.assertIsNotNone(
             mock_predicted_pareto.call_args[1].get("objective_thresholds")
         )
         mock_predicted_pareto.reset_mock()
         mock_observed_pareto.assert_not_called()
         self.assertGreater(len(predicted_pareto), 0)
 
-        with manual_seed(seed=RANDOM_SEED):
+        with with_rng_seed(seed=RANDOM_SEED):
             observed_pareto = ax_client.get_pareto_optimal_parameters(
                 use_model_predictions=False
             )
         # Check that we specified objective threshold overrides (because we
         # inferred them)
         self.assertIsNotNone(
             mock_observed_pareto.call_args[1].get("objective_thresholds")
@@ -2823,19 +2844,28 @@
                 {"name": "y", "type": "range", "bounds": [0.0, 15.0]},
             ],
             name="fixed_features",
         )
         with mock.patch.object(
             GenerationStrategy, "gen", wraps=ax_client.generation_strategy.gen
         ) as mock_gen:
-            params, idx = ax_client.get_next_trial()
-            call_kwargs = mock_gen.call_args_list[0][1]
-            ff = call_kwargs["fixed_features"]
-            self.assertEqual(ff.parameters, {})
-            self.assertEqual(ff.trial_index, 0)
+            with self.subTest("fixed_features is None"):
+                params, idx = ax_client.get_next_trial()
+                call_kwargs = mock_gen.call_args_list[0][1]
+                ff = call_kwargs["fixed_features"]
+                self.assertIsNone(ff)
+            with self.subTest("fixed_features is set"):
+                fixed_features = FixedFeatures(
+                    parameters={"x": 0.0, "y": 5.0}, trial_index=0
+                )
+                params, idx = ax_client.get_next_trial(fixed_features=fixed_features)
+                call_kwargs = mock_gen.call_args_list[1][1]
+                ff = call_kwargs["fixed_features"]
+                self.assertEqual(ff.parameters, fixed_features.parameters)
+                self.assertEqual(ff.trial_index, 0)
 
     def test_get_optimization_trace_discard_infeasible_trials(self) -> None:
         ax_client = AxClient()
         ax_client.create_experiment(
             name="test_experiment",
             parameters=[
                 {
@@ -2900,14 +2930,41 @@
             )
 
             if "are no unordered categorical parameters." in log:
                 found_no_log = True
 
         self.assertTrue(found_no_log)
 
+    def test_with_node_based_gs(self) -> None:
+        sobol_gs = GenerationStrategy(
+            name="Sobol",
+            nodes=[
+                GenerationNode(
+                    node_name="Sobol",
+                    model_specs=[ModelSpec(model_enum=Models.SOBOL)],
+                )
+            ],
+        )
+        ax_client = get_branin_optimization(generation_strategy=sobol_gs)
+        params, idx = ax_client.get_next_trial()
+        ax_client.complete_trial(trial_index=idx, raw_data={"branin": (0, 0.0)})
+
+        self.assertEqual(ax_client.generation_strategy.name, "Sobol")
+        self.assertEqual(
+            checked_cast(
+                Trial, ax_client.experiment.trials[0]
+            )._generator_run._model_key,
+            "Sobol",
+        )
+        with mock.patch(
+            "ax.service.ax_client.optimization_trace_single_method"
+        ) as mock_plot:
+            ax_client.get_optimization_trace()
+        mock_plot.assert_called_once()
+
 
 # Utility functions for testing get_model_predictions without calling
 # get_next_trial. Create Ax Client with an experiment where
 # num_initial_trials kwarg is zero. Note that this kwarg is
 # needed to be able to instantiate the model for the first time
 # without calling get_next_trial().
 def _set_up_client_for_get_model_predictions_no_next_trial() -> AxClient:
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_best_point.py` & `ax-platform-0.4.0/ax/service/tests/test_best_point.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest.mock import Mock
 
 import pandas as pd
 
 from ax.core.arm import Arm
 from ax.core.batch_trial import BatchTrial
 from ax.core.data import Data
@@ -121,15 +123,15 @@
                 ]
             )
         exp.attach_data(Data(df=pd.DataFrame.from_records(df_dict)))
         self.assertEqual(get_trace(exp), [len(trial.arms) - 1])
         # test that there is performance metric in the trace for each
         # completed/early-stopped trial
         trial1 = checked_cast(BatchTrial, trial).clone_to()
-        trial1.mark_abandoned()
+        trial1.mark_abandoned(unsafe=True)
         arms = get_arms_from_dict(get_arm_weights2())
         trial2 = exp.new_batch_trial(GeneratorRun(arms))
         trial2.mark_running(no_runner_required=True).mark_completed()
         df_dict2 = []
         for i, arm in enumerate(trial2.arms):
             df_dict2.extend(
                 [
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_best_point_utils.py` & `ax-platform-0.4.0/ax/service/tests/test_best_point_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import random
 from typing import List
 from unittest.mock import MagicMock, patch
 
 import pandas as pd
 import torch
 from ax.core.arm import Arm
@@ -352,17 +354,19 @@
         for trial_idx in range(20):
             for metric_name in ["foo", "bar"]:
                 df_dicts.append(
                     {
                         "trial_index": trial_idx,
                         "metric_name": metric_name,
                         "arm_name": f"{trial_idx}_0",
-                        "mean": float(trial_idx)
-                        if metric_name == "foo"
-                        else trial_idx + 5.0,
+                        "mean": (
+                            float(trial_idx)
+                            if metric_name == "foo"
+                            else trial_idx + 5.0
+                        ),
                         "sem": 0.0,
                     }
                 )
         experiment.attach_data(Data(df=pd.DataFrame.from_records(df_dicts)))
 
         expected_Y = torch.stack(
             [
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_early_stopping.py` & `ax-platform-0.4.0/ax/service/tests/test_early_stopping.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, Optional
 
 from ax.service.utils import early_stopping as early_stopping_utils
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import (
     DummyEarlyStoppingStrategy,
     get_branin_experiment,
@@ -14,14 +16,15 @@
 
 
 class TestEarlyStoppingUtils(TestCase):
     """Testing the early stopping utilities functionality that is not tested in
     main `AxClient` testing suite (`TestServiceAPI`)."""
 
     def setUp(self) -> None:
+        super().setUp()
         self.branin_experiment = get_branin_experiment()
 
     def test_should_stop_trials_early(self) -> None:
         expected: Dict[int, Optional[str]] = {
             1: "Stopped due to testing.",
             3: "Stopped due to testing.",
         }
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_global_stopping.py` & `ax-platform-0.4.0/ax/service/tests/test_global_stopping.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, Tuple
 
 import numpy as np
 from ax.core.types import TParameterization
 from ax.exceptions.core import OptimizationShouldStop
 from ax.global_stopping.strategies.base import BaseGlobalStoppingStrategy
 from ax.service.ax_client import AxClient
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_instantiation_utils.py` & `ax-platform-0.4.0/ax/service/tests/test_instantiation_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict
 
 from ax.core.metric import Metric
 from ax.core.optimization_config import MultiObjectiveOptimizationConfig
 from ax.core.parameter import (
     ChoiceParameter,
     FixedParameter,
@@ -26,14 +28,27 @@
 
     def test_parameter_type_validation(self) -> None:
         with self.assertRaisesRegex(ValueError, "No AE parameter type"):
             # pyre-fixme[6]: For 1st param expected `Union[Type[bool], Type[float],
             #  Type[int], Type[str]]` but got `Type[list]`.
             InstantiationBase._get_parameter_type(list)
 
+    def test_make_search_space(self) -> None:
+        with self.assertRaisesRegex(ValueError, "cannot contain spaces"):
+            InstantiationBase.make_search_space(
+                parameters=[
+                    {
+                        "name": "x space 1",
+                        "type": "range",
+                        "bounds": [0.0, 1.0],
+                    }
+                ],
+                parameter_constraints=None,
+            )
+
     def test_constraint_from_str(self) -> None:
         with self.assertRaisesRegex(ValueError, "Bound for the constraint"):
             InstantiationBase.constraint_from_str(
                 "x1 + x2 <= not_numerical_bound",
                 # pyre-fixme[6]: For 2nd param expected `Dict[str, Parameter]` but
                 #  got `Dict[str, None]`.
                 {"x1": None, "x2": None},
@@ -150,14 +165,18 @@
             experiment._tracking_metrics,
             {metric_name: Metric(name=metric_name) for metric_name in metrics_names},
         )
 
     def test_make_objectives(self) -> None:
         with self.assertRaisesRegex(ValueError, "specify 'minimize' or 'maximize'"):
             InstantiationBase.make_objectives({"branin": "unknown"})
+
+        with self.assertRaisesRegex(ValueError, "cannot contain spaces"):
+            InstantiationBase.make_objectives({"branin space": "maximize"})
+
         objectives = InstantiationBase.make_objectives(
             {"branin": "minimize", "currin": "maximize"}
         )
         branin_metric = [o.minimize for o in objectives if o.metric.name == "branin"]
         self.assertTrue(branin_metric[0])
         currin_metric = [o.minimize for o in objectives if o.metric.name == "currin"]
         self.assertFalse(currin_metric[0])
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_interactive_loop.py` & `ax-platform-0.4.0/ax/service/tests/test_interactive_loop.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import functools
 import time
 from logging import WARN
 from typing import Optional, Tuple
 
 import numpy as np
@@ -19,17 +21,14 @@
 from ax.service.utils.instantiation import ObjectiveProperties
 from ax.utils.common.testutils import TestCase
 from ax.utils.measurement.synthetic_functions import hartmann6
 from ax.utils.testing.mock import fast_botorch_optimize
 
 
 class TestInteractiveLoop(TestCase):
-    def setUp(self) -> None:
-        super().setUp()
-
     @fast_botorch_optimize
     def test_interactive_loop(self) -> None:
         def _elicit(
             parameterization_with_trial_index: Tuple[TParameterization, int]
         ) -> Optional[Tuple[int, TEvaluationOutcome]]:
             parameterization, trial_index = parameterization_with_trial_index
             x = np.array([parameterization.get(f"x{i+1}") for i in range(6)])
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_managed_loop.py` & `ax-platform-0.4.0/ax/service/tests/test_managed_loop.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, Tuple, Union
 from unittest.mock import Mock, patch
 
 import numpy as np
 from ax.exceptions.core import UserInputError
 from ax.metrics.branin import branin
 from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_report_utils.py` & `ax-platform-0.4.0/ax/service/tests/test_report_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import copy
 import itertools
 from collections import namedtuple
 from logging import INFO, WARN
 from typing import Dict, List
 from unittest import mock
 from unittest.mock import patch
@@ -161,15 +163,15 @@
 
         df = exp_to_df(
             exp=exp,
             additional_fields_callables={  # pyre-ignore
                 "timestamp": compute_maximum_map_values_timestamp
             },
         )
-        self.assertEqual(df["timestamp"].tolist(), [5.0, 5.0, 5.0])
+        self.assertEqual(df["timestamp"].tolist(), [4.0, 4.0, 4.0])
 
     def test_exp_to_df_trial_timing(self) -> None:
         # 1. test all have started, none have completed
         exp = get_test_map_data_experiment(num_trials=3, num_fetches=5, num_complete=0)
         df = exp_to_df(
             exp=exp,
             trial_attribute_fields=["time_run_started", "time_completed"],
@@ -417,24 +419,35 @@
         # analysis raises an INFO level log entry here. Leaving level=WARN here
         # actually passes on Python 3.8 because of a language internal bug. See
         # https://bugs.python.org/issue41943 for more information.
         with self.assertLogs(logger="ax", level=INFO) as log:
             plots = get_standard_plots(
                 experiment=exp, model=Models.MOO(experiment=exp, data=exp.fetch_data())
             )
-            self.assertEqual(len(log.output), 2)
+            self.assertEqual(len(log.output), 5)
             self.assertIn(
                 "Pareto plotting not supported for experiments with relative objective "
                 "thresholds.",
                 log.output[0],
             )
             self.assertIn(
-                "Failed to compute global feature sensitivities:",
+                "Failed to compute signed global feature sensitivities",
                 log.output[1],
             )
+            self.assertIn(
+                "Failed to compute unsigned feature sensitivities:",
+                log.output[2],
+            )
+            created_plots_logs = set(log.output[2:])
+            for metric_suffix in ("a", "b"):
+                expected_msg = (
+                    "Created contour plots for metric branin_"
+                    f"{metric_suffix} and parameters ['x2', 'x1']"
+                )
+                self.assertTrue(any(expected_msg in msg for msg in created_plots_logs))
         self.assertEqual(len(plots), 6)
 
     @fast_botorch_optimize
     def test_get_standard_plots_moo_relative_constraints(self) -> None:
         exp = get_branin_experiment_with_multi_objective(with_batch=True)
         exp.optimization_config.objective.objectives[0].minimize = False
         exp.optimization_config.objective.objectives[1].minimize = True
@@ -510,35 +523,52 @@
         sobol = Models.SOBOL(search_space=exp.search_space)
         for _ in range(1):
             exp.new_trial(sobol.gen(1)).run()
         model = Models.GPEI(
             experiment=exp,
             data=exp.fetch_data(),
         )
+        with self.assertLogs(logger="ax", level=INFO) as log:
+            _get_objective_v_param_plots(
+                experiment=exp, model=model, max_num_contour_plots=2
+            )
+            self.assertEqual(len(log.output), 1)
+            self.assertIn(
+                "Created contour plots for metric objective and parameters",
+                log.output[0],
+            )
         with self.assertLogs(logger="ax", level=WARN) as log:
-            _get_objective_v_param_plots(experiment=exp, model=model)
+            _get_objective_v_param_plots(
+                experiment=exp, model=model, max_num_contour_plots=1
+            )
             self.assertEqual(len(log.output), 1)
-            self.assertIn("Skipping creation of 2450 contour plots", log.output[0])
+            self.assertIn(
+                "Skipping creation of contour plots",
+                log.output[0],
+            )
+        with self.assertLogs(logger="ax", level=WARN) as log:
             _get_objective_v_param_plots(
-                experiment=exp, model=model, max_num_slice_plots=10
+                experiment=exp,
+                model=model,
+                max_num_contour_plots=1,
+                max_num_slice_plots=10,
             )
-            # Adds two more warnings.
-            self.assertEqual(len(log.output), 3)
-            self.assertIn("Skipping creation of 50 slice plots", log.output[1])
+            # Creates two warnings, one for slice plots and one for contour plots.
+            self.assertEqual(len(log.output), 2)
 
     def test_get_metric_name_pairs(self) -> None:
         exp = get_branin_experiment(with_trial=True)
         exp._optimization_config = MultiObjectiveOptimizationConfig(
             objective=MultiObjective(
                 objectives=[
-                    Objective(metric=Metric("m0")),
-                    Objective(metric=Metric("m1")),
-                    Objective(metric=Metric("m2")),
-                    Objective(metric=Metric("m3")),
-                    Objective(metric=Metric("m4")),
+                    Objective(metric=Metric("m0"), minimize=False),
+                    Objective(metric=Metric("m1"), minimize=False),
+                    Objective(metric=Metric("m2"), minimize=False),
+                    Objective(metric=Metric("m3"), minimize=False),
+                    Objective(metric=Metric("m4"), minimize=False),
                 ]
             )
         )
         with self.assertLogs(logger="ax", level=WARN) as log:
             metric_name_pairs = _get_metric_name_pairs(experiment=exp)
             self.assertEqual(len(log.output), 1)
             self.assertIn(
@@ -1018,17 +1048,17 @@
                 search_space=get_branin_search_space(),
                 tracking_metrics=[m0, m1, m2, m3],
             )
 
             optimization_config = MultiObjectiveOptimizationConfig(
                 objective=MultiObjective(
                     objectives=[
-                        Objective(metric=Metric("m0")),
+                        Objective(metric=Metric("m0"), minimize=False),
                         Objective(metric=Metric("m1"), minimize=True),
-                        Objective(metric=Metric("m3")),
+                        Objective(metric=Metric("m3"), minimize=False),
                     ]
                 )
             )
             experiment.optimization_config = optimization_config
             self.assertEqual(True, experiment.is_moo_problem)
 
             comparison_arm_names = ["opt_0", "opt_1_min", "opt_3"]
@@ -1118,34 +1148,22 @@
 
         comparison_arm_names = ["equal"]
 
         with patch(
             "ax.service.utils.report_utils.exp_to_df",
             return_value=arms_df,
         ):
-            with self.assertLogs("ax", level=INFO) as log:
-                result = compare_to_baseline(
-                    experiment=experiment,
-                    optimization_config=optimization_config,
-                    comparison_arm_names=comparison_arm_names,
-                    baseline_arm_name=custom_baseline_arm_name,
-                )
-                self.assertEqual(result, None)
-                self.assertTrue(
-                    any(
-                        (
-                            "compare_to_baseline:"
-                            f" comparison arm equal"
-                            f" did not beat baseline arm {custom_baseline_arm_name}."
-                        )
-                        in log_str
-                        for log_str in log.output
-                    ),
-                    log.output,
-                )
+            result = compare_to_baseline(
+                experiment=experiment,
+                optimization_config=optimization_config,
+                comparison_arm_names=comparison_arm_names,
+                baseline_arm_name=custom_baseline_arm_name,
+            )
+
+            self.assertIsNone(result)
 
     def test_compare_to_baseline_select_baseline_arm(self) -> None:
         OBJECTIVE_METRIC = "objective"
         true_obj_metric = Metric(name=OBJECTIVE_METRIC, lower_is_better=True)
         experiment = Experiment(
             search_space=get_branin_search_space(),
             tracking_metrics=[true_obj_metric],
```

### Comparing `ax-platform-0.3.7/ax/service/tests/test_with_db_settings_base.py` & `ax-platform-0.4.0/ax/service/tests/test_with_db_settings_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import random
 import string
 from typing import Tuple
 from unittest.mock import patch
 
 from ax.core.base_trial import TrialStatus
 from ax.core.experiment import Experiment
```

### Comparing `ax-platform-0.3.7/ax/service/utils/best_point.py` & `ax-platform-0.4.0/ax/service/utils/best_point.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from collections import OrderedDict
 from functools import reduce
 
 from logging import Logger
 from typing import Dict, Iterable, List, Optional, Tuple, Type
 
 import pandas as pd
@@ -46,15 +48,15 @@
 from ax.modelbridge.torch import TorchModelBridge
 from ax.modelbridge.transforms.utils import (
     derelativize_optimization_config_with_raw_status_quo,
 )
 from ax.plot.pareto_utils import get_tensor_converter_model
 from ax.utils.common.logger import get_logger
 from ax.utils.common.typeutils import checked_cast, not_none
-from numpy import NaN
+from numpy import nan
 from torch import Tensor
 
 logger: Logger = get_logger(__name__)
 
 
 def get_best_raw_objective_point_with_trial_index(
     experiment: Experiment,
@@ -729,15 +731,15 @@
     """Noiseless is defined as SEM = 0 or SEM = NaN on a given metric (usually
     the objective).
     """
 
     name_mask = df["metric_name"] == metric_name
     df_metric_arms_sems = df[name_mask]["sem"]
 
-    return ((df_metric_arms_sems == 0) | df_metric_arms_sems == NaN).all()
+    return ((df_metric_arms_sems == 0) | df_metric_arms_sems == nan).all()
 
 
 def _derel_opt_config_wrapper(
     optimization_config: OptimizationConfig,
     modelbridge: Optional[ModelBridge] = None,
     experiment: Optional[Experiment] = None,
     observations: Optional[List[Observation]] = None,
@@ -895,17 +897,19 @@
     optimization_config = checked_cast(
         MultiObjectiveOptimizationConfig, optimization_config
     )
     provided_thresholds = {
         obj_t.metric.name: obj_t for obj_t in optimization_config.objective_thresholds
     }
     objective_thresholds = [
-        provided_thresholds[objective.metric.name]
-        if objective.metric.name in provided_thresholds
-        else _objective_threshold_from_nadir(
-            experiment=experiment,
-            objective=objective,
-            optimization_config=optimization_config,
+        (
+            provided_thresholds[objective.metric.name]
+            if objective.metric.name in provided_thresholds
+            else _objective_threshold_from_nadir(
+                experiment=experiment,
+                objective=objective,
+                optimization_config=optimization_config,
+            )
         )
         for objective in objectives
     ]
     return objective_thresholds
```

### Comparing `ax-platform-0.3.7/ax/service/utils/best_point_mixin.py` & `ax-platform-0.4.0/ax/service/utils/best_point_mixin.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from abc import ABCMeta, abstractmethod
 from functools import partial
 from logging import Logger
 from typing import Dict, Iterable, List, Optional, Tuple
 
 import numpy as np
 import torch
@@ -262,15 +264,15 @@
             raise NotImplementedError(
                 "Please use `get_pareto_optimal_parameters` for multi-objective "
                 "problems."
             )
         # TODO[drfreund]: Find a way to include data for last trial in the
         # calculation of best parameters.
         if use_model_predictions:
-            current_model = generation_strategy._curr.model_enum
+            current_model = generation_strategy._curr.model_spec_to_gen_from.model_enum
             # Cover for the case where source of `self._curr.model` was not a `Models`
             # enum but a factory function, in which case we cannot do
             # `get_model_from_generator_run` (since we don't have model type and inputs
             # recorded on the generator run.
             models_enum = (
                 current_model.__class__
                 if isinstance(current_model, ModelRegistryBase)
@@ -377,15 +379,15 @@
             return 0.0
         moo_optimization_config = checked_cast(
             MultiObjectiveOptimizationConfig,
             optimization_config or experiment.optimization_config,
         )
 
         if use_model_predictions:
-            current_model = generation_strategy._curr.model_enum
+            current_model = generation_strategy._curr.model_spec_to_gen_from.model_enum
             # Cover for the case where source of `self._curr.model` was not a `Models`
             # enum but a factory function, in which case we cannot do
             # `get_model_from_generator_run` (since we don't have model type and inputs
             # recorded on the generator run.
             models_enum = (
                 current_model.__class__
                 if isinstance(current_model, ModelRegistryBase)
```

### Comparing `ax-platform-0.3.7/ax/service/utils/early_stopping.py` & `ax-platform-0.4.0/ax/service/utils/early_stopping.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Optional, Set
 
 from ax.core.experiment import Experiment
 from ax.early_stopping.strategies import BaseEarlyStoppingStrategy
 from ax.utils.common.typeutils import not_none
 
 
@@ -41,11 +43,12 @@
 ) -> List[str]:
     """A helper function that returns a list of metric names on which a given
     `early_stopping_strategy` is operating."""
     if early_stopping_strategy is None:
         return []
     if early_stopping_strategy.metric_names is not None:
         return list(early_stopping_strategy.metric_names)
+    # TODO: generalize this to multi-objective ess
     default_objective, _ = early_stopping_strategy._default_objective_and_direction(
         experiment=experiment
     )
     return [default_objective]
```

### Comparing `ax-platform-0.3.7/ax/service/utils/instantiation.py` & `ax-platform-0.4.0/ax/service/utils/instantiation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 from dataclasses import dataclass
 
 from logging import Logger
 from typing import Any, Dict, List, Optional, Sequence, Type, Union
 
 from ax.core.arm import Arm
@@ -103,14 +105,15 @@
             or maximized.
         - threshold: Optional `float` representing the smallest objective value
             (resp. largest if minimize=True) that is considered valuable in the context
             of multi-objective optimization. In BoTorch and in the literature, this is
             also known as an element of the reference point vector that defines the
             hyper-volume of the Pareto front.
     """
+
     minimize: bool
     threshold: Optional[float] = None
 
 
 @dataclass(frozen=True)
 class FixedFeatures:
     """Class for representing fixed features via the Service API."""
@@ -146,14 +149,20 @@
         cls,
         name: str,
         lower_is_better: Optional[bool] = None,
         metric_class: Type[Metric] = Metric,
         for_opt_config: bool = False,
         metric_definitions: Optional[Dict[str, Dict[str, Any]]] = None,
     ) -> Metric:
+        if " " in name:
+            raise ValueError(
+                "Metric names cannot contain spaces when used with AxClient. Got "
+                f"{name!r}."
+            )
+
         return metric_class(
             name=name,
             lower_is_better=lower_is_better,
             **cls._get_deserialized_metric_kwargs(
                 name=name,
                 metric_definitions=metric_definitions,
                 metric_class=metric_class,
@@ -261,17 +270,20 @@
         value = representation["value"]
         assert type(value) in PARAM_TYPES.values(), (
             f"Cannot parse fixed parameter {name}: for fixed parameters, json "
             "representation should include a single value."
         )
         return FixedParameter(
             name=name,
-            parameter_type=cls._get_parameter_type(type(value))  # pyre-ignore[6]
-            if parameter_type is None
-            else cls._get_parameter_type(PARAM_TYPES[parameter_type]),  # pyre-ignore[6]
+            parameter_type=(
+                cls._get_parameter_type(type(value))  # pyre-ignore[6]
+                if parameter_type is None
+                # pyre-ignore[6]
+                else cls._get_parameter_type(PARAM_TYPES[parameter_type])
+            ),
             value=value,  # pyre-ignore[6]
             is_fidelity=checked_cast(bool, representation.get("is_fidelity", False)),
             target_value=representation.get("target_value", None),  # pyre-ignore[6]
             dependents=representation.get("dependents", None),  # pyre-ignore[6]
         )
 
     @classmethod
@@ -304,14 +316,20 @@
         parameter_type = representation.get("value_type", None)
         if parameter_type is not None:
             assert isinstance(parameter_type, str) and parameter_type in PARAM_TYPES, (
                 "Value type in parameter JSON representation must be 'int', 'float', "
                 "'bool' or 'str'."
             )
 
+        if " " in name:
+            raise ValueError(
+                "Parameter names cannot contain spaces when used with AxClient. Got "
+                f"{name!r}."
+            )
+
         if parameter_class == "range":
             return cls._make_range_param(
                 name=name,
                 representation=representation,
                 parameter_type=parameter_type,
             )
 
@@ -849,17 +867,17 @@
         default_data_type = (
             DataType.MAP_DATA if support_intermediate_data else DataType.DATA
         )
 
         properties: Dict[str, Any] = {}
 
         if immutable_search_space_and_opt_config:
-            properties[
-                Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF
-            ] = immutable_search_space_and_opt_config
+            properties[Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF] = (
+                immutable_search_space_and_opt_config
+            )
 
         if owners is not None:
             properties["owners"] = owners
 
         return Experiment(
             name=name,
             description=description,
@@ -918,11 +936,13 @@
             fixed_features: The fixed features for generation.
 
         Returns:
             The new ObservationFeatures object.
         """
         return ObservationFeatures(
             parameters=fixed_features.parameters,
-            trial_index=None
-            if fixed_features.trial_index is None
-            else fixed_features.trial_index,
+            trial_index=(
+                None
+                if fixed_features.trial_index is None
+                else fixed_features.trial_index
+            ),
         )
```

### Comparing `ax-platform-0.3.7/ax/service/utils/report_utils.py` & `ax-platform-0.4.0/ax/service/utils/report_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import itertools
 import logging
 from collections import defaultdict
 from datetime import timedelta
 from logging import Logger
 from typing import (
     Any,
@@ -19,15 +21,14 @@
     Optional,
     Tuple,
     TYPE_CHECKING,
     Union,
 )
 
 import gpytorch
-
 import numpy as np
 import pandas as pd
 import plotly.graph_objects as go
 from ax.core.base_trial import TrialStatus
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRunType
@@ -104,15 +105,14 @@
         )
     ]
 
 
 def _get_objective_trace_plot(
     experiment: Experiment,
     data: Data,
-    model_transitions: List[int],
     true_objective_metric_name: Optional[str] = None,
 ) -> Iterable[go.Figure]:
     if experiment.is_moo_problem:
         return [
             scatter_plot_with_hypervolume_trace_plotly(experiment=experiment),
             *_pairwise_pareto_plotly_scatter(experiment=experiment),
         ]
@@ -135,30 +135,37 @@
         if metric_name is not None
     )
 
     plots = [
         plot_objective_value_vs_trial_index(
             exp_df=exp_df,
             metric_colname=metric_name,
-            minimize=optimization_config.objective.minimize
-            if optimization_config.objective.metric.name == metric_name
-            else experiment.metrics[metric_name].lower_is_better,
+            minimize=not_none(
+                optimization_config.objective.minimize
+                if optimization_config.objective.metric.name == metric_name
+                else experiment.metrics[metric_name].lower_is_better
+            ),
             title=f"Best {metric_name} found vs. trial index",
             hover_data_colnames=run_metadata_report_keys,
         )
         for metric_name in metric_names
     ]
 
     return [plot for plot in plots if plot is not None]
 
 
 def _get_objective_v_param_plots(
     experiment: Experiment,
     model: ModelBridge,
-    max_num_slice_plots: int = 50,
+    importance: Optional[
+        Union[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]
+    ] = None,
+    # Chosen to take ~1min on local benchmarks.
+    max_num_slice_plots: int = 200,
+    # Chosen to take ~2min on local benchmarks.
     max_num_contour_plots: int = 20,
 ) -> List[go.Figure]:
     search_space = experiment.search_space
 
     range_params = [
         checked_cast(Parameter, param)
         for param in search_space.range_parameters.values()
@@ -167,16 +174,17 @@
     if len(range_params) < 1:
         # if search space contains no range params
         logger.warning(
             "`_get_objective_v_param_plot` requires a search space with at least one "
             "`RangeParameter`. Returning an empty list."
         )
         return []
+    range_param_names = [param.name for param in range_params]
     num_range_params = len(range_params)
-    num_metrics = len(experiment.metrics)
+    num_metrics = len(model.metric_names)
     num_slice_plots = num_range_params * num_metrics
     output_plots = []
     if num_slice_plots <= max_num_slice_plots:
         # parameter slice plot
         output_plots += [
             interact_slice_plotly(
                 model=model,
@@ -189,40 +197,64 @@
             "<br>Users can plot individual slice plots with the <br>python "
             "function ax.plot.slice.plot_slice_plotly."
         )
         # TODO: return a warning here then convert to a plot/message/etc. downstream.
         warning_plot = _warn_and_create_warning_plot(warning_msg=warning_msg)
         output_plots.append(warning_plot)
 
-    num_contour_plots = num_range_params * (num_range_params - 1) * num_metrics
-    if num_range_params > 1 and num_contour_plots <= max_num_contour_plots:
-        # contour plots
-        try:
-            with gpytorch.settings.max_eager_kernel_size(float("inf")):
-                output_plots += [
-                    interact_contour_plotly(
-                        model=not_none(model),
-                        metric_name=metric_name,
-                    )
-                    for metric_name in model.metric_names
-                ]
-        # `mean shape torch.Size` RunTimeErrors, pending resolution of
-        # https://github.com/cornellius-gp/gpytorch/issues/1853
-        except RuntimeError as e:
-            logger.warning(f"Contour plotting failed with error: {e}.")
-    elif num_contour_plots > max_num_contour_plots:
+    # contour plots
+    num_contour_per_metric = max_num_contour_plots // num_metrics
+    if num_contour_per_metric < 2:
         warning_msg = (
-            f"Skipping creation of {num_contour_plots} contour plots since that "
-            f"exceeds <br>`max_num_contour_plots = {max_num_contour_plots}`."
+            "Skipping creation of contour plots since that requires <br>"
+            "`max_num_contour_plots >= 2 * num_metrics`. Got "
+            f"{max_num_contour_plots=} and {num_metrics=}."
             "<br>Users can plot individual contour plots with the <br>python "
             "function ax.plot.contour.plot_contour_plotly."
         )
         # TODO: return a warning here then convert to a plot/message/etc. downstream.
         warning_plot = _warn_and_create_warning_plot(warning_msg=warning_msg)
         output_plots.append(warning_plot)
+    elif num_range_params > 1:
+        # Using n params yields n * (n - 1) contour plots, so we use the number of
+        # params that yields the desired number of plots (solved using quadratic eqn)
+        num_params_per_metric = int(0.5 + (0.25 + num_contour_per_metric) ** 0.5)
+        try:
+            for metric_name in model.metric_names:
+                if importance is not None:
+                    range_params_sens_for_metric = {
+                        k: v
+                        for k, v in importance[metric_name].items()
+                        if k in range_param_names
+                    }
+                    # sort the params by their sensitivity
+                    params_to_use = sorted(
+                        range_params_sens_for_metric,
+                        key=lambda x: range_params_sens_for_metric[x],
+                        reverse=True,
+                    )[:num_params_per_metric]
+                # if sens is not available, just use the first num_features_per_metric.
+                else:
+                    params_to_use = range_param_names[:num_params_per_metric]
+                with gpytorch.settings.max_eager_kernel_size(float("inf")):
+                    output_plots.append(
+                        interact_contour_plotly(
+                            model=not_none(model),
+                            metric_name=metric_name,
+                            parameters_to_use=params_to_use,
+                        )
+                    )
+                logger.info(
+                    f"Created contour plots for metric {metric_name} and parameters "
+                    f"{params_to_use}."
+                )
+        # `mean shape torch.Size` RunTimeErrors, pending resolution of
+        # https://github.com/cornellius-gp/gpytorch/issues/1853
+        except RuntimeError as e:
+            logger.warning(f"Contour plotting failed with error: {e}.")
     return output_plots
 
 
 def _get_suffix(input_str: str, delim: str = ".", n_chunks: int = 1) -> str:
     return delim.join(input_str.split(delim)[-n_chunks:])
 
 
@@ -287,15 +319,14 @@
     return {istr: istr for istr in input_str_list}
 
 
 def get_standard_plots(
     experiment: Experiment,
     model: Optional[ModelBridge],
     data: Optional[Data] = None,
-    model_transitions: Optional[List[int]] = None,
     true_objective_metric_name: Optional[str] = None,
     early_stopping_strategy: Optional[BaseEarlyStoppingStrategy] = None,
     limit_points_per_plot: Optional[int] = None,
     global_sensitivity_analysis: bool = True,
 ) -> List[go.Figure]:
     """Extract standard plots for single-objective optimization.
 
@@ -303,17 +334,14 @@
     interest to an Ax user. Currently not supported are
     - TODO: multi-objective optimization
     - TODO: ChoiceParameter plots
 
     Args:
         - experiment: The ``Experiment`` from which to obtain standard plots.
         - model: The ``ModelBridge`` used to suggest trial parameters.
-        - data: If specified, data, to which to fit the model before generating plots.
-        - model_transitions: The arm numbers at which shifts in generation_strategy
-            occur.
         - true_objective_metric_name: Name of the metric to use as the true objective.
         - early_stopping_strategy: Early stopping strategy used throughout the
             experiment; used for visualizing when curves are stopped.
         - limit_points_per_plot: Limit the number of points used per metric in
             each curve plot. Passed to `_get_curve_plot_dropdown`.
         - global_sensitivity_analysis: If True, plot total Variance-based sensitivity
             analysis for the model parameters. If False, plot sensitivities based on
@@ -357,17 +385,14 @@
 
     output_plot_list = []
     try:
         output_plot_list.extend(
             _get_objective_trace_plot(
                 experiment=experiment,
                 data=data,
-                model_transitions=model_transitions
-                if model_transitions is not None
-                else [],
                 true_objective_metric_name=true_objective_metric_name,
             )
         )
     except Exception as e:
         # Allow model-based plotting to proceed if objective_trace plotting fails.
         logger.exception(f"Plotting `objective_trace` failed with error {e}")
 
@@ -386,47 +411,75 @@
                         true_objective_metric_name=true_objective_metric_name,
                     )
                 )
                 logger.debug("Finished with objective vs. true objective scatter plot.")
         except Exception as e:
             logger.exception(f"Scatter plot failed with error: {e}")
 
+        # Compute feature importance ("sensitivity") to select most important
+        # features to plot.
+        sens = None
+        importance_measure = ""
+        if global_sensitivity_analysis and isinstance(model, TorchModelBridge):
+            try:
+                logger.debug("Starting global sensitivity analysis.")
+                sens = ax_parameter_sens(model, order="total")
+                importance_measure = (
+                    '<a href="https://en.wikipedia.org/wiki/Variance-based_'
+                    'sensitivity_analysis">Variance-based sensitivity analysis</a>'
+                )
+                logger.debug("Finished global sensitivity analysis.")
+            except Exception as e:
+                logger.info(
+                    f"Failed to compute signed global feature sensitivities: {e}"
+                    "Trying to get unsigned feature sensitivities."
+                )
+                try:
+                    sens = ax_parameter_sens(model, order="total", signed=False)
+                except Exception as e:
+                    logger.exception(
+                        f"Failed to compute unsigned feature sensitivities: {e}"
+                    )
+        if sens is None:
+            try:
+                sens = {
+                    metric_name: model.feature_importances(metric_name)
+                    for i, metric_name in enumerate(sorted(model.metric_names))
+                }
+            except Exception as e:
+                logger.info(f"Failed to compute feature importances: {e}")
+
         try:
             logger.debug("Starting objective vs. param plots.")
+            # importance is the absolute value of sensitivity.
+            importance = None
+            if sens is not None:
+                importance = {
+                    k: {j: np.absolute(sens[k][j]) for j in sens[k].keys()}
+                    for k in sens.keys()
+                }
             output_plot_list.extend(
                 _get_objective_v_param_plots(
                     experiment=experiment,
                     model=model,
+                    importance=importance,
                 )
             )
             logger.debug("Finished objective vs. param plots.")
         except Exception as e:
             logger.exception(f"Slice plot failed with error: {e}")
 
         try:
             logger.debug("Starting cross validation plot.")
             output_plot_list.extend(_get_cross_validation_plots(model=model))
             logger.debug("Finished cross validation plot.")
         except Exception as e:
             logger.exception(f"Cross-validation plot failed with error: {e}")
 
         # sensitivity plot
-        sens = None
-        importance_measure = ""
-        try:
-            if global_sensitivity_analysis and isinstance(model, TorchModelBridge):
-                logger.debug("Starting global sensitivity analysis.")
-                sens = ax_parameter_sens(model, order="total", signed=True)
-                importance_measure = (
-                    '<a href="https://en.wikipedia.org/wiki/Variance-based_'
-                    'sensitivity_analysis">Variance-based sensitivity analysis</a>'
-                )
-                logger.debug("Finished global sensitivity analysis.")
-        except Exception as e:
-            logger.info(f"Failed to compute global feature sensitivities: {e}")
         try:
             logger.debug("Starting feature importance plot.")
             feature_importance_plot = plot_feature_importance_by_feature_plotly(
                 model=model,
                 # pyre-ignore [6]:
                 # In call for argument `sensitivity_values`, expected
                 # `Optional[Dict[str, Dict[str, Union[float, ndarray]]]]`
@@ -846,17 +899,19 @@
     )
 
     # Add trial reason for failed or abandoned trials
     trial_to_reason = {
         index: (
             f"{trial.failed_reason[:15]}..."
             if trial.status.is_failed and trial.failed_reason is not None
-            else f"{trial.abandoned_reason[:15]}..."
-            if trial.status.is_abandoned and trial.abandoned_reason is not None
-            else None
+            else (
+                f"{trial.abandoned_reason[:15]}..."
+                if trial.status.is_abandoned and trial.abandoned_reason is not None
+                else None
+            )
         )
         for index, trial in trials
     }
 
     _merge_trials_dict_with_df(
         df=arms_df,
         trials_dict=trial_to_reason,
@@ -1188,15 +1243,15 @@
             + ", can't compute percent change."
         )
         return None
 
     if (objective_minimize and (baseline_value <= comparison_value)) or (
         not objective_minimize and (baseline_value >= comparison_value)
     ):
-        logger.info(
+        logger.debug(
             f"compare_to_baseline: comparison arm {comparison_arm_name}"
             + f" did not beat baseline arm {baseline_arm_name}. "
         )
         return None
     percent_change = ((abs(comparison_value - baseline_value)) / baseline_value) * 100
 
     return _format_comparison_string(
@@ -1456,15 +1511,15 @@
     """
     # Get fit quality dict.
     model_bridge = generation_strategy.model  # Optional[ModelBridge]
     if model_bridge is None:  # Need to re-fit the model.
         generation_strategy._fit_current_model(data=None)
         model_bridge = cast(ModelBridge, generation_strategy.model)
     if isinstance(model_bridge, RandomModelBridge):
-        logger.info(
+        logger.debug(
             "Current modelbridge on GenerationStrategy is RandomModelBridge. "
             "Not checking metric predictability."
         )
         return None
     model_fit_dict = compute_model_fit_metrics_from_modelbridge(
         model_bridge=model_bridge,
         experiment=experiment,
```

### Comparing `ax-platform-0.3.7/ax/service/utils/scheduler_options.py` & `ax-platform-0.4.0/ax/service/utils/scheduler_options.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from dataclasses import dataclass, field
 from enum import Enum
 from logging import INFO
 from typing import Any, Dict, Optional
 
 from ax.early_stopping.strategies import BaseEarlyStoppingStrategy
 from ax.global_stopping.strategies.base import BaseGlobalStoppingStrategy
```

### Comparing `ax-platform-0.3.7/ax/service/utils/with_db_settings_base.py` & `ax-platform-0.4.0/ax/service/utils/with_db_settings_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import re
 import time
 
 from logging import INFO, Logger
 from typing import Any, Dict, List, Optional, Tuple, Type
 
 from ax.core.base_trial import BaseTrial
@@ -67,15 +69,15 @@
     from ax.storage.sqa_store.sqa_config import SQAConfig
     from ax.storage.sqa_store.structs import DBSettings
     from sqlalchemy.exc import OperationalError
     from sqlalchemy.orm.exc import StaleDataError
 
     # We retry on `OperationalError` if saving to DB.
     RETRY_EXCEPTION_TYPES = (OperationalError, StaleDataError)
-except (ModuleNotFoundError, IncompatibleDependencyVersion):
+except (ModuleNotFoundError, IncompatibleDependencyVersion, TypeError):
     DBSettings = None
     Decoder = None
     Encoder = None
     SQAConfig = None
 
 
 STORAGE_MINI_BATCH_SIZE = 50
@@ -251,18 +253,18 @@
             reduced_state=reduced_state,
             load_trials_in_batches_of_size=LOADING_MINI_BATCH_SIZE,
             ax_object_field_overrides=self.AX_OBJECT_FIELD_OVERRIDES,
             skip_runners_and_metrics=skip_runners_and_metrics,
         )
         if not isinstance(experiment, Experiment):
             raise ValueError("Service API only supports `Experiment`.")
+        num_trials = len(experiment.trials)
         logger.info(
-            f"Loaded experiment {experiment_name} in "
-            f"{_round_floats_for_logging(time.time() - start_time)} seconds, "
-            f"loading trials in mini-batches of {LOADING_MINI_BATCH_SIZE}."
+            f"Loaded experiment {experiment_name} & {num_trials} trials in "
+            f"{_round_floats_for_logging(time.time() - start_time)} seconds."
         )
 
         try:
             start_time = time.time()
             generation_strategy = _load_generation_strategy_by_experiment_name(
                 experiment_name=experiment_name,
                 decoder=self.db_settings.decoder,
```

### Comparing `ax-platform-0.3.7/ax/storage/botorch_modular_registry.py` & `ax-platform-0.4.0/ax/storage/botorch_modular_registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, Dict, Type
 
 import torch
 
 # Ax `Acquisition` imports
 from ax.models.torch.botorch_modular.acquisition import Acquisition
 from ax.models.torch.botorch_modular.sebo import SEBOAcquisition
@@ -41,28 +43,29 @@
     qLogExpectedHypervolumeImprovement,
     qLogNoisyExpectedHypervolumeImprovement,
 )
 from botorch.acquisition.multi_objective.monte_carlo import (
     qExpectedHypervolumeImprovement,
     qNoisyExpectedHypervolumeImprovement,
 )
+from botorch.acquisition.preference import AnalyticExpectedUtilityOfBestOption
 from botorch.models import SaasFullyBayesianSingleTaskGP
 from botorch.models.contextual import LCEAGP
 from botorch.models.fully_bayesian_multitask import SaasFullyBayesianMultiTaskGP
 
 # BoTorch `Model` imports
 from botorch.models.gp_regression import FixedNoiseGP, SingleTaskGP
 from botorch.models.gp_regression_fidelity import (
     FixedNoiseMultiFidelityGP,
     SingleTaskMultiFidelityGP,
 )
 from botorch.models.gp_regression_mixed import MixedSingleTaskGP
 from botorch.models.model import Model
 from botorch.models.model_list_gp_regression import ModelListGP
-from botorch.models.multitask import FixedNoiseMultiTaskGP, MultiTaskGP
+from botorch.models.multitask import MultiTaskGP
 from botorch.models.transforms.input import (
     ChainedInputTransform,
     InputPerturbation,
     InputTransform,
     Normalize,
     Round,
     Warp,
@@ -101,15 +104,14 @@
 """
 MODEL_REGISTRY: Dict[Type[Model], str] = {
     # NOTE: Fixed noise models are deprecated. They point to their
     # supported parent classes, so that we can reap them with minimal
     # concern for backwards compatibility when the time comes.
     FixedNoiseGP: "SingleTaskGP",
     FixedNoiseMultiFidelityGP: "SingleTaskMultiFidelityGP",
-    FixedNoiseMultiTaskGP: "MultiTaskGP",
     MixedSingleTaskGP: "MixedSingleTaskGP",
     ModelListGP: "ModelListGP",
     MultiTaskGP: "MultiTaskGP",
     SingleTaskGP: "SingleTaskGP",
     SingleTaskMultiFidelityGP: "SingleTaskMultiFidelityGP",
     SaasFullyBayesianSingleTaskGP: "SaasFullyBayesianSingleTaskGP",
     SaasFullyBayesianMultiTaskGP: "SaasFullyBayesianMultiTaskGP",
@@ -118,14 +120,15 @@
 
 
 """
 Mapping of Botorch `AcquisitionFunction` classes to class name strings.
 """
 ACQUISITION_FUNCTION_REGISTRY: Dict[Type[AcquisitionFunction], str] = {
     ExpectedImprovement: "ExpectedImprovement",
+    AnalyticExpectedUtilityOfBestOption: "AnalyticExpectedUtilityOfBestOption",
     NoisyExpectedImprovement: "NoisyExpectedImprovement",
     qExpectedHypervolumeImprovement: "qExpectedHypervolumeImprovement",
     qNoisyExpectedHypervolumeImprovement: "qNoisyExpectedHypervolumeImprovement",
     qExpectedImprovement: "qExpectedImprovement",
     qKnowledgeGradient: "qKnowledgeGradient",
     qMaxValueEntropy: "qMaxValueEntropy",
     qMultiFidelityKnowledgeGradient: "qMultiFidelityKnowledgeGradient",
@@ -200,14 +203,15 @@
 """
 REVERSE_ACQUISITION_REGISTRY: Dict[str, Type[Acquisition]] = {
     v: k for k, v in ACQUISITION_REGISTRY.items()
 }
 
 
 REVERSE_MODEL_REGISTRY: Dict[str, Type[Model]] = {
+    # NOTE: These ensure backwards compatibility. Keep them around.
     "FixedNoiseGP": SingleTaskGP,
     "FixedNoiseMultiFidelityGP": SingleTaskMultiFidelityGP,
     "FixedNoiseMultiTaskGP": MultiTaskGP,
     **{v: k for k, v in MODEL_REGISTRY.items()},
 }
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/decoder.py` & `ax-platform-0.4.0/ax/storage/json_store/decoder.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,31 +1,35 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import datetime
 from collections import OrderedDict
 from enum import Enum
 from inspect import isclass
+from io import StringIO
 from logging import Logger
-from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
 import numpy as np
 import pandas as pd
 import torch
 from ax.benchmark.problems.hpo.torchvision import (
     PyTorchCNNTorchvisionBenchmarkProblem as TorchvisionBenchmarkProblem,
 )
 from ax.core.base_trial import BaseTrial
 from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
 from ax.core.multi_type_experiment import MultiTypeExperiment
+from ax.core.objective import Objective
 from ax.core.parameter import Parameter
 from ax.core.parameter_constraint import (
     OrderConstraint,
     ParameterConstraint,
     SumConstraint,
 )
 from ax.core.search_space import SearchSpace
@@ -33,54 +37,46 @@
 from ax.modelbridge.generation_strategy import (
     GenerationNode,
     GenerationStep,
     GenerationStrategy,
 )
 from ax.modelbridge.model_spec import ModelSpec
 from ax.modelbridge.registry import _decode_callables_from_references
-from ax.modelbridge.transition_criterion import (
-    MaxGenerationParallelism,
-    MaxTrials,
-    MinimumPreferenceOccurances,
-    MinimumTrialsInStatus,
-    MinTrials,
-    TransitionCriterion,
-)
+from ax.modelbridge.transition_criterion import TransitionCriterion, TrialBasedCriterion
 from ax.models.torch.botorch_modular.model import SurrogateSpec
 from ax.models.torch.botorch_modular.surrogate import Surrogate
 from ax.storage.json_store.decoders import (
     batch_trial_from_json,
     botorch_component_from_json,
     tensor_from_json,
     trial_from_json,
 )
-
 from ax.storage.json_store.registry import (
     CORE_CLASS_DECODER_REGISTRY,
     CORE_DECODER_REGISTRY,
 )
 from ax.utils.common.logger import get_logger
-from ax.utils.common.serialization import SerializationMixin
+from ax.utils.common.serialization import (
+    SerializationMixin,
+    TClassDecoderRegistry,
+    TDecoderRegistry,
+)
 from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.common.typeutils_torch import torch_type_from_str
 
+
 logger: Logger = get_logger(__name__)
 
 
 # pyre-fixme[3]: Return annotation cannot be `Any`.
 def object_from_json(
     # pyre-fixme[2]: Parameter annotation cannot be `Any`.
     object_json: Any,
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Any:
     """Recursively load objects from a JSON-serializable dictionary."""
     if type(object_json) in (str, int, float, bool, type(None)) or isinstance(
         object_json, Enum
     ):
         return object_json
     elif isinstance(object_json, list):
@@ -133,15 +129,15 @@
                     )
                     for k, v in object_json["value"]
                 ]
             )
         elif _type == "DataFrame":
             # Need dtype=False, otherwise infers arm_names like "4_1"
             # should be int 41
-            return pd.read_json(object_json["value"], dtype=False)
+            return pd.read_json(StringIO(object_json["value"]), dtype=False)
         elif _type == "ndarray":
             return np.array(object_json["value"])
         elif _type == "Tensor":
             return tensor_from_json(json=object_json)
         elif _type.startswith("torch"):
             # Torch types will be encoded as "torch_<type_name>", so we drop prefix
             return torch_type_from_str(
@@ -219,19 +215,24 @@
             )
         elif _class == SearchSpace:
             return search_space_from_json(
                 search_space_json=object_json,
                 decoder_registry=decoder_registry,
                 class_decoder_registry=class_decoder_registry,
             )
+        elif _class == Objective:
+            return objective_from_json(
+                object_json=object_json,
+                decoder_registry=decoder_registry,
+                class_decoder_registry=class_decoder_registry,
+            )
         elif _class == TorchvisionBenchmarkProblem:
             return TorchvisionBenchmarkProblem.from_dataset_name(
                 name=object_json["name"],
                 num_trials=object_json["num_trials"],
-                infer_noise=object_json["infer_noise"],
             )
         elif _class in (SurrogateSpec, Surrogate):
             if "input_transform" in object_json:
                 (
                     input_transform_classes_json,
                     input_transform_options_json,
                 ) = get_input_transform_json_components(
@@ -246,20 +247,29 @@
                     outcome_transform_classes_json,
                     outcome_transform_options_json,
                 ) = get_outcome_transform_json_components(
                     outcome_transforms_json=object_json.pop("outcome_transform"),
                     decoder_registry=decoder_registry,
                     class_decoder_registry=class_decoder_registry,
                 )
-                object_json[
-                    "outcome_transform_classes"
-                ] = outcome_transform_classes_json
-                object_json[
-                    "outcome_transform_options"
-                ] = outcome_transform_options_json
+                object_json["outcome_transform_classes"] = (
+                    outcome_transform_classes_json
+                )
+                object_json["outcome_transform_options"] = (
+                    outcome_transform_options_json
+                )
+        elif isclass(_class) and issubclass(_class, TrialBasedCriterion):
+            # TrialBasedCriterion contain a list of `TrialStatus` for args.
+            # This list needs to be unpacked by hand to properly retain the types.
+            return trial_transition_criteria_from_json(
+                class_=_class,
+                transition_criteria_json=object_json,
+                decoder_registry=decoder_registry,
+                class_decoder_registry=class_decoder_registry,
+            )
         elif isclass(_class) and issubclass(_class, SerializationMixin):
             return _class(
                 **_class.deserialize_init_args(
                     args=object_json,
                     decoder_registry=decoder_registry,
                     class_decoder_registry=class_decoder_registry,
                 )
@@ -281,21 +291,16 @@
 
 # pyre-fixme[3]: Return annotation cannot be `Any`.
 def ax_class_from_json_dict(
     # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
     #  `typing.Type` to avoid runtime subscripting errors.
     _class: Type,
     object_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Any:
     """Reinstantiates an Ax class registered in `DECODER_REGISTRY` from a JSON
     dict.
     """
     return _class(
         **{
             k: object_from_json(
@@ -306,21 +311,16 @@
             for k, v in object_json.items()
         }
     )
 
 
 def generator_run_from_json(
     object_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> GeneratorRun:
     """Load Ax GeneratorRun from JSON."""
     time_created_json = object_json.pop("time_created")
     type_json = object_json.pop("generator_run_type")
     index_json = object_json.pop("index")
     generator_run = GeneratorRun(
         **{
@@ -346,148 +346,46 @@
         index_json,
         decoder_registry=decoder_registry,
         class_decoder_registry=class_decoder_registry,
     )
     return generator_run
 
 
-def transition_criteria_from_json(
-    transition_criteria_json: List[Dict[str, Any]],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
-) -> Optional[List[TransitionCriterion]]:
-    """Load Ax TransitionCriteria from JSON.
-
-    This function is necessary due to the loading of TrialStatus in
-    some, but not all, TransitionCriterion.
+def trial_transition_criteria_from_json(
+    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use `typing.Type` to
+    #  avoid runtime subscripting errors.
+    class_: Type,
+    transition_criteria_json: Dict[str, Any],
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
+) -> Optional[TransitionCriterion]:
+    """Load Ax transition criteria that depend on Trials from JSON.
+
+    Since ```TrialBasedCriterion``` contain lists of ```TrialStatus``,
+    the json for these criterion needs to be carefully unpacked and
+    re-processed via ```object_from_json``` in order to maintain correct
+    typing. We pass in ```class_``` in order to correctly handle all classes
+    which inherit from ```TrialBasedCriterion``` (ex: ```MaxTrials```).
     """
-    if transition_criteria_json is None:
-        return None
+    new_dict = {}
+    for key, value in transition_criteria_json.items():
+        new_val = object_from_json(
+            object_json=value,
+            decoder_registry=decoder_registry,
+            class_decoder_registry=class_decoder_registry,
+        )
+        new_dict[key] = new_val
 
-    # TODO: @mgarrard see if this can use deserialize_init_args
-    # and be moved up to a higher level so we don't construct a list
-    criterion_list = []
-    for criterion_json in transition_criteria_json:
-        criterion_type = criterion_json.pop("__type")
-        if criterion_type == "MinimumTrialsInStatus":
-            criterion_list.append(
-                MinimumTrialsInStatus(
-                    status=object_from_json(criterion_json.pop("status")),
-                    threshold=criterion_json.pop("threshold"),
-                    transition_to=criterion_json.pop("transition_to")
-                    if "transition_to" in criterion_json.keys()
-                    else None,
-                )
-            )
-        elif criterion_type == "MinimumPreferenceOccurances":
-            criterion_list.append(
-                MinimumPreferenceOccurances(
-                    metric_name=criterion_json.pop("metric_name"),
-                    threshold=criterion_json.pop("threshold"),
-                    transition_to=criterion_json.pop("transition_to")
-                    if "transition_to" in criterion_json.keys()
-                    else None,
-                    block_transition_if_unmet=criterion_json.pop(
-                        "block_transition_if_unmet", True
-                    ),
-                    block_gen_if_met=criterion_json.pop("block_gen_if_met", False),
-                )
-            )
-        elif criterion_type == "MaxGenerationParallelism":
-            criterion_list.append(
-                MaxGenerationParallelism(
-                    threshold=criterion_json.pop("threshold"),
-                    only_in_statuses=object_from_json(
-                        criterion_json.pop("only_in_statuses"),
-                        decoder_registry=decoder_registry,
-                        class_decoder_registry=class_decoder_registry,
-                    )
-                    if "only_in_statuses" in criterion_json.keys()
-                    else None,
-                    not_in_statuses=object_from_json(
-                        criterion_json.pop("not_in_statuses"),
-                        decoder_registry=decoder_registry,
-                        class_decoder_registry=class_decoder_registry,
-                    )
-                    if "not_in_statuses" in criterion_json.keys()
-                    else None,
-                    transition_to=criterion_json.pop("transition_to", None),
-                    block_transition_if_unmet=criterion_json.pop(
-                        "block_transition_if_unmet", False
-                    ),
-                    block_gen_if_met=criterion_json.pop("block_gen_if_met", True),
-                )
-            )
-        elif criterion_type == "MaxTrials":
-            criterion_list.append(
-                MaxTrials(
-                    threshold=criterion_json.pop("threshold"),
-                    only_in_statuses=object_from_json(
-                        criterion_json.pop("only_in_statuses"),
-                        decoder_registry=decoder_registry,
-                        class_decoder_registry=class_decoder_registry,
-                    )
-                    if "only_in_statuses" in criterion_json.keys()
-                    else None,
-                    not_in_statuses=object_from_json(
-                        criterion_json.pop("not_in_statuses"),
-                        decoder_registry=decoder_registry,
-                        class_decoder_registry=class_decoder_registry,
-                    )
-                    if "not_in_statuses" in criterion_json.keys()
-                    else None,
-                    transition_to=criterion_json.pop("transition_to", None),
-                    block_transition_if_unmet=criterion_json.pop(
-                        "block_transition_if_unmet", False
-                    ),
-                    block_gen_if_met=criterion_json.pop("block_gen_if_met", True),
-                )
-            )
-        elif criterion_type == "MinTrials":
-            criterion_list.append(
-                MinTrials(
-                    threshold=criterion_json.pop("threshold"),
-                    only_in_statuses=object_from_json(
-                        criterion_json.pop("only_in_statuses"),
-                        decoder_registry=decoder_registry,
-                        class_decoder_registry=class_decoder_registry,
-                    )
-                    if "only_in_statuses" in criterion_json.keys()
-                    else None,
-                    not_in_statuses=object_from_json(
-                        criterion_json.pop("not_in_statuses"),
-                        decoder_registry=decoder_registry,
-                        class_decoder_registry=class_decoder_registry,
-                    )
-                    if "not_in_statuses" in criterion_json.keys()
-                    else None,
-                    transition_to=criterion_json.pop("transition_to", None),
-                    block_transition_if_unmet=criterion_json.pop(
-                        "block_transition_if_unmet", False
-                    ),
-                    block_gen_if_met=criterion_json.pop("block_gen_if_met", True),
-                )
-            )
-    return criterion_list
+    return class_(**new_dict)
 
 
 def search_space_from_json(
     search_space_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> SearchSpace:
     """Load a SearchSpace from JSON.
 
     This function is necessary due to the coupled loading of SearchSpace
     and parameter constraints.
     """
     parameters = object_from_json(
@@ -506,21 +404,16 @@
         ),
     )
 
 
 def parameter_constraints_from_json(
     parameter_constraint_json: List[Dict[str, Any]],
     parameters: List[Parameter],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> List[ParameterConstraint]:
     """Load ParameterConstraints from JSON.
 
     Order and SumConstraint are tied to a search space,
     and require that SearchSpace's parameters to be passed in for decoding.
 
     Args:
@@ -560,21 +453,16 @@
             )
     return parameter_constraints
 
 
 def trials_from_json(
     experiment: Experiment,
     trials_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Dict[int, BaseTrial]:
     """Load Ax Trials from JSON."""
     loaded_trials = {}
     for index, batch_json in trials_json.items():
         is_trial = batch_json["__type"] == "Trial"
         batch_json = {
             k: object_from_json(
@@ -591,21 +479,16 @@
             else batch_trial_from_json(experiment=experiment, **batch_json)
         )
     return loaded_trials
 
 
 def data_from_json(
     data_by_trial_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Dict[int, "OrderedDict[int, Data]"]:
     """Load Ax Data from JSON."""
     data_by_trial = object_from_json(
         data_by_trial_json,
         decoder_registry=decoder_registry,
         class_decoder_registry=class_decoder_registry,
     )
@@ -615,21 +498,16 @@
         int(k): OrderedDict({int(k2): v2 for k2, v2 in v.items()})
         for k, v in data_by_trial.items()
     }
 
 
 def multi_type_experiment_from_json(
     object_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> MultiTypeExperiment:
     """Load AE MultiTypeExperiment from JSON."""
     experiment_info = _get_experiment_info(object_json)
 
     _metric_to_canonical_name = object_json.pop("_metric_to_canonical_name")
     _metric_to_trial_type = object_json.pop("_metric_to_trial_type")
     _trial_type_to_runner = object_from_json(
@@ -669,21 +547,16 @@
         class_decoder_registry=class_decoder_registry,
     )
     return experiment
 
 
 def experiment_from_json(
     object_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Experiment:
     """Load Ax Experiment from JSON."""
     experiment_info = _get_experiment_info(object_json)
 
     experiment = Experiment(
         **{
             k: object_from_json(
@@ -714,21 +587,16 @@
         "data_by_trial_json": object_json.pop("data_by_trial"),
     }
 
 
 def _load_experiment_info(
     exp: Experiment,
     exp_info: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> None:
     """Loads `Experiment` object with basic information."""
     exp._time_created = object_from_json(
         exp_info.get("time_created_json"),
         decoder_registry=decoder_registry,
         class_decoder_registry=class_decoder_registry,
     )
@@ -774,153 +642,148 @@
         if k == "recommended_max_parallelism":
             object_json["max_parallelism"] = object_json.pop(k)
     return object_json
 
 
 def generation_node_from_json(
     generation_node_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> GenerationNode:
     """Load GenerationNode object from JSON."""
     return GenerationNode(
         node_name=generation_node_json.pop("node_name"),
         model_specs=object_from_json(
             generation_node_json.pop("model_specs"),
             decoder_registry=decoder_registry,
             class_decoder_registry=class_decoder_registry,
         ),
         # TODO @mgarrad this should probably be a object_from_json but bestmodelselector
         # isn't implemented
         best_model_selector=generation_node_json.pop("best_model_selector", None),
         should_deduplicate=generation_node_json.pop("should_deduplicate", False),
-        gen_unlimited_trials=generation_node_json.pop("gen_unlimited_trials", True),
-        transition_criteria=transition_criteria_from_json(
-            generation_node_json.pop("transition_criteria"),
-            decoder_registry=decoder_registry,
-            class_decoder_registry=class_decoder_registry,
-        )
-        if "transition_criteria" in generation_node_json.keys()
-        else None,
+        transition_criteria=(
+            object_from_json(
+                generation_node_json.pop("transition_criteria"),
+                decoder_registry=decoder_registry,
+                class_decoder_registry=class_decoder_registry,
+            )
+            if "transition_criteria" in generation_node_json.keys()
+            else None
+        ),
     )
 
 
 def generation_step_from_json(
     generation_step_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> GenerationStep:
     """Load generation step from JSON."""
     generation_step_json = _convert_generation_step_keys_for_backwards_compatibility(
         generation_step_json
     )
     kwargs = generation_step_json.pop("model_kwargs", None)
+    kwargs.pop("fit_on_update", None)  # Remove deprecated fit_on_update.
     gen_kwargs = generation_step_json.pop("model_gen_kwargs", None)
     completion_criteria = (
-        transition_criteria_from_json(generation_step_json.pop("completion_criteria"))
+        object_from_json(
+            generation_step_json.pop("completion_criteria"),
+            decoder_registry=decoder_registry,
+            class_decoder_registry=class_decoder_registry,
+        )
         if "completion_criteria" in generation_step_json.keys()
         else []
     )
     generation_step = GenerationStep(
         model=object_from_json(
             generation_step_json.pop("model"),
             decoder_registry=decoder_registry,
             class_decoder_registry=class_decoder_registry,
         ),
         num_trials=generation_step_json.pop("num_trials"),
         min_trials_observed=generation_step_json.pop("min_trials_observed", 0),
-        completion_criteria=completion_criteria
-        if completion_criteria is not None
-        else [],
+        completion_criteria=(
+            completion_criteria if completion_criteria is not None else []
+        ),
         max_parallelism=(generation_step_json.pop("max_parallelism", None)),
         enforce_num_trials=generation_step_json.pop("enforce_num_trials", True),
-        model_kwargs=_decode_callables_from_references(
-            object_from_json(
-                kwargs,
-                decoder_registry=decoder_registry,
-                class_decoder_registry=class_decoder_registry,
-            ),
-        )
-        if kwargs
-        else None,
-        model_gen_kwargs=_decode_callables_from_references(
-            object_from_json(
-                gen_kwargs,
-                decoder_registry=decoder_registry,
-                class_decoder_registry=class_decoder_registry,
-            ),
-        )
-        if gen_kwargs
-        else None,
+        model_kwargs=(
+            _decode_callables_from_references(
+                object_from_json(
+                    kwargs,
+                    decoder_registry=decoder_registry,
+                    class_decoder_registry=class_decoder_registry,
+                ),
+            )
+            if kwargs
+            else None
+        ),
+        model_gen_kwargs=(
+            _decode_callables_from_references(
+                object_from_json(
+                    gen_kwargs,
+                    decoder_registry=decoder_registry,
+                    class_decoder_registry=class_decoder_registry,
+                ),
+            )
+            if gen_kwargs
+            else None
+        ),
         index=generation_step_json.pop("index", -1),
         should_deduplicate=generation_step_json.pop("should_deduplicate", False),
     )
     return generation_step
 
 
 def model_spec_from_json(
     model_spec_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> ModelSpec:
     """Load ModelSpec from JSON."""
     kwargs = model_spec_json.pop("model_kwargs", None)
+    kwargs.pop("fit_on_update", None)  # Remove deprecated fit_on_update.
     gen_kwargs = model_spec_json.pop("model_gen_kwargs", None)
     return ModelSpec(
         model_enum=object_from_json(
             model_spec_json.pop("model_enum"),
             decoder_registry=decoder_registry,
             class_decoder_registry=class_decoder_registry,
         ),
-        model_kwargs=_decode_callables_from_references(
-            object_from_json(
-                kwargs,
-                decoder_registry=decoder_registry,
-                class_decoder_registry=class_decoder_registry,
-            ),
-        )
-        if kwargs
-        else None,
-        model_gen_kwargs=_decode_callables_from_references(
-            object_from_json(
-                gen_kwargs,
-                decoder_registry=decoder_registry,
-                class_decoder_registry=class_decoder_registry,
-            ),
-        )
-        if gen_kwargs
-        else None,
+        model_kwargs=(
+            _decode_callables_from_references(
+                object_from_json(
+                    kwargs,
+                    decoder_registry=decoder_registry,
+                    class_decoder_registry=class_decoder_registry,
+                ),
+            )
+            if kwargs
+            else None
+        ),
+        model_gen_kwargs=(
+            _decode_callables_from_references(
+                object_from_json(
+                    gen_kwargs,
+                    decoder_registry=decoder_registry,
+                    class_decoder_registry=class_decoder_registry,
+                ),
+            )
+            if gen_kwargs
+            else None
+        ),
     )
 
 
 def generation_strategy_from_json(
     generation_strategy_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
     experiment: Optional[Experiment] = None,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> GenerationStrategy:
     """Load generation strategy from JSON."""
     nodes = (
         object_from_json(
             generation_strategy_json.pop("nodes"),
             decoder_registry=decoder_registry,
             class_decoder_registry=class_decoder_registry,
@@ -961,21 +824,16 @@
         class_decoder_registry=class_decoder_registry,
     )
     return gs
 
 
 def surrogate_from_list_surrogate_json(
     list_surrogate_json: Dict[str, Any],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Surrogate:
     logger.warning(
         "`ListSurrogate` has been deprecated. Reconstructing a `Surrogate` "
         "with as similar properties as possible."
     )
     if "submodel_input_transforms" in list_surrogate_json:
         (
@@ -1041,21 +899,16 @@
         ),
         likelihood_options=list_surrogate_json.get("submodel_likelihood_options"),
     )
 
 
 def get_input_transform_json_components(
     input_transforms_json: Optional[Union[List[Dict[str, Any]], Dict[str, Any]]],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Tuple[Optional[List[Dict[str, Any]]], Optional[Dict[str, Any]]]:
     if input_transforms_json is None:
         return None, None
     if isinstance(input_transforms_json, dict):
         # This is a single input transform.
         input_transforms_json = [input_transforms_json]
     else:
@@ -1074,21 +927,16 @@
         for input_transform_json in input_transforms_json
     }
     return input_transform_classes_json, input_transform_options_json
 
 
 def get_outcome_transform_json_components(
     outcome_transforms_json: Optional[List[Dict[str, Any]]],
-    # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use
-    #  `typing.Type` to avoid runtime subscripting errors.
-    decoder_registry: Dict[str, Type] = CORE_DECODER_REGISTRY,
-    # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
-    class_decoder_registry: Dict[
-        str, Callable[[Dict[str, Any]], Any]
-    ] = CORE_CLASS_DECODER_REGISTRY,
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
 ) -> Tuple[Optional[List[Dict[str, Any]]], Optional[Dict[str, Any]]]:
     if outcome_transforms_json is None:
         return None, None
 
     outcome_transforms_json = [
         outcome_transform_json
         for outcome_transform_json in outcome_transforms_json
@@ -1101,7 +949,43 @@
     outcome_transform_options_json = {
         checked_cast(str, outcome_transform_json["__type"]): outcome_transform_json[
             "state_dict"
         ]
         for outcome_transform_json in outcome_transforms_json
     }
     return outcome_transform_classes_json, outcome_transform_options_json
+
+
+def objective_from_json(
+    object_json: Dict[str, Any],
+    decoder_registry: TDecoderRegistry = CORE_DECODER_REGISTRY,
+    class_decoder_registry: TClassDecoderRegistry = CORE_CLASS_DECODER_REGISTRY,
+) -> Objective:
+    """Load an ``Objective`` from JSON in a backwards compatible way.
+
+    If both ``minimize`` and ``lower_is_better`` are specified but have conflicting
+    values, this will overwrite ``lower_is_better=minimize`` to resolve the conflict.
+
+    # TODO: Do we need to do this for scalarized objective as well?
+    """
+    input_args = {
+        k: object_from_json(
+            v,
+            decoder_registry=decoder_registry,
+            class_decoder_registry=class_decoder_registry,
+        )
+        for k, v in object_json.items()
+    }
+    metric = input_args.pop("metric")
+    minimize = input_args.pop("minimize")
+    if metric.lower_is_better is not None and metric.lower_is_better != minimize:
+        logger.warning(
+            f"Metric {metric.name} has {metric.lower_is_better=} but objective "
+            f"specifies {minimize=}. Overwriting ``lower_is_better`` to match "
+            f"the optimization direction {minimize=}."
+        )
+        metric.lower_is_better = minimize
+    return Objective(
+        metric=metric,
+        minimize=minimize,
+        **input_args,  # For future compatibility.
+    )
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/decoders.py` & `ax-platform-0.4.0/ax/storage/json_store/decoders.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import inspect
 import logging
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Type, TYPE_CHECKING, Union
@@ -298,17 +300,19 @@
             "For gpytorch objects, this is likely because the object's "
             "`state_dict` method returns these extra args, which could "
             "indicate that the object's state will not be fully recreated "
             "by this serialization/deserialization method."
         )
     return botorch_class(
         **{
-            k: tensor_or_size_from_json(json=v)
-            if isinstance(v, dict) and "__type" in v
-            else v
+            k: (
+                tensor_or_size_from_json(json=v)
+                if isinstance(v, dict) and "__type" in v
+                else v
+            )
             for k, v in state_dict.items()
         }
     )
 
 
 def pathlib_from_json(pathsegments: Union[str, Iterable[str]]) -> Path:
     if isinstance(pathsegments, str):
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/encoder.py` & `ax-platform-0.4.0/ax/storage/json_store/encoder.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import dataclasses
 import datetime
 import enum
 from collections import OrderedDict
 from inspect import isclass
 from typing import Any, Callable, Dict, Type
 
@@ -17,15 +19,15 @@
 from ax.exceptions.storage import JSONEncodeError
 from ax.storage.json_store.encoders import tensor_to_dict
 from ax.storage.json_store.registry import (
     CORE_CLASS_ENCODER_REGISTRY,
     CORE_ENCODER_REGISTRY,
 )
 from ax.utils.common.serialization import _is_named_tuple
-from ax.utils.common.typeutils import numpy_type_to_python_type
+from ax.utils.common.typeutils_nonnative import numpy_type_to_python_type
 from ax.utils.common.typeutils_torch import torch_type_to_str
 
 
 # pyre-fixme[3]: Return annotation cannot be `Any`.
 def object_to_json(  # noqa C901
     # pyre-fixme[2]: Parameter annotation cannot be `Any`.
     obj: Any,
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/encoders.py` & `ax-platform-0.4.0/ax/storage/json_store/encoders.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import re
 import warnings
 from pathlib import Path
 from typing import Any, Dict, Type
 
 from ax.benchmark.benchmark_problem import (
     BenchmarkProblem,
@@ -135,15 +137,17 @@
     return {
         "__type": benchmark_problem.__class__.__name__,
         "name": benchmark_problem.name,
         "search_space": benchmark_problem.search_space,
         "optimization_config": benchmark_problem.optimization_config,
         "runner": benchmark_problem.runner,
         "num_trials": benchmark_problem.num_trials,
-        "infer_noise": benchmark_problem.infer_noise,
+        "is_noiseless": benchmark_problem.is_noiseless,
+        "observe_noise_sd": benchmark_problem.observe_noise_sd,
+        "has_ground_truth": benchmark_problem.has_ground_truth,
         "tracking_metrics": benchmark_problem.tracking_metrics,
     }
 
 
 def multi_objective_benchmark_problem_to_dict(
     moo_benchmark_problem: MultiObjectiveBenchmarkProblem,
 ) -> Dict[str, Any]:
@@ -151,15 +155,17 @@
     return {
         "__type": moo_benchmark_problem.__class__.__name__,
         "name": moo_benchmark_problem.name,
         "search_space": moo_benchmark_problem.search_space,
         "optimization_config": moo_benchmark_problem.optimization_config,
         "runner": moo_benchmark_problem.runner,
         "num_trials": moo_benchmark_problem.num_trials,
-        "infer_noise": moo_benchmark_problem.infer_noise,
+        "is_noiseless": moo_benchmark_problem.is_noiseless,
+        "observe_noise_sd": moo_benchmark_problem.observe_noise_sd,
+        "has_ground_truth": moo_benchmark_problem.has_ground_truth,
         "tracking_metrics": moo_benchmark_problem.tracking_metrics,
         "maximum_hypervolume": moo_benchmark_problem.maximum_hypervolume,
         "reference_point": moo_benchmark_problem.reference_point,
     }
 
 
 def single_objective_benchmark_problem_to_dict(
@@ -168,15 +174,17 @@
     return {
         "__type": soo_benchmark_problem.__class__.__name__,
         "name": soo_benchmark_problem.name,
         "search_space": soo_benchmark_problem.search_space,
         "optimization_config": soo_benchmark_problem.optimization_config,
         "runner": soo_benchmark_problem.runner,
         "num_trials": soo_benchmark_problem.num_trials,
-        "infer_noise": soo_benchmark_problem.infer_noise,
+        "is_noiseless": soo_benchmark_problem.is_noiseless,
+        "observe_noise_sd": soo_benchmark_problem.observe_noise_sd,
+        "has_ground_truth": soo_benchmark_problem.has_ground_truth,
         "tracking_metrics": soo_benchmark_problem.tracking_metrics,
         "optimal_value": soo_benchmark_problem.optimal_value,
     }
 
 
 def trial_to_dict(trial: Trial) -> Dict[str, Any]:
     """Convert Ax trial to a dictionary."""
@@ -486,15 +494,14 @@
 def generation_node_to_dict(generation_node: GenerationNode) -> Dict[str, Any]:
     """Convert Ax generation node to a dictionary."""
     return {
         "__type": generation_node.__class__.__name__,
         "model_specs": generation_node.model_specs,
         "should_deduplicate": generation_node.should_deduplicate,
         "node_name": generation_node.node_name,
-        "gen_unlimited_trials": generation_node.gen_unlimited_trials,
         "model_spec_to_gen_from": generation_node._model_spec_to_gen_from,
         "transition_criteria": generation_node.transition_criteria,
     }
 
 
 def generation_strategy_to_dict(
     generation_strategy: GenerationStrategy,
@@ -507,17 +514,17 @@
         )
     node_based_gs = generation_strategy.is_node_based
     return {
         "__type": generation_strategy.__class__.__name__,
         "db_id": generation_strategy._db_id,
         "name": generation_strategy.name,
         "steps": generation_strategy._steps if not node_based_gs else [],
-        "curr_index": generation_strategy.current_step_index
-        if not node_based_gs
-        else -1,
+        "curr_index": (
+            generation_strategy.current_step_index if not node_based_gs else -1
+        ),
         "generator_runs": generation_strategy._generator_runs,
         "had_initialized_model": generation_strategy.model is not None,
         "experiment": generation_strategy._experiment,
         "nodes": generation_strategy._nodes,
         "curr_node_name": generation_strategy.current_node_name,
     }
 
@@ -559,22 +566,23 @@
 
 def botorch_model_to_dict(model: BoTorchModel) -> Dict[str, Any]:
     """Convert Ax model to a dictionary."""
     return {
         "__type": model.__class__.__name__,
         "acquisition_class": model.acquisition_class,
         "acquisition_options": model.acquisition_options or {},
-        "surrogate": model._surrogates[Keys.ONLY_SURROGATE]
-        if Keys.ONLY_SURROGATE in model._surrogates
-        else None,
-        "surrogate_specs": model.surrogate_specs
-        if len(model.surrogate_specs) > 0
-        else None,
+        "surrogate": (
+            model._surrogates[Keys.ONLY_SURROGATE]
+            if Keys.ONLY_SURROGATE in model._surrogates
+            else None
+        ),
+        "surrogate_specs": (
+            model.surrogate_specs if len(model.surrogate_specs) > 0 else None
+        ),
         "botorch_acqf_class": model._botorch_acqf_class,
-        "refit_on_update": model.refit_on_update,
         "refit_on_cv": model.refit_on_cv,
         "warm_start_refit": model.warm_start_refit,
     }
 
 
 def surrogate_to_dict(surrogate: Surrogate) -> Dict[str, Any]:
     """Convert Ax surrogate to a dictionary."""
@@ -585,14 +593,15 @@
 
 def tensor_to_dict(obj: Tensor) -> Dict[str, Any]:
     if obj.numel() > 1e4:
         warnings.warn(
             f"Attempting to serialize a tensor with {obj.numel()} elements. "
             "This may result in storage issues.",
             AxStorageWarning,
+            stacklevel=3,
         )
     return {
         "__type": "Tensor",
         "value": obj.tolist(),
         "dtype": {"__type": "torch_dtype", "value": torch_type_to_str(obj.dtype)},
         "device": {"__type": "torch_device", "value": torch_type_to_str(obj.device)},
     }
@@ -661,15 +670,14 @@
     return {
         "__type": strategy.__class__.__name__,
         "metric_names": strategy.metric_names,
         "percentile_threshold": strategy.percentile_threshold,
         "min_progression": strategy.min_progression,
         "min_curves": strategy.min_curves,
         "trial_indices_to_ignore": strategy.trial_indices_to_ignore,
-        "true_objective_metric_name": strategy.true_objective_metric_name,
         "seconds_between_polls": strategy.seconds_between_polls,
         "normalize_progressions": strategy.normalize_progressions,
     }
 
 
 def threshold_early_stopping_strategy_to_dict(
     strategy: ThresholdEarlyStoppingStrategy,
@@ -677,15 +685,14 @@
     """Convert Ax metric-threshold early stopping strategy to a dictionary."""
     return {
         "__type": strategy.__class__.__name__,
         "metric_names": strategy.metric_names,
         "metric_threshold": strategy.metric_threshold,
         "min_progression": strategy.min_progression,
         "trial_indices_to_ignore": strategy.trial_indices_to_ignore,
-        "true_objective_metric_name": strategy.true_objective_metric_name,
         "normalize_progressions": strategy.normalize_progressions,
     }
 
 
 def logical_early_stopping_strategy_to_dict(
     strategy: LogicalEarlyStoppingStrategy,
 ) -> Dict[str, Any]:
@@ -724,15 +731,15 @@
     problem: PyTorchCNNTorchvisionBenchmarkProblem,
 ) -> Dict[str, Any]:
     # unit tests for this in benchmark suite
     return {
         "__type": problem.__class__.__name__,
         "name": not_none(re.compile("(?<=::).*").search(problem.name)).group(),
         "num_trials": problem.num_trials,
-        "infer_noise": problem.infer_noise,
+        "observe_noise_sd": problem.observe_noise_sd,
     }
 
 
 def risk_measure_to_dict(
     risk_measure: RiskMeasure,
 ) -> Dict[str, Any]:
     """Convert a RiskMeasure to a dictionary."""
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/load.py` & `ax-platform-0.4.0/ax/storage/json_store/load.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from typing import Any, Callable, Dict, Type
 
 from ax.core.experiment import Experiment
 from ax.storage.json_store.decoder import object_from_json
 from ax.storage.json_store.registry import (
     CORE_CLASS_DECODER_REGISTRY,
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/registry.py` & `ax-platform-0.4.0/ax/storage/json_store/registry.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,36 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import pathlib
 from typing import Any, Callable, Dict, Type
 
 import torch
 
 from ax.benchmark.benchmark_method import BenchmarkMethod
 from ax.benchmark.benchmark_problem import (
     BenchmarkProblem,
     MultiObjectiveBenchmarkProblem,
     SingleObjectiveBenchmarkProblem,
 )
 from ax.benchmark.benchmark_result import AggregatedBenchmarkResult, BenchmarkResult
+from ax.benchmark.metrics.benchmark import BenchmarkMetric, GroundTruthBenchmarkMetric
+from ax.benchmark.metrics.jenatton import JenattonMetric
 from ax.benchmark.problems.hpo.pytorch_cnn import PyTorchCNNMetric
 from ax.benchmark.problems.hpo.torchvision import (
     PyTorchCNNTorchvisionBenchmarkProblem,
     PyTorchCNNTorchvisionRunner,
 )
-from ax.benchmark.problems.surrogate import SurrogateMetric, SurrogateRunner
+from ax.benchmark.runners.botorch_test import BotorchTestProblemRunner
+from ax.benchmark.runners.surrogate import SurrogateRunner
 from ax.core import ObservationFeatures
 from ax.core.arm import Arm
 from ax.core.base_trial import TrialStatus
 from ax.core.batch_trial import (
     AbandonedArm,
     BatchTrial,
     GeneratorRunStruct,
@@ -65,22 +70,20 @@
     ThresholdEarlyStoppingStrategy,
 )
 from ax.early_stopping.strategies.logical import (
     AndEarlyStoppingStrategy,
     OrEarlyStoppingStrategy,
 )
 from ax.global_stopping.strategies.improvement import ImprovementGlobalStoppingStrategy
-from ax.metrics.botorch_test_problem import BotorchTestProblemMetric
 from ax.metrics.branin import AugmentedBraninMetric, BraninMetric, NegativeBraninMetric
 from ax.metrics.branin_map import BraninTimestampMapMetric
 from ax.metrics.chemistry import ChemistryMetric, ChemistryProblemType
 from ax.metrics.dict_lookup import DictLookupMetric
 from ax.metrics.factorial import FactorialMetric
 from ax.metrics.hartmann6 import AugmentedHartmann6Metric, Hartmann6Metric
-from ax.metrics.jenatton import JenattonMetric
 from ax.metrics.l2norm import L2NormMetric
 from ax.metrics.noisy_function import NoisyFunctionMetric
 from ax.metrics.sklearn import SklearnDataset, SklearnMetric, SklearnModelType
 from ax.modelbridge.factory import Models
 from ax.modelbridge.generation_node import GenerationNode, GenerationStep
 from ax.modelbridge.generation_strategy import GenerationStrategy
 from ax.modelbridge.model_spec import ModelSpec
@@ -94,15 +97,14 @@
     MinTrials,
     TransitionCriterion,
 )
 from ax.models.torch.botorch_modular.acquisition import Acquisition
 from ax.models.torch.botorch_modular.model import BoTorchModel, SurrogateSpec
 from ax.models.torch.botorch_modular.surrogate import Surrogate
 from ax.models.winsorization_config import WinsorizationConfig
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
 from ax.runners.synthetic import SyntheticRunner
 from ax.service.utils.scheduler_options import SchedulerOptions, TrialType
 from ax.storage.json_store.decoders import (
     class_from_json,
     input_transform_type_from_json,
     outcome_transform_type_from_json,
     pathlib_from_json,
@@ -175,15 +177,15 @@
     Arm: arm_to_dict,
     AndEarlyStoppingStrategy: logical_early_stopping_strategy_to_dict,
     AugmentedBraninMetric: metric_to_dict,
     AugmentedHartmann6Metric: metric_to_dict,
     BatchTrial: batch_to_dict,
     BenchmarkProblem: benchmark_problem_to_dict,
     BoTorchModel: botorch_model_to_dict,
-    BotorchTestProblemMetric: metric_to_dict,
+    BenchmarkMetric: metric_to_dict,
     BotorchTestProblemRunner: runner_to_dict,
     BraninMetric: metric_to_dict,
     BraninTimestampMapMetric: metric_to_dict,
     ChainedInputTransform: botorch_component_to_dict,
     ChoiceParameter: choice_parameter_to_dict,
     Data: data_to_dict,
     DictLookupMetric: metric_to_dict,
@@ -191,14 +193,15 @@
     FactorialMetric: metric_to_dict,
     FixedParameter: fixed_parameter_to_dict,
     GammaPrior: botorch_component_to_dict,
     GenerationStep: generation_step_to_dict,
     GenerationNode: generation_node_to_dict,
     GenerationStrategy: generation_strategy_to_dict,
     GeneratorRun: generator_run_to_dict,
+    GroundTruthBenchmarkMetric: metric_to_dict,
     Hartmann6Metric: metric_to_dict,
     ImprovementGlobalStoppingStrategy: improvement_global_stopping_strategy_to_dict,
     Interval: botorch_component_to_dict,
     JenattonMetric: metric_to_dict,
     L2NormMetric: metric_to_dict,
     MapData: map_data_to_dict,
     MapKeyInfo: map_key_info_to_dict,
@@ -244,15 +247,15 @@
     TransitionCriterion: transition_criterion_to_dict,
     ScalarizedObjective: scalarized_objective_to_dict,
     SearchSpace: search_space_to_dict,
     SingleObjectiveBenchmarkProblem: single_objective_benchmark_problem_to_dict,
     HierarchicalSearchSpace: search_space_to_dict,
     SumConstraint: sum_parameter_constraint_to_dict,
     Surrogate: surrogate_to_dict,
-    SurrogateMetric: metric_to_dict,
+    BenchmarkMetric: metric_to_dict,
     SurrogateRunner: runner_to_dict,
     SyntheticRunner: runner_to_dict,
     ThresholdEarlyStoppingStrategy: threshold_early_stopping_strategy_to_dict,
     Trial: trial_to_dict,
     ObservationFeatures: observation_features_to_dict,
     WinsorizationConfig: winsorization_config_to_dict,
 }
@@ -282,18 +285,19 @@
     "AndEarlyStoppingStrategy": AndEarlyStoppingStrategy,
     "AugmentedBraninMetric": AugmentedBraninMetric,
     "AugmentedHartmann6Metric": AugmentedHartmann6Metric,
     "Arm": Arm,
     "BatchTrial": BatchTrial,
     "AggregatedBenchmarkResult": AggregatedBenchmarkResult,
     "BenchmarkMethod": BenchmarkMethod,
+    "BenchmarkMetric": BenchmarkMetric,
     "BenchmarkProblem": BenchmarkProblem,
     "BenchmarkResult": BenchmarkResult,
     "BoTorchModel": BoTorchModel,
-    "BotorchTestProblemMetric": BotorchTestProblemMetric,
+    "BotorchTestProblemMetric": BenchmarkMetric,  # backward-compatibility
     "BotorchTestProblemRunner": BotorchTestProblemRunner,
     "BraninMetric": BraninMetric,
     "BraninTimestampMapMetric": BraninTimestampMapMetric,
     "ChainedInputTransform": ChainedInputTransform,
     "ChemistryMetric": ChemistryMetric,
     "ChemistryProblemType": ChemistryProblemType,
     "ChoiceParameter": ChoiceParameter,
@@ -307,26 +311,29 @@
     "FixedParameter": FixedParameter,
     "GammaPrior": GammaPrior,
     "GenerationNode": GenerationNode,
     "GenerationStrategy": GenerationStrategy,
     "GenerationStep": GenerationStep,
     "GeneratorRun": GeneratorRun,
     "GeneratorRunStruct": GeneratorRunStruct,
+    "GroundTruthBenchmarkMetric": GroundTruthBenchmarkMetric,
+    "GroundTruthBotorchTestProblemMetric": GroundTruthBenchmarkMetric,  # for BC
     "Hartmann6Metric": Hartmann6Metric,
     "HierarchicalSearchSpace": HierarchicalSearchSpace,
     "ImprovementGlobalStoppingStrategy": ImprovementGlobalStoppingStrategy,
     "Interval": Interval,
     "JenattonMetric": JenattonMetric,
     "LifecycleStage": LifecycleStage,
     "ListSurrogate": Surrogate,  # For backwards compatibility
     "L2NormMetric": L2NormMetric,
     "MapData": MapData,
     "MapMetric": MapMetric,
     "MapKeyInfo": MapKeyInfo,
     "MaxTrials": MaxTrials,
+    "MaxGenerationParallelism": MaxGenerationParallelism,
     "Metric": Metric,
     "MinTrials": MinTrials,
     "MinimumTrialsInStatus": MinimumTrialsInStatus,
     "MinimumPreferenceOccurances": MinimumPreferenceOccurances,
     "Models": Models,
     "ModelRegistryBase": ModelRegistryBase,
     "ModelSpec": ModelSpec,
@@ -366,15 +373,15 @@
     "SearchSpace": SearchSpace,
     "SingleObjectiveBenchmarkProblem": SingleObjectiveBenchmarkProblem,
     "SklearnDataset": SklearnDataset,
     "SklearnMetric": SklearnMetric,
     "SklearnModelType": SklearnModelType,
     "SumConstraint": SumConstraint,
     "Surrogate": Surrogate,
-    "SurrogateMetric": SurrogateMetric,
+    "SurrogateMetric": BenchmarkMetric,  # backward-compatiblity
     # NOTE: SurrogateRunners -> SyntheticRunner on load due to complications
     "SurrogateRunner": SyntheticRunner,
     "SyntheticRunner": SyntheticRunner,
     "SurrogateSpec": SurrogateSpec,
     "Trial": Trial,
     "TrialType": TrialType,
     "TrialStatus": TrialStatus,
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/save.py` & `ax-platform-0.4.0/ax/storage/json_store/save.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from typing import Any, Callable, Dict, Type
 
 from ax.core.experiment import Experiment
 from ax.storage.json_store.encoder import object_to_json
 from ax.storage.json_store.registry import (
     CORE_CLASS_ENCODER_REGISTRY,
```

### Comparing `ax-platform-0.3.7/ax/storage/json_store/tests/test_json_store.py` & `ax-platform-0.4.0/ax/storage/json_store/tests/test_json_store.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 import tempfile
 from functools import partial
 
 import numpy as np
 import torch
+from ax.benchmark.metrics.jenatton import JenattonMetric
 from ax.core.metric import Metric
+from ax.core.objective import Objective
 from ax.core.runner import Runner
 from ax.exceptions.core import AxStorageWarning
 from ax.exceptions.storage import JSONDecodeError, JSONEncodeError
-from ax.metrics.jenatton import JenattonMetric
+from ax.modelbridge.generation_node import GenerationStep
 from ax.modelbridge.generation_strategy import GenerationStrategy
 from ax.modelbridge.registry import Models
 from ax.storage.json_store.decoder import (
     generation_strategy_from_json,
     object_from_json,
 )
 from ax.storage.json_store.decoders import botorch_component_from_json, class_from_json
@@ -95,28 +99,28 @@
     get_order_constraint,
     get_outcome_constraint,
     get_parameter_constraint,
     get_parameter_distribution,
     get_pathlib_path,
     get_percentile_early_stopping_strategy,
     get_percentile_early_stopping_strategy_with_non_objective_metric_name,
-    get_percentile_early_stopping_strategy_with_true_objective_metric_name,
     get_range_parameter,
     get_risk_measure,
     get_robust_search_space,
     get_scalarized_objective,
     get_scheduler_options_batch_trial,
     get_search_space,
     get_sebo_acquisition_class,
     get_sum_constraint1,
     get_sum_constraint2,
     get_surrogate,
     get_synthetic_runner,
     get_threshold_early_stopping_strategy,
     get_trial,
+    get_trial_based_criterion,
     get_winsorization_config,
 )
 from ax.utils.testing.modeling_stubs import (
     get_generation_strategy,
     get_input_transform_type,
     get_observation_features,
     get_outcome_transfrom_type,
@@ -190,18 +194,14 @@
     ("OrEarlyStoppingStrategy", get_or_early_stopping_strategy),
     ("OrderConstraint", get_order_constraint),
     ("OutcomeConstraint", get_outcome_constraint),
     ("Path", get_pathlib_path),
     ("PercentileEarlyStoppingStrategy", get_percentile_early_stopping_strategy),
     (
         "PercentileEarlyStoppingStrategy",
-        get_percentile_early_stopping_strategy_with_true_objective_metric_name,
-    ),
-    (
-        "PercentileEarlyStoppingStrategy",
         get_percentile_early_stopping_strategy_with_non_objective_metric_name,
     ),
     ("ParameterConstraint", get_parameter_constraint),
     ("ParameterDistribution", get_parameter_distribution),
     ("RangeParameter", get_range_parameter),
     ("RiskMeasure", get_risk_measure),
     ("RobustSearchSpace", get_robust_search_space),
@@ -217,23 +217,25 @@
     ("Type[Acquisition]", get_acquisition_type),
     ("Type[AcquisitionFunction]", get_acquisition_function_type),
     ("Type[Model]", get_model_type),
     ("Type[MarginalLogLikelihood]", get_mll_type),
     ("Type[Transform]", get_transform_type),
     ("Type[InputTransform]", get_input_transform_type),
     ("Type[OutcomeTransform]", get_outcome_transfrom_type),
+    ("TransitionCriterionList", get_trial_based_criterion),
     ("ThresholdEarlyStoppingStrategy", get_threshold_early_stopping_strategy),
     ("Trial", get_trial),
     ("WinsorizationConfig", get_winsorization_config),
     ("SEBOAcquisition", get_sebo_acquisition_class),
 ]
 
 
 class JSONStoreTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.experiment = get_experiment_with_batch_and_single_trial()
 
     def test_JSONEncodeFailure(self) -> None:
         with self.assertRaises(JSONEncodeError):
             object_to_json(
                 obj=RuntimeError("foobar"),
                 encoder_registry=CORE_ENCODER_REGISTRY,
@@ -649,7 +651,41 @@
             expected_json = botorch_component_to_dict(interval)
             expected_json["state_dict"]["foo"] = "bar"
             botorch_component_from_json(interval.__class__, expected_json)
         with self.assertRaisesRegex(ValueError, "Missing required initialization args"):
             expected_json = botorch_component_to_dict(interval)
             del expected_json["state_dict"]["lower_bound"]
             botorch_component_from_json(interval.__class__, expected_json)
+
+    def test_objective_backwards_compatibility(self) -> None:
+        # Test that we can load an objective that has conflicting
+        # ``lower_is_better`` and ``minimize`` fields.
+        objective = get_objective(minimize=True)
+        objective.metric.lower_is_better = False  # for conflict!
+        objective_json = object_to_json(objective)
+        self.assertTrue(objective_json["minimize"])
+        self.assertFalse(objective_json["metric"]["lower_is_better"])
+        objective_loaded = object_from_json(objective_json)
+        self.assertIsInstance(objective_loaded, Objective)
+        self.assertNotEqual(objective, objective_loaded)
+        self.assertTrue(objective_loaded.minimize)
+        self.assertTrue(objective_loaded.metric.lower_is_better)
+
+    def test_generation_step_backwards_compatibility(self) -> None:
+        # Test that we can load a generation step with fit_on_update.
+        json = {
+            "__type": "GenerationStep",
+            "model": {"__type": "Models", "name": "BOTORCH_MODULAR"},
+            "num_trials": 5,
+            "min_trials_observed": 0,
+            "completion_criteria": [],
+            "max_parallelism": None,
+            "use_update": False,
+            "enforce_num_trials": True,
+            "model_kwargs": {"fit_on_update": False, "other_kwarg": 5},
+            "model_gen_kwargs": {},
+            "index": -1,
+            "should_deduplicate": False,
+        }
+        generation_step = object_from_json(json)
+        self.assertIsInstance(generation_step, GenerationStep)
+        self.assertEqual(generation_step.model_kwargs, {"other_kwarg": 5})
```

### Comparing `ax-platform-0.3.7/ax/storage/metric_registry.py` & `ax-platform-0.4.0/ax/storage/metric_registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Callable, Dict, Optional, Tuple, Type
 
 from ax.core.map_metric import MapMetric
 from ax.core.metric import Metric
 from ax.metrics.branin import BraninMetric
 from ax.metrics.branin_map import BraninTimestampMapMetric
@@ -101,17 +103,15 @@
     """Add custom metric classes to the SQA and JSON registries.
     For the SQA registry, if no int is specified, use a hash of the class name.
     """
     metric_registry = metric_registry or {Metric: 1}
 
     new_metric_registry = {
         **{
-            metric_cls: val
-            if val
-            else abs(stable_hash(metric_cls.__name__)) % (10**5)
+            metric_cls: val if val else abs(stable_hash(metric_cls.__name__)) % (10**5)
             for metric_cls, val in metric_clss.items()
         },
         **metric_registry,
     }
     new_encoder_registry = {
         **{metric_cls: metric_to_dict for metric_cls in metric_clss},
         **encoder_registry,
```

### Comparing `ax-platform-0.3.7/ax/storage/registry_bundle.py` & `ax-platform-0.4.0/ax/storage/registry_bundle.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from abc import ABC, abstractproperty
 from typing import Any, Callable, ChainMap, Dict, Optional, Type
 
 from ax.core.metric import Metric
 from ax.core.runner import Runner
```

### Comparing `ax-platform-0.3.7/ax/storage/runner_registry.py` & `ax-platform-0.4.0/ax/storage/runner_registry.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Callable, Dict, Optional, Tuple, Type
 
 from ax.core.runner import Runner
 from ax.runners.synthetic import SyntheticRunner
 from ax.storage.json_store.encoders import runner_to_dict
 from ax.storage.json_store.registry import CORE_DECODER_REGISTRY, CORE_ENCODER_REGISTRY
@@ -82,17 +84,15 @@
     Dict[str, Type],
 ]:
     """Add custom runner classes to the SQA and JSON registries.
     For the SQA registry, if no int is specified, use a hash of the class name.
     """
     new_runner_registry = {
         **{
-            runner_cls: val
-            if val
-            else abs(stable_hash(runner_cls.__name__)) % (10**5)
+            runner_cls: val if val else abs(stable_hash(runner_cls.__name__)) % (10**5)
             for runner_cls, val in runner_clss.items()
         },
         **runner_registry,
     }
     new_encoder_registry = {
         **{runner_cls: runner_to_dict for runner_cls in runner_clss},
         **encoder_registry,
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/db.py` & `ax-platform-0.4.0/ax/storage/sqa_store/db.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from contextlib import contextmanager
 from typing import Any, Callable, Generator, Optional, TypeVar
 
 from sqlalchemy import create_engine
 from sqlalchemy.engine.base import Engine
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/decoder.py` & `ax-platform-0.4.0/ax/storage/sqa_store/decoder.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from collections import defaultdict, OrderedDict
 from enum import Enum
+from io import StringIO
 from logging import Logger
 from typing import Any, cast, Dict, List, Optional, Tuple, Type, Union
 
 import pandas as pd
 from ax.core.arm import Arm
 from ax.core.base_trial import BaseTrial, TrialStatus
 from ax.core.batch_trial import AbandonedArm, BatchTrial, GeneratorRunStruct
@@ -629,15 +632,14 @@
         weights = []
         opt_config = None
         search_space = None
 
         for arm_sqa in generator_run_sqa.arms:
             arms.append(self.arm_from_sqa(arm_sqa=arm_sqa))
             weights.append(arm_sqa.weight)
-
         if not reduced_state and not immutable_search_space_and_opt_config:
             (
                 opt_config,
                 tracking_metrics,
             ) = self.opt_config_and_tracking_metrics_from_sqa(
                 metrics_sqa=generator_run_sqa.metrics
             )
@@ -679,41 +681,49 @@
             best_arm_predictions=best_arm_predictions,  # pyre-ignore[6]
             # pyre-fixme[6]: Expected `Optional[Tuple[typing.Dict[str, List[float]],
             #  typing.Dict[str, typing.Dict[str, List[float]]]]]` for 8th param but got
             #  `Optional[typing.Tuple[Union[typing.Dict[str, List[float]],
             #  typing.Dict[str, typing.Dict[str, List[float]]]], ...]]`.
             model_predictions=model_predictions,
             model_key=generator_run_sqa.model_key,
-            model_kwargs=None
-            if reduced_state
-            else object_from_json(
-                generator_run_sqa.model_kwargs,
-                decoder_registry=self.config.json_decoder_registry,
-                class_decoder_registry=self.config.json_class_decoder_registry,
+            model_kwargs=(
+                None
+                if reduced_state
+                else object_from_json(
+                    generator_run_sqa.model_kwargs,
+                    decoder_registry=self.config.json_decoder_registry,
+                    class_decoder_registry=self.config.json_class_decoder_registry,
+                )
             ),
-            bridge_kwargs=None
-            if reduced_state
-            else object_from_json(
-                generator_run_sqa.bridge_kwargs,
-                decoder_registry=self.config.json_decoder_registry,
-                class_decoder_registry=self.config.json_class_decoder_registry,
+            bridge_kwargs=(
+                None
+                if reduced_state
+                else object_from_json(
+                    generator_run_sqa.bridge_kwargs,
+                    decoder_registry=self.config.json_decoder_registry,
+                    class_decoder_registry=self.config.json_class_decoder_registry,
+                )
             ),
-            gen_metadata=None
-            if reduced_state
-            else object_from_json(
-                generator_run_sqa.gen_metadata,
-                decoder_registry=self.config.json_decoder_registry,
-                class_decoder_registry=self.config.json_class_decoder_registry,
+            gen_metadata=(
+                None
+                if reduced_state
+                else object_from_json(
+                    generator_run_sqa.gen_metadata,
+                    decoder_registry=self.config.json_decoder_registry,
+                    class_decoder_registry=self.config.json_class_decoder_registry,
+                )
             ),
-            model_state_after_gen=None
-            if reduced_state
-            else object_from_json(
-                generator_run_sqa.model_state_after_gen,
-                decoder_registry=self.config.json_decoder_registry,
-                class_decoder_registry=self.config.json_class_decoder_registry,
+            model_state_after_gen=(
+                None
+                if reduced_state
+                else object_from_json(
+                    generator_run_sqa.model_state_after_gen,
+                    decoder_registry=self.config.json_decoder_registry,
+                    class_decoder_registry=self.config.json_class_decoder_registry,
+                )
             ),
             generation_step_index=generator_run_sqa.generation_step_index,
             candidate_metadata_by_arm_signature=object_from_json(
                 generator_run_sqa.candidate_metadata_by_arm_signature,
                 decoder_registry=self.config.json_decoder_registry,
                 class_decoder_registry=self.config.json_class_decoder_registry,
             ),
@@ -969,15 +979,15 @@
                 else {}
             )
         )
 
         # Override df from deserialize_init_args with `data_json`.
         # NOTE: Need dtype=False, otherwise infers arm_names like
         # "4_1" should be int 41.
-        kwargs["df"] = pd.read_json(data_sqa.data_json, dtype=False)
+        kwargs["df"] = pd.read_json(StringIO(data_sqa.data_json), dtype=False)
 
         dat = data_constructor(**kwargs)
 
         dat.db_id = data_sqa.id
         return dat
 
     def _metric_from_sqa_util(self, metric_sqa: SQAMetric) -> Metric:
@@ -1014,15 +1024,24 @@
                 "Cannot decode SQAMetric to Objective because minimize is None."
             )
         if metric_sqa.scalarized_objective_weight is not None:
             raise SQADecodeError(
                 f"The metric {metric.name} corresponding to regular objective does not "
                 "have weight attribute"
             )
-        return Objective(metric=metric, minimize=metric_sqa.minimize)
+        # Resolve any conflicts between ``lower_is_better`` and ``minimize``.
+        minimize = metric_sqa.minimize
+        if metric.lower_is_better is not None and metric.lower_is_better != minimize:
+            logger.warning(
+                f"Metric {metric.name} has {metric.lower_is_better=} but objective "
+                f"specifies {minimize=}. Overwriting ``lower_is_better`` to match "
+                f"the optimization direction {minimize=}."
+            )
+            metric.lower_is_better = minimize
+        return Objective(metric=metric, minimize=minimize)
 
     def _multi_objective_from_sqa(self, parent_metric_sqa: SQAMetric) -> Objective:
         try:
             metrics_sqa_children = (
                 parent_metric_sqa.scalarized_objective_children_metrics
             )
         except DetachedInstanceError:
@@ -1040,17 +1059,17 @@
             "skip_runners_and_metrics"
         ):
             for child_metric in metrics_sqa_children:
                 child_metric.metric_type = self.config.metric_registry[Metric]
 
         # Extracting metric and weight for each child
         objectives = [
-            Objective(
+            self._objective_from_sqa(
                 metric=self._metric_from_sqa_util(parent_metric_sqa),
-                minimize=parent_metric_sqa.minimize,
+                metric_sqa=parent_metric_sqa,
             )
             for parent_metric_sqa in metrics_sqa_children
         ]
 
         multi_objective = MultiObjective(objectives=objectives)
         multi_objective.db_id = parent_metric_sqa.id
         return multi_objective
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/delete.py` & `ax-platform-0.4.0/ax/storage/sqa_store/delete.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Optional
 
 from ax.core.experiment import Experiment
 from ax.modelbridge.generation_strategy import GenerationStrategy
 from ax.storage.sqa_store.db import session_scope
 from ax.storage.sqa_store.decoder import Decoder
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/encoder.py` & `ax-platform-0.4.0/ax/storage/sqa_store/encoder.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from enum import Enum
 
 from logging import Logger
 from typing import Any, cast, Dict, List, Optional, Tuple, Type
 
 from ax.core.arm import Arm
 from ax.core.base_trial import BaseTrial
@@ -783,42 +785,50 @@
             fit_time=generator_run.fit_time,
             gen_time=generator_run.gen_time,
             best_arm_name=best_arm_name,
             best_arm_parameters=best_arm_parameters,
             best_arm_predictions=best_arm_predictions,
             model_predictions=model_predictions,
             model_key=generator_run._model_key,
-            model_kwargs=object_to_json(
-                generator_run._model_kwargs,
-                encoder_registry=self.config.json_encoder_registry,
-                class_encoder_registry=self.config.json_class_encoder_registry,
-            )
-            if not reduced_state
-            else None,
-            bridge_kwargs=object_to_json(
-                generator_run._bridge_kwargs,
-                encoder_registry=self.config.json_encoder_registry,
-                class_encoder_registry=self.config.json_class_encoder_registry,
-            )
-            if not reduced_state
-            else None,
-            gen_metadata=object_to_json(
-                generator_run._gen_metadata,
-                encoder_registry=self.config.json_encoder_registry,
-                class_encoder_registry=self.config.json_class_encoder_registry,
-            )
-            if not reduced_state
-            else None,
-            model_state_after_gen=object_to_json(
-                generator_run._model_state_after_gen,
-                encoder_registry=self.config.json_encoder_registry,
-                class_encoder_registry=self.config.json_class_encoder_registry,
-            )
-            if not reduced_state
-            else None,
+            model_kwargs=(
+                object_to_json(
+                    generator_run._model_kwargs,
+                    encoder_registry=self.config.json_encoder_registry,
+                    class_encoder_registry=self.config.json_class_encoder_registry,
+                )
+                if not reduced_state
+                else None
+            ),
+            bridge_kwargs=(
+                object_to_json(
+                    generator_run._bridge_kwargs,
+                    encoder_registry=self.config.json_encoder_registry,
+                    class_encoder_registry=self.config.json_class_encoder_registry,
+                )
+                if not reduced_state
+                else None
+            ),
+            gen_metadata=(
+                object_to_json(
+                    generator_run._gen_metadata,
+                    encoder_registry=self.config.json_encoder_registry,
+                    class_encoder_registry=self.config.json_class_encoder_registry,
+                )
+                if not reduced_state
+                else None
+            ),
+            model_state_after_gen=(
+                object_to_json(
+                    generator_run._model_state_after_gen,
+                    encoder_registry=self.config.json_encoder_registry,
+                    class_encoder_registry=self.config.json_class_encoder_registry,
+                )
+                if not reduced_state
+                else None
+            ),
             generation_step_index=generator_run._generation_step_index,
             candidate_metadata_by_arm_signature=object_to_json(
                 generator_run._candidate_metadata_by_arm_signature,
                 encoder_registry=self.config.json_encoder_registry,
                 class_encoder_registry=self.config.json_class_encoder_registry,
             ),
             generation_node_name=generator_run._generation_node_name,
@@ -850,33 +860,39 @@
             gr_sqa = self.generator_run_to_sqa(gr, reduced_state=reduced_state)
             generator_runs_sqa.append(gr_sqa)
 
         # pyre-fixme[29]: `SQAGenerationStrategy` is not a function.
         gs_sqa = gs_class(
             id=generation_strategy.db_id,
             name=generation_strategy.name,
-            steps=object_to_json(
-                generation_strategy._steps,
-                encoder_registry=self.config.json_encoder_registry,
-                class_encoder_registry=self.config.json_class_encoder_registry,
-            )
-            if not node_based_strategy
-            else [],
-            curr_index=generation_strategy.current_step_index
-            if not node_based_strategy
-            else -1,
+            steps=(
+                object_to_json(
+                    generation_strategy._steps,
+                    encoder_registry=self.config.json_encoder_registry,
+                    class_encoder_registry=self.config.json_class_encoder_registry,
+                )
+                if not node_based_strategy
+                else []
+            ),
+            curr_index=(
+                generation_strategy.current_step_index
+                if not node_based_strategy
+                else -1
+            ),
             generator_runs=generator_runs_sqa,
             experiment_id=experiment_id,
-            nodes=object_to_json(
-                generation_strategy._nodes,
-                encoder_registry=self.config.json_encoder_registry,
-                class_encoder_registry=self.config.json_class_encoder_registry,
-            )
-            if node_based_strategy
-            else [],
+            nodes=(
+                object_to_json(
+                    generation_strategy._nodes,
+                    encoder_registry=self.config.json_encoder_registry,
+                    class_encoder_registry=self.config.json_class_encoder_registry,
+                )
+                if node_based_strategy
+                else []
+            ),
             curr_node_name=generation_strategy.current_node_name,
         )
         return gs_sqa
 
     def runner_to_sqa(
         self, runner: Runner, trial_type: Optional[str] = None
     ) -> SQARunner:
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/json.py` & `ax-platform-0.4.0/ax/storage/sqa_store/json.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from json import JSONDecodeError
 from typing import Any, Dict, List, Optional
 
 from ax.storage.sqa_store.db import JSON_FIELD_LENGTH, LONGTEXT_BYTES, MEDIUMTEXT_BYTES
 from sqlalchemy.ext.mutable import MutableDict, MutableList
 from sqlalchemy.types import Text, TypeDecorator, VARCHAR
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/load.py` & `ax-platform-0.4.0/ax/storage/sqa_store/load.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from math import ceil
 from typing import Any, cast, Dict, List, Optional, Type
 
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
 from ax.core.metric import Metric
 from ax.core.trial import Trial
@@ -19,14 +21,15 @@
     get_query_options_to_defer_immutable_duplicates,
     get_query_options_to_defer_large_model_cols,
 )
 from ax.storage.sqa_store.sqa_classes import (
     SQAExperiment,
     SQAGenerationStrategy,
     SQAGeneratorRun,
+    SQAMetric,
     SQATrial,
 )
 from ax.storage.sqa_store.sqa_config import SQAConfig
 
 from ax.storage.utils import MetricIntent
 from ax.utils.common.constants import Keys
 from ax.utils.common.typeutils import checked_cast, not_none
@@ -133,29 +136,27 @@
     # NOTE: Currently, we load metrics and then convert mostly go get
     # "lower_is_better". Alternatively, we can "nolod" all attributes except
     # "lower_is_better" or any other attribute we want to include. This can be
     # implemented in the future if we need to.
     if skip_runners_and_metrics:
         base_metric_type_int = decoder.config.metric_registry[Metric]
         for sqa_metric in experiment_sqa.metrics:
-            sqa_metric.metric_type = base_metric_type_int
-            # Handle multi-objective metrics that are not directly attached to
-            # the experiment
-            if sqa_metric.intent == MetricIntent.MULTI_OBJECTIVE:
-                if sqa_metric.properties is None:
-                    sqa_metric.properties = {}
-                sqa_metric.properties["skip_runners_and_metrics"] = True
+            _set_sqa_metric_to_base_type(
+                sqa_metric, base_metric_type_int=base_metric_type_int
+            )
 
         assign_metric_on_gr = not reduced_state and not imm_OC_and_SS
         if assign_metric_on_gr:
             try:
                 for sqa_trial in experiment_sqa.trials:
                     for sqa_generator_run in sqa_trial.generator_runs:
                         for sqa_metric in sqa_generator_run.metrics:
-                            sqa_metric.metric_type = base_metric_type_int
+                            _set_sqa_metric_to_base_type(
+                                sqa_metric, base_metric_type_int=base_metric_type_int
+                            )
             except DetachedInstanceError as e:
                 raise DetachedInstanceError(
                     "Unable to retrieve metric from SQA generator run, possibly due "
                     "to parts of the experiment being lazy-loaded. This is not "
                     f"expected state, please contact Ax support. Original error: {e}"
                 )
 
@@ -208,15 +209,14 @@
     experiment_id: int,
     trial_sqa_class: Type[SQATrial],
     load_trials_in_batches_of_size: Optional[int] = None,
     # pyre-fixme[2]: Parameter annotation cannot contain `Any`.
     trials_query_options: Optional[List[Any]] = None,
     skip_runners_and_metrics: bool = False,
 ) -> List[SQATrial]:
-
     """Obtains SQLAlchemy trial objects for given experiment ID from DB,
     optionally in mini-batches and with specified query options.
     """
     with session_scope() as session:
         query = session.query(trial_sqa_class.id).filter_by(experiment_id=experiment_id)
         trial_db_ids = query.all()
         trial_db_ids = [db_id_tuple[0] for db_id_tuple in trial_db_ids]
@@ -245,14 +245,29 @@
                 query = query.options(noload("runner"))
 
             sqa_trials.extend(query.all())
 
     return sqa_trials
 
 
+def _set_sqa_metric_to_base_type(
+    sqa_metric: SQAMetric, base_metric_type_int: int
+) -> None:
+    """Sets metric type to base type, since we don't want to load
+    the metric class from the DB.
+    """
+    sqa_metric.metric_type = base_metric_type_int
+    # Handle multi-objective metrics that are not directly attached to
+    # the experiment
+    if sqa_metric.intent == MetricIntent.MULTI_OBJECTIVE:
+        if sqa_metric.properties is None:
+            sqa_metric.properties = {}
+        sqa_metric.properties["skip_runners_and_metrics"] = True
+
+
 def _get_experiment_sqa_reduced_state(
     experiment_name: str,
     exp_sqa_class: Type[SQAExperiment],
     trial_sqa_class: Type[SQATrial],
     load_trials_in_batches_of_size: Optional[int] = None,
     skip_runners_and_metrics: bool = False,
 ) -> SQAExperiment:
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/reduced_state.py` & `ax-platform-0.4.0/ax/storage/sqa_store/reduced_state.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List
 
 from ax.storage.sqa_store.sqa_classes import SQAGeneratorRun
 from sqlalchemy.orm import defaultload, lazyload, strategy_options
 from sqlalchemy.orm.attributes import InstrumentedAttribute
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/save.py` & `ax-platform-0.4.0/ax/storage/sqa_store/save.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 
 from logging import Logger
-from typing import Any, Callable, Dict, List, Optional, Sequence, Union
+from typing import Any, Callable, cast, Dict, List, Optional, Sequence, Type, Union
 
 from ax.core.base_trial import BaseTrial
+from ax.core.data import Data
 from ax.core.experiment import Experiment
 from ax.core.generator_run import GeneratorRun
 from ax.core.metric import Metric
 from ax.core.outcome_constraint import ObjectiveThreshold, OutcomeConstraint
 from ax.core.runner import Runner
 from ax.core.trial import Trial
 from ax.exceptions.core import UserInputError
@@ -209,29 +212,27 @@
 ) -> None:
     """Add new trials to the experiment, or update if they already exist.
 
     Note that new data objects (whether attached to existing or new trials)
     will also be added to the experiment, but existing data objects in the
     database will *not* be updated or removed.
     """
-    experiment_id = experiment._db_id
-    if experiment_id is None:
-        raise ValueError("Must save experiment first.")
-
-    # pyre-fixme[53]: Captured variable `experiment_id` is not annotated.
-    # pyre-fixme[3]: Return type must be annotated.
-    def add_experiment_id(sqa: Union[SQATrial, SQAData]):
+    if experiment._db_id is None:
+        raise ValueError("Must save experiment before saving/updating its trials.")
+
+    experiment_id: int = experiment._db_id
+
+    def add_experiment_id(sqa: Union[SQATrial, SQAData]) -> None:
         sqa.experiment_id = experiment_id
 
     if reduce_state_generator_runs:
         latest_trial = trials[-1]
         trials_to_reduce_state = trials[0:-1]
 
-        # pyre-fixme[3]: Return type must be annotated.
-        def trial_to_reduced_state_sqa_encoder(t: BaseTrial):
+        def trial_to_reduced_state_sqa_encoder(t: BaseTrial) -> SQATrial:
             return encoder.trial_to_sqa(t, generator_run_reduced_state=True)
 
         _bulk_merge_into_session(
             objs=trials_to_reduce_state,
             encode_func=trial_to_reduced_state_sqa_encoder,
             decode_func=decoder.trial_from_sqa,
             decode_args_list=[{"experiment": experiment} for _ in range(len(trials))],
@@ -253,24 +254,40 @@
             encode_func=encoder.trial_to_sqa,
             decode_func=decoder.trial_from_sqa,
             decode_args_list=[{"experiment": experiment} for _ in range(len(trials))],
             modify_sqa=add_experiment_id,
             batch_size=batch_size,
         )
 
-    datas = []
-    data_encode_args = []
+    datas, data_encode_args, datas_to_keep, trial_idcs = [], [], [], []
+    data_sqa_class: Type[SQAData] = cast(
+        Type[SQAData], encoder.config.class_to_sqa_class[Data]
+    )
     for trial in trials:
+        trial_idcs.append(trial.index)
         trial_datas = experiment.data_by_trial.get(trial.index, {})
         for ts, data in trial_datas.items():
             if data.db_id is None:
-                # Only need to worry about new data, since it's not really possible
-                # or supported to modify or remove existing data.
+                # This is data we have not saved before; we should add it to the
+                # database. Previously saved data for this experiment can be removed.
                 datas.append(data)
                 data_encode_args.append({"trial_index": trial.index, "timestamp": ts})
+            else:
+                datas_to_keep.append(data.db_id)
+
+        # For trials, for which we saved new data, we can first remove previously
+        # saved data if it's no longer on the experiment.
+        with session_scope() as session:
+            session.query(data_sqa_class).filter_by(experiment_id=experiment_id).filter(
+                data_sqa_class.trial_index.isnot(None)  # pyre-ignore[16]
+            ).filter(
+                data_sqa_class.trial_index.in_(trial_idcs)  # pyre-ignore[16]
+            ).filter(
+                data_sqa_class.id.not_in(datas_to_keep)  # pyre-ignore[16]
+            ).delete()
 
     _bulk_merge_into_session(
         objs=datas,
         encode_func=encoder.data_to_sqa,
         decode_func=decoder.data_from_sqa,
         encode_args_list=data_encode_args,
         decode_args_list=[
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/sqa_classes.py` & `ax-platform-0.4.0/ax/storage/sqa_store/sqa_classes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from datetime import datetime
 from typing import Any, Dict, List, Optional
 
 from ax.core.base_trial import TrialStatus
 from ax.core.batch_trial import LifecycleStage
 from ax.core.parameter import ParameterType
 from ax.core.types import (
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/sqa_config.py` & `ax-platform-0.4.0/ax/storage/sqa_store/sqa_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from dataclasses import dataclass, field
 from enum import Enum
 from typing import Any, Callable, Dict, Optional, Type
 
 from ax.core.arm import Arm
 from ax.core.batch_trial import AbandonedArm
 from ax.core.data import Data
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/sqa_enum.py` & `ax-platform-0.4.0/ax/storage/sqa_store/sqa_enum.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 from typing import Any, Dict, List
 
 from ax.storage.sqa_store.db import NAME_OR_TYPE_FIELD_LENGTH
 from sqlalchemy import types
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/structs.py` & `ax-platform-0.4.0/ax/storage/sqa_store/structs.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Callable, NamedTuple, Optional
 
 from ax.storage.sqa_store.decoder import Decoder
 from ax.storage.sqa_store.encoder import Encoder
 from ax.storage.sqa_store.sqa_config import SQAConfig
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/tests/test_sqa_store.py` & `ax-platform-0.4.0/ax/storage/sqa_store/tests/test_sqa_store.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from datetime import datetime
 from logging import Logger
 from typing import Any
 from unittest.mock import MagicMock, Mock, patch
 
 from ax.core.arm import Arm
 from ax.core.batch_trial import BatchTrial, LifecycleStage
 from ax.core.generator_run import GeneratorRun
 from ax.core.metric import Metric
-from ax.core.objective import Objective
+from ax.core.objective import MultiObjective, Objective
 from ax.core.outcome_constraint import OutcomeConstraint
 from ax.core.parameter import ParameterType, RangeParameter
 from ax.core.runner import Runner
 from ax.core.types import ComparisonOp
 from ax.exceptions.core import ObjectNotFoundError
 from ax.exceptions.storage import SQADecodeError, SQAEncodeError
 from ax.metrics.branin import BraninMetric
@@ -114,14 +116,15 @@
 logger: Logger = get_logger(__name__)
 
 GET_GS_SQA_IMM_FUNC = _get_generation_strategy_sqa_immutable_opt_config_and_search_space
 
 
 class SQAStoreTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         init_test_engine_and_session_factory(force_init=True)
         self.config = SQAConfig()
         self.encoder = Encoder(config=self.config)
         self.decoder = Decoder(config=self.config)
         self.experiment = get_experiment_with_batch_trial()
         self.dummy_parameters = [
             get_range_parameter(),  # w
@@ -242,19 +245,27 @@
             json_decoder_registry=decoder_registry,
             metric_registry=metric_registry,
             runner_registry=runner_registry,
         )
 
         for immutable in [True, False]:
             for multi_objective in [True, False]:
+                custom_metric_names = ["custom_test_metric"]
                 experiment = get_experiment_with_custom_runner_and_metric(
                     constrain_search_space=False,
                     immutable=immutable,
                     multi_objective=multi_objective,
+                    num_trials=1,
                 )
+                if multi_objective:
+                    custom_metric_names.extend(["m1", "m3"])
+                    for metric_name in custom_metric_names:
+                        self.assertEqual(
+                            experiment.metrics[metric_name].__class__, CustomTestMetric
+                        )
 
                 # Save the experiment to db using the updated registries.
                 save_experiment(experiment, config=sqa_config)
 
                 # At this point try to load the experiment back without specifying
                 # updated registries. Confirm that this attempt fails.
                 with self.assertRaises(SQADecodeError):
@@ -266,21 +277,35 @@
                     experiment.name, skip_runners_and_metrics=True
                 )
 
                 # Validate that:
                 #   - the runner is not loaded
                 #   - the metric is loaded as a base Metric class, not CustomTestMetric
                 self.assertIs(loaded_experiment.runner, None)
-                self.assertTrue("custom_test_metric" in loaded_experiment.metrics)
-                self.assertEqual(
-                    loaded_experiment.metrics["custom_test_metric"].__class__, Metric
-                )
+
+                for metric_name in custom_metric_names:
+                    self.assertTrue(metric_name in loaded_experiment.metrics)
+                    self.assertEqual(
+                        loaded_experiment.metrics["custom_test_metric"].__class__,
+                        Metric,
+                    )
                 self.assertEqual(len(loaded_experiment.trials), 1)
-                self.assertIs(loaded_experiment.trials[0].runner, None)
+                trial = loaded_experiment.trials[0]
+                self.assertIs(trial.runner, None)
                 delete_experiment(exp_name=experiment.name)
+                # check generator runs
+                gr = trial.generator_runs[0]
+                if multi_objective and not immutable:
+                    objectives = checked_cast(
+                        MultiObjective, not_none(gr.optimization_config).objective
+                    ).objectives
+                    for i, objective in enumerate(objectives):
+                        metric = objective.metric
+                        self.assertEqual(metric.name, f"m{1 + 2 * i}")
+                        self.assertEqual(metric.__class__, Metric)
 
     @patch(
         f"{Decoder.__module__}.Decoder.generator_run_from_sqa",
         side_effect=Decoder(SQAConfig()).generator_run_from_sqa,
     )
     @patch(
         f"{Decoder.__module__}.Decoder.trial_from_sqa",
@@ -723,15 +748,17 @@
         save_experiment(experiment)
         self.assertEqual(
             get_session().query(SQAMetric).count(), len(experiment.metrics)
         )
 
         # replace objective
         # (old one should become tracking metric)
-        optimization_config.objective = Objective(metric=Metric(name="objective"))
+        optimization_config.objective = Objective(
+            metric=Metric(name="objective"), minimize=False
+        )
         experiment.optimization_config = optimization_config
         save_experiment(experiment)
         self.assertEqual(
             get_session().query(SQAMetric).count(), len(experiment.metrics)
         )
 
         loaded_experiment = load_experiment(experiment.name)
@@ -1356,15 +1383,17 @@
             experiment_name=experiment.name
         )
         # Some fields of the reloaded GS are not expected to be set (both will be
         # set during next model fitting call), so we unset them on the original GS as
         # well.
         generation_strategy._unset_non_persistent_state_fields()
         self.assertEqual(generation_strategy, new_generation_strategy)
-        self.assertIsInstance(new_generation_strategy._nodes[0].model_enum, Models)
+        self.assertIsInstance(
+            new_generation_strategy._nodes[0].model_spec_to_gen_from.model_enum, Models
+        )
         self.assertEqual(len(new_generation_strategy._generator_runs), 2)
         self.assertEqual(
             not_none(new_generation_strategy._experiment)._name, experiment._name
         )
 
     def test_EncodeDecodeGenerationStrategyReducedState(self) -> None:
         """Try restoring the generation strategy using the experiment its attached to,
@@ -1399,15 +1428,14 @@
         # set during next model fitting call), so we unset them on the original GS as
         # well.
         generation_strategy._unset_non_persistent_state_fields()
         # Now the generation strategies should be equal.
         # Reloaded generation strategy will not have attributes associated with fitting
         # the model until after it's used to fit the model or generate candidates, so
         # we unset those attributes here and compare equality of the rest.
-        generation_strategy._seen_trial_indices_by_status = None
         generation_strategy._model = None
         self.assertEqual(new_generation_strategy, generation_strategy)
         # Model should be successfully restored in generation strategy even with
         # the reduced state.
         self.assertIsInstance(new_generation_strategy._steps[0].model, Models)
         self.assertEqual(len(new_generation_strategy._generator_runs), 2)
         self.assertEqual(
@@ -1514,15 +1542,14 @@
         save_experiment(experiment)
         loaded_generation_strategy = load_generation_strategy_by_experiment_name(
             experiment_name=experiment.name
         )
         # Reloaded generation strategy will not have attributes associated with fitting
         # the model until after it's used to fit the model or generate candidates, so
         # we unset those attributes here and compare equality of the rest.
-        generation_strategy._seen_trial_indices_by_status = None
         generation_strategy._model = None
         self.assertEqual(generation_strategy, loaded_generation_strategy)
         self.assertIsNotNone(loaded_generation_strategy._experiment)
         self.assertEqual(
             not_none(generation_strategy._experiment).description,
             experiment.description,
         )
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/tests/test_utils.py` & `ax-platform-0.4.0/ax/storage/sqa_store/tests/test_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.storage.sqa_store.db import init_test_engine_and_session_factory
 from ax.storage.sqa_store.load import load_experiment
 from ax.storage.sqa_store.save import save_experiment
 from ax.storage.sqa_store.utils import copy_db_ids
 from ax.utils.common.base import Base
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import (
@@ -23,14 +25,15 @@
         self.baseline_workflow_inputs = baseline_workflow_inputs
         # pyre-fixme[4]: Attribute must be annotated.
         self.dummy_db_id = db_id
 
 
 class SQAStoreUtilsTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         init_test_engine_and_session_factory(force_init=True)
 
     def test_CopyDBIDsBatchTrialExp(self) -> None:
         exp1 = get_experiment_with_batch_trial()
         save_experiment(exp1)
         exp2 = load_experiment(exp1.name)
         self.assertEqual(exp1, exp2)
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/tests/utils.py` & `ax-platform-0.4.0/ax/storage/sqa_store/tests/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.storage.sqa_store.decoder import Decoder
 from ax.storage.sqa_store.encoder import Encoder
 from ax.utils.testing.core_stubs import (
     get_abandoned_arm,
     get_arm,
     get_batch_trial,
     get_branin_metric,
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/timestamp.py` & `ax-platform-0.4.0/ax/storage/sqa_store/timestamp.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import datetime
 from typing import Optional
 
 from sqlalchemy.engine.interfaces import Dialect
 from sqlalchemy.types import Integer, TypeDecorator
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/utils.py` & `ax-platform-0.4.0/ax/storage/sqa_store/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import warnings
 from typing import Any, List, Optional
 
 from ax.core.experiment import Experiment
 from ax.core.search_space import SearchSpace
 from ax.exceptions.storage import SQADecodeError
 from ax.utils.common.base import Base, SortableBase
```

### Comparing `ax-platform-0.3.7/ax/storage/sqa_store/validation.py` & `ax-platform-0.4.0/ax/storage/sqa_store/validation.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Callable, List, TypeVar
 
 from ax.storage.sqa_store.db import SQABase
 from ax.storage.sqa_store.reduced_state import GR_LARGE_MODEL_ATTRS
 from ax.storage.sqa_store.sqa_classes import (
     ONLY_ONE_FIELDS,
@@ -30,15 +32,15 @@
 logger: Logger = get_logger(__name__)
 
 
 def listens_for_multiple(
     targets: List[InstrumentedAttribute],
     identifier: str,
     *args: Any,
-    **kwargs: Any
+    **kwargs: Any,
     # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.
 ) -> Callable:
     """Analogue of SQLAlchemy `listen_for`, but applies the same listening handler
     function to multiple instrumented attributes.
     """
 
     # pyre-fixme[3]: Return type must be annotated.
```

### Comparing `ax-platform-0.3.7/ax/storage/tests/test_botorch_modular_registry.py` & `ax-platform-0.4.0/ax/storage/tests/test_botorch_modular_registry.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.models.torch.botorch_modular.acquisition import Acquisition
 from ax.storage.botorch_modular_registry import (
     ACQUISITION_FUNCTION_REGISTRY,
     ACQUISITION_REGISTRY,
     MODEL_REGISTRY,
     register_acquisition,
     register_acquisition_function,
```

### Comparing `ax-platform-0.3.7/ax/storage/tests/test_registry_bundle.py` & `ax-platform-0.4.0/ax/storage/tests/test_registry_bundle.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from ax.metrics.botorch_test_problem import BotorchTestProblemMetric
+# pyre-strict
+
+from ax.benchmark.metrics.benchmark import BenchmarkMetric
+from ax.benchmark.runners.botorch_test import BotorchTestProblemRunner
 from ax.metrics.branin import BraninMetric
-from ax.runners.botorch_test_problem import BotorchTestProblemRunner
 from ax.runners.synthetic import SyntheticRunner
 from ax.storage.registry_bundle import RegistryBundle
 from ax.utils.common.testutils import TestCase
 
 
 class RegistryBundleTest(TestCase):
     def test_from_registry_bundles(self) -> None:
@@ -19,24 +21,24 @@
             json_encoder_registry={},
             json_class_encoder_registry={},
             json_decoder_registry={},
             json_class_decoder_registry={},
         )
 
         right = RegistryBundle(
-            metric_clss={BotorchTestProblemMetric: None},
+            metric_clss={BenchmarkMetric: None},
             runner_clss={BotorchTestProblemRunner: None},
             json_encoder_registry={},
             json_class_encoder_registry={},
             json_decoder_registry={},
             json_class_decoder_registry={},
         )
 
         self.assertIn(BraninMetric, left.encoder_registry)
-        self.assertNotIn(BotorchTestProblemMetric, left.encoder_registry)
+        self.assertNotIn(BenchmarkMetric, left.encoder_registry)
 
         combined = RegistryBundle.from_registry_bundles(left, right)
 
         self.assertIn(BraninMetric, combined.encoder_registry)
         self.assertIn(SyntheticRunner, combined.encoder_registry)
-        self.assertIn(BotorchTestProblemMetric, combined.encoder_registry)
+        self.assertIn(BenchmarkMetric, combined.encoder_registry)
         self.assertIn(BotorchTestProblemRunner, combined.encoder_registry)
```

### Comparing `ax-platform-0.3.7/ax/storage/utils.py` & `ax-platform-0.4.0/ax/storage/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import enum
 from hashlib import md5
 
 from ax.core.formatting_utils import DataType  # noqa F401
 
 
 class DomainType(enum.Enum):
```

### Comparing `ax-platform-0.3.7/ax/telemetry/ax_client.py` & `ax-platform-0.4.0/ax/telemetry/ax_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import asdict, dataclass
 from typing import Any, Dict, Optional
 
 from ax.service.ax_client import AxClient
 from ax.telemetry.common import _get_max_transformed_dimensionality
```

### Comparing `ax-platform-0.3.7/ax/telemetry/common.py` & `ax-platform-0.4.0/ax/telemetry/common.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from datetime import datetime
 from typing import Any, Dict, List, Tuple, Type
 
 from ax.core.experiment import Experiment
 
 from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
```

### Comparing `ax-platform-0.3.7/ax/telemetry/experiment.py` & `ax-platform-0.4.0/ax/telemetry/experiment.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import dataclass
 from math import inf
 from typing import Dict, List, Optional, Tuple
 
 from ax.core.base_trial import TrialStatus
@@ -102,29 +104,33 @@
                 num_unordered_choice_parameters_medium
             ),
             num_unordered_choice_parameters_large=num_unordered_choice_parameters_large,
             num_fixed_parameters=num_fixed_parameters,
             dimensionality=sum(
                 1 for param in experiment.parameters.values() if param.cardinality() > 1
             ),
-            hierarchical_tree_height=experiment.search_space.height
-            if isinstance(experiment.search_space, HierarchicalSearchSpace)
-            else 1,
+            hierarchical_tree_height=(
+                experiment.search_space.height
+                if isinstance(experiment.search_space, HierarchicalSearchSpace)
+                else 1
+            ),
             num_parameter_constraints=len(
                 experiment.search_space.parameter_constraints
             ),
-            num_objectives=len(experiment.optimization_config.objective.metrics)
-            if experiment.optimization_config is not None
-            else 0,
+            num_objectives=(
+                len(experiment.optimization_config.objective.metrics)
+                if experiment.optimization_config is not None
+                else 0
+            ),
             num_tracking_metrics=len(experiment.tracking_metrics),
-            num_outcome_constraints=len(
-                experiment.optimization_config.outcome_constraints
-            )
-            if experiment.optimization_config is not None
-            else 0,
+            num_outcome_constraints=(
+                len(experiment.optimization_config.outcome_constraints)
+                if experiment.optimization_config is not None
+                else 0
+            ),
             num_map_metrics=sum(
                 1
                 for metric in experiment.metrics.values()
                 if isinstance(metric, MapMetric)
             ),
             metric_cls_to_quantity={
                 cls_name: sum(
@@ -171,33 +177,33 @@
         num_int_range_parameters_small = sum(
             1
             for param in search_space.parameters.values()
             if (
                 isinstance(param, RangeParameter)
                 or (isinstance(param, ChoiceParameter) and param.is_ordered)
             )
-            and (1 < param.cardinality() <= 3)
+            and (1.0 < param.cardinality() <= 3.0)
         )
         num_int_range_parameters_medium = sum(
             1
             for param in search_space.parameters.values()
             if (
                 isinstance(param, RangeParameter)
                 or (isinstance(param, ChoiceParameter) and param.is_ordered)
             )
-            and (3 < param.cardinality() <= 7)
+            and (3.0 < param.cardinality() <= 7.0)
         )
         num_int_range_parameters_large = sum(
             1
             for param in search_space.parameters.values()
             if (
                 isinstance(param, RangeParameter)
                 or (isinstance(param, ChoiceParameter) and param.is_ordered)
             )
-            and (7 < param.cardinality() < inf)
+            and (7.0 < param.cardinality() < inf)
         )
         num_log_scale_range_parameters = sum(
             1
             for param in search_space.parameters.values()
             if isinstance(param, RangeParameter) and param.log_scale
         )
         num_unordered_choice_parameters_small = sum(
```

### Comparing `ax-platform-0.3.7/ax/telemetry/generation_strategy.py` & `ax-platform-0.4.0/ax/telemetry/generation_strategy.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import dataclass
 from math import inf
 
 from ax.modelbridge.generation_strategy import GenerationStrategy
 from ax.telemetry.common import INITIALIZATION_MODELS, OTHER_MODELS
@@ -53,11 +55,11 @@
                 if step.model not in INITIALIZATION_MODELS + OTHER_MODELS
             ),
             num_requested_other_trials=sum(
                 step.num_trials
                 for step in generation_strategy._steps
                 if step.model in OTHER_MODELS
             ),
-            max_parallelism=true_max_parallelism
-            if isinstance(true_max_parallelism, int)
-            else -1,
+            max_parallelism=(
+                true_max_parallelism if isinstance(true_max_parallelism, int) else -1
+            ),
         )
```

### Comparing `ax-platform-0.3.7/ax/telemetry/optimization.py` & `ax-platform-0.4.0/ax/telemetry/optimization.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import dataclass
 from typing import Dict, Optional, Union
 
 from ax.core.experiment import Experiment
 from ax.modelbridge.generation_strategy import GenerationStrategy
@@ -386,19 +388,21 @@
             max_parallelism=(
                 None
                 if generation_strategy_created_record is None
                 else generation_strategy_created_record.max_parallelism
             ),
             early_stopping_strategy_cls=None,
             global_stopping_strategy_cls=None,
-            transformed_dimensionality=None
-            if generation_strategy is None
-            else _get_max_transformed_dimensionality(
-                search_space=experiment.search_space,
-                generation_strategy=generation_strategy,
+            transformed_dimensionality=(
+                None
+                if generation_strategy is None
+                else _get_max_transformed_dimensionality(
+                    search_space=experiment.search_space,
+                    generation_strategy=generation_strategy,
+                )
             ),
             arms_per_trial=arms_per_trial,
             unique_identifier=unique_identifier,
             owner=owner,
             product_surface=product_surface,
             launch_surface=launch_surface,
             deployed_job_id=deployed_job_id,
```

### Comparing `ax-platform-0.3.7/ax/telemetry/scheduler.py` & `ax-platform-0.4.0/ax/telemetry/scheduler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import asdict, dataclass
 from typing import Any, Dict, Optional
 from warnings import warn
 
 import numpy as np
```

### Comparing `ax-platform-0.3.7/ax/telemetry/tests/test_ax_client.py` & `ax-platform-0.4.0/ax/telemetry/tests/test_ax_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Dict, List, Sequence, Union
 
 import numpy as np
 
 from ax.core.types import TParamValue
+from ax.exceptions.core import UnsupportedError
 from ax.service.ax_client import AxClient, ObjectiveProperties
 from ax.telemetry.ax_client import AxClientCompletedRecord, AxClientCreatedRecord
 from ax.telemetry.experiment import ExperimentCompletedRecord, ExperimentCreatedRecord
 from ax.telemetry.generation_strategy import GenerationStrategyCreatedRecord
 from ax.utils.common.testutils import TestCase
 
 
@@ -124,14 +127,33 @@
             model_fit_quality=float("nan"),
             model_std_quality=float("nan"),
             model_fit_generalization=float("nan"),
             model_std_generalization=float("nan"),
         )
         self._compare_axclient_completed_records(record, expected)
 
+    def test_batch_trial_warning(self) -> None:
+        ax_client = AxClient()
+        error_msg = (
+            "AxClient API does not support batch trials yet."
+            " We plan to add this support in coming versions."
+        )
+        with self.assertRaisesRegex(UnsupportedError, error_msg):
+            ax_client.create_experiment(
+                name="test_experiment",
+                parameters=[
+                    {"name": "x", "type": "range", "bounds": [-5.0, 10.0]},
+                ],
+                objectives={"branin": ObjectiveProperties(minimize=True)},
+                is_test=True,
+                choose_generation_strategy_kwargs={
+                    "use_batch_trials": True,
+                },
+            )
+
     def _compare_axclient_completed_records(
         self, record: AxClientCompletedRecord, expected: AxClientCompletedRecord
     ) -> None:
         self.assertEqual(
             record.experiment_completed_record, expected.experiment_completed_record
         )
         numeric_fields = [
```

### Comparing `ax-platform-0.3.7/ax/telemetry/tests/test_experiment.py` & `ax-platform-0.4.0/ax/telemetry/tests/test_experiment.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.core.utils import get_model_times
 from ax.telemetry.experiment import ExperimentCompletedRecord, ExperimentCreatedRecord
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.core_stubs import get_experiment_with_custom_runner_and_metric
 
 
 class TestExperiment(TestCase):
     def test_experiment_created_record_from_experiment(self) -> None:
-        experiment = get_experiment_with_custom_runner_and_metric()
+        experiment = get_experiment_with_custom_runner_and_metric(
+            has_outcome_constraint=True
+        )
 
         record = ExperimentCreatedRecord.from_experiment(experiment=experiment)
         expected = ExperimentCreatedRecord(
             experiment_name="test",
             experiment_type=None,
             num_continuous_range_parameters=1,
             num_int_range_parameters_small=0,
@@ -36,16 +40,17 @@
             num_map_metrics=0,
             metric_cls_to_quantity={"Metric": 2, "CustomTestMetric": 1},
             runner_cls="CustomTestRunner",
         )
         self.assertEqual(record, expected)
 
     def test_experiment_completed_record_from_experiment(self) -> None:
-        experiment = get_experiment_with_custom_runner_and_metric()
-
+        experiment = get_experiment_with_custom_runner_and_metric(
+            has_outcome_constraint=True, num_trials=1
+        )
         record = ExperimentCompletedRecord.from_experiment(experiment=experiment)
 
         # Calculate these here, may change from run to run
         fit_time, gen_time = get_model_times(experiment=experiment)
         expected = ExperimentCompletedRecord(
             num_initialization_trials=1,
             num_bayesopt_trials=0,
```

### Comparing `ax-platform-0.3.7/ax/telemetry/tests/test_generation_strategy.py` & `ax-platform-0.4.0/ax/telemetry/tests/test_generation_strategy.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.telemetry.generation_strategy import GenerationStrategyCreatedRecord
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.modeling_stubs import get_generation_strategy
 
 
 class TestGenerationStrategy(TestCase):
     def test_generation_strategy_created_record_from_generation_strategy(self) -> None:
```

### Comparing `ax-platform-0.3.7/ax/telemetry/tests/test_optimization.py` & `ax-platform-0.4.0/ax/telemetry/tests/test_optimization.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env fbpython
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from dataclasses import asdict
 from datetime import datetime
 
 from ax.service.ax_client import AxClient, ObjectiveProperties
 from ax.service.scheduler import Scheduler, SchedulerOptions
 from ax.telemetry.ax_client import AxClientCompletedRecord, AxClientCreatedRecord
 from ax.telemetry.common import get_unique_identifier
```

### Comparing `ax-platform-0.3.7/ax/utils/common/base.py` & `ax-platform-0.4.0/ax/utils/common/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import abc
 from typing import Optional
 
 from ax.utils.common.equality import equality_typechecker, object_attribute_dicts_equal
 
@@ -43,13 +45,23 @@
 class SortableBase(Base, metaclass=abc.ABCMeta):
     """Extension to the base class that also provides an inequality check."""
 
     @property
     @abc.abstractmethod
     def _unique_id(self) -> str:
         """Returns an identification string that can be used to uniquely
-        identify this instance from others attached to the same experiment.
+        identify this instance from others attached to the same parent
+        object. For example, for ``Trials`` this can be their index,
+
+        since that is unique w.r.t. to parent ``Experiment`` object.
+        For ``GenerationNode``-s attached to a ``GenerationStrategy``,
+        this can be their name since we ensure uniqueness of it upon
+        ``GenerationStrategy`` instantiation.
+
+        This method is needed to correctly update SQLAlchemy objects
+        that appear as children of other objects, in lists or other
+        sortable collections or containers.
         """
         pass
 
     def __lt__(self, other: SortableBase) -> bool:
         return self._unique_id < other._unique_id
```

### Comparing `ax-platform-0.3.7/ax/utils/common/constants.py` & `ax-platform-0.4.0/ax/utils/common/constants.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from enum import Enum, unique
 from typing import Tuple
 
 
 # -------------------------- Warnings --------------------------
 
 
@@ -67,15 +69,14 @@
     PREFERENCE_DATA = "preference_data"
     PRIMARY_SURROGATE = "primary"
     PROJECT = "project"
     TRIAL_COMPLETION_TIMESTAMP = "trial_completion_timestamp"
     QMC = "qmc"
     RAW_INNER_SAMPLES = "raw_inner_samples"
     RAW_SAMPLES = "raw_samples"
-    REFIT_ON_UPDATE = "refit_on_update"
     SAMPLER = "sampler"
     SEED_INNER = "seed_inner"
     SEQUENTIAL = "sequential"
     STATE_DICT = "state_dict"
     SUBCLASS = "subclass"
     SUBSET_MODEL = "subset_model"
     TASK_FEATURES = "task_features"
```

### Comparing `ax-platform-0.3.7/ax/utils/common/decorator.py` & `ax-platform-0.4.0/ax/utils/common/decorator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from abc import ABC, abstractmethod
 from typing import Any, Callable, TypeVar
 
 T = TypeVar("T")
 
 
 class ClassDecorator(ABC):
```

### Comparing `ax-platform-0.3.7/ax/utils/common/docutils.py` & `ax-platform-0.4.0/ax/utils/common/docutils.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/utils/common/equality.py` & `ax-platform-0.4.0/ax/utils/common/equality.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from datetime import datetime
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import numpy as np
 import pandas as pd
-from ax.utils.common.typeutils import numpy_type_to_python_type
+from ax.utils.common.typeutils_nonnative import numpy_type_to_python_type
 
 
 # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.
 def equality_typechecker(eq_func: Callable) -> Callable:
     """A decorator to wrap all __eq__ methods to ensure that the inputs
     are of the right type.
     """
```

### Comparing `ax-platform-0.3.7/ax/utils/common/executils.py` & `ax-platform-0.4.0/ax/utils/common/executils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import asyncio
 import functools
 import time
 from contextlib import contextmanager
 from logging import Logger
 from typing import Any, Generator, List, Optional, Tuple, Type
```

### Comparing `ax-platform-0.3.7/ax/utils/common/kwargs.py` & `ax-platform-0.4.0/ax/utils/common/kwargs.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from inspect import Parameter, signature
 
 from logging import Logger
 from typing import Any, Callable, Dict, Iterable, List, Optional
 
 from ax.utils.common.logger import get_logger
-from ax.utils.common.typeutils import version_safe_check_type
+from ax.utils.common.typeutils_nonnative import version_safe_check_type
 
 logger: Logger = get_logger(__name__)
 
 TKwargs = Dict[str, Any]
 
 
 def consolidate_kwargs(
```

### Comparing `ax-platform-0.3.7/ax/utils/common/logger.py` & `ax-platform-0.4.0/ax/utils/common/logger.py`

 * *Files 1% similar despite different names*

```diff
@@ -82,15 +82,15 @@
     formatter = _build_stream_formatter()
     console.setFormatter(formatter)
     return console
 
 
 def build_file_handler(
     filepath: str,
-    level: int = DEFAULT_LOG_LEVEL
+    level: int = DEFAULT_LOG_LEVEL,
     # pyre-fixme[24]: Generic type `logging.StreamHandler` expects 1 type parameter.
 ) -> logging.StreamHandler:
     """Build a file handle that logs entries to the given file, using the
     same formatting as the stream handler.
 
     Args:
         filepath: Location of the file to log output to. If the file exists, output
```

### Comparing `ax-platform-0.3.7/ax/utils/common/mock.py` & `ax-platform-0.4.0/ax/utils/common/mock.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/utils/common/result.py` & `ax-platform-0.4.0/ax/utils/common/result.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from abc import ABC, abstractmethod, abstractproperty
 
 from typing import Any, Callable, cast, Generic, NoReturn, Optional, TypeVar, Union
```

### Comparing `ax-platform-0.3.7/ax/utils/common/serialization.py` & `ax-platform-0.4.0/ax/utils/common/serialization.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 import inspect
 import pydoc
 from types import FunctionType
 from typing import Any, Callable, Dict, List, Optional, Type
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_docutils.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_docutils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.utils.common.docutils import copy_doc
 from ax.utils.common.testutils import TestCase
 
 
 def has_doc() -> None:
     """I have a docstring"""
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_equality.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_equality.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from datetime import datetime
 
 import numpy as np
 import pandas as pd
 from ax.utils.common.equality import (
     dataframe_equals,
     datetime_equals,
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_executils.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_executils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 import time
 from asyncio import iscoroutinefunction
 from unittest.mock import Mock
 
 from ax.utils.common.executils import retry_on_exception
 from ax.utils.common.testutils import TestCase
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_kwargutils.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_kwargutils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 from logging import Logger
 from typing import Callable, Dict, Optional
 from unittest.mock import patch
 
 from ax.utils.common.kwargs import validate_kwarg_typing, warn_on_kwargs
 from ax.utils.common.logger import get_logger
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_logger.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_logger.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 from tempfile import NamedTemporaryFile
 from unittest.mock import patch
 
 from ax.utils.common.logger import build_file_handler, get_logger
 from ax.utils.common.testutils import TestCase
 
 
 BASE_LOGGER_NAME = f"ax.{__name__}"
 
 
 class LoggerTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.warning_string = "Test warning"
 
     def test_Logger(self) -> None:
         logger = get_logger(BASE_LOGGER_NAME + ".testLogger")
         # Verify it doesn't crash
         logger.warning(self.warning_string)
         # Patch it, verify we actually called it
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_result.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_result.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,22 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.utils.common.result import Err, Ok, Result, UnwrapError
 from ax.utils.common.testutils import TestCase
 
 
 class ResultTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
+
         def safeDivide(a: float, b: float) -> Result[float, str]:
             if b == 0:
                 return Err("yikes")
 
             return Ok(a / b)
 
         self.ok = safeDivide(0, 2)
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_serialization.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_serialization.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import NamedTuple
 
 from ax.utils.common.serialization import named_tuple_to_dict
 from ax.utils.common.testutils import TestCase
 
 
 class TestSerializationUtils(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_testutils.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_testutils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import io
 import sys
 
 from ax.utils.common.base import Base
 from ax.utils.common.testutils import TestCase
 
 
 # pyre-fixme[3]: Return type must be annotated.
 def _f():
     e = RuntimeError("Test")
     raise e
 
 
-F_FAILURE_LINENO = 17  # Line # for the error in `_f`.
+F_FAILURE_LINENO = 19  # Line # for the error in `_f`.
 
 
 def _g() -> None:
     _f()  # Lines along the path are matched too
 
 
 class MyBase(Base):
```

### Comparing `ax-platform-0.3.7/ax/utils/common/tests/test_typeutils.py` & `ax-platform-0.4.0/ax/utils/common/tests/test_typeutils.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 
 import numpy as np
 from ax.utils.common.testutils import TestCase
 from ax.utils.common.typeutils import (
     checked_cast,
     checked_cast_dict,
     checked_cast_list,
     checked_cast_optional,
     not_none,
-    numpy_type_to_python_type,
 )
+from ax.utils.common.typeutils_nonnative import numpy_type_to_python_type
 
 
 class TestTypeUtils(TestCase):
     def test_not_none(self) -> None:
         self.assertEqual(not_none("not_none"), "not_none")
         with self.assertRaises(ValueError):
             not_none(None)
```

### Comparing `ax-platform-0.3.7/ax/utils/common/testutils.py` & `ax-platform-0.4.0/ax/utils/common/testutils.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 import io
 import linecache
 import logging
 import signal
 import sys
 import types
 import unittest
+import warnings
 from functools import wraps
 from logging import Logger
 from types import FrameType
 from typing import (
     Any,
     Callable,
     ContextManager,
@@ -33,20 +34,20 @@
     Type,
     TypeVar,
     Union,
 )
 from unittest.mock import MagicMock
 
 import numpy as np
-
 import yappi
-
+from ax.exceptions.core import AxParameterWarning
 from ax.utils.common.base import Base
 from ax.utils.common.equality import object_attribute_dicts_find_unequal_fields
 from ax.utils.common.logger import get_logger
+from botorch.exceptions.warnings import InputDataWarning
 from pyfakefs import fake_filesystem_unittest
 
 
 T_AX_BASE_OR_ATTR_DICT = Union[Base, Dict[str, Any]]
 COMPARISON_STR_MAX_LEVEL = 8
 T = TypeVar("T")
 
@@ -96,18 +97,14 @@
         self.filename = None
         super().__init__(
             expected=expected, test_case=test_case, expected_regex=expected_regex
         )
 
     # pyre-fixme[14]: `__exit__` overrides method defined in `_AssertRaisesContext`
     #  inconsistently.
-    # pyre-fixme[14]: `__exit__` overrides method defined in `_AssertRaisesContext`
-    #  inconsistently.
-    # pyre-fixme[14]: `__exit__` overrides method defined in `_AssertRaisesContext`
-    #  inconsistently.
     def __exit__(
         self,
         exc_type: Optional[Type[Exception]],
         exc_value: Optional[Exception],
         tb: Optional[types.TracebackType],
     ) -> bool:
         """This is called when the context closes. If an exception was raised
@@ -130,16 +127,14 @@
 
         return True
 
 
 # Instead of showing a warning (like in the standard library) we throw an error when
 # deprecated functions are called.
 # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.
-# pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.
-# pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.
 def _deprecate(original_func: Callable) -> Callable:
     def _deprecated_func(*args: List[Any], **kwargs: Dict[str, Any]) -> None:
         raise RuntimeError(
             f"This function is deprecated please use {original_func.__name__} "
             "instead."
         )
 
@@ -323,21 +318,43 @@
         signal.signal(signal.SIGALRM, signal_handler)
 
     def setUp(self) -> None:
         """
         Only show log messages of WARNING or higher while testing.
 
         Ax prints a lot of INFO logs that are not relevant for unit tests.
+
+        Also silences a number of common warnings originating from Ax & BoTorch.
         """
+        super().setUp()
         logger = get_logger(__name__, level=logging.WARNING)
         # Parent handlers are shared, so setting the level this
         # way applies it to all Ax loggers.
         if logger.parent is not None and hasattr(logger.parent, "handlers"):
             logger.parent.handlers[0].setLevel(logging.WARNING)
 
+        # Choice parameter default parameter type / is_ordered warnings.
+        warnings.filterwarnings(
+            "ignore",
+            message=".*is not specified for .ChoiceParameter.*",
+            category=AxParameterWarning,
+        )
+        # BoTorch float32 warning.
+        warnings.filterwarnings(
+            "ignore",
+            message="The model inputs are of type",
+            category=InputDataWarning,
+        )
+        # BoTorch input standardization warnings.
+        warnings.filterwarnings(
+            "ignore",
+            message="Input data is not",
+            category=InputDataWarning,
+        )
+
     def run(
         self, result: Optional[unittest.result.TestResult] = ...
     ) -> Optional[unittest.result.TestResult]:
         # Arrange for a SIGALRM signal to be delivered to the calling process
         # in specified number of seconds.
         signal.alarm(self.MAX_TEST_SECONDS)
         try:
@@ -468,31 +485,26 @@
             print(new_err.getvalue(), file=old_err, flush=True)
             raise
         finally:
             sys.stderr = old_err
 
     # This list is taken from the python standard library
     # pyre-fixme[4]: Attribute must be annotated.
-    # pyre-fixme[4]: Attribute must be annotated.
     failUnlessEqual = assertEquals = _deprecate(unittest.TestCase.assertEqual)
     # pyre-fixme[4]: Attribute must be annotated.
-    # pyre-fixme[4]: Attribute must be annotated.
     failIfEqual = assertNotEquals = _deprecate(unittest.TestCase.assertNotEqual)
     # pyre-fixme[4]: Attribute must be annotated.
-    # pyre-fixme[4]: Attribute must be annotated.
     failUnlessAlmostEqual = assertAlmostEquals = _deprecate(
         unittest.TestCase.assertAlmostEqual
     )
     # pyre-fixme[4]: Attribute must be annotated.
-    # pyre-fixme[4]: Attribute must be annotated.
     failIfAlmostEqual = assertNotAlmostEquals = _deprecate(
         unittest.TestCase.assertNotAlmostEqual
     )
     # pyre-fixme[4]: Attribute must be annotated.
-    # pyre-fixme[4]: Attribute must be annotated.
     failUnless = assert_ = _deprecate(unittest.TestCase.assertTrue)
     # pyre-fixme[4]: Attribute must be annotated.
     failUnlessRaises = _deprecate(unittest.TestCase.assertRaises)
     # pyre-fixme[4]: Attribute must be annotated.
     failIf = _deprecate(unittest.TestCase.assertFalse)
     # pyre-fixme[4]: Attribute must be annotated.
     assertRaisesRegexp = _deprecate(unittest.TestCase.assertRaisesRegex)
```

### Comparing `ax-platform-0.3.7/ax/utils/common/timeutils.py` & `ax-platform-0.4.0/ax/utils/common/timeutils.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from datetime import datetime, timedelta
 from time import time
 from typing import Generator
 
 import pandas as pd
```

### Comparing `ax-platform-0.3.7/ax/utils/common/typeutils.py` & `ax-platform-0.4.0/ax/utils/common/typeutils.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,18 +1,16 @@
-#!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
-from inspect import signature
+# pyre-strict
+
 from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar
 
-import numpy as np
-from typeguard import check_type
 
 T = TypeVar("T")
 V = TypeVar("V")
 K = TypeVar("K")
 X = TypeVar("X")
 Y = TypeVar("Y")
 
@@ -51,16 +49,18 @@
         exception: override exception to raise if  typecheck fails
     Returns:
         the ``val`` argument, unchanged
 
     .. _typing.cast: https://docs.python.org/3/library/typing.html#typing.cast
     """
     if not isinstance(val, typ):
-        raise exception if exception is not None else ValueError(
-            f"Value was not of type {typ}:\n{val}"
+        raise (
+            exception
+            if exception is not None
+            else ValueError(f"Value was not of type {typ}:\n{val}")
         )
     return val
 
 
 def checked_cast_optional(typ: Type[T], val: Optional[V]) -> Optional[T]:
     """Calls checked_cast only if value is not None."""
     if val is None:
@@ -105,41 +105,14 @@
     """
     if not isinstance(val, typ):
         raise ValueError(f"Value was not of type {type!r}:\n{val!r}")
     # pyre-fixme[7]: Expected `T` but got `V`.
     return val
 
 
-def version_safe_check_type(argname: str, value: T, expected_type: Type[T]) -> None:
-    """Excecute the check_type function if it has the expected signature, otherwise
-    warn.  This is done to support newer versions of typeguard with minimal loss
-    of functionality for users that have dependency conflicts"""
-    # Get the signature of the check_type function
-    sig = signature(check_type)
-    # Get the parameters of the check_type function
-    params = sig.parameters
-    # Check if the check_type function has the expected signature
-    params = set(params.keys())
-    if all(arg in params for arg in ["argname", "value", "expected_type"]):
-        check_type(argname, value, expected_type)
-
-
-# pyre-fixme[3]: Return annotation cannot be `Any`.
-# pyre-fixme[2]: Parameter annotation cannot be `Any`.
-def numpy_type_to_python_type(value: Any) -> Any:
-    """If `value` is a Numpy int or float, coerce to a Python int or float.
-    This is necessary because some of our transforms return Numpy values.
-    """
-    if isinstance(value, np.integer):
-        value = int(value)  # pragma: nocover (covered by generator tests)
-    if isinstance(value, np.floating):
-        value = float(value)  # pragma: nocover  (covered by generator tests)
-    return value
-
-
 # pyre-fixme[2]: Parameter annotation cannot be `Any`.
 # pyre-fixme[24]: Generic type `type` expects 1 type parameter, use `typing.Type` to
 #  avoid runtime subscripting errors.
 def _argparse_type_encoder(arg: Any) -> Type:
     """
     Transforms arguments passed to `optimizer_argparse.__call__`
     at runtime to construct the key used for method lookup as
```

### Comparing `ax-platform-0.3.7/ax/utils/common/typeutils_torch.py` & `ax-platform-0.4.0/ax/utils/common/typeutils_torch.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import json
 from typing import Union
 
 import torch
 from ax.utils.common.typeutils import checked_cast
```

### Comparing `ax-platform-0.3.7/ax/utils/flake8_plugins/docstring_checker.py` & `ax-platform-0.4.0/ax/utils/flake8_plugins/docstring_checker.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/utils/measurement/synthetic_functions.py` & `ax-platform-0.4.0/ax/utils/measurement/synthetic_functions.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,28 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from abc import ABC, abstractmethod
 from typing import Any, Callable, List, Optional, Tuple, Union
 
 import numpy as np
 import torch
 from ax.utils.common.docutils import copy_doc
 from ax.utils.common.typeutils import checked_cast, not_none
 from botorch.test_functions import synthetic as botorch_synthetic
+from pyre_extensions import override
 
 
-# pyre-fixme[3]: Return annotation cannot be `Any`.
 # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.
-def informative_failure_on_none(func: Callable) -> Any:
+def informative_failure_on_none(func: Callable) -> Callable:
     # pyre-fixme[3]: Return annotation cannot be `Any`.
     # pyre-fixme[2]: Parameter must be annotated.
     def function_wrapper(*args, **kwargs) -> Any:
         res = func(*args, **kwargs)
         if res is None:
             raise NotImplementedError(
                 f"{args[0].name} does not specify property " f'"{func.__name__}".'
@@ -28,16 +30,15 @@
         return not_none(res)
 
     return function_wrapper
 
 
 class SyntheticFunction(ABC):
 
-    # pyre-fixme[4]: Attribute must be annotated.
-    _required_dimensionality = None
+    _required_dimensionality: Optional[int] = None
     # pyre-fixme[4]: Attribute must be annotated.
     _domain = None
     # pyre-fixme[4]: Attribute must be annotated.
     _minimums = None
     # pyre-fixme[4]: Attribute must be annotated.
     _maximums = None
     # pyre-fixme[4]: Attribute must be annotated.
@@ -159,45 +160,45 @@
 
     @property
     @informative_failure_on_none
     def fmax(self) -> float:
         """Value at global minimum(s)."""
         return self._fmax
 
-    @classmethod
     @abstractmethod
     def _f(self, X: np.ndarray) -> float:
         """Implementation of the synthetic function. Must be implemented in subclass.
 
         Args:
-            X (numpy.ndarray): an n by d array, where n represents the number
-                of observations and d is the dimensionality of the inputs.
+            X: A one-dimensional array with `d` elements, where d is the
+                dimensionality of the inputs.
 
         Returns:
-            numpy.ndarray: an n-dimensional array.
+            float: Function value.
         """
         ...
 
 
 class FromBotorch(SyntheticFunction):
     def __init__(
         self, botorch_synthetic_function: botorch_synthetic.SyntheticTestFunction
     ) -> None:
         self._botorch_function = botorch_synthetic_function
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._required_dimensionality = self._botorch_function.dim
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._domain = self._botorch_function._bounds
-        # pyre-fixme[4]: Attribute must be annotated.
-        self._fmin = self._botorch_function._optimal_value
+        self._required_dimensionality: int = self._botorch_function.dim
+        self._domain: Optional[List[Tuple[float, float]]] = (
+            self._botorch_function._bounds
+        )
+        self._fmin: float = self._botorch_function._optimal_value
 
+    @override
     @property
     def name(self) -> str:
         return f"{self.__class__.__name__}_{self._botorch_function.__class__.__name__}"
 
+    @override
     def _f(self, X: np.ndarray) -> float:
         # TODO: support batch evaluation
         return float(self._botorch_function(X=torch.from_numpy(X)).item())
 
 
 def from_botorch(
     botorch_synthetic_function: botorch_synthetic.SyntheticTestFunction,
@@ -206,41 +207,37 @@
     return FromBotorch(botorch_synthetic_function=botorch_synthetic_function)
 
 
 class Hartmann6(SyntheticFunction):
     """Hartmann6 function (6-dimensional with 1 global minimum)."""
 
     _required_dimensionality = 6
-    # pyre-fixme[4]: Attribute must be annotated.
-    _domain = [(0, 1) for i in range(6)]
+    _domain: List[Tuple[int, int]] = [(0, 1) for i in range(6)]
     _minimums = [(0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573)]
-    # pyre-fixme[4]: Attribute must be annotated.
-    _fmin = -3.32237
+    _fmin: float = -3.32237
     _fmax = 0.0
-    # pyre-fixme[4]: Attribute must be annotated.
-    _alpha = np.array([1.0, 1.2, 3.0, 3.2])
-    # pyre-fixme[4]: Attribute must be annotated.
-    _A = np.array(
+    _alpha: np.ndarray = np.array([1.0, 1.2, 3.0, 3.2])
+    _A: np.ndarray = np.array(
         [
             [10, 3, 17, 3.5, 1.7, 8],
             [0.05, 10, 17, 0.1, 8, 14],
             [3, 3.5, 1.7, 10, 17, 8],
             [17, 8, 0.05, 10, 0.1, 14],
         ]
     )
-    # pyre-fixme[4]: Attribute must be annotated.
-    _P = 10 ** (-4) * np.array(
+    _P: np.ndarray = 10 ** (-4) * np.array(
         [
             [1312, 1696, 5569, 124, 8283, 5886],
             [2329, 4135, 8307, 3736, 1004, 9991],
             [2348, 1451, 3522, 2883, 3047, 6650],
             [4047, 8828, 8732, 5743, 1091, 381],
         ]
     )
 
+    @override
     @copy_doc(SyntheticFunction._f)
     def _f(self, X: np.ndarray) -> float:
         y = 0.0
         for j, alpha_j in enumerate(self._alpha):
             t = 0
             for k in range(6):
                 t += self._A[j, k] * ((X[k] - self._P[j, k]) ** 2)
@@ -248,23 +245,22 @@
         return float(y)
 
 
 class Aug_Hartmann6(Hartmann6):
     """Augmented Hartmann6 function (7-dimensional with 1 global minimum)."""
 
     _required_dimensionality = 7
-    # pyre-fixme[4]: Attribute must be annotated.
-    _domain = [(0, 1) for i in range(7)]
+    _domain: List[Tuple[int, int]] = [(0, 1) for i in range(7)]
     # pyre-fixme[15]: `_minimums` overrides attribute defined in `Hartmann6`
     #  inconsistently.
     _minimums = [(0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573, 1.0)]
-    # pyre-fixme[4]: Attribute must be annotated.
-    _fmin = -3.32237
+    _fmin: float = -3.32237
     _fmax = 0.0
 
+    @override
     @copy_doc(SyntheticFunction._f)
     def _f(self, X: np.ndarray) -> float:
         y = 0.0
         alpha_0 = self._alpha[0] - 0.1 * (1 - X[-1])
         for j, alpha_j in enumerate(self._alpha):
             t = 0
             for k in range(6):
@@ -277,19 +273,23 @@
 
 
 class Branin(SyntheticFunction):
     """Branin function (2-dimensional with 3 global minima)."""
 
     _required_dimensionality = 2
     _domain = [(-5, 10), (0, 15)]
-    # pyre-fixme[4]: Attribute must be annotated.
-    _minimums = [(-np.pi, 12.275), (np.pi, 2.275), (9.42478, 2.475)]
+    _minimums: List[Tuple[float, float]] = [
+        (-np.pi, 12.275),
+        (np.pi, 2.275),
+        (9.42478, 2.475),
+    ]
     _fmin = 0.397887
     _fmax = 308.129
 
+    @override
     @copy_doc(SyntheticFunction._f)
     def _f(self, X: np.ndarray) -> float:
         x_1 = X[0]
         x_2 = X[1]
         return float(
             (x_2 - 5.1 / (4 * np.pi**2) * x_1**2 + 5.0 / np.pi * x_1 - 6.0) ** 2
             + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x_1)
@@ -298,19 +298,23 @@
 
 
 class Aug_Branin(SyntheticFunction):
     """Augmented Branin function (3-dimensional with infinitely many global minima)."""
 
     _required_dimensionality = 3
     _domain = [(-5, 10), (0, 15), (0, 1)]
-    # pyre-fixme[4]: Attribute must be annotated.
-    _minimums = [(-np.pi, 12.275, 1), (np.pi, 2.275, 1), (9.42478, 2.475, 1)]
+    _minimums: List[Tuple[float, float, int]] = [
+        (-np.pi, 12.275, 1),
+        (np.pi, 2.275, 1),
+        (9.42478, 2.475, 1),
+    ]
     _fmin = 0.397887
     _fmax = 308.129
 
+    @override
     @copy_doc(SyntheticFunction._f)
     def _f(self, X: np.ndarray) -> float:
         x_1 = X[0]
         x_2 = X[1]
         return float(
             (
                 x_2
```

### Comparing `ax-platform-0.3.7/ax/utils/measurement/tests/test_synthetic_functions.py` & `ax-platform-0.4.0/ax/utils/measurement/tests/test_synthetic_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 from ax.utils.common.testutils import TestCase
 from ax.utils.measurement.synthetic_functions import (
     aug_branin,
     aug_hartmann6,
     branin,
     FromBotorch,
```

### Comparing `ax-platform-0.3.7/ax/utils/notebook/plotting.py` & `ax-platform-0.4.0/ax/utils/notebook/plotting.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 
 from ax.plot.base import AxPlotConfig, AxPlotTypes
 from ax.plot.render import _js_requires, _wrap_js, plot_config_to_html
 from ax.utils.common.logger import get_logger
 from IPython.display import display
 from plotly.offline import init_notebook_mode, iplot
```

### Comparing `ax-platform-0.3.7/ax/utils/report/render.py` & `ax-platform-0.4.0/ax/utils/report/render.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 import pkgutil
 from typing import List, Optional
 
 from ax.plot.render import _js_requires, _load_css_resource as _load_plot_css_resource
 from jinja2 import Environment, FunctionLoader
```

### Comparing `ax-platform-0.3.7/ax/utils/report/resources/base_template.html` & `ax-platform-0.4.0/ax/utils/report/resources/base_template.html`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/utils/report/resources/report.css` & `ax-platform-0.4.0/ax/utils/report/resources/report.css`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/utils/report/resources/sufficient_statistic.html` & `ax-platform-0.4.0/ax/utils/report/resources/sufficient_statistic.html`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/ax/utils/report/tests/test_render.py` & `ax-platform-0.4.0/ax/utils/report/tests/test_render.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from ax.utils.common.testutils import TestCase
 from ax.utils.report.render import (
     h2_html,
     h3_html,
     link_html,
     list_item_html,
     p_html,
```

### Comparing `ax-platform-0.3.7/ax/utils/sensitivity/derivative_gp.py` & `ax-platform-0.4.0/ax/utils/sensitivity/derivative_gp.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import math
 
 import torch
 from botorch.models.model import Model
 from gpytorch.distributions import MultivariateNormal
 from torch import Tensor
```

### Comparing `ax-platform-0.3.7/ax/utils/sensitivity/derivative_measures.py` & `ax-platform-0.4.0/ax/utils/sensitivity/derivative_measures.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,27 +1,58 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
+from functools import partial
 from typing import Any, Callable, List, Optional, Union
 
 import torch
 from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.sensitivity.derivative_gp import posterior_derivative
 from botorch.models.model import Model
 from botorch.posteriors.gpytorch import GPyTorchPosterior
 from botorch.posteriors.posterior import Posterior
 from botorch.sampling.normal import SobolQMCNormalSampler
 from botorch.utils.sampling import draw_sobol_samples
 from botorch.utils.transforms import unnormalize
 from gpytorch.distributions import MultivariateNormal
 
 
+def sample_discrete_parameters(
+    input_mc_samples: torch.Tensor,
+    discrete_features: Union[None, List[int]],
+    bounds: torch.Tensor,
+    num_mc_samples: int,
+) -> torch.Tensor:
+    r"""Samples the input parameters uniformly at random for the discrete features.
+
+    Args:
+        input_mc_samples: The input mc samples tensor to be modified.
+        discrete_features: A list of integers (or None) of indices corresponding
+            to discrete features.
+        bounds: The parameter bounds.
+        num_mc_samples: The number of Monte Carlo grid samples.
+
+    Returns:
+        A modified input mc samples tensor.
+    """
+    if discrete_features is None:
+        return input_mc_samples
+    all_low = bounds[0, discrete_features].to(dtype=torch.int).tolist()
+    all_high = (bounds[1, discrete_features]).to(dtype=torch.int).tolist()
+    for i, low, high in zip(discrete_features, all_low, all_high):
+        randint = partial(torch.randint, low=low, high=high + 1)
+        input_mc_samples[:, i] = randint(size=torch.Size([num_mc_samples]))
+    return input_mc_samples
+
+
 class GpDGSMGpMean(object):
 
     mean_gradients: Optional[torch.Tensor] = None
     bootstrap_indices: Optional[torch.Tensor] = None
     mean_gradients_btsp: Optional[List[torch.Tensor]] = None
 
     def __init__(
@@ -31,14 +62,15 @@
         derivative_gp: bool = False,
         kernel_type: Optional[str] = None,
         Y_scale: float = 1.0,
         num_mc_samples: int = 10**4,
         input_qmc: bool = False,
         dtype: torch.dtype = torch.double,
         num_bootstrap_samples: int = 1,
+        discrete_features: Optional[List[int]] = None,
     ) -> None:
         r"""Computes three types of derivative based measures:
         the gradient, the gradient square and the gradient absolute measures.
 
         Args:
             model: A BoTorch model.
             bounds: Parameter bounds over which to evaluate model sensitivity.
@@ -50,14 +82,17 @@
             num_mc_samples: The number of MonteCarlo grid samples
             input_qmc: If True, a qmc Sobol grid is use instead of uniformly random.
             dtype: Can be provided if the GP is fit to data of type `torch.float`.
             num_bootstrap_samples: If higher than 1, the method will compute the
                 dgsm measure `num_bootstrap_samples` times by selecting subsamples
                 from the `input_mc_samples` and return the variance and standard error
                 across all computed measures.
+            discrete_features: If specified, the inputs associated with the indices in
+                this list are generated using an integer-valued uniform distribution,
+                rather than the default (pseudo-)random continuous uniform distribution.
         """
         # pyre-fixme[4]: Attribute must be annotated.
         self.dim = checked_cast(tuple, model.train_inputs)[0].shape[-1]
         self.derivative_gp = derivative_gp
         self.kernel_type = kernel_type
         # pyre-fixme[4]: Attribute must be annotated.
         self.bootstrap = num_bootstrap_samples > 1
@@ -67,23 +102,32 @@
         )  # deduct 1 because the first is meant to be the full grid
         if self.derivative_gp and (self.kernel_type is None):
             raise ValueError("Kernel type has to be specified to use derivative GP")
         self.num_mc_samples = num_mc_samples
         if input_qmc:
             # pyre-fixme[4]: Attribute must be annotated.
             self.input_mc_samples = (
-                draw_sobol_samples(bounds=bounds, n=num_mc_samples, q=1)
+                draw_sobol_samples(bounds=bounds, n=num_mc_samples, q=1, seed=1234)
                 .squeeze(1)
                 .to(dtype)
             )
         else:
             self.input_mc_samples = unnormalize(
                 torch.rand(num_mc_samples, self.dim, dtype=dtype),
                 bounds=bounds,
             )
+
+        # uniform integral distribution for discrete features
+        self.input_mc_samples = sample_discrete_parameters(
+            input_mc_samples=self.input_mc_samples,
+            discrete_features=discrete_features,
+            bounds=bounds,
+            num_mc_samples=num_mc_samples,
+        )
+
         if self.derivative_gp:
             posterior = posterior_derivative(
                 model, self.input_mc_samples, not_none(self.kernel_type)
             )
         else:
             self.input_mc_samples.requires_grad = True
             posterior = checked_cast(
@@ -162,15 +206,15 @@
 
         Returns:
             if `self.num_bootstrap_samples > 1`
                 Tensor: (values, var_mc, stderr_mc) x dim
             else
                 Tensor: (values) x dim
         """
-        return self.aggregation(torch.tensor)
+        return self.aggregation(torch.as_tensor)
 
     def gradient_absolute_measure(self) -> torch.Tensor:
         r"""Computes the gradient absolute measure:
 
         Returns:
             if `self.num_bootstrap_samples > 1`
                 Tensor: (values, var_mc, stderr_mc) x dim
@@ -367,27 +411,33 @@
             )
             return gradients_measure_mean_vargp_segp_varmc_segp
 
 
 def compute_derivatives_from_model_list(
     model_list: List[Model],
     bounds: torch.Tensor,
+    discrete_features: Optional[List[int]] = None,
     **kwargs: Any,
 ) -> torch.Tensor:
     """
     Computes average derivatives of a list of models on a bounded domain. Estimation
     is according to the GP posterior mean function.
 
     Args:
         model_list: A list of m botorch.models.model.Model types for which to compute
             the average derivative.
         bounds: A 2 x d Tensor of lower and upper bounds of the domain of the models.
+        discrete_features: If specified, the inputs associated with the indices in
+            this list are generated using an integer-valued uniform distribution,
+            rather than the default (pseudo-)random continuous uniform distribution.
         kwargs: Passed along to GpDGSMGpMean.
 
     Returns:
         A (m x d) tensor of gradient measures.
     """
     indices = []
     for model in model_list:
-        sens_class = GpDGSMGpMean(model=model, bounds=bounds, **kwargs)
+        sens_class = GpDGSMGpMean(
+            model=model, bounds=bounds, discrete_features=discrete_features, **kwargs
+        )
         indices.append(sens_class.gradient_measure())
     return torch.stack(indices)
```

### Comparing `ax-platform-0.3.7/ax/utils/sensitivity/sobol_measures.py` & `ax-platform-0.4.0/ax/utils/sensitivity/sobol_measures.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,49 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from copy import deepcopy
 
 from typing import Any, Callable, Dict, List, Optional, Union
 
 import numpy as np
 
 import torch
 
 from ax.modelbridge.torch import TorchModelBridge
 from ax.models.torch.botorch import BotorchModel
 from ax.models.torch.botorch_modular.model import BoTorchModel as ModularBoTorchModel
 from ax.utils.common.typeutils import checked_cast
-from ax.utils.sensitivity.derivative_measures import compute_derivatives_from_model_list
+from ax.utils.sensitivity.derivative_measures import (
+    compute_derivatives_from_model_list,
+    sample_discrete_parameters,
+)
 from botorch.models.model import Model, ModelList
 from botorch.posteriors.gpytorch import GPyTorchPosterior
 from botorch.sampling.normal import SobolQMCNormalSampler
 from botorch.utils.sampling import draw_sobol_samples
-from botorch.utils.transforms import unnormalize
+from botorch.utils.transforms import is_ensemble, unnormalize
 from torch import Tensor
 
 
 class SobolSensitivity(object):
     def __init__(
         self,
         bounds: torch.Tensor,
         input_function: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
         num_mc_samples: int = 10**4,
         input_qmc: bool = False,
         second_order: bool = False,
         num_bootstrap_samples: int = 1,
         bootstrap_array: bool = False,
+        discrete_features: Optional[List[int]] = None,
     ) -> None:
         r"""Computes three types of Sobol indices:
         first order indices, total indices and second order indices (if specified ).
 
         Args:
             bounds: Parameter bounds over which to evaluate model sensitivity.
             input_function: The objective function.
@@ -45,14 +51,17 @@
             input_qmc: If True, a qmc Sobol grid is use instead of uniformly random.
             second_order: If True, the second order indices are computed.
             bootstrap: If true, the MC error is returned.
             num_bootstrap_samples: If bootstrap is true, the number of bootstraps has
                 to be specified.
             bootstrap_array: If true, all the num_bootstrap_samples extimated indices
                 are returned instead of their mean and Var.
+            discrete_features: If specified, the inputs associated with the indices in
+                this list are generated using an integer-valued uniform distribution,
+                rather than the default (pseudo-)random continuous uniform distribution.
         """
         self.input_function = input_function
         self.dim: int = bounds.shape[-1]
         self.num_mc_samples = num_mc_samples
         self.second_order = second_order
         self.bootstrap: bool = num_bootstrap_samples > 1
         self.num_bootstrap_samples: int = (
@@ -65,14 +74,29 @@
             # pyre-ignore
             self.A = draw_sobol_samples(**sobol_kwargs, seed=seed_A).squeeze(1)
             # pyre-ignore
             self.B = draw_sobol_samples(**sobol_kwargs, seed=seed_B).squeeze(1)
         else:
             self.A = unnormalize(torch.rand(num_mc_samples, self.dim), bounds=bounds)
             self.B = unnormalize(torch.rand(num_mc_samples, self.dim), bounds=bounds)
+
+        # uniform integral distribution for discrete features
+        self.A = sample_discrete_parameters(
+            input_mc_samples=self.A,
+            discrete_features=discrete_features,
+            bounds=bounds,
+            num_mc_samples=num_mc_samples,
+        )
+        self.B = sample_discrete_parameters(
+            input_mc_samples=self.B,
+            discrete_features=discrete_features,
+            bounds=bounds,
+            num_mc_samples=num_mc_samples,
+        )
+
         # pyre-fixme[4]: Attribute must be annotated.
         self.A_B_ABi = self.generate_all_input_matrix().to(torch.double)
 
         if self.bootstrap:
             subset_size = 4
             # pyre-fixme[4]: Attribute must be annotated.
             self.bootstrap_indices = torch.randint(
@@ -389,14 +413,15 @@
         second_order: bool = False,
         input_qmc: bool = False,
         num_bootstrap_samples: int = 1,
         link_function: Callable[
             [torch.Tensor, torch.Tensor], torch.Tensor
         ] = GaussianLinkMean,
         mini_batch_size: int = 128,
+        discrete_features: Optional[List[int]] = None,
     ) -> None:
         r"""Computes three types of Sobol indices:
         first order indices, total indices and second order indices (if specified ).
 
         Args:
             model: Botorch model
             bounds: `2 x d` parameter bounds over which to evaluate model sensitivity.
@@ -405,14 +430,17 @@
             num_mc_samples: The number of montecarlo grid samples
             second_order: If True, the second order indices are computed.
             input_qmc: If True, a qmc Sobol grid is use instead of uniformly random.
             num_bootstrap_samples: If bootstrap is true, the number of bootstraps has
                 to be specified.
             mini_batch_size: The size of the mini-batches used while evaluating the
                 model posterior. Increasing this will increase the memory usage.
+            discrete_features: If specified, the inputs associated with the indices in
+                this list are generated using an integer-valued uniform distribution,
+                rather than the default (pseudo-)random continuous uniform distribution.
         """
         self.model = model
         self.second_order = second_order
         self.input_qmc = input_qmc
         # pyre-fixme[4]: Attribute must be annotated.
         self.bootstrap = num_bootstrap_samples > 1
         self.num_bootstrap_samples = num_bootstrap_samples
@@ -423,23 +451,28 @@
                 means, variances = [], []
                 # Since we're only looking at mean & variance, we can freely
                 # use mini-batches.
                 for x_split in x.split(split_size=mini_batch_size):
                     p = checked_cast(GPyTorchPosterior, self.model.posterior(x_split))
                     means.append(p.mean)
                     variances.append(p.variance)
-            return link_function(torch.cat(means), torch.cat(variances))
+
+            cat_dim = 1 if is_ensemble(self.model) else 0
+            return link_function(
+                torch.cat(means, dim=cat_dim), torch.cat(variances, dim=cat_dim)
+            )
 
         self.sensitivity = SobolSensitivity(
             bounds=bounds,
             num_mc_samples=self.num_mc_samples,
             input_function=input_function,
             second_order=self.second_order,
             input_qmc=self.input_qmc,
             num_bootstrap_samples=self.num_bootstrap_samples,
+            discrete_features=discrete_features,
         )
         self.sensitivity.evalute_function()
 
     def first_order_indices(self) -> Tensor:
         r"""Computes the first order Sobol indices:
 
         Returns:
@@ -480,14 +513,15 @@
         bounds: torch.Tensor,
         num_gp_samples: int = 10**3,
         num_mc_samples: int = 10**4,
         second_order: bool = False,
         input_qmc: bool = False,
         gp_sample_qmc: bool = False,
         num_bootstrap_samples: int = 1,
+        discrete_features: Optional[List[int]] = None,
     ) -> None:
         r"""Computes three types of Sobol indices:
         first order indices, total indices and second order indices (if specified ).
 
         Args:
             model: Botorch model.
             bounds: `2 x d` parameter bounds over which to evaluate model sensitivity.
@@ -496,14 +530,17 @@
             num_mc_samples: The number of montecarlo grid samples
             second_order: If True, the second order indices are computed.
             input_qmc: If True, a qmc Sobol grid is use instead of uniformly random.
             gp_sample_qmc: If True, the posterior sampling is done using
                 SobolQMCNormalSampler.
             num_bootstrap_samples: If bootstrap is true, the number of bootstraps has
                 to be specified.
+            discrete_features: If specified, the inputs associated with the indices in
+                this list are generated using an integer-valued uniform distribution,
+                rather than the default (pseudo-)random continuous uniform distribution.
         """
         self.model = model
         self.second_order = second_order
         self.input_qmc = input_qmc
         self.gp_sample_qmc = gp_sample_qmc
         # pyre-fixme[4]: Attribute must be annotated.
         self.bootstrap = num_bootstrap_samples > 1
@@ -513,14 +550,15 @@
         self.sensitivity = SobolSensitivity(
             bounds=bounds,
             num_mc_samples=self.num_mc_samples,
             second_order=self.second_order,
             input_qmc=self.input_qmc,
             num_bootstrap_samples=self.num_bootstrap_samples,
             bootstrap_array=True,
+            discrete_features=discrete_features,
         )
         # TODO: Ideally, we would reduce the memory consumption here as well
         # but this is a tricky since it uses joint posterior sampling.
         posterior = self.model.posterior(self.sensitivity.A_B_ABi)
         if self.gp_sample_qmc:
             sampler = SobolQMCNormalSampler(
                 sample_shape=torch.Size([self.num_gp_samples]), seed=0
@@ -711,36 +749,41 @@
             return second_order_idxs_mean_vargp_segp_varmc_segp
 
 
 def compute_sobol_indices_from_model_list(
     model_list: List[Model],
     bounds: Tensor,
     order: str = "first",
+    discrete_features: Optional[List[int]] = None,
     **sobol_kwargs: Any,
 ) -> Tensor:
     """
     Computes Sobol indices of a list of models on a bounded domain.
 
     Args:
         model_list: A list of botorch.models.model.Model types for which to compute
             the Sobol indices.
         bounds: A 2 x d Tensor of lower and upper bounds of the domain of the models.
         order: A string specifying the order of the Sobol indices to be computed.
             Supports "first" and "total" and defaults to "first".
+        discrete_features: If specified, the inputs associated with the indices in
+            this list are generated using an integer-valued uniform distribution,
+            rather than the default (pseudo-)random continuous uniform distribution.
         sobol_kwargs: keyword arguments passed on to SobolSensitivityGPMean.
 
     Returns:
         With m GPs, returns a (m x d) tensor of `order`-order Sobol indices.
     """
     indices = []
     method = getattr(SobolSensitivityGPMean, f"{order}_order_indices")
     for model in model_list:
         sens_class = SobolSensitivityGPMean(
             model=model,
             bounds=bounds,
+            discrete_features=discrete_features,
             **sobol_kwargs,
         )
         indices.append(method(sens_class))
     return torch.stack(indices)
 
 
 def ax_parameter_sens(
@@ -783,22 +826,32 @@
     digest = torch_model.search_space_digest
     model_list = _get_model_per_metric(torch_model, metrics)
     bounds = torch.tensor(digest.bounds).T  # transposing to make it 2 x d
     ind = compute_sobol_indices_from_model_list(
         model_list=model_list,
         bounds=bounds,
         order=order,
+        discrete_features=digest.categorical_features + digest.ordinal_features,
         **sobol_kwargs,
     )
     if signed:
         ind_deriv = compute_derivatives_from_model_list(
             model_list=model_list,
             bounds=bounds,
+            discrete_features=digest.categorical_features + digest.ordinal_features,
             **sobol_kwargs,
         )
+        # categorical features don't have a direction, so we set the derivative to 1.0
+        # in order not to zero our their sensitivity. We treat categorical features
+        # separately in the sensitivity analysis plot as well, to make clear that they
+        # are affecting the metric, but neither increasing nor decreasing. Note that the
+        # orginal variables have a well defined direction, so we do not need to treat
+        # them differently here.
+        for i in digest.categorical_features:
+            ind_deriv[:, i] = 1.0
         ind *= torch.sign(ind_deriv)
     return _array_with_string_indices_to_dict(
         rows=metrics, cols=digest.feature_names, A=ind.numpy()
     )
 
 
 def _get_torch_model(
@@ -847,20 +900,14 @@
                     i = outcomes.index(m)
                     metric_model = model.surrogates[label].model
                     # since model is a ModularBoTorchModel, metric_model will be a
                     # `botorch.models.model.Model` object, which have the `num_outputs`
                     # property and `subset_outputs` method.
                     if metric_model.num_outputs > 1:  # subset to relevant output
                         metric_model = metric_model.subset_output([i])
-                    if isinstance(metric_model, ModelList):
-                        # any multi-output metric_model will have been modified to be
-                        # single-output model after the if statement right above, so if
-                        # metric_model is a ModelList, it has to contain only one model.
-                        assert len(metric_model.models) == 1
-                        metric_model = metric_model.models[0]
                     model_list.append(metric_model)
                     continue  # found surrogate for `m`, so we can move on to next `m`.
         return model_list
 
 
 def _array_with_string_indices_to_dict(
     rows: List[str], cols: List[str], A: np.ndarray
```

### Comparing `ax-platform-0.3.7/ax/utils/sensitivity/tests/test_sensitivity.py` & `ax-platform-0.4.0/ax/utils/sensitivity/tests/test_sensitivity.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,57 +1,74 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
 
+
+import copy
 import math
 from typing import cast
 from unittest.mock import patch, PropertyMock
 
 import torch
 from ax.modelbridge.base import ModelBridge
 from ax.modelbridge.registry import Models
 from ax.modelbridge.torch import TorchModelBridge
 from ax.models.torch.botorch import BotorchModel
+from ax.utils.common.random import set_rng_seed
 from ax.utils.common.testutils import TestCase
 from ax.utils.sensitivity.derivative_gp import posterior_derivative
 from ax.utils.sensitivity.derivative_measures import (
     compute_derivatives_from_model_list,
     GpDGSMGpMean,
     GpDGSMGpSampling,
+    sample_discrete_parameters,
 )
 from ax.utils.sensitivity.sobol_measures import (
     _get_model_per_metric,
     ax_parameter_sens,
     compute_sobol_indices_from_model_list,
     ProbitLinkMean,
     SobolSensitivityGPMean,
     SobolSensitivityGPSampling,
 )
 from ax.utils.testing.core_stubs import get_branin_experiment
 from botorch.models.gpytorch import BatchedMultiOutputGPyTorchModel, GPyTorchModel
 from botorch.models.model_list_gp_regression import ModelListGP
+from botorch.utils.transforms import unnormalize
 from gpytorch.distributions import MultivariateNormal
 from torch import Tensor
 
 
-def get_modelbridge(modular: bool = False) -> ModelBridge:
+def get_modelbridge(modular: bool = False, saasbo: bool = False) -> ModelBridge:
     exp = get_branin_experiment(with_batch=True)
     exp.trials[0].run()
-    return (Models.BOTORCH_MODULAR if modular else Models.LEGACY_BOTORCH)(
-        # Model bridge kwargs
-        experiment=exp,
-        data=exp.fetch_data(),
-    )
+    if modular:
+        return Models.BOTORCH_MODULAR(
+            experiment=exp,
+            data=exp.fetch_data(),
+        )
+    if saasbo:
+        return Models.SAASBO(
+            experiment=exp,
+            data=exp.fetch_data(),
+        )
+    else:
+        return Models.LEGACY_BOTORCH(experiment=exp, data=exp.fetch_data())
 
 
-class SensitivityAnanlysisTest(TestCase):
+class SensitivityAnalysisTest(TestCase):
     def setUp(self) -> None:
+        super().setUp()
         self.model = get_modelbridge().model.model
+        self.saas_model = (
+            get_modelbridge(saasbo=True).model.surrogates["SAASBO_Surrogate"].model
+        )
 
     def test_DgsmGpMean(self) -> None:
         bounds = torch.tensor([(0.0, 1.0) for _ in range(2)]).t()
         sensitivity_mean = GpDGSMGpMean(self.model, bounds=bounds, num_mc_samples=10)
         gradients_measure = sensitivity_mean.gradient_measure()
         gradients_absolute_measure = sensitivity_mean.gradient_absolute_measure()
         gradients_square_measure = sensitivity_mean.gradients_square_measure()
@@ -131,14 +148,27 @@
         self.assertIsInstance(first_order, Tensor)
         self.assertIsInstance(total_order, Tensor)
         self.assertIsInstance(second_order, Tensor)
         self.assertEqual(first_order.shape, torch.Size([2]))
         self.assertEqual(total_order.shape, torch.Size([2]))
         self.assertEqual(second_order.shape, torch.Size([1]))
 
+        sensitivity_mean_saas = SobolSensitivityGPMean(
+            self.saas_model, num_mc_samples=10, bounds=bounds, second_order=True
+        )
+        first_order = sensitivity_mean_saas.first_order_indices()
+        total_order = sensitivity_mean_saas.total_order_indices()
+        second_order = sensitivity_mean_saas.second_order_indices()
+        self.assertIsInstance(first_order, Tensor)
+        self.assertIsInstance(total_order, Tensor)
+        self.assertIsInstance(second_order, Tensor)
+        self.assertEqual(first_order.shape, torch.Size([2]))
+        self.assertEqual(total_order.shape, torch.Size([2]))
+        self.assertEqual(second_order.shape, torch.Size([1]))
+
         sensitivity_mean_bootstrap = SobolSensitivityGPMean(
             self.model,
             num_mc_samples=10,
             bounds=bounds,
             second_order=True,
             num_bootstrap_samples=10,
             input_qmc=True,
@@ -264,43 +294,64 @@
                     for i, row in enumerate(ind_dict):
                         for j, col in enumerate(ind_dict[row]):
                             self.assertAlmostEqual(
                                 ind_dict[row][col], ind_tnsr[i, j].item()
                             )
         # Test with signed
         model_bridge = get_modelbridge(modular=True)
-        # Unsigned
-        ind_dict = ax_parameter_sens(
-            model_bridge,  # pyre-ignore
-            input_qmc=True,
-            num_mc_samples=10,
-            order="total",
-            signed=False,
-        )
-        ind_deriv = compute_derivatives_from_model_list(
-            model_list=[model_bridge.model.surrogate.model],
-            bounds=torch.tensor(model_bridge.model.search_space_digest.bounds).T,
-        )
-        ind_dict_signed = ax_parameter_sens(
-            model_bridge,  # pyre-ignore
-            input_qmc=True,
-            num_mc_samples=10,
-            order="total",
-            # signed=True
-        )
-        for i, pname in enumerate(["x1", "x2"]):
-            self.assertEqual(
-                torch.sign(ind_deriv[0, i]).item(),
-                math.copysign(1, ind_dict_signed["branin"][pname]),
-            )
-            self.assertAlmostEqual(
-                (torch.sign(ind_deriv[0, i]) * ind_dict["branin"][pname]).item(),
-                ind_dict_signed["branin"][pname],
-            )  # signed
-            self.assertTrue(ind_dict["branin"][pname] >= 0)  # unsigned
+
+        # adding a categorical feature
+        cat_model_bridge = copy.deepcopy(model_bridge)
+        digest = cat_model_bridge.model.search_space_digest
+        digest.categorical_features = [0]
+
+        sobol_kwargs = {"input_qmc": True, "num_mc_samples": 10}
+        seed = 1234
+        for bridge in [model_bridge, cat_model_bridge]:
+            discrete_features = bridge.model.search_space_digest.categorical_features
+            with self.subTest(model_bridge=bridge):
+                set_rng_seed(seed)
+                # Unsigned
+                ind_dict = ax_parameter_sens(
+                    model_bridge=bridge,  # pyre-ignore
+                    metrics=None,
+                    order="total",
+                    signed=False,
+                    **sobol_kwargs,
+                )
+                ind_deriv = compute_derivatives_from_model_list(
+                    model_list=[bridge.model.surrogate.model],
+                    bounds=torch.tensor(bridge.model.search_space_digest.bounds).T,
+                    discrete_features=discrete_features,
+                    **sobol_kwargs,
+                )
+                set_rng_seed(seed)  # reset seed to keep discrete features the same
+                cat_indices = bridge.model.search_space_digest.categorical_features
+                ind_dict_signed = ax_parameter_sens(
+                    model_bridge=bridge,  # pyre-ignore
+                    metrics=None,
+                    order="total",
+                    signed=True,
+                    **sobol_kwargs,
+                )
+                for i, pname in enumerate(["x1", "x2"]):
+                    if i in cat_indices:  # special case for categorical features
+                        expected_sign = 1
+                    else:
+                        expected_sign = torch.sign(ind_deriv[0, i]).item()
+
+                    self.assertEqual(
+                        expected_sign,
+                        math.copysign(1, ind_dict_signed["branin"][pname]),
+                    )
+                    self.assertAlmostEqual(
+                        (expected_sign * ind_dict["branin"][pname]).item(),
+                        ind_dict_signed["branin"][pname],
+                    )  # signed
+                    self.assertTrue(ind_dict["branin"][pname] >= 0)  # unsigned
 
     def test_SobolGPSampling(self) -> None:
         bounds = torch.tensor([(0.0, 1.0) for _ in range(2)]).t()
         sensitivity_sampling = SobolSensitivityGPSampling(
             self.model,
             num_mc_samples=10,
             num_gp_samples=10,
@@ -343,14 +394,68 @@
                 bounds=bounds,
                 second_order=False,
             )
             first_order = sensitivity_sampling.first_order_indices()
             total_order = sensitivity_sampling.total_order_indices()
             second_order = sensitivity_sampling.second_order_indices()
 
+        discrete_feature = 0
+        sensitivity_sampling_discrete = SobolSensitivityGPSampling(
+            self.model,
+            num_mc_samples=10,
+            num_gp_samples=10,
+            bounds=bounds,
+            second_order=True,
+            discrete_features=[discrete_feature],
+        )
+        sens = sensitivity_sampling_discrete.sensitivity
+        A = sens.A
+        B = sens.B
+        Arnd = A.round()
+        Brnd = B.round()
+        # testing that the discrete feature is integer valued
+        self.assertTrue(
+            torch.allclose(Arnd[:, discrete_feature], A[:, discrete_feature])
+        )
+        self.assertTrue(
+            torch.allclose(Brnd[:, discrete_feature], B[:, discrete_feature])
+        )
+
+        # testing that the other features are not integer valued
+        self.assertFalse(torch.allclose(Arnd, A))
+        self.assertFalse(torch.allclose(Brnd, B))
+
     def test_DerivativeGp(self) -> None:
         test_x = torch.rand(2, 2)
         posterior = posterior_derivative(self.model, test_x, kernel_type="matern")
         self.assertIsInstance(posterior, MultivariateNormal)
 
         with self.assertRaises(ValueError):
             posterior = posterior_derivative(self.model, test_x, kernel_type="xyz")
+
+    def test_sample_discrete_parameters(self) -> None:
+        dim = 5
+        bounds = torch.stack((torch.zeros(dim), torch.arange(1, dim + 1)))
+        num_mc_samples = 8
+        A = unnormalize(torch.rand(num_mc_samples, dim), bounds=bounds)
+        discrete_features, continuous_features = [1, 3], [0, 2, 4]
+        B = sample_discrete_parameters(
+            input_mc_samples=A.clone(),
+            discrete_features=discrete_features,
+            bounds=bounds,
+            num_mc_samples=num_mc_samples,
+        )
+        self.assertTrue(  # Non-discrete parameters should be untouched
+            torch.equal(A[:, continuous_features], B[:, continuous_features])
+        )
+        for i in discrete_features:  # Make sure we sampled integers in the right range
+            self.assertTrue(B[:, i].min() >= bounds[0, i])
+            self.assertTrue(B[:, i].max() <= bounds[1, i])
+            self.assertTrue(torch.allclose(B[:, i], B[:, i].round()))
+        # discrete_features=None should be a no-op
+        B = sample_discrete_parameters(
+            input_mc_samples=A.clone(),
+            discrete_features=None,
+            bounds=bounds,
+            num_mc_samples=num_mc_samples,
+        )
+        self.assertTrue(torch.equal(A, B))
```

### Comparing `ax-platform-0.3.7/ax/utils/stats/model_fit_stats.py` & `ax-platform-0.4.0/ax/utils/stats/model_fit_stats.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,29 +1,40 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+from logging import Logger
 from typing import Dict, Mapping, Optional, Protocol
 
 import numpy as np
+
+from ax.utils.common.logger import get_logger
 from scipy.stats import fisher_exact, norm, pearsonr, spearmanr
+from sklearn.neighbors import KernelDensity
+
+
+logger: Logger = get_logger(__name__)
+
+
+DEFAULT_KDE_BANDWIDTH = 0.1  # default bandwidth for kernel density estimators
 
 """
 ################################ Model Fit Metrics ###############################
 """
 
 
 class ModelFitMetricProtocol(Protocol):
     """Structural type for model fit metrics."""
 
     def __call__(
         self, y_obs: np.ndarray, y_pred: np.ndarray, se_pred: np.ndarray
-    ) -> float:
-        ...
+    ) -> float: ...
 
 
 def compute_model_fit_metrics(
     y_obs: Mapping[str, np.ndarray],
     y_pred: Mapping[str, np.ndarray],
     se_pred: Mapping[str, np.ndarray],
     fit_metrics_dict: Mapping[str, ModelFitMetricProtocol],
@@ -122,14 +133,68 @@
 
     Returns:
         The scalar standard deviation of the standardized error.
     """
     return ((y_obs - y_pred) / se_pred).std()
 
 
+def entropy_of_observations(
+    y_obs: np.ndarray,
+    y_pred: np.ndarray,
+    se_pred: np.ndarray,
+    bandwidth: float = DEFAULT_KDE_BANDWIDTH,
+) -> float:
+    """Computes the entropy of the observations y_obs using a kernel density estimator.
+    This can be used to quantify how "clustered" the outcomes are. NOTE: y_pred and
+    se_pred are not used, but are required for the API.
+
+    Args:
+        y_obs: An array of observations for a single metric.
+        y_pred: Unused.
+        se_pred: Unused.
+        bandwidth: The kernel bandwidth. Defaults to 0.1, which is a reasonable value
+            for standardized outcomes y_obs. The rank ordering of the results on a set
+            of y_obs data sets is not generally sensitive to the bandwidth, if it is
+            held fixed across the data sets. The absolute value of the results however
+            changes significantly with the bandwidth.
+
+    Returns:
+        The scalar entropy of the observations.
+    """
+    if y_obs.ndim == 1:
+        y_obs = y_obs[:, np.newaxis]
+
+    # Check if standardization was applied to the observations.
+    if bandwidth == DEFAULT_KDE_BANDWIDTH:
+        y_std = np.std(y_obs, axis=0, ddof=1)
+        if np.any(y_std < 0.5) or np.any(2.0 < y_std):  # allowing a fudge factor of 2.
+            logger.warning(
+                "Standardization of observations was not applied. "
+                f"The default bandwidth of {DEFAULT_KDE_BANDWIDTH} is a reasonable "
+                "choice if observations are standardize, but may not be otherwise."
+            )
+    return _entropy_via_kde(y_obs, bandwidth=bandwidth)
+
+
+def _entropy_via_kde(y: np.ndarray, bandwidth: float = DEFAULT_KDE_BANDWIDTH) -> float:
+    """Computes the entropy of the kernel density estimate of the input data.
+
+    Args:
+        y: An (n x m) array of observations.
+        bandwidth: The kernel bandwidth.
+
+    Returns:
+        The scalar entropy of the kernel density estimate.
+    """
+    kde = KernelDensity(kernel="gaussian", bandwidth=bandwidth)
+    kde.fit(y)
+    log_p = kde.score_samples(y)  # computes the log probability of each data point
+    return -np.sum(np.exp(log_p) * log_p)  # compute entropy, the negated sum of p log p
+
+
 def _mean_prediction_ci(
     y_obs: np.ndarray, y_pred: np.ndarray, se_pred: np.ndarray
 ) -> float:
     # Pyre does not allow float * np.ndarray.
     return float(np.mean(1.96 * 2 * se_pred / np.abs(y_obs)))
```

### Comparing `ax-platform-0.3.7/ax/utils/stats/statstools.py` & `ax-platform-0.4.0/ax/utils/stats/statstools.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import List, Tuple, Union
 
 import numpy as np
 import pandas as pd
 from ax.core.data import Data
 from ax.utils.common.logger import get_logger
```

### Comparing `ax-platform-0.3.7/ax/utils/stats/tests/test_statstools.py` & `ax-platform-0.4.0/ax/utils/stats/tests/test_statstools.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from itertools import product
 
 import numpy as np
 import pandas as pd
 from ax.core.data import Data
 from ax.utils.common.testutils import TestCase
 from ax.utils.stats.statstools import (
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/backend_scheduler.py` & `ax-platform-0.4.0/ax/utils/testing/backend_scheduler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from dataclasses import replace as dataclass_replace
 
 from logging import Logger
 from typing import Dict, Optional, Set
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/backend_simulator.py` & `ax-platform-0.4.0/ax/utils/testing/backend_simulator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import logging
 import random
 import time
 from dataclasses import dataclass
 
 from logging import Logger
 from typing import Dict, List, Optional
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/benchmark_stubs.py` & `ax-platform-0.4.0/ax/utils/testing/benchmark_stubs.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+from typing import Any, Dict, Optional
+
 import numpy as np
 from ax.benchmark.benchmark_method import BenchmarkMethod
 from ax.benchmark.benchmark_problem import (
     BenchmarkProblem,
     MultiObjectiveBenchmarkProblem,
     SingleObjectiveBenchmarkProblem,
 )
@@ -25,44 +29,60 @@
 from ax.utils.testing.core_stubs import (
     get_branin_multi_objective_optimization_config,
     get_branin_optimization_config,
     get_branin_search_space,
 )
 from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement
 from botorch.models.gp_regression import SingleTaskGP
-from botorch.test_functions.multi_objective import BraninCurrin
+from botorch.test_functions.multi_objective import BraninCurrin, ConstrainedBraninCurrin
 from botorch.test_functions.synthetic import Branin
 
 
 def get_benchmark_problem() -> BenchmarkProblem:
     return BenchmarkProblem.from_botorch(
-        test_problem_class=Branin, test_problem_kwargs={}, num_trials=4
+        test_problem_class=Branin,
+        test_problem_kwargs={},
+        lower_is_better=True,
+        num_trials=4,
     )
 
 
 def get_single_objective_benchmark_problem(
-    infer_noise: bool = True,
+    observe_noise_sd: bool = False,
     num_trials: int = 4,
+    test_problem_kwargs: Optional[Dict[str, Any]] = None,
 ) -> SingleObjectiveBenchmarkProblem:
     return SingleObjectiveBenchmarkProblem.from_botorch_synthetic(
         test_problem_class=Branin,
-        test_problem_kwargs={},
+        test_problem_kwargs=test_problem_kwargs or {},
+        lower_is_better=True,
         num_trials=num_trials,
-        infer_noise=infer_noise,
+        observe_noise_sd=observe_noise_sd,
     )
 
 
 def get_multi_objective_benchmark_problem(
-    infer_noise: bool = True, num_trials: int = 4
+    observe_noise_sd: bool = False, num_trials: int = 4
 ) -> MultiObjectiveBenchmarkProblem:
     return MultiObjectiveBenchmarkProblem.from_botorch_multi_objective(
         test_problem_class=BraninCurrin,
         test_problem_kwargs={},
         num_trials=num_trials,
-        infer_noise=infer_noise,
+        observe_noise_sd=observe_noise_sd,
+    )
+
+
+def get_constrained_multi_objective_benchmark_problem(
+    observe_noise_sd: bool = False, num_trials: int = 4
+) -> MultiObjectiveBenchmarkProblem:
+    return MultiObjectiveBenchmarkProblem.from_botorch_multi_objective(
+        test_problem_class=ConstrainedBraninCurrin,
+        test_problem_kwargs={},
+        num_trials=num_trials,
+        observe_noise_sd=observe_noise_sd,
     )
 
 
 def get_sobol_benchmark_method() -> BenchmarkMethod:
     return BenchmarkMethod(
         name="SOBOL",
         generation_strategy=GenerationStrategy(
@@ -72,59 +92,63 @@
         scheduler_options=SchedulerOptions(
             total_trials=4, init_seconds_between_polls=0
         ),
     )
 
 
 def get_soo_surrogate() -> SOOSurrogateBenchmarkProblem:
-    surrogate = Surrogate(
-        botorch_model_class=SingleTaskGP,
-    )
+    surrogate = Surrogate(botorch_model_class=SingleTaskGP)
     return SOOSurrogateBenchmarkProblem(
         name="test",
         search_space=get_branin_search_space(),
         optimization_config=get_branin_optimization_config(),
         num_trials=6,
-        infer_noise=False,
-        metric_names=[],
+        outcome_names=["branin"],
+        observe_noise_stds=True,
         get_surrogate_and_datasets=lambda: (surrogate, []),
         optimal_value=0.0,
     )
 
 
 def get_moo_surrogate() -> MOOSurrogateBenchmarkProblem:
     surrogate = Surrogate(botorch_model_class=SingleTaskGP)
     return MOOSurrogateBenchmarkProblem(
         name="test",
         search_space=get_branin_search_space(),
         optimization_config=get_branin_multi_objective_optimization_config(),
         num_trials=10,
-        infer_noise=False,
-        metric_names=[],
+        outcome_names=["branin_a", "branin_b"],
+        observe_noise_stds=True,
         get_surrogate_and_datasets=lambda: (surrogate, []),
         maximum_hypervolume=1.0,
         reference_point=[],
     )
 
 
 def get_sobol_gpei_benchmark_method() -> BenchmarkMethod:
     return BenchmarkMethod(
         name="MBO_SOBOL_GPEI",
         generation_strategy=GenerationStrategy(
             name="Modular::Sobol+GPEI",
             steps=[
-                GenerationStep(model=Models.SOBOL, num_trials=3, min_trials_observed=3),
+                GenerationStep(
+                    model=Models.SOBOL,
+                    num_trials=3,
+                    model_kwargs={"fit_tracking_metrics": False},
+                    min_trials_observed=3,
+                ),
                 GenerationStep(
                     model=Models.BOTORCH_MODULAR,
                     num_trials=-1,
                     model_kwargs={
                         "surrogate": Surrogate(SingleTaskGP),
                         # TODO: tests should better reflect defaults and not
                         # re-implement this logic.
                         "botorch_acqf_class": qNoisyExpectedImprovement,
+                        "model_kwargs": {"fit_tracking_metrics": False},
                     },
                     model_gen_kwargs={
                         "model_gen_options": {
                             Keys.OPTIMIZER_KWARGS: {
                                 "num_restarts": 50,
                                 "raw_samples": 1024,
                             },
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/core_stubs.py` & `ax-platform-0.4.0/ax/utils/testing/core_stubs.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
+
+from __future__ import annotations
 
 from collections import OrderedDict
 from datetime import datetime, timedelta
-
 from logging import Logger
 from pathlib import Path
 from typing import (
     Any,
     cast,
     Dict,
     Iterable,
@@ -68,14 +71,15 @@
 from ax.core.types import (
     ComparisonOp,
     TModelCov,
     TModelMean,
     TModelPredict,
     TModelPredictArm,
     TParameterization,
+    TParamValue,
 )
 from ax.early_stopping.strategies import (
     BaseEarlyStoppingStrategy,
     PercentileEarlyStoppingStrategy,
     ThresholdEarlyStoppingStrategy,
 )
 from ax.early_stopping.strategies.logical import (
@@ -88,23 +92,29 @@
 from ax.metrics.branin import AugmentedBraninMetric, BraninMetric
 from ax.metrics.branin_map import BraninTimestampMapMetric
 from ax.metrics.dict_lookup import DictLookupMetric
 from ax.metrics.factorial import FactorialMetric
 from ax.metrics.hartmann6 import AugmentedHartmann6Metric, Hartmann6Metric
 from ax.modelbridge.factory import Cont_X_trans, get_factorial, get_sobol
 from ax.modelbridge.generation_strategy import GenerationStrategy
+from ax.modelbridge.transition_criterion import (
+    MaxGenerationParallelism,
+    MaxTrials,
+    TrialBasedCriterion,
+)
 from ax.models.torch.botorch_modular.acquisition import Acquisition
 from ax.models.torch.botorch_modular.model import BoTorchModel, SurrogateSpec
 from ax.models.torch.botorch_modular.sebo import SEBOAcquisition
 from ax.models.torch.botorch_modular.surrogate import Surrogate
 from ax.models.winsorization_config import WinsorizationConfig
 from ax.runners.synthetic import SyntheticRunner
 from ax.service.utils.scheduler_options import SchedulerOptions, TrialType
 from ax.utils.common.constants import Keys
 from ax.utils.common.logger import get_logger
+from ax.utils.common.random import set_rng_seed
 from ax.utils.common.typeutils import checked_cast, not_none
 from ax.utils.measurement.synthetic_functions import branin
 from botorch.acquisition.acquisition import AcquisitionFunction
 from botorch.acquisition.monte_carlo import qExpectedImprovement
 from botorch.models.gp_regression import SingleTaskGP
 from botorch.models.model import Model
 from botorch.models.transforms.input import ChainedInputTransform, Normalize, Round
@@ -146,45 +156,86 @@
         description="test description",
         tracking_metrics=[MapMetric(name="tracking")],
         is_test=True,
         default_data_type=DataType.MAP_DATA,
     )
 
 
+def get_trial_based_criterion() -> List[TrialBasedCriterion]:
+    return [
+        MaxTrials(
+            threshold=3,
+            only_in_statuses=[TrialStatus.RUNNING, TrialStatus.COMPLETED],
+            not_in_statuses=None,
+        ),
+        MaxGenerationParallelism(
+            threshold=5,
+            only_in_statuses=None,
+            not_in_statuses=[
+                TrialStatus.RUNNING,
+            ],
+        ),
+    ]
+
+
 def get_experiment_with_custom_runner_and_metric(
     constrain_search_space: bool = True,
     immutable: bool = False,
     multi_objective: bool = False,
+    num_trials: int = 3,
+    has_outcome_constraint: bool = False,
 ) -> Experiment:
-
     # Create experiment with custom runner and metric
     experiment = Experiment(
         name="test",
-        # Omit constraints to prevent Sobol rejection sampling below,
-        # which floods logs with "Unable to round" warnings.
         search_space=get_search_space(constrain_search_space=constrain_search_space),
-        optimization_config=get_multi_objective_optimization_config(custom_metric=True)
-        if multi_objective
-        else get_optimization_config(),
+        optimization_config=(
+            get_multi_objective_optimization_config(
+                custom_metric=True,
+                outcome_constraint=has_outcome_constraint,
+                relative=False,
+            )
+            if multi_objective
+            else get_optimization_config(
+                outcome_constraint=has_outcome_constraint, relative=False
+            )
+        ),
         description="test description",
         tracking_metrics=[
             CustomTestMetric(name="custom_test_metric", test_attribute="test")
         ],
         runner=CustomTestRunner(test_attribute="test"),
         is_test=True,
     )
 
     # Create a trial, set its runner and complete it.
-    sobol_generator = get_sobol(search_space=experiment.search_space)
-    sobol_run = sobol_generator.gen(n=1)
-    trial = experiment.new_trial(generator_run=sobol_run)
-    trial.runner = experiment.runner
-    trial.mark_running()
-    experiment.attach_data(get_data(metric_name="custom_test_metric"))
-    trial.mark_completed()
+    for _ in range(num_trials):
+        sobol_generator = get_sobol(
+            search_space=experiment.search_space,
+        )
+        sobol_run = sobol_generator.gen(
+            n=1,
+            optimization_config=(
+                experiment.optimization_config if not immutable else None
+            ),
+        )
+        trial = experiment.new_trial(generator_run=sobol_run)
+        trial.runner = experiment.runner
+        trial.mark_running()
+        data = Data.from_multiple_data(
+            get_data(
+                metric_name=metric_name,
+                trial_index=trial.index,
+                num_non_sq_arms=len(trial.arms),
+                include_sq=False,
+            )
+            for metric_name in experiment.metrics
+        )
+        experiment.attach_data(data)
+        trial.mark_completed()
 
     if immutable:
         experiment._properties = {Keys.IMMUTABLE_SEARCH_SPACE_AND_OPT_CONF: True}
 
     return experiment
 
 
@@ -205,17 +256,19 @@
         with_fidelity_parameter=with_fidelity_parameter,
         with_choice_parameter=with_choice_parameter,
         with_str_choice_param=with_str_choice_param,
     )
     exp = Experiment(
         name="branin_test_experiment" if named else None,
         search_space=search_space,
-        optimization_config=get_branin_optimization_config(minimize=minimize)
-        if has_optimization_config
-        else None,
+        optimization_config=(
+            get_branin_optimization_config(minimize=minimize)
+            if has_optimization_config
+            else None
+        ),
         runner=SyntheticRunner(),
         is_test=True,
     )
 
     if with_status_quo:
         exp.status_quo = Arm(parameters={"x1": 0.0, "x2": 0.0})
 
@@ -324,15 +377,15 @@
             rate=rate,
             lower_is_better=True,
         )
 
     tracking_metric = (
         get_map_metric("tracking_branin_map")
         if map_tracking_metric
-        else BraninMetric(name="branin", param_names=["x1", "x2"])
+        else BraninMetric(name="branin", param_names=["x1", "x2"], lower_is_better=True)
     )
     exp = Experiment(
         name="branin_with_timestamp_map_metric",
         search_space=get_branin_search_space(),
         optimization_config=OptimizationConfig(
             objective=Objective(metric=get_map_metric("branin_map"), minimize=True)
         ),
@@ -378,22 +431,21 @@
         trial = experiment.new_trial().add_arm(arm=get_branin_arms(n=1, seed=i)[0])
         trial.run()
     for _ in range(num_fetches):
         # each time we call fetch, we grab another timestamp
         experiment.fetch_data()
     for i in range(num_complete):
         experiment.trials[i].mark_as(status=TrialStatus.COMPLETED)
-    experiment.attach_data(data=experiment.fetch_data())
     return experiment
 
 
 def get_multi_type_experiment(
     add_trial_type: bool = True, add_trials: bool = False, num_arms: int = 10
 ) -> MultiTypeExperiment:
-    oc = OptimizationConfig(Objective(BraninMetric("m1", ["x1", "x2"])))
+    oc = OptimizationConfig(Objective(BraninMetric("m1", ["x1", "x2"]), minimize=True))
     experiment = MultiTypeExperiment(
         name="test_exp",
         search_space=get_branin_search_space(),
         default_trial_type="type1",
         default_runner=SyntheticRunner(dummy_metadata="dummy1"),
         optimization_config=oc,
     )
@@ -450,19 +502,21 @@
     has_optimization_config: bool = True,
     with_batch: bool = False,
     with_status_quo: bool = False,
 ) -> Experiment:
     exp = Experiment(
         name="factorial_test_experiment",
         search_space=get_factorial_search_space(),
-        optimization_config=OptimizationConfig(
-            objective=Objective(metric=get_factorial_metric())
-        )
-        if has_optimization_config
-        else None,
+        optimization_config=(
+            OptimizationConfig(
+                objective=Objective(metric=get_factorial_metric(), minimize=False)
+            )
+            if has_optimization_config
+            else None
+        ),
         runner=SyntheticRunner(),
         is_test=True,
         tracking_metrics=[get_factorial_metric("secondary_metric")],
     )
 
     if with_status_quo:
         exp.status_quo = Arm(
@@ -553,20 +607,22 @@
     num_objectives: int = 2,
 ) -> Experiment:
     exp = Experiment(
         name="branin_test_experiment",
         search_space=get_branin_search_space(
             with_fidelity_parameter=with_fidelity_parameter
         ),
-        optimization_config=get_branin_multi_objective_optimization_config(
-            has_objective_thresholds=has_objective_thresholds,
-            num_objectives=num_objectives,
-        )
-        if has_optimization_config
-        else None,
+        optimization_config=(
+            get_branin_multi_objective_optimization_config(
+                has_objective_thresholds=has_objective_thresholds,
+                num_objectives=num_objectives,
+            )
+            if has_optimization_config
+            else None
+        ),
         runner=SyntheticRunner(),
         is_test=True,
     )
 
     if with_status_quo:
         # Experiment chooses the name "status_quo" by default
         exp.status_quo = Arm(parameters={"x1": 0.0, "x2": 0.0})
@@ -581,19 +637,21 @@
     return exp
 
 
 def get_branin_with_multi_task(with_multi_objective: bool = False) -> Experiment:
     exp = Experiment(
         name="branin_test_experiment",
         search_space=get_branin_search_space(),
-        optimization_config=get_branin_multi_objective_optimization_config(
-            has_objective_thresholds=True,
-        )
-        if with_multi_objective
-        else get_branin_optimization_config(),
+        optimization_config=(
+            get_branin_multi_objective_optimization_config(
+                has_objective_thresholds=True,
+            )
+            if with_multi_objective
+            else get_branin_optimization_config()
+        ),
         runner=SyntheticRunner(),
         is_test=True,
     )
 
     exp.status_quo = Arm(parameters={"x1": 0.0, "x2": 0.0}, name="status_quo")
 
     sobol_generator = get_sobol(search_space=exp.search_space, seed=TEST_SOBOL_SEED)
@@ -662,15 +720,18 @@
     observations: List[List[float]],
     minimize: bool = False,
     scalarized: bool = False,
     constrained: bool = False,
     with_tracking_metrics: bool = False,
     search_space: Optional[SearchSpace] = None,
 ) -> Experiment:
-    multi_objective = (len(observations[0]) - constrained) > 1
+    if observations:
+        multi_objective = (len(observations[0]) - constrained) > 1
+    else:
+        multi_objective = False
     if multi_objective:
         metrics = [
             Metric(name="m1", lower_is_better=minimize),
             Metric(name="m2", lower_is_better=False),
         ]
         if scalarized:
             optimization_config = OptimizationConfig(
@@ -682,30 +743,34 @@
             optimization_config = MultiObjectiveOptimizationConfig(
                 objective=MultiObjective(metrics=metrics),
                 objective_thresholds=[
                     ObjectiveThreshold(
                         metric=metrics[i],
                         bound=0.0,
                         relative=False,
-                        op=ComparisonOp.LEQ
-                        if metrics[i].lower_is_better
-                        else ComparisonOp.GEQ,
+                        op=(
+                            ComparisonOp.LEQ
+                            if metrics[i].lower_is_better
+                            else ComparisonOp.GEQ
+                        ),
                     )
                     for i in [0, 1]
                 ],
-                outcome_constraints=[
-                    OutcomeConstraint(
-                        metric=Metric(name="m3"),
-                        op=ComparisonOp.GEQ,
-                        bound=0.0,
-                        relative=False,
-                    )
-                ]
-                if constrained
-                else None,
+                outcome_constraints=(
+                    [
+                        OutcomeConstraint(
+                            metric=Metric(name="m3"),
+                            op=ComparisonOp.GEQ,
+                            bound=0.0,
+                            relative=False,
+                        )
+                    ]
+                    if constrained
+                    else None
+                ),
             )
     else:
         if scalarized:
             raise NotImplementedError
         objective = Objective(metric=Metric(name="m1"), minimize=minimize)
         if constrained:
             constraint = OutcomeConstraint(
@@ -719,19 +784,19 @@
             )
         else:
             optimization_config = OptimizationConfig(objective=objective)
     search_space = search_space or get_search_space_for_range_values(min=0.0, max=1.0)
     exp = Experiment(
         search_space=search_space,
         optimization_config=optimization_config,
-        tracking_metrics=[
-            Metric(name=f"m{len(observations[0])}", lower_is_better=False)
-        ]
-        if with_tracking_metrics
-        else None,
+        tracking_metrics=(
+            [Metric(name=f"m{len(observations[0])}", lower_is_better=False)]
+            if with_tracking_metrics
+            else None
+        ),
         runner=SyntheticRunner(),
         is_test=True,
     )
     sobol_generator = get_sobol(search_space=search_space)
     for i, obs in enumerate(observations):
         # Create a dummy trial to add the observation.
         trial = exp.new_trial(generator_run=sobol_generator.gen(n=1))
@@ -750,15 +815,15 @@
             )
         )
         exp.attach_data(data)
         trial.run().complete()
     return exp
 
 
-def get_high_dimensional_branin_experiment() -> Experiment:
+def get_high_dimensional_branin_experiment(with_batch: bool = False) -> Experiment:
     search_space = SearchSpace(
         # pyre-fixme[6]: In call `SearchSpace.__init__`, for 1st parameter `parameters`
         # expected `List[Parameter]` but got `List[RangeParameter]`.
         parameters=[
             RangeParameter(
                 name=f"x{i}",
                 parameter_type=ParameterType.FLOAT,
@@ -784,20 +849,25 @@
                 name="objective",
                 param_names=["x19", "x44"],
             ),
             minimize=True,
         )
     )
 
-    return Experiment(
+    exp = Experiment(
         name="high_dimensional_branin_experiment",
         search_space=search_space,
         optimization_config=optimization_config,
         runner=SyntheticRunner(),
     )
+    if with_batch:
+        sobol_generator = get_sobol(search_space=exp.search_space)
+        sobol_run = sobol_generator.gen(n=15)
+        exp.new_batch_trial().add_generator_run(sobol_run)
+    return exp
 
 
 ##############################
 # Search Spaces
 ##############################
 
 
@@ -826,22 +896,24 @@
     with_choice_parameter: bool = False,
     with_str_choice_param: bool = False,
 ) -> SearchSpace:
     parameters = [
         RangeParameter(
             name="x1", parameter_type=ParameterType.FLOAT, lower=-5, upper=10
         ),
-        ChoiceParameter(
-            name="x2",
-            parameter_type=ParameterType.FLOAT,
-            values=[float(x) for x in range(0, 16)],
-        )
-        if with_choice_parameter
-        else RangeParameter(
-            name="x2", parameter_type=ParameterType.FLOAT, lower=0, upper=15
+        (
+            ChoiceParameter(
+                name="x2",
+                parameter_type=ParameterType.FLOAT,
+                values=[float(x) for x in range(0, 16)],
+            )
+            if with_choice_parameter
+            else RangeParameter(
+                name="x2", parameter_type=ParameterType.FLOAT, lower=0, upper=15
+            )
         ),
     ]
     if with_str_choice_param:
         parameters.append(
             ChoiceParameter(
                 name="str_param",
                 parameter_type=ParameterType.STRING,
@@ -924,15 +996,15 @@
         ]
     )
 
 
 def get_hartmann_search_space(with_fidelity_parameter: bool = False) -> SearchSpace:
     parameters = [
         RangeParameter(
-            name=f"x{idx+1}", parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0
+            name=f"x{idx + 1}", parameter_type=ParameterType.FLOAT, lower=0.0, upper=1.0
         )
         for idx in range(6)
     ]
     if with_fidelity_parameter:
         parameters.append(
             RangeParameter(
                 name="fidelity",
@@ -1420,29 +1492,20 @@
 ) -> AugmentedHartmann6Metric:
     param_names = [f"x{idx + 1}" for idx in range(6)]
     param_names.append("fidelity")
     return AugmentedHartmann6Metric(name=name, param_names=param_names, noise_sd=0.01)
 
 
 def get_factorial_metric(name: str = "success_metric") -> FactorialMetric:
-    coefficients = {
+    coefficients: Dict[str, Dict[TParamValue, float]] = {
         "factor1": {"level11": 0.1, "level12": 0.2, "level13": 0.3},
         "factor2": {"level21": 0.1, "level22": 0.2},
         "factor3": {"level31": 0.1, "level32": 0.2, "level33": 0.3, "level34": 0.4},
     }
-    return FactorialMetric(
-        name=name,
-        # Expected `Dict[str, Dict[typing.Optional[typing.Union[bool, float, str]],
-        # float]]` for 3rd parameter `coefficients` to call
-        # `ax.metrics.factorial.FactorialMetric.__init__` but got `Dict[str,
-        # Dict[str, float]]`.
-        # pyre-fixme[6]:
-        coefficients=coefficients,
-        batch_size=int(1e4),
-    )
+    return FactorialMetric(name=name, coefficients=coefficients, batch_size=int(1e4))
 
 
 def get_dict_lookup_metric() -> DictLookupMetric:
     return DictLookupMetric(
         name="test metric",
         param_names=["p1", "p2"],
         lookup_dict={
@@ -1463,16 +1526,18 @@
     comparison_op: ComparisonOp = ComparisonOp.GEQ,
 ) -> ObjectiveThreshold:
     return ObjectiveThreshold(
         metric=Metric(name=metric_name), bound=bound, op=comparison_op
     )
 
 
-def get_outcome_constraint() -> OutcomeConstraint:
-    return OutcomeConstraint(metric=Metric(name="m2"), op=ComparisonOp.GEQ, bound=-0.25)
+def get_outcome_constraint(relative: bool = True) -> OutcomeConstraint:
+    return OutcomeConstraint(
+        metric=Metric(name="m2"), op=ComparisonOp.GEQ, bound=-0.25, relative=relative
+    )
 
 
 def get_scalarized_outcome_constraint() -> ScalarizedOutcomeConstraint:
     return ScalarizedOutcomeConstraint(
         metrics=[Metric(name="oc_m3"), Metric(name="oc_m4")],
         weights=[0.2, 0.8],
         op=ComparisonOp.GEQ,
@@ -1485,35 +1550,38 @@
 
 
 ##############################
 # Objectives
 ##############################
 
 
-def get_objective() -> Objective:
-    return Objective(metric=Metric(name="m1"), minimize=False)
+def get_objective(minimize: bool = False) -> Objective:
+    return Objective(metric=Metric(name="m1"), minimize=minimize)
 
 
-def get_map_objective() -> Objective:
-    return Objective(metric=MapMetric(name="m1"), minimize=False)
+def get_map_objective(minimize: bool = False) -> Objective:
+    return Objective(metric=MapMetric(name="m1"), minimize=minimize)
 
 
 def get_multi_objective() -> Objective:
     return MultiObjective(
         objectives=[
-            Objective(metric=Metric(name="m1")),
+            Objective(metric=Metric(name="m1"), minimize=False),
             Objective(metric=Metric(name="m3", lower_is_better=True), minimize=True),
         ],
     )
 
 
 def get_custom_multi_objective() -> Objective:
     return MultiObjective(
         objectives=[
-            Objective(metric=CustomTestMetric(name="m1", test_attribute="test")),
+            Objective(
+                metric=CustomTestMetric(name="m1", test_attribute="test"),
+                minimize=False,
+            ),
             Objective(
                 metric=CustomTestMetric(
                     name="m3", lower_is_better=True, test_attribute="test"
                 ),
                 minimize=True,
             ),
         ],
@@ -1535,15 +1603,17 @@
         metrics=[Metric(name="m1"), Metric(name="m3")],
         weights=[1.0, 2.0],
         minimize=False,
     )
 
 
 def get_branin_objective(name: str = "branin", minimize: bool = False) -> Objective:
-    return Objective(metric=get_branin_metric(name=name), minimize=minimize)
+    return Objective(
+        metric=get_branin_metric(name=name, lower_is_better=minimize), minimize=minimize
+    )
 
 
 def get_branin_multi_objective(num_objectives: int = 2) -> Objective:
     _validate_num_objectives(num_objectives=num_objectives)
     objectives = [
         Objective(metric=get_branin_metric(name="branin_a"), minimize=True),
         Objective(metric=get_branin_metric(name="branin_b"), minimize=True),
@@ -1568,46 +1638,58 @@
 
 
 ##############################
 # Optimization Configs
 ##############################
 
 
-def get_optimization_config() -> OptimizationConfig:
+def get_optimization_config(
+    outcome_constraint: bool = True, relative: bool = True
+) -> OptimizationConfig:
     objective = get_objective()
-    outcome_constraints = [get_outcome_constraint()]
+    outcome_constraints = (
+        [get_outcome_constraint(relative=relative)] if outcome_constraint else []
+    )
     return OptimizationConfig(
         objective=objective, outcome_constraints=outcome_constraints
     )
 
 
 def get_map_optimization_config() -> OptimizationConfig:
     objective = get_map_objective()
     return OptimizationConfig(objective=objective)
 
 
 def get_multi_objective_optimization_config(
     custom_metric: bool = False,
+    relative: bool = True,
+    outcome_constraint: bool = True,
 ) -> MultiObjectiveOptimizationConfig:
 
     objective = get_custom_multi_objective() if custom_metric else get_multi_objective()
-    outcome_constraints = [get_outcome_constraint()]
+    outcome_constraints = (
+        [get_outcome_constraint(relative=relative)] if outcome_constraint else []
+    )
     objective_thresholds = [
         get_objective_threshold(metric_name="m1"),
         get_objective_threshold(metric_name="m3", comparison_op=ComparisonOp.LEQ),
     ]
     return MultiObjectiveOptimizationConfig(
         objective=objective,
         outcome_constraints=outcome_constraints,
         objective_thresholds=objective_thresholds,
     )
 
 
-def get_optimization_config_no_constraints() -> OptimizationConfig:
-    return OptimizationConfig(objective=Objective(metric=Metric("test_metric")))
+def get_optimization_config_no_constraints(
+    minimize: bool = False,
+) -> OptimizationConfig:
+    return OptimizationConfig(
+        objective=Objective(metric=Metric("test_metric"), minimize=minimize)
+    )
 
 
 def get_branin_optimization_config(minimize: bool = False) -> OptimizationConfig:
     return OptimizationConfig(objective=get_branin_objective(minimize=minimize))
 
 
 def _validate_num_objectives(num_objectives: int) -> None:
@@ -1722,15 +1804,14 @@
 
 
 def get_weights() -> List[float]:
     return list(get_arm_weights1().values())
 
 
 def get_branin_arms(n: int, seed: int) -> List[Arm]:
-    # TODO replace with sobol
     np.random.seed(seed)
     x1_raw = np.random.rand(n)
     x2_raw = np.random.rand(n)
     return [
         Arm(parameters={"x1": -5 + x1_raw[i] * 15, "x2": x2_raw[i] * 15})
         for i in range(n)
     ]
@@ -1899,16 +1980,16 @@
     if trials:
         df_dicts = [
             {
                 "trial_index": trial.index,
                 "metric_name": "branin",
                 "arm_name": not_none(checked_cast(Trial, trial).arm).name,
                 "mean": branin(
-                    float(not_none(trial.arm).parameters["x1"]),  # pyre-ignore[6]
-                    float(not_none(trial.arm).parameters["x2"]),  # pyre-ignore[6]
+                    float(not_none(not_none(trial.arm).parameters["x1"])),
+                    float(not_none(not_none(trial.arm).parameters["x2"])),
                 ),
                 "sem": 0.0,
             }
             for trial in trials
         ]
     else:
         df_dicts = [
@@ -1928,17 +2009,18 @@
     return Data(
         pd.DataFrame(
             {
                 "trial_index": batch.index,
                 "arm_name": [arm.name for arm in batch.arms],
                 "metric_name": "branin",
                 "mean": [
-                    # pyre-ignore[6]: This function can fail if a parameter value
-                    # does not support conversion to float.
-                    branin(float(arm.parameters["x1"]), float(arm.parameters["x2"]))
+                    branin(
+                        float(not_none(arm.parameters["x1"])),
+                        float(not_none(arm.parameters["x2"])),
+                    )
                     for arm in batch.arms
                 ],
                 "sem": 0.1,
             }
         )
     )
 
@@ -1970,34 +2052,29 @@
         min_progression=0.2,
         min_curves=10,
         trial_indices_to_ignore=[0, 1, 2],
         normalize_progressions=True,
     )
 
 
-def get_percentile_early_stopping_strategy_with_true_objective_metric_name() -> PercentileEarlyStoppingStrategy:  # noqa
-    strategy = get_percentile_early_stopping_strategy()
-    strategy.true_objective_metric_name = "true_objective"
-    return strategy
-
-
-def get_percentile_early_stopping_strategy_with_non_objective_metric_name() -> PercentileEarlyStoppingStrategy:  # noqa
+def get_percentile_early_stopping_strategy_with_non_objective_metric_name() -> (
+    PercentileEarlyStoppingStrategy
+):
     return PercentileEarlyStoppingStrategy(
         metric_names=["foo"],
         percentile_threshold=0.25,
         min_progression=0.2,
         min_curves=10,
         trial_indices_to_ignore=[0, 1, 2],
         normalize_progressions=True,
     )
 
 
 def get_threshold_early_stopping_strategy() -> ThresholdEarlyStoppingStrategy:
     return ThresholdEarlyStoppingStrategy(
-        true_objective_metric_name="true_objective",
         metric_threshold=0.1,
         min_progression=0.2,
         trial_indices_to_ignore=[0, 1, 2],
         normalize_progressions=True,
     )
 
 
@@ -2238,24 +2315,26 @@
         has_observation_noise: If True, includes Yvar in the dataset.
         feature_names: A list of feature names. Defaults to x0, x1...
         outcome_names: A list of outcome names. Defaults to y0, y1...
         tkwargs: Optional dictionary of tensor kwargs, such as dtype and device.
         seed: An optional seed used to generate the data.
     """
     if seed is not None:
-        torch.manual_seed(seed)
+        set_rng_seed(seed)
     feature_names = feature_names or [f"x{i}" for i in range(d)]
     outcome_names = outcome_names or [f"y{i}" for i in range(m)]
     tkwargs = tkwargs or {}
     return SupervisedDataset(
         X=torch.rand(num_samples, d, **tkwargs),
         Y=torch.rand(num_samples, m, **tkwargs),
-        Yvar=torch.rand(num_samples, m, **tkwargs) * 0.01
-        if has_observation_noise
-        else None,
+        Yvar=(
+            torch.rand(num_samples, m, **tkwargs) * 0.01
+            if has_observation_noise
+            else None
+        ),
         feature_names=feature_names,
         outcome_names=outcome_names,
     )
 
 
 ##############################
 # Custom runner and metric
@@ -2294,7 +2373,12 @@
         self,
         experiment: Experiment,
         num_generator_runs: int,
         data: Optional[Data] = None,
         n: int = 1,
     ) -> List[List[GeneratorRun]]:
         return []
+
+    def clone_reset(self) -> SpecialGenerationStrategy:
+        clone = SpecialGenerationStrategy()
+        clone._name = self._name
+        return clone
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/metrics/backend_simulator_map.py` & `ax-platform-0.4.0/ax/utils/testing/metrics/backend_simulator_map.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any, List
 
 from ax.core.base_trial import BaseTrial
 from ax.core.map_metric import MapMetricFetchResult
 from ax.metrics.noisy_function_map import NoisyFunctionMapMetric
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/metrics/branin_backend_map.py` & `ax-platform-0.4.0/ax/utils/testing/metrics/branin_backend_map.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import List, Optional
 
 import numpy as np
 from ax.metrics.branin_map import BraninTimestampMapMetric
 from ax.utils.testing.metrics.backend_simulator_map import (
     BackendSimulatorTimestampMapMetric,
 )
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/mock.py` & `ax-platform-0.4.0/ax/utils/testing/mock.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from contextlib import contextmanager, ExitStack
 from functools import wraps
 from typing import Any, Callable, Dict, Generator, Optional
 from unittest import mock
 
 from ax.models.torch.fully_bayesian import run_inference
 from botorch.fit import fit_fully_bayesian_model_nuts
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/modeling_stubs.py` & `ax-platform-0.4.0/ax/utils/testing/modeling_stubs.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from logging import Logger
 from typing import Any, Dict, List, Optional, Type
 
 import numpy as np
 from ax.core.base_trial import TrialStatus
 from ax.core.experiment import Experiment
 from ax.core.observation import Observation, ObservationData, ObservationFeatures
@@ -250,21 +252,19 @@
         model_kwargs=step_model_kwargs,
         model_gen_kwargs={},
     )
     sobol_node = GenerationNode(
         node_name="sobol_node",
         transition_criteria=sobol_criterion,
         model_specs=[sobol_model_spec],
-        gen_unlimited_trials=False,
     )
     gpei_node = GenerationNode(
         node_name="GPEI_node",
         transition_criteria=gpei_criterion,
         model_specs=[gpei_model_spec],
-        gen_unlimited_trials=False,
     )
 
     sobol_GPEI_GS_nodes = GenerationStrategy(
         name="Sobol+GPEI_Nodes",
         nodes=[sobol_node, gpei_node],
         steps=None,
     )
@@ -502,15 +502,16 @@
 
 
 class transform_1(Transform):
     def transform_search_space(self, search_space: SearchSpace) -> SearchSpace:
         new_ss = search_space.clone()
         for param in new_ss.parameters.values():
             if isinstance(param, FixedParameter):
-                param._value += 1.0
+                if param._value is not None and not isinstance(param._value, str):
+                    param._value += 1.0
             elif isinstance(param, RangeParameter):
                 param._lower += 1.0
                 param._upper += 1.0
         return new_ss
 
     def transform_optimization_config(
         self,
@@ -560,15 +561,16 @@
 
 
 class transform_2(Transform):
     def transform_search_space(self, search_space: SearchSpace) -> SearchSpace:
         new_ss = search_space.clone()
         for param in new_ss.parameters.values():
             if isinstance(param, FixedParameter):
-                param._value *= 2.0
+                if param._value is not None and not isinstance(param._value, str):
+                    param._value *= 2.0
             elif isinstance(param, RangeParameter):
                 param._lower *= 2.0
                 param._upper *= 2.0
         return new_ss
 
     def transform_optimization_config(
         self,
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/test_init_files.py` & `ax-platform-0.4.0/ax/utils/testing/test_init_files.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import os
 from glob import glob
 
 from ax.utils.common.testutils import TestCase
 
 
 class InitTest(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/tests/test_backend_simulator.py` & `ax-platform-0.4.0/ax/utils/testing/tests/test_backend_simulator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from unittest.mock import Mock, patch
 
 from ax.core.base_trial import TrialStatus
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.backend_simulator import BackendSimulator, BackendSimulatorOptions
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/tests/test_utils.py` & `ax-platform-0.4.0/ax/utils/testing/tests/test_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 import numpy as np
 import torch
 from ax.utils.common.testutils import TestCase
 from ax.utils.testing.utils import generic_equals
 
 
 class TestUtils(TestCase):
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/torch_stubs.py` & `ax-platform-0.4.0/ax/utils/testing/torch_stubs.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple
 
 import torch
```

### Comparing `ax-platform-0.3.7/ax/utils/testing/utils.py` & `ax-platform-0.4.0/ax/utils/testing/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from typing import Any
 
 import numpy as np
 import torch
 from torch import Tensor
```

### Comparing `ax-platform-0.3.7/ax/utils/tutorials/cnn_utils.py` & `ax-platform-0.4.0/ax/utils/tutorials/cnn_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+# pyre-strict
+
 from itertools import accumulate
 from typing import Dict, List, Optional, Tuple
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
```

### Comparing `ax-platform-0.3.7/ax_platform.egg-info/PKG-INFO` & `ax-platform-0.4.0/ax_platform.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 Metadata-Version: 2.1
 Name: ax-platform
-Version: 0.3.7
+Version: 0.4.0
 Summary: Adaptive Experimentation
 Home-page: https://github.com/facebook/Ax
 Author: Facebook, Inc.
 License: MIT
 Keywords: Experimentation,Optimization
-Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Programming Language :: Python :: 3
-Requires-Python: >=3.9
+Requires-Python: >=3.10
 Description-Content-Type: text/markdown
 Provides-Extra: dev
 Provides-Extra: mysql
 Provides-Extra: notebook
 Provides-Extra: unittest
 Provides-Extra: unittest_minimal
 Provides-Extra: tutorial
@@ -82,15 +81,15 @@
 
 # best_parameters contains {'x1': 1.02, 'x2': 2.97}; the global min is (1, 3)
 ```
 
 ## Installation
 
 ### Requirements
-You need Python 3.9 or later to run Ax.
+You need Python 3.10 or later to run Ax.
 
 The required Python dependencies are:
 
 * [botorch](https://www.botorch.org)
 * jinja2
 * pandas
 * scipy
@@ -200,9 +199,7 @@
 [`--depth`](https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt)
 argument to `git clone`. If you require the entire commit history you may remove this
 argument.
 
 ## License
 
 Ax is licensed under the [MIT license](./LICENSE).
-
-
```

### Comparing `ax-platform-0.3.7/ax_platform.egg-info/SOURCES.txt` & `ax-platform-0.4.0/ax_platform.egg-info/SOURCES.txt`

 * *Files 11% similar despite different names*

```diff
@@ -15,45 +15,75 @@
 .github/workflows/reusable_test.yml
 .github/workflows/reusable_tutorials.yml
 .github/workflows/tutorials.yml
 ax/__init__.py
 ax/version.py
 ax/analysis/base_analysis.py
 ax/analysis/base_plotly_visualization.py
+ax/analysis/cross_validation_plot.py
 ax/analysis/parallel_coordinates_plot.py
+ax/analysis/predicted_outcomes_dot_plot.py
+ax/analysis/helpers/color_helpers.py
+ax/analysis/helpers/constants.py
+ax/analysis/helpers/cross_validation_helpers.py
+ax/analysis/helpers/layout_helpers.py
+ax/analysis/helpers/plot_data_df_helpers.py
+ax/analysis/helpers/plot_helpers.py
+ax/analysis/helpers/scatter_helpers.py
+ax/analysis/helpers/tests/test_cross_validation_helpers.py
+ax/analysis/helpers/tests/test_cv_consistency_checks.py
+ax/analysis/tests/test_cross_validation_plot.py
 ax/analysis/tests/test_parallel_coordinates_plot.py
+ax/analysis/tests/test_predicted_outcomes_dot_plot.py
 ax/benchmark/__init__.py
 ax/benchmark/benchmark.py
 ax/benchmark/benchmark_method.py
 ax/benchmark/benchmark_problem.py
 ax/benchmark/benchmark_result.py
 ax/benchmark/methods/__init__.py
 ax/benchmark/methods/modular_botorch.py
+ax/benchmark/methods/sobol.py
+ax/benchmark/metrics/__init__.py
+ax/benchmark/metrics/base.py
+ax/benchmark/metrics/benchmark.py
+ax/benchmark/metrics/jenatton.py
+ax/benchmark/metrics/utils.py
 ax/benchmark/problems/__init__.py
 ax/benchmark/problems/hd_embedding.py
 ax/benchmark/problems/registry.py
 ax/benchmark/problems/surrogate.py
 ax/benchmark/problems/hpo/__init__.py
 ax/benchmark/problems/hpo/pytorch_cnn.py
 ax/benchmark/problems/hpo/torchvision.py
 ax/benchmark/problems/synthetic/__init__.py
 ax/benchmark/problems/synthetic/discretized/__init__.py
 ax/benchmark/problems/synthetic/discretized/mixed_integer.py
 ax/benchmark/problems/synthetic/hss/__init__.py
 ax/benchmark/problems/synthetic/hss/jenatton.py
+ax/benchmark/runners/__init__.py
+ax/benchmark/runners/base.py
+ax/benchmark/runners/botorch_test.py
+ax/benchmark/runners/surrogate.py
 ax/benchmark/tests/__init__.py
 ax/benchmark/tests/test_benchmark.py
 ax/benchmark/tests/test_benchmark_method.py
 ax/benchmark/tests/test_benchmark_problem.py
-ax/benchmark/tests/test_methods.py
-ax/benchmark/tests/test_mixed_integer_problems.py
-ax/benchmark/tests/test_problem_storage.py
-ax/benchmark/tests/test_problems.py
-ax/benchmark/tests/test_surrogate_problems.py
-ax/benchmark/tests/test_surrogate_runner.py
+ax/benchmark/tests/methods/__init__.py
+ax/benchmark/tests/methods/test_methods.py
+ax/benchmark/tests/metrics/__init__.py
+ax/benchmark/tests/metrics/test_benchmark_metric.py
+ax/benchmark/tests/metrics/test_jennaton.py
+ax/benchmark/tests/problems/__init__.py
+ax/benchmark/tests/problems/test_mixed_integer_problems.py
+ax/benchmark/tests/problems/test_problem_storage.py
+ax/benchmark/tests/problems/test_problems.py
+ax/benchmark/tests/problems/test_surrogate_problems.py
+ax/benchmark/tests/runners/__init__.py
+ax/benchmark/tests/runners/test_botorch_test_problem.py
+ax/benchmark/tests/runners/test_surrogate_runner.py
 ax/core/__init__.py
 ax/core/arm.py
 ax/core/base_trial.py
 ax/core/batch_trial.py
 ax/core/data.py
 ax/core/experiment.py
 ax/core/formatting_utils.py
@@ -78,14 +108,15 @@
 ax/core/utils.py
 ax/core/tests/__init__.py
 ax/core/tests/test_arm.py
 ax/core/tests/test_batch_trial.py
 ax/core/tests/test_data.py
 ax/core/tests/test_experiment.py
 ax/core/tests/test_formatting_utils.py
+ax/core/tests/test_generation_strategy_interface.py
 ax/core/tests/test_generator_run.py
 ax/core/tests/test_map_data.py
 ax/core/tests/test_map_metric.py
 ax/core/tests/test_metric.py
 ax/core/tests/test_multi_type_experiment.py
 ax/core/tests/test_objective.py
 ax/core/tests/test_observation.py
@@ -117,28 +148,27 @@
 ax/exceptions/model.py
 ax/exceptions/storage.py
 ax/global_stopping/__init__.py
 ax/global_stopping/strategies/__init__.py
 ax/global_stopping/strategies/base.py
 ax/global_stopping/strategies/improvement.py
 ax/global_stopping/tests/__init__.py
-ax/global_stopping/tests/tests_strategies.py
+ax/global_stopping/tests/test_strategies.py
 ax/health_check/search_space.py
 ax/health_check/tests/test_search_space.py
 ax/metrics/__init__.py
 ax/metrics/botorch_test_problem.py
 ax/metrics/branin.py
 ax/metrics/branin_map.py
 ax/metrics/chemistry.py
 ax/metrics/chemistry_data.zip
 ax/metrics/curve.py
 ax/metrics/dict_lookup.py
 ax/metrics/factorial.py
 ax/metrics/hartmann6.py
-ax/metrics/jenatton.py
 ax/metrics/l2norm.py
 ax/metrics/noisy_function.py
 ax/metrics/noisy_function_map.py
 ax/metrics/sklearn.py
 ax/metrics/tensorboard.py
 ax/metrics/torchx.py
 ax/metrics/tests/__init__.py
@@ -149,14 +179,15 @@
 ax/metrics/tests/test_sklearn.py
 ax/metrics/tests/test_tensorboard.py
 ax/modelbridge/__init__.py
 ax/modelbridge/base.py
 ax/modelbridge/cross_validation.py
 ax/modelbridge/discrete.py
 ax/modelbridge/dispatch_utils.py
+ax/modelbridge/external_generation_node.py
 ax/modelbridge/factory.py
 ax/modelbridge/generation_node.py
 ax/modelbridge/generation_strategy.py
 ax/modelbridge/map_torch.py
 ax/modelbridge/model_spec.py
 ax/modelbridge/modelbridge_utils.py
 ax/modelbridge/pairwise.py
@@ -171,17 +202,19 @@
 ax/modelbridge/tests/__init__.py
 ax/modelbridge/tests/test_aepsych_criterion.py
 ax/modelbridge/tests/test_alebo_strategy.py
 ax/modelbridge/tests/test_base_modelbridge.py
 ax/modelbridge/tests/test_cross_validation.py
 ax/modelbridge/tests/test_discrete_modelbridge.py
 ax/modelbridge/tests/test_dispatch_utils.py
+ax/modelbridge/tests/test_external_generation_node.py
 ax/modelbridge/tests/test_factory.py
 ax/modelbridge/tests/test_generation_node.py
 ax/modelbridge/tests/test_generation_strategy.py
+ax/modelbridge/tests/test_hierarchical_search_space.py
 ax/modelbridge/tests/test_map_torch_modelbridge.py
 ax/modelbridge/tests/test_model_fit_metrics.py
 ax/modelbridge/tests/test_model_spec.py
 ax/modelbridge/tests/test_modelbridge_utils.py
 ax/modelbridge/tests/test_pairwise_modelbridge.py
 ax/modelbridge/tests/test_prediction_utils.py
 ax/modelbridge/tests/test_random_modelbridge.py
@@ -196,14 +229,15 @@
 ax/modelbridge/transforms/__init__.py
 ax/modelbridge/transforms/base.py
 ax/modelbridge/transforms/cap_parameter.py
 ax/modelbridge/transforms/cast.py
 ax/modelbridge/transforms/centered_unit_x.py
 ax/modelbridge/transforms/choice_encode.py
 ax/modelbridge/transforms/convert_metric_names.py
+ax/modelbridge/transforms/deprecated_transform_mixin.py
 ax/modelbridge/transforms/derelativize.py
 ax/modelbridge/transforms/int_range_to_choice.py
 ax/modelbridge/transforms/int_to_float.py
 ax/modelbridge/transforms/inverse_gaussian_cdf_y.py
 ax/modelbridge/transforms/ivw.py
 ax/modelbridge/transforms/log.py
 ax/modelbridge/transforms/log_y.py
@@ -228,14 +262,15 @@
 ax/modelbridge/transforms/winsorize.py
 ax/modelbridge/transforms/tests/test_base_transform.py
 ax/modelbridge/transforms/tests/test_cap_parameter_transform.py
 ax/modelbridge/transforms/tests/test_cast_transform.py
 ax/modelbridge/transforms/tests/test_centered_unit_x_transform.py
 ax/modelbridge/transforms/tests/test_choice_encode_transform.py
 ax/modelbridge/transforms/tests/test_convert_metric_names_transform.py
+ax/modelbridge/transforms/tests/test_deprecated_transform_mixin.py
 ax/modelbridge/transforms/tests/test_derelativize_transform.py
 ax/modelbridge/transforms/tests/test_int_range_to_choice_transform.py
 ax/modelbridge/transforms/tests/test_int_to_float_transform.py
 ax/modelbridge/transforms/tests/test_inverse_gaussian_cdf_y_transform.py
 ax/modelbridge/transforms/tests/test_ivw_transform.py
 ax/modelbridge/transforms/tests/test_log_transform.py
 ax/modelbridge/transforms/tests/test_log_y_transform.py
@@ -384,21 +419,19 @@
 ax/plot/tests/test_parallel_coordinates.py
 ax/plot/tests/test_pareto_utils.py
 ax/plot/tests/test_slices.py
 ax/plot/tests/test_tile_fitted.py
 ax/plot/tests/test_traces.py
 ax/plot/tests/long_running/test_pareto_utils.py
 ax/runners/__init__.py
-ax/runners/botorch_test_problem.py
 ax/runners/simulated_backend.py
 ax/runners/single_running_trial_mixin.py
 ax/runners/synthetic.py
 ax/runners/torchx.py
 ax/runners/tests/__init__.py
-ax/runners/tests/test_botorch_test_problem.py
 ax/runners/tests/test_single_running_trial_mixin.py
 ax/runners/tests/test_torchx.py
 ax/service/__init__.py
 ax/service/ax_client.py
 ax/service/interactive_loop.py
 ax/service/managed_loop.py
 ax/service/scheduler.py
@@ -480,26 +513,29 @@
 ax/utils/common/decorator.py
 ax/utils/common/docutils.py
 ax/utils/common/equality.py
 ax/utils/common/executils.py
 ax/utils/common/kwargs.py
 ax/utils/common/logger.py
 ax/utils/common/mock.py
+ax/utils/common/random.py
 ax/utils/common/result.py
 ax/utils/common/serialization.py
 ax/utils/common/testutils.py
 ax/utils/common/timeutils.py
 ax/utils/common/typeutils.py
+ax/utils/common/typeutils_nonnative.py
 ax/utils/common/typeutils_torch.py
 ax/utils/common/tests/__init__.py
 ax/utils/common/tests/test_docutils.py
 ax/utils/common/tests/test_equality.py
 ax/utils/common/tests/test_executils.py
 ax/utils/common/tests/test_kwargutils.py
 ax/utils/common/tests/test_logger.py
+ax/utils/common/tests/test_random.py
 ax/utils/common/tests/test_result.py
 ax/utils/common/tests/test_serialization.py
 ax/utils/common/tests/test_testutils.py
 ax/utils/common/tests/test_typeutils.py
 ax/utils/flake8_plugins/__init__.py
 ax/utils/flake8_plugins/docstring_checker.py
 ax/utils/measurement/__init__.py
@@ -573,27 +609,24 @@
 docs/assets/fitted.js
 docs/assets/gp_opt.png
 docs/assets/gp_posterior.png
 docs/assets/mab_animate.gif
 docs/assets/mab_probs.png
 docs/assets/mab_regret.png
 docs/assets/slice.js
-scripts/build_ax.sh
-scripts/docker_install.sh
 scripts/import_ax.py
 scripts/insert_api_refs.py
 scripts/make_docs.sh
 scripts/make_tutorials.py
 scripts/parse_sphinx.py
 scripts/patch_site_config.py
 scripts/publish_site.sh
 scripts/update_versions_html.py
 scripts/validate_sphinx.py
 scripts/versions.js
-scripts/wheels_build.ps1
 sphinx/Makefile
 sphinx/make.bat
 sphinx/source/ax.rst
 sphinx/source/benchmark.rst
 sphinx/source/conf.py
 sphinx/source/core.rst
 sphinx/source/early_stopping.rst
@@ -605,14 +638,15 @@
 sphinx/source/models.rst
 sphinx/source/plot.rst
 sphinx/source/runners.rst
 sphinx/source/service.rst
 sphinx/source/storage.rst
 sphinx/source/telemetry.rst
 sphinx/source/utils.rst
+tutorials/external_generation_node.ipynb
 tutorials/factorial.ipynb
 tutorials/generation_strategy.ipynb
 tutorials/gpei_hartmann_developer.ipynb
 tutorials/gpei_hartmann_loop.ipynb
 tutorials/gpei_hartmann_service.ipynb
 tutorials/gss.ipynb
 tutorials/modular_botax.ipynb
```

### Comparing `ax-platform-0.3.7/ax_platform.egg-info/requires.txt` & `ax-platform-0.4.0/ax_platform.egg-info/requires.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-botorch==0.10.0
+botorch==0.11.0
 jinja2
 pandas
 scipy
 scikit-learn
 ipywidgets
 plotly>=5.12.0
 typeguard
 pyre-extensions
 
 [dev]
 beautifulsoup4
-black==22.3.0
+black==24.2.0
 flake8
 hypothesis
 Jinja2
 pyfakefs==5.1.0
 pytest>=4.6
 pytest-cov
 sphinx==5.3.0
@@ -28,15 +28,15 @@
 SQLAlchemy==1.4.17
 
 [notebook]
 jupyter
 
 [tutorial]
 beautifulsoup4
-black==22.3.0
+black==24.2.0
 flake8
 hypothesis
 Jinja2
 pyfakefs==5.1.0
 pytest>=4.6
 pytest-cov
 sphinx==5.3.0
@@ -58,15 +58,15 @@
 pyro-ppl
 pytorch-lightning
 papermill
 submitit
 
 [unittest]
 beautifulsoup4
-black==22.3.0
+black==24.2.0
 flake8
 hypothesis
 Jinja2
 pyfakefs==5.1.0
 pytest>=4.6
 pytest-cov
 sphinx==5.3.0
```

### Comparing `ax-platform-0.3.7/docs/api.md` & `ax-platform-0.4.0/docs/api.md`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,22 @@
 ---
 id: api
 title: APIs
 ---
 
 The modular design of Ax enables three different usage modes, with different
 balances of structure to flexibility and reproducibility. Navigate to the
-["Tutorials" page](/tutorials) for the in-depth walk-throughs of each API and
-usage mode. From most lightweight to fullest functionality, they are:
+["Tutorials" page](/tutorials) for an in-depth walk-through of each API and
+usage mode.
+
+**NOTE: We recommend the Service API for the vast majority of use cases.** This
+API provides an ideal balance of flexibility and simplicity for most users, and
+we are in the process of consolidating Ax usage around it more formally.
+
+From most lightweight to fullest functionality, our APIs are:
 
 - **Loop API** ([tutorial](/tutorials/gpei_hartmann_loop.html)) is intended for
   synchronous optimization loops, where [trials](glossary.md#trial) can be
   evaluated right away. With this API, optimization can be executed in a single
   call and [experiment](glossary.md#experiment) introspection is available once
   optimization is complete. **Use this API only for the simplest use cases where
   running a single trial is fast and only one trial should be running at a
```

### Comparing `ax-platform-0.3.7/docs/assets/bandit_allocation.png` & `ax-platform-0.4.0/docs/assets/bandit_allocation.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/bo_1d_opt.gif` & `ax-platform-0.4.0/docs/assets/bo_1d_opt.gif`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/contour.js` & `ax-platform-0.4.0/docs/assets/contour.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/cv.js` & `ax-platform-0.4.0/docs/assets/cv.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/example_shrinkage.png` & `ax-platform-0.4.0/docs/assets/example_shrinkage.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/fitted.js` & `ax-platform-0.4.0/docs/assets/fitted.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/gp_opt.png` & `ax-platform-0.4.0/docs/assets/gp_opt.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/gp_posterior.png` & `ax-platform-0.4.0/docs/assets/gp_posterior.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/mab_animate.gif` & `ax-platform-0.4.0/docs/assets/mab_animate.gif`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/mab_probs.png` & `ax-platform-0.4.0/docs/assets/mab_probs.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/mab_regret.png` & `ax-platform-0.4.0/docs/assets/mab_regret.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/assets/slice.js` & `ax-platform-0.4.0/docs/assets/slice.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/banditopt.md` & `ax-platform-0.4.0/docs/banditopt.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/bayesopt.md` & `ax-platform-0.4.0/docs/bayesopt.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/core.md` & `ax-platform-0.4.0/docs/core.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/data.md` & `ax-platform-0.4.0/docs/data.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/glossary.md` & `ax-platform-0.4.0/docs/glossary.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/installation.md` & `ax-platform-0.4.0/docs/installation.md`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 ---
 id: installation
 title: Installation
 ---
 
 ## Requirements
-You need Python 3.9 or later to run Ax.
+You need Python 3.10 or later to run Ax.
 
 The required Python dependencies are:
 
 * [botorch][def]
 * jinja2
 * pandas
 * scipy
```

### Comparing `ax-platform-0.3.7/docs/models.md` & `ax-platform-0.4.0/docs/models.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/storage.md` & `ax-platform-0.4.0/docs/storage.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/trial-evaluation.md` & `ax-platform-0.4.0/docs/trial-evaluation.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/docs/why-ax.md` & `ax-platform-0.4.0/docs/why-ax.md`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/insert_api_refs.py` & `ax-platform-0.4.0/scripts/insert_api_refs.py`

 * *Files 9% similar despite different names*

```diff
@@ -19,30 +19,30 @@
     for sp in glob.glob(source_glob):
         module_name = sp[:-3]
         module_name = module_name.replace("/", ".")
         #  Parse the source file into an AST
         node = ast.parse(open(sp).read())
         #  Extract the names of all functions and classes defined in this file
         defined.extend(
-            (n.name, module_name + "." + n.name)
+            (n.name, f"{module_name}.{n.name}")
             for n in node.body
             if (isinstance(n, ast.FunctionDef) or isinstance(n, ast.ClassDef))
         )
     return defined
 
 
 def replace_backticks(source_path, docs_path):
-    markdown_glob = docs_path + "/*.md"
-    source_glob = source_path + "/**/*.py"
+    markdown_glob = f"{docs_path}/*.md"
+    source_glob = f"{source_path}/**/*.py"
     methods = list_functions(source_glob)
     for f in glob.glob(markdown_glob):
         for n, m in methods:
             #  Match backquoted mentions of the function/class name which are
             #  not already links
-            pattern = "(?<![[`])(`" + n + "`)"
+            pattern = f"(?<![[`])(`{n}`)"
             link = f"[`{n}`](/api/{m.split('.')[1]}.html#{m})"
             lines = open(f).readlines()
             for i, l in enumerate(lines):
                 match = re.search(pattern, l)
                 if match:
                     print(f"{f}:{i+1} s/{match.group(0)}/{link}")
                     lines[i] = re.sub(pattern, link, l)
```

### Comparing `ax-platform-0.3.7/scripts/make_docs.sh` & `ax-platform-0.4.0/scripts/make_docs.sh`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/make_tutorials.py` & `ax-platform-0.4.0/scripts/make_tutorials.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/parse_sphinx.py` & `ax-platform-0.4.0/scripts/parse_sphinx.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/patch_site_config.py` & `ax-platform-0.4.0/scripts/patch_site_config.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/publish_site.sh` & `ax-platform-0.4.0/scripts/publish_site.sh`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/update_versions_html.py` & `ax-platform-0.4.0/scripts/update_versions_html.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/validate_sphinx.py` & `ax-platform-0.4.0/scripts/validate_sphinx.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/scripts/versions.js` & `ax-platform-0.4.0/scripts/versions.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/setup.py` & `ax-platform-0.4.0/setup.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,16 +4,15 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 import os
 
 from setuptools import find_packages, setup
 
-# TODO: read pinned Botorch version from a shared source
-PINNED_BOTORCH_VERSION = "0.10.0"
+PINNED_BOTORCH_VERSION = "0.11.0"
 
 if os.environ.get("ALLOW_BOTORCH_LATEST"):
     # allows a more recent previously installed version of botorch to remain
     # if there is no previously installed version, installs the latest release
     botorch_req = f"botorch>={PINNED_BOTORCH_VERSION}"
 else:
     botorch_req = f"botorch=={PINNED_BOTORCH_VERSION}"
@@ -31,15 +30,15 @@
     "typeguard",
     "pyre-extensions",
 ]
 
 # pytest-cov requires pytest >= 3.6
 DEV_REQUIRES = [
     "beautifulsoup4",
-    "black==22.3.0",
+    "black==24.2.0",
     "flake8",
     "hypothesis",
     "Jinja2",
     "pyfakefs==5.1.0",
     "pytest>=4.6",
     "pytest-cov",
     "sphinx==5.3.0",
@@ -101,15 +100,15 @@
             "Development Status :: 4 - Beta",
             "Operating System :: POSIX :: Linux",
             "Operating System :: MacOS :: MacOS X",
             "Programming Language :: Python :: 3",
         ],
         long_description=long_description,
         long_description_content_type="text/markdown",
-        python_requires=">=3.9",
+        python_requires=">=3.10",
         install_requires=REQUIRES,
         packages=find_packages(),
         package_data={
             # include all js, css, and html files in the package
             "": ["*.js", "*.css", "*.html"]
         },
         extras_require={
```

### Comparing `ax-platform-0.3.7/sphinx/Makefile` & `ax-platform-0.4.0/sphinx/Makefile`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/sphinx/make.bat` & `ax-platform-0.4.0/sphinx/make.bat`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/sphinx/source/benchmark.rst` & `ax-platform-0.4.0/sphinx/source/benchmark.rst`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.benchmark
-===================================
+============
 
 .. automodule:: ax.benchmark
 .. currentmodule:: ax.benchmark
 
 Benchmark
-----------------
+---------
+
 
 Benchmark Method
-~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.benchmark.benchmark_method
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Problem
@@ -24,65 +25,73 @@
 .. automodule:: ax.benchmark.benchmark_problem
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Benchmark Result
-~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.benchmark.benchmark_result
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark
 ~~~~~~~~~
 
 .. automodule:: ax.benchmark.benchmark
     :members:
     :undoc-members:
     :show-inheritance:
 
-Scored Benchmark
-~~~~~~~~~~~~~~~~
+Benchmark Methods Modular BoTorch
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. automodule:: ax.benchmark.methods.modular_botorch
+    :members:
+    :undoc-members:
+    :show-inheritance:
 
-.. automodule:: ax.benchmark.scored_benchmark
+Benchmark Methods Sobol
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. automodule:: ax.benchmark.methods.sobol
     :members:
     :undoc-members:
     :show-inheritance:
 
-Benchmark Methods GPEI and MOO
+Benchmark Metrics Base
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. automodule:: ax.benchmark.methods.gpei_and_moo
+.. automodule:: ax.benchmark.metrics.base
     :members:
     :undoc-members:
     :show-inheritance:
 
-Benchmark Methods Modular BoTorch
+Benchmark Metrics Benchmark
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. automodule:: ax.benchmark.methods.modular_botorch
+.. automodule:: ax.benchmark.metrics.benchmark
     :members:
     :undoc-members:
     :show-inheritance:
 
-Benchmark Methods SAASBO
+Benchmark Metrics Jenatton
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. automodule:: ax.benchmark.methods.saasbo
+.. automodule:: ax.benchmark.metrics.jenatton
     :members:
     :undoc-members:
     :show-inheritance:
 
-Benchmark Methods Choose Generation Strategy
+Benchmark Metrics Utils
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. automodule:: ax.benchmark.methods.choose_generation_strategy
+.. automodule:: ax.benchmark.metrics.utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Problems Registry
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
@@ -104,15 +113,15 @@
 
 .. automodule:: ax.benchmark.problems.surrogate
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Problems Mixed Integer Synthetic
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.benchmark.problems.synthetic.discretized.mixed_integer
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Problems Jenatton
@@ -120,21 +129,45 @@
 
 .. automodule:: ax.benchmark.problems.synthetic.hss.jenatton
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Problems PyTorchCNN
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.benchmark.problems.hpo.pytorch_cnn
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Problems PyTorchCNN TorchVision
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.benchmark.problems.hpo.torchvision
     :members:
     :undoc-members:
     :show-inheritance:
+
+Benchmark Runners Base
+~~~~~~~~~~~~~~~~~~~~~~
+
+.. automodule:: ax.benchmark.runners.base
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
+Benchmark Runners BoTorch Test
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. automodule:: ax.benchmark.runners.botorch_test
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
+Benchmark Runners Surrogate
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. automodule:: ax.benchmark.runners.surrogate
+    :members:
+    :undoc-members:
+    :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/conf.py` & `ax-platform-0.4.0/sphinx/source/conf.py`

 * *Files 2% similar despite different names*

```diff
@@ -74,15 +74,15 @@
 release = "0.0.1"
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 #
 # This is also used if you do content translation via gettext catalogs.
 # Usually you set "language" from the command line for these cases.
-language = None
+language = 'en'
 
 # There are two options for replacing |today|: either, you set today to some
 # non-false value, then it is used:
 # today = ''
 # Else, today_fmt is used as the format for a strftime call.
 # today_fmt = '%B %d, %Y'
 
@@ -238,23 +238,25 @@
         "Ax",
         "Platform for automated optimization and experimentation.",
         "Miscellaneous",
     )
 ]
 
 # Example configuration for intersphinx: refer to the Python standard library.
-intersphinx_mapping = {"https://docs.python.org/": None}
+intersphinx_mapping = {'<name>': ('https://docs.python.org/', None)}
 
 # -- Autodocs Configuration -------------------------------------------
 
 # Turns on conditional imports via TYPE_CHECKING flag;
 # Used for sphinx_autodoc_typehints
 set_type_checking_flag = True
 
 # Mock SQLAlchemy base; otherwise have issues with trying to register same
 # class (with same table) multiple times through autodocs
 # Also mock Pandas to avoid circular imports due to TYPE_CHECKING = True
 autodoc_mock_imports = [
     "sqlalchemy",
     "pandas",
+    "plotly",
+    "tensorboard",
     "__test_modules__",
 ]
```

### Comparing `ax-platform-0.3.7/sphinx/source/core.rst` & `ax-platform-0.4.0/sphinx/source/core.rst`

 * *Files 5% similar despite different names*

```diff
@@ -1,31 +1,22 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.core
-===================================
+=======
 
 .. automodule:: ax.core
 .. currentmodule:: ax.core
 
 
 Core Classes
--------------
-
-`AbstractData`
-~~~~~~~~~~~~~~~~~~~~~~~
-
-.. automodule:: ax.core.abstract_data
-    :members:
-    :undoc-members:
-    :show-inheritance:
-    :noindex:
+------------
 
 `Arm`
-~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~
 
 .. automodule:: ax.core.arm
     :members:
     :undoc-members:
     :show-inheritance:
 
 `BaseTrial`
@@ -33,40 +24,40 @@
 
 .. automodule:: ax.core.base_trial
     :members:
     :undoc-members:
     :show-inheritance:
 
 `BatchTrial`
-~~~~~~~~~~~~~
+~~~~~~~~~~~~
 
 .. automodule:: ax.core.batch_trial
     :members:
     :undoc-members:
     :show-inheritance:
 
 `Data`
-~~~~~~~~~~
+~~~~~~
 
 .. automodule:: ax.core.data
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 `Experiment`
-~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~
 
 .. automodule:: ax.core.experiment
     :members:
     :undoc-members:
     :show-inheritance:
 
 `GenerationStrategyInterface`
-~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.core.generation_strategy_interface
     :members:
     :undoc-members:
     :show-inheritance:
 
 `GeneratorRun`
@@ -74,33 +65,33 @@
 
 .. automodule:: ax.core.generator_run
     :members:
     :undoc-members:
     :show-inheritance:
 
 `MapData`
-~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.core.map_data
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 `MapMetric`
-~~~~~~~~~~~~~~
+~~~~~~~~~~~
 
 .. automodule:: ax.core.map_metric
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 `Metric`
-~~~~~~~~~~~~~~
+~~~~~~~~
 
 .. automodule:: ax.core.metric
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
@@ -109,24 +100,24 @@
 
 .. automodule:: ax.core.multi_type_experiment
     :members:
     :undoc-members:
     :show-inheritance:
 
 `Objective`
-~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~
 
 .. automodule:: ax.core.objective
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 `Observation`
-~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.core.observation
     :members:
     :undoc-members:
     :show-inheritance:
 
 `OptimizationConfig`
@@ -134,73 +125,73 @@
 
 .. automodule:: ax.core.optimization_config
     :members:
     :undoc-members:
     :show-inheritance:
 
 `OutcomeConstraint`
-~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.core.outcome_constraint
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 `Parameter`
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~
 
 .. automodule:: ax.core.parameter
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ParameterConstraint`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.core.parameter_constraint
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ParameterDistribution`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.core.parameter_distribution
     :members:
     :undoc-members:
     :show-inheritance:
 
 `RiskMeasure`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.core.risk_measures
     :members:
     :undoc-members:
     :show-inheritance:
 
 `Runner`
-~~~~~~~~~~~~~~~~
+~~~~~~~~
 
 .. automodule:: ax.core.runner
     :members:
     :undoc-members:
     :show-inheritance:
 
 `SearchSpace`
-~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.core.search_space
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 `Trial`
-~~~~~~~~
+~~~~~~~
 
 .. automodule:: ax.core.trial
     :members:
     :undoc-members:
     :show-inheritance:
 
 
@@ -218,13 +209,13 @@
 
 .. automodule:: ax.core.utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Formatting Utils
-----------
+----------------
 
 .. automodule:: ax.core.formatting_utils
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/early_stopping.rst` & `ax-platform-0.4.0/sphinx/source/early_stopping.rst`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.early_stopping
-===================================
+=================
 
 .. automodule:: ax.early_stopping
 .. currentmodule:: ax.early_stopping
 
 Strategies
 ----------
```

### Comparing `ax-platform-0.3.7/sphinx/source/exceptions.rst` & `ax-platform-0.4.0/sphinx/source/exceptions.rst`

 * *Files 1% similar despite different names*

```diff
@@ -1,58 +1,58 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.exceptions
-===================================
+=============
 
 .. automodule:: ax.exceptions
 .. currentmodule:: ax.exceptions
 
 
 Constants
-~~~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.exceptions.constants
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Core
-~~~~~~~~~~~~~~~~~~
+~~~~
 
 .. automodule:: ax.exceptions.core
     :members:
     :undoc-members:
     :show-inheritance:
 
 Data
-~~~~~~~~~~~~~~~~~~
+~~~~
 
 .. automodule:: ax.exceptions.data_provider
     :members:
     :undoc-members:
     :show-inheritance:
 
 Generation Strategy
-~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.exceptions.generation_strategy
     :members:
     :undoc-members:
     :show-inheritance:
 
 Model
-~~~~~~~~~~~~~~~~~~
+~~~~~
 
 .. automodule:: ax.exceptions.model
     :members:
     :undoc-members:
     :show-inheritance:
 
 Storage
-~~~~~~~~~~~~~~~~~~
+~~~~~~~
 
 .. automodule:: ax.exceptions.storage
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/metrics.rst` & `ax-platform-0.4.0/sphinx/source/metrics.rst`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.metrics
-===================================
+==========
 
 .. automodule:: ax.metrics
 .. currentmodule:: ax.metrics
 
 
 BoTorch Test Problem
-~~~~~~
+~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.metrics.botorch_test_problem
     :members:
     :undoc-members:
     :show-inheritance:
 
 Branin
@@ -37,23 +37,23 @@
 
 .. automodule:: ax.metrics.chemistry
     :members:
     :undoc-members:
     :show-inheritance:
 
 Curve
-~~~~~~~~~
+~~~~~
 
 .. automodule:: ax.metrics.curve
     :members:
     :undoc-members:
     :show-inheritance:
 
 Dictionary Lookup
-~~~~~~~~~
+~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.metrics.dict_lookup
     :members:
     :undoc-members:
     :show-inheritance:
 
 Factorial
@@ -69,15 +69,15 @@
 
 .. automodule:: ax.metrics.hartmann6
     :members:
     :undoc-members:
     :show-inheritance:
 
 Jenatton
-~~~~~~~~~
+~~~~~~~~
 
 .. automodule:: ax.metrics.jenatton
     :members:
     :undoc-members:
     :show-inheritance:
 
 
@@ -94,15 +94,15 @@
 
 .. automodule:: ax.metrics.noisy_function
     :members:
     :undoc-members:
     :show-inheritance:
 
 Noisy Function Map
-~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.metrics.noisy_function_map
     :members:
     :undoc-members:
     :show-inheritance:
 
 Sklearn
@@ -119,13 +119,13 @@
 .. automodule:: ax.metrics.tensorboard
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 TorchX
-~~~~~~~~~~~
+~~~~~~
 
 .. automodule:: ax.metrics.torchx
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/modelbridge.rst` & `ax-platform-0.4.0/sphinx/source/modelbridge.rst`

 * *Files 6% similar despite different names*

```diff
@@ -1,74 +1,81 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.modelbridge
-===================================
+==============
 
 .. automodule:: ax.modelbridge
 .. currentmodule:: ax.modelbridge
 
 
 Generation Strategy, Registry, and Factory
--------------------------------------------
+------------------------------------------
 
 Generation Strategy
-~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~
 .. automodule:: ax.modelbridge.generation_strategy
     :members:
     :undoc-members:
     :show-inheritance:
 
 Generation Node
 ~~~~~~~~~~~~~~~
 .. automodule:: ax.modelbridge.generation_node
     :members:
     :undoc-members:
     :show-inheritance:
 
+External Generation Node
+~~~~~~~~~~~~~~~
+.. automodule:: ax.modelbridge.external_generation_node
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
 Transition Criterion
 .. automodule:: ax.modelbridge.transition_criterion
     :members:
     :undoc-members:
     :show-inheritance:
 
 Registry
-~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~
 .. automodule:: ax.modelbridge.registry
     :members:
     :undoc-members:
     :show-inheritance:
 
 Factory
-~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~
 .. automodule:: ax.modelbridge.factory
     :members:
     :undoc-members:
     :show-inheritance:
 
 ModelSpec
 ~~~~~~~~~
 .. automodule:: ax.modelbridge.model_spec
     :members:
     :undoc-members:
     :show-inheritance:
 
 Model Bridges
---------------
+-------------
 
 Base Model Bridge
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.base
     :members:
     :undoc-members:
     :show-inheritance:
 
 Discrete Model Bridge
-~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.discrete
     :members:
     :undoc-members:
     :show-inheritance:
 
 Random Model Bridge
@@ -77,82 +84,89 @@
 .. automodule:: ax.modelbridge.random
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 Torch Model Bridge
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.torch
     :members:
     :undoc-members:
     :show-inheritance:
 
 Pairwise Model Bridge
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.pairwise
     :members:
     :undoc-members:
     :show-inheritance:
 
 Map Torch Model Bridge
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.map_torch
     :members:
     :undoc-members:
     :show-inheritance:
 
 Utilities
----------------
+---------
 
 General Utilities
-~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~
 .. automodule:: ax.modelbridge.modelbridge_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Prediction Utilities
-~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~
 .. automodule:: ax.modelbridge.prediction_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Cross Validation
-~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~
 .. automodule:: ax.modelbridge.cross_validation
     :members:
     :undoc-members:
     :show-inheritance:
 
 Dispatch Utilities
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.dispatch_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Transforms
--------------
+----------
+`ax.modelbridge.transforms.deprecated_transform_mixin`
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.. automodule:: ax.modelbridge.transforms.deprecated_transform_mixin
+    :members:
+    :undoc-members:
+    :show-inheritance:
 
 `ax.modelbridge.transforms.base`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.base
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.cast`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.cast
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.cap_parameter`
@@ -168,71 +182,71 @@
 
 .. automodule:: ax.modelbridge.transforms.centered_unit_x
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.choice\_encode`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.choice_encode
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.convert_metric_names`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.convert_metric_names
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.derelativize`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.derelativize
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.int\_range\_to\_choice`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.int_range_to_choice
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.int\_to\_float`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.int_to_float
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.ivw`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.ivw
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.inverse_gaussian_cdf_y`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.inverse_gaussian_cdf_y
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.log`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.log
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.log_y`
@@ -240,15 +254,15 @@
 
 .. automodule:: ax.modelbridge.transforms.log_y
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.logit`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.logit
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.map_unit_x`
@@ -256,79 +270,79 @@
 
 .. automodule:: ax.modelbridge.transforms.map_unit_x
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.merge_repeated_measurements`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.merge_repeated_measurements
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.metrics_as_task`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.metrics_as_task
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.one\_hot`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.one_hot
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.percentile_y`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.percentile_y
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.power\_transform\_y`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.power_transform_y
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.remove\_fixed`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.remove_fixed
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.rounding`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.rounding
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.search\_space\_to\_choice`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.search_space_to_choice
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.search\_space\_to\_float`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.search_space_to_float
     :members:
     :undoc-members:
     :show-inheritance:
 
  `ax.modelbridge.transforms.standardize\_y`
@@ -336,31 +350,31 @@
 
 .. automodule:: ax.modelbridge.transforms.standardize_y
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.stratified\_standardize\_y`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.stratified_standardize_y
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.task\_encode`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.task_encode
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.trial\_as\_task`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.trial_as_task
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.unit\_x`
@@ -368,23 +382,23 @@
 
 .. automodule:: ax.modelbridge.transforms.unit_x
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.utils`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.winsorize`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.transforms.winsorize
     :members:
     :undoc-members:
     :show-inheritance:
 
 `ax.modelbridge.transforms.relativize`
@@ -395,15 +409,15 @@
     :undoc-members:
     :show-inheritance:
 
 Strategies
 -------------
 
 `ax.modelbridge.strategies.alebo`
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.modelbridge.strategies.alebo
     :members:
     :undoc-members:
     :show-inheritance:
 
 .. automodule:: ax.modelbridge.strategies.rembo
```

### Comparing `ax-platform-0.3.7/sphinx/source/models.rst` & `ax-platform-0.4.0/sphinx/source/models.rst`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.models
-===================================
+=========
 
 .. automodule:: ax.models
 .. currentmodule:: ax.models
 
 
 Base Models & Utilities
 -----------------------
 
 ax.models.base
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.base
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.discrete\_base module
@@ -24,15 +24,15 @@
 
 .. automodule:: ax.models.discrete_base
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch\_base module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch_base
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.model\_utils module
@@ -40,64 +40,64 @@
 
 .. automodule:: ax.models.model_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.types
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.types
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.winsorization\_config module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.winsorization_config
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Discrete Models
-----------------
+---------------
 
 ax.models.discrete.eb\_thompson module
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.discrete.eb_thompson
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.discrete.full\_factorial module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.discrete.full_factorial
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.discrete.thompson module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.discrete.thompson
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 
 Random Models
-----------------
+-------------
 
 ax.models.random.base module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.random.base
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.random.uniform module
@@ -105,273 +105,265 @@
 
 .. automodule:: ax.models.random.uniform
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.random.sobol module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.random.sobol
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.random.alebo_initializer module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.random.alebo_initializer
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.random.rembo_initializer module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.random.rembo_initializer
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Torch Models & Utilities
 ------------------------
 
 ax.models.torch.alebo module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.alebo
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_defaults module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_defaults
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_kg module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_kg
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_mes module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_mes
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_moo module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_moo
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_moo_defaults module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_moo_defaults
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.acquisition module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.acquisition
     :members:
     :undoc-members:
     :show-inheritance:
 
-ax.models.torch.botorch_modular.default_options module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-.. automodule:: ax.models.torch.botorch_modular.default_options
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
 ax.models.torch.botorch_modular.list_surrogate module
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.list_surrogate
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.randomforest module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.randomforest
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.model module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.model
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.multi_fidelity module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.multi_fidelity
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.optimizer_argparse module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.optimizer_argparse
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.sebo module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.sebo
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.surrogate module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.surrogate
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.utils module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.kernels module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.kernels
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.input_constructors.covar_modules module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.input_constructors.covar_modules
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.input_constructors.input_transforms module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.input_constructors.input_transforms
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.botorch_modular.input_constructors.outcome_transform module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.botorch_modular.input_constructors.outcome_transform
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.cbo_lcea module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.cbo_lcea
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.cbo_lcem module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.cbo_lcem
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.cbo_sac module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.cbo_sac
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.frontier_utils module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.frontier_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.fully_bayesian module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.fully_bayesian
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.fully_bayesian_model_utils module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.fully_bayesian_model_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.posterior_mean module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.posterior_mean
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.rembo module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.rembo
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.models.torch.utils module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.models.torch.utils
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/plot.rst` & `ax-platform-0.4.0/sphinx/source/plot.rst`

 * *Files 2% similar despite different names*

```diff
@@ -1,53 +1,46 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.plot
-===================================
+=======
 
 .. automodule:: ax.plot
 .. currentmodule:: ax.plot
 
 
 Rendering
-------------
+---------
 
 .. automodule:: ax.plot.render
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Plots
-----------
+-----
 
 Base
-~~~~~~~~~~~~~~~~~~~
+~~~~
 
 .. automodule:: ax.plot.base
     :members:
     :undoc-members:
     :show-inheritance:
 
 Bandit Rollout
-~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~
 .. automodule:: ax.plot.bandit_rollout
     :members:
     :undoc-members:
     :show-inheritance:
 
-Benchmark
-~~~~~~~~~~~~~~~~
-.. automodule:: ax.plot.benchmark
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
 Contour Plot
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~
 
 .. automodule:: ax.plot.contour
     :members:
     :undoc-members:
     :show-inheritance:
 
 Feature Importances
@@ -55,15 +48,15 @@
 
 .. automodule:: ax.plot.feature_importances
     :members:
     :undoc-members:
     :show-inheritance:
 
 Marginal Effects
-~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.plot.marginal_effects
     :members:
     :undoc-members:
     :show-inheritance:
 
 Model Diagnostics
@@ -71,68 +64,67 @@
 
 .. automodule:: ax.plot.diagnostic
     :members:
     :undoc-members:
     :show-inheritance:
 
 Pareto Plots
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~
 .. automodule:: ax.plot.pareto_frontier
     :members:
     :undoc-members:
     :show-inheritance:
 
 .. automodule:: ax.plot.pareto_utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Scatter Plots
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.plot.scatter
     :members:
     :undoc-members:
     :show-inheritance:
 
 Slice Plot
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~
 
 .. automodule:: ax.plot.slice
     :members:
     :undoc-members:
     :show-inheritance:
 
 Table
-~~~~~~~~~~~~~~~~~~~
+~~~~~
 
 .. automodule:: ax.plot.table_view
     :members:
     :undoc-members:
     :show-inheritance:
 
 Trace Plots
-~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~
 
 .. automodule:: ax.plot.trace
     :members:
     :undoc-members:
     :show-inheritance:
 
 Parallel Coordinates
 ~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.plot.parallel_coordinates
     :members:
     :undoc-members:
     :show-inheritance:
 
-
 Plotting Utilities
--------------------
+------------------
 
 .. automodule:: ax.plot.color
     :members:
     :undoc-members:
     :show-inheritance:
 
 .. automodule:: ax.plot.helper
```

### Comparing `ax-platform-0.3.7/sphinx/source/runners.rst` & `ax-platform-0.4.0/sphinx/source/runners.rst`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.runners
-===================================
+==========
 
 .. automodule:: ax.runners
 .. currentmodule:: ax.runners
 
 BoTorch Test Problem
-~~~~~~
+~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.runners.botorch_test_problem
     :members:
     :undoc-members:
     :show-inheritance:
 
 SingleRunningTrialMixin
-~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.runners.single_running_trial_mixin
     :members:
     :undoc-members:
     :show-inheritance:
 
 Synthetic Runner
@@ -28,21 +28,21 @@
 
 .. automodule:: ax.runners.synthetic
     :members:
     :undoc-members:
     :show-inheritance:
 
 Simulated Backend Runner
-~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.runners.simulated_backend
     :members:
     :undoc-members:
     :show-inheritance:
 
 TorchX Runner
-~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.runners.torchx
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/service.rst` & `ax-platform-0.4.0/sphinx/source/service.rst`

 * *Files 11% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.service
-===================================
+==========
 
 .. automodule:: ax.service
 .. currentmodule:: ax.service
 
 
 Ax Client
-~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.service.ax_client
     :members:
     :undoc-members:
     :show-inheritance:
 
 Managed Loop
-~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~
 
 .. automodule:: ax.service.managed_loop
     :members:
     :undoc-members:
     :show-inheritance:
 
 Interactive Loop
@@ -30,28 +30,28 @@
 .. automodule:: ax.service.interactive_loop
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Scheduler
-~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.service.scheduler
     :members:
     :undoc-members:
     :show-inheritance:
 
 .. automodule:: ax.service.utils.scheduler_options
     :members:
     :undoc-members:
     :show-inheritance:
 
 Utils
-------
+-----
 
 Best Point Identification
 ~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.service.utils.best_point_mixin
     :members:
     :undoc-members:
@@ -61,24 +61,24 @@
 .. automodule:: ax.service.utils.best_point
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Instantiation
-~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.service.utils.instantiation
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Reporting
-~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.service.utils.report_utils
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/storage.rst` & `ax-platform-0.4.0/sphinx/source/storage.rst`

 * *Files 5% similar despite different names*

```diff
@@ -1,70 +1,70 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.storage
-===================================
+==========
 
 .. automodule:: ax.storage
 .. currentmodule:: ax.storage
 
 
 JSON
------
+----
 
 ax.storage.json\_store.decoder module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.decoder
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.json\_store.decoders module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.decoders
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.json\_store.encoder module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.encoder
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.json\_store.encoders module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.encoders
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.json\_store.load module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.load
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.json\_store.registry module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.registry
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.json\_store.save module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.json_store.save
     :members:
     :undoc-members:
     :show-inheritance:
 
 
@@ -82,97 +82,97 @@
 
 .. automodule:: ax.storage.sqa_store.encoder
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.db module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.db
     :members:
     :undoc-members:
     :show-inheritance:
     :exclude-members: Base
 
 ax.storage.sqa\_store.delete module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.delete
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.json module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.json
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.load module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.load
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.save module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.save
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.structs module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.structs
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.sqa\_classes module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.sqa_classes
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.sqa\_config module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.sqa_config
     :members:
     :undoc-members:
     :show-inheritance:
     :noindex:
 
 ax.storage.sqa\_store.sqa\_enum module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.sqa_enum
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.timestamp module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.timestamp
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.utils module
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.storage.sqa_store.utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 ax.storage.sqa\_store.validation module
@@ -188,15 +188,15 @@
 
 .. automodule:: ax.storage.sqa_store.reduced_state
     :members:
     :undoc-members:
     :show-inheritance:
 
 Registries
------------------
+----------
 
 .. automodule:: ax.storage.botorch_modular_registry
     :members:
     :undoc-members:
     :show-inheritance:
 
 .. automodule:: ax.storage.metric_registry
@@ -216,13 +216,13 @@
 
 .. automodule:: ax.storage.transform_registry
     :members:
     :undoc-members:
     :show-inheritance:
 
 Utilities
------------------
+---------
 
 .. automodule:: ax.storage.utils
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/sphinx/source/telemetry.rst` & `ax-platform-0.4.0/sphinx/source/telemetry.rst`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/sphinx/source/utils.rst` & `ax-platform-0.4.0/sphinx/source/utils.rst`

 * *Files 12% similar despite different names*

```diff
@@ -1,244 +1,248 @@
 .. role:: hidden
     :class: hidden-section
 
 ax.utils
-===================================
+========
 
 .. automodule:: ax.utils
 .. currentmodule:: ax.utils
 
 
 Common
----------------
+------
 
 Base
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~
 
 .. automodule:: ax.utils.common.base
     :members:
     :undoc-members:
     :show-inheritance:
 
 Constants
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.utils.common.constants
     :members:
     :undoc-members:
     :show-inheritance:
 
 Decorator
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.utils.common.decorator
     :members:
     :undoc-members:
     :show-inheritance:
 
 Docutils
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~
 
 .. automodule:: ax.utils.common.docutils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Equality
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~
 
 .. automodule:: ax.utils.common.equality
     :members:
     :undoc-members:
     :show-inheritance:
 
-Equality
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+Executils
+~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.common.executils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Kwargs
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~
 
 .. automodule:: ax.utils.common.kwargs
     :members:
     :undoc-members:
     :show-inheritance:
 
 Logger
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~
 
 .. automodule:: ax.utils.common.logger
     :members:
     :undoc-members:
     :show-inheritance:
 
 Mock Torch
 ~~~~~~~~~~
 
 .. automodule:: ax.utils.common.mock
     :members:
     :undoc-members:
     :show-inheritance:
 
+Random
+~~~~~~
+
+.. automodule:: ax.utils.common.random
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
 Result
 ~~~~~~
 
 .. automodule:: ax.utils.common.result
     :members:
     :undoc-members:
     :show-inheritance:
 
 Serialization
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.common.serialization
     :members:
     :undoc-members:
     :show-inheritance:
 
 Testutils
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.utils.common.testutils
     :members:
     :no-undoc-members:
     :show-inheritance:
 
 Timeutils
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.utils.common.timeutils
     :members:
     :undoc-members:
     :show-inheritance:
 
 Typeutils
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~
 
 .. automodule:: ax.utils.common.typeutils
     :members:
     :undoc-members:
     :show-inheritance:
 
+Typeutils Non-Native
+~~~~~~~~~~~~~~~
+
+.. automodule:: ax.utils.common.typeutils_nonnative
+    :members:
+    :undoc-members:
+    :show-inheritance:
+
 Typeutils Torch
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.common.typeutils_torch
     :members:
     :undoc-members:
     :show-inheritance:
 
 Flake8 Plugins
----------------
+--------------
 
 Docstring Checker
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.flake8_plugins.docstring_checker
     :members:
     :undoc-members:
     :show-inheritance:
 
 Measurement
----------------
+-----------
 
 Synthetic Functions
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.measurement.synthetic_functions
     :members:
     :undoc-members:
     :show-inheritance:
 
 Notebook
----------------
+--------
 
 Plotting
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~
 
 .. automodule:: ax.utils.notebook.plotting
     :members:
     :undoc-members:
     :show-inheritance:
 
 Report
----------------
+------
 
 Render
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~
 
 .. automodule:: ax.utils.report.render
     :members:
     :undoc-members:
     :show-inheritance:
 
 Sensitivity
----------------
+-----------
 
 Derivative GP
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.sensitivity.derivative_gp
     :members:
     :undoc-members:
     :show-inheritance:
 
 Derivative Measures
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.sensitivity.derivative_measures
     :members:
     :undoc-members:
     :show-inheritance:
 
 Sobol Measures
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.sensitivity.sobol_measures
     :members:
     :undoc-members:
     :show-inheritance:
 
 Stats
----------------
+-----
 
 Statstools
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~
 
 .. automodule:: ax.utils.stats.statstools
     :members:
     :undoc-members:
     :show-inheritance:
 
 Model Fit Metrics
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.stats.model_fit_stats
     :members:
     :undoc-members:
     :show-inheritance:
 
-Storage
----------------
-
-Deletion
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-.. automodule:: ax.utils.storage.sqa.delete
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
-
 Testing
----------------
+-------
 
 Backend Scheduler
 ~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.testing.backend_scheduler
     :members:
     :undoc-members:
@@ -249,40 +253,40 @@
 
 .. automodule:: ax.utils.testing.backend_simulator
     :members:
     :undoc-members:
     :show-inheritance:
 
 Benchmark Stubs
-~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.testing.benchmark_stubs
     :members:
     :undoc-members:
     :show-inheritance:
 
 Core Stubs
-~~~~~~~~~~~
+~~~~~~~~~~
 
 .. automodule:: ax.utils.testing.core_stubs
     :members:
     :undoc-members:
     :show-inheritance:
 
 Modeling Stubs
-~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.testing.modeling_stubs
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Mocking
-~~~~~~~~~~~~~~~
+~~~~~~~
 
 .. automodule:: ax.utils.testing.mock
     :members:
     :undoc-members:
     :show-inheritance:
 
 
@@ -291,40 +295,32 @@
 
 .. automodule:: ax.utils.testing.test_init_files
     :members:
     :undoc-members:
     :show-inheritance:
 
 Torch Stubs
-~~~~~~~~~~~~~~~
+~~~~~~~~~~~
 
 .. automodule:: ax.utils.testing.torch_stubs
     :members:
     :undoc-members:
     :show-inheritance:
 
-Unittest Conventions
-~~~~~~~~~~~~~~~~~~~~~
-
-.. automodule:: ax.utils.testing.unittest_conventions
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
 Utils
-~~~~~~~~~~~~~~~~~~~~~
+~~~~~
 
 .. automodule:: ax.utils.testing.utils
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Test Metrics
----------------
+------------
 
 Backend Simulator Map
 ~~~~~~~~~~~~~~~~~~~~~
 
 .. automodule:: ax.utils.testing.metrics.backend_simulator_map
     :members:
     :undoc-members:
@@ -337,16 +333,16 @@
 .. automodule:: ax.utils.testing.metrics.branin_backend_map
     :members:
     :undoc-members:
     :show-inheritance:
 
 
 Tutorials
----------------
+---------
 
 Neural Net
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~
 
 .. automodule:: ax.utils.tutorials.cnn_utils
     :members:
     :undoc-members:
     :show-inheritance:
```

### Comparing `ax-platform-0.3.7/tutorials/early_stopping/early_stopping.ipynb` & `ax-platform-0.4.0/tutorials/early_stopping/early_stopping.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/early_stopping/mnist_train_nas.py` & `ax-platform-0.4.0/tutorials/early_stopping/mnist_train_nas.py`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/factorial.ipynb` & `ax-platform-0.4.0/tutorials/factorial.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.999967824967825%*

 * *Differences: {"'cells'": "{12: {'source': {insert: [(13, '        "*

 * *            'objective=Objective(metric=FactorialMetric(name="success_metric"), '*

 * *            "minimize=False)\\n')], delete: [13]}}}"}*

```diff
@@ -266,15 +266,15 @@
                 "        return trial_metadata\n",
                 "\n",
                 "\n",
                 "exp = Experiment(\n",
                 "    name=\"my_factorial_closed_loop_experiment\",\n",
                 "    search_space=search_space,\n",
                 "    optimization_config=OptimizationConfig(\n",
-                "        objective=Objective(metric=FactorialMetric(name=\"success_metric\"))\n",
+                "        objective=Objective(metric=FactorialMetric(name=\"success_metric\"), minimize=False)\n",
                 "    ),\n",
                 "    runner=MyRunner(),\n",
                 ")\n",
                 "exp.status_quo = Arm(\n",
                 "    parameters={\"factor1\": \"level11\", \"factor2\": \"level21\", \"factor3\": \"level31\"}\n",
                 ")"
             ]
```

### Comparing `ax-platform-0.3.7/tutorials/generation_strategy.ipynb` & `ax-platform-0.4.0/tutorials/generation_strategy.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/gpei_hartmann_developer.ipynb` & `ax-platform-0.4.0/tutorials/gpei_hartmann_developer.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/gpei_hartmann_loop.ipynb` & `ax-platform-0.4.0/tutorials/gpei_hartmann_loop.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/gpei_hartmann_service.ipynb` & `ax-platform-0.4.0/tutorials/gpei_hartmann_service.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/gss.ipynb` & `ax-platform-0.4.0/tutorials/gss.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/human_in_the_loop/hitl_data.json` & `ax-platform-0.4.0/tutorials/human_in_the_loop/hitl_data.json`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/human_in_the_loop/hitl_exp.json` & `ax-platform-0.4.0/tutorials/human_in_the_loop/hitl_exp.json`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/human_in_the_loop/human_in_the_loop.ipynb` & `ax-platform-0.4.0/tutorials/human_in_the_loop/human_in_the_loop.ipynb`

 * *Files 9% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9997652116402116%*

 * *Differences: {"'cells'": "{14: {'source': {insert: [(4, '    objective=Objective(objective_metric, "*

 * *            "minimize=False),\\n')], delete: [4]}}, 30: {'source': {insert: [(3, '        "*

 * *            "objective=Objective(objective_metric, minimize=False),\\n')], delete: [3]}}, 35: "*

 * *            "{'source': {insert: [(4, '        objective=Objective(objective_metric, "*

 * *            "minimize=False), outcome_constraints=[constraint_5]\\n')], delete: [4]}}, 39: "*

 * *            "{'source': {insert: [(4, '        objective […]*

```diff
@@ -175,15 +175,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "objective_metric = Metric(name=\"metric_1\")\n",
                 "constraint_metric = Metric(name=\"metric_2\")\n",
                 "\n",
                 "experiment.optimization_config = OptimizationConfig(\n",
-                "    objective=Objective(objective_metric),\n",
+                "    objective=Objective(objective_metric, minimize=False),\n",
                 "    outcome_constraints=[\n",
                 "        OutcomeConstraint(metric=constraint_metric, op=ComparisonOp.LEQ, bound=5),\n",
                 "    ],\n",
                 ")"
             ]
         },
         {
@@ -352,15 +352,15 @@
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": [
                 "unconstrained = gp.gen(\n",
                 "    n=3,\n",
                 "    optimization_config=OptimizationConfig(\n",
-                "        objective=Objective(objective_metric),\n",
+                "        objective=Objective(objective_metric, minimize=False),\n",
                 "    ),\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -406,15 +406,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "constraint_5 = OutcomeConstraint(metric=constraint_metric, op=ComparisonOp.LEQ, bound=5)\n",
                 "constraint_5_results = gp.gen(\n",
                 "    n=3,\n",
                 "    optimization_config=OptimizationConfig(\n",
-                "        objective=Objective(objective_metric), outcome_constraints=[constraint_5]\n",
+                "        objective=Objective(objective_metric, minimize=False), outcome_constraints=[constraint_5]\n",
                 "    ),\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
@@ -455,15 +455,15 @@
             "metadata": {},
             "outputs": [],
             "source": [
                 "constraint_1 = OutcomeConstraint(metric=constraint_metric, op=ComparisonOp.LEQ, bound=1)\n",
                 "constraint_1_results = gp.gen(\n",
                 "    n=3,\n",
                 "    optimization_config=OptimizationConfig(\n",
-                "        objective=Objective(objective_metric),\n",
+                "        objective=Objective(objective_metric, minimize=False),\n",
                 "        outcome_constraints=[constraint_1],\n",
                 "    ),\n",
                 ")"
             ]
         },
         {
             "cell_type": "code",
```

### Comparing `ax-platform-0.3.7/tutorials/modular_botax.ipynb` & `ax-platform-0.4.0/tutorials/modular_botax.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9999914965986394%*

 * *Differences: {"'cells'": "{12: {'source': {delete: [21]}}}"}*

```diff
@@ -232,15 +232,14 @@
                 "    # constructor for the given BoTorch `AcquisitionFunction`\n",
                 "    acquisition_options={},\n",
                 "    # Optional Ax `Acquisition` subclass (if the given BoTorch\n",
                 "    # `AcquisitionFunction` requires one, which is rare)\n",
                 "    acquisition_class=None,\n",
                 "    # Less common model settings shown with default values, refer\n",
                 "    # to `BoTorchModel` documentation for detail\n",
-                "    refit_on_update=True,\n",
                 "    refit_on_cv=False,\n",
                 "    warm_start_refit=True,\n",
                 ")"
             ]
         },
         {
             "cell_type": "markdown",
```

### Comparing `ax-platform-0.3.7/tutorials/multi_task.ipynb` & `ax-platform-0.4.0/tutorials/multi_task.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/multiobjective_optimization.ipynb` & `ax-platform-0.4.0/tutorials/multiobjective_optimization.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/raytune_pytorch_cnn.ipynb` & `ax-platform-0.4.0/tutorials/raytune_pytorch_cnn.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/saasbo.ipynb` & `ax-platform-0.4.0/tutorials/saasbo.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/saasbo_nehvi.ipynb` & `ax-platform-0.4.0/tutorials/saasbo_nehvi.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/scheduler.ipynb` & `ax-platform-0.4.0/tutorials/scheduler.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/sebo.ipynb` & `ax-platform-0.4.0/tutorials/sebo.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/submitit.ipynb` & `ax-platform-0.4.0/tutorials/submitit.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/tune_cnn_service.ipynb` & `ax-platform-0.4.0/tutorials/tune_cnn_service.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/tutorials/visualizations.ipynb` & `ax-platform-0.4.0/tutorials/visualizations.ipynb`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/core/Footer.js` & `ax-platform-0.4.0/website/core/Footer.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/core/Tutorial.js` & `ax-platform-0.4.0/website/core/Tutorial.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/core/TutorialSidebar.js` & `ax-platform-0.4.0/website/core/TutorialSidebar.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/pages/en/index.js` & `ax-platform-0.4.0/website/pages/en/index.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/pages/tutorials/index.js` & `ax-platform-0.4.0/website/pages/tutorials/index.js`

 * *Files 9% similar despite different names*

#### js-beautify {}

```diff
@@ -7,15 +7,17 @@
  * @format
  */
 
 const React = require('react');
 
 const CWD = process.cwd();
 
-const CompLibrary = require(`${CWD}/node_modules/docusaurus/lib/core/CompLibrary.js`);
+const CompLibrary = require(
+    `${CWD}/node_modules/docusaurus/lib/core/CompLibrary.js`,
+);
 const Container = CompLibrary.Container;
 
 const TutorialSidebar = require(`${CWD}/core/TutorialSidebar.js`);
 
 class TutorialHome extends React.Component {
     render() {
         return ( <
@@ -43,14 +45,24 @@
             a href = "gpei_hartmann_service.html" > Service < /a>, and&nbsp; <
             a href = "gpei_hartmann_developer.html" > Developer < /a> &mdash; are a
             good place to start.Each tutorial showcases optimization on a constrained Hartmann6 problem, with the Loop API being the simplest to use and the Developer API being the most customizable. <
             /p> <
             p >
             <
             b >
+            NOTE: We recommend the <
+            a href = "gpei_hartmann_service.html" > Service API < /a> for the
+            vast majority of use cases. <
+            /b>
+            This API provides an ideal balance of flexibility and simplicity
+            for most users, and we are in the process of consolidating Ax usage around it more formally. <
+            /p> <
+            p >
+            <
+            b >
             Further, we explore the different components available in Ax in
             more detail. <
             /b>{' '}
             The components explored below serve to set up an experiment,
             visualize its results, configure an optimization algorithm, run an entire experiment in a managed closed loop, and combine BoTorch components in Ax in a modular way. <
             /p> <
             ul >
@@ -84,16 +96,18 @@
             ul >
             <
             li >
             <
             a href = "modular_botax.html" >
             Modular < code > BoTorchModel < /code> <
             /a> &
-            nbsp; walks though a new beta - feature & mdash; an improved interface between Ax and < a href = "https://botorch.org/" > BoTorch < /a>{' '} &
-            mdash; which allows
+            nbsp; walks though a new beta - feature & mdash; an improved interface between Ax and {
+                ' '
+            } <
+            a href = "https://botorch.org/" > BoTorch < /a> &mdash; which allows
             for combining arbitrary BoTorch components like <
             code > AcquisitionFunction < /code>, <code>Model</code > , <
             code > AcquisitionObjective < /code> etc. into a single{' '} <
             code > Model < /code> in Ax. <
             /li> <
             /ul> <
             p >
```

### Comparing `ax-platform-0.3.7/website/siteConfig.js` & `ax-platform-0.4.0/website/siteConfig.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/css/base_sphinx.css` & `ax-platform-0.4.0/website/static/css/base_sphinx.css`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/css/custom.css` & `ax-platform-0.4.0/website/static/css/custom.css`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/css/custom_sphinx.css` & `ax-platform-0.4.0/website/static/css/custom_sphinx.css`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/css/pygments.css` & `ax-platform-0.4.0/website/static/css/pygments.css`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/ax.svg` & `ax-platform-0.4.0/website/static/img/ax.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/ax_lockup.svg` & `ax-platform-0.4.0/website/static/img/ax_lockup.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/ax_lockup_white.svg` & `ax-platform-0.4.0/website/static/img/ax_lockup_white.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/ax_logo_lockup.svg` & `ax-platform-0.4.0/website/static/img/ax_logo_lockup.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/database-solid.svg` & `ax-platform-0.4.0/website/static/img/database-solid.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/dice-solid.svg` & `ax-platform-0.4.0/website/static/img/dice-solid.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/favicon/favicon.ico` & `ax-platform-0.4.0/website/static/img/favicon/favicon.ico`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/favicon.png` & `ax-platform-0.4.0/website/static/img/favicon.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/oss_logo.png` & `ax-platform-0.4.0/website/static/img/oss_logo.png`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/img/th-large-solid.svg` & `ax-platform-0.4.0/website/static/img/th-large-solid.svg`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/js/mathjax.js` & `ax-platform-0.4.0/website/static/js/mathjax.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/static/js/plotUtils.js` & `ax-platform-0.4.0/website/static/js/plotUtils.js`

 * *Files identical despite different names*

### Comparing `ax-platform-0.3.7/website/tutorials.json` & `ax-platform-0.4.0/website/tutorials.json`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8%*

 * *Differences: {"'Integrating External Strategies'": "[OrderedDict([('id', 'external_generation_node'), ('title', "*

 * *                                      "'RandomForest with ExternalGenerationNode')])]"}*

```diff
@@ -80,9 +80,15 @@
             "title": "Bandit Optimization"
         },
         {
             "dir": "human_in_the_loop",
             "id": "human_in_the_loop",
             "title": "Human-in-the-Loop Optimization"
         }
+    ],
+    "Integrating External Strategies": [
+        {
+            "id": "external_generation_node",
+            "title": "RandomForest with ExternalGenerationNode"
+        }
     ]
 }
```


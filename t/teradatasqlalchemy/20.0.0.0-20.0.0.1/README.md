# Comparing `tmp/teradatasqlalchemy-20.0.0.0-py3-none-any.whl.zip` & `tmp/teradatasqlalchemy-20.0.0.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,23 @@
-Zip file size: 60183 bytes, number of entries: 21
--rw-rw-rw-  2.0 fat     1476 b- defN 24-Mar-14 14:15 teradatasqlalchemy/__init__.py
--rw-rw-rw-  2.0 fat     2795 b- defN 24-Mar-14 14:15 teradatasqlalchemy/base.py
--rw-rw-rw-  2.0 fat    23904 b- defN 24-Mar-14 14:15 teradatasqlalchemy/compiler.py
--rw-rw-rw-  2.0 fat    76174 b- defN 24-Mar-14 14:15 teradatasqlalchemy/dialect.py
--rw-rw-rw-  2.0 fat     1635 b- defN 24-Mar-14 14:15 teradatasqlalchemy/options.py
--rw-rw-rw-  2.0 fat      967 b- defN 24-Mar-14 14:15 teradatasqlalchemy/requirements.py
--rw-rw-rw-  2.0 fat     5846 b- defN 24-Mar-14 14:15 teradatasqlalchemy/resolver.py
--rw-rw-rw-  2.0 fat     5138 b- defN 24-Mar-14 14:15 teradatasqlalchemy/restricted_words.py
--rw-rw-rw-  2.0 fat    51479 b- defN 24-Mar-14 14:15 teradatasqlalchemy/types.py
--rw-rw-rw-  2.0 fat      938 b- defN 24-Mar-14 14:15 teradatasqlalchemy/utils.py
--rw-rw-rw-  2.0 fat       95 b- defN 24-Mar-14 14:15 teradatasqlalchemy/vernumber.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-14 14:15 teradatasqlalchemy/telemetry/__init__.py
--rw-rw-rw-  2.0 fat    15208 b- defN 24-Mar-14 14:15 teradatasqlalchemy/telemetry/queryband.py
--rw-rw-rw-  2.0 fat    13962 b- defN 24-Mar-14 14:15 teradatasqlalchemy-20.0.0.0.data/data/teradatasqlalchemy/LICENSE
--rw-rw-rw-  2.0 fat     9666 b- defN 24-Mar-14 14:15 teradatasqlalchemy-20.0.0.0.data/data/teradatasqlalchemy/README.md
--rw-rw-rw-  2.0 fat    13962 b- defN 24-Mar-14 14:20 teradatasqlalchemy-20.0.0.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    10682 b- defN 24-Mar-14 14:20 teradatasqlalchemy-20.0.0.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Mar-14 14:20 teradatasqlalchemy-20.0.0.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       79 b- defN 24-Mar-14 14:20 teradatasqlalchemy-20.0.0.0.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       19 b- defN 24-Mar-14 14:20 teradatasqlalchemy-20.0.0.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     1962 b- defN 24-Mar-14 14:20 teradatasqlalchemy-20.0.0.0.dist-info/RECORD
-21 files, 236079 bytes uncompressed, 56915 bytes compressed:  75.9%
+Zip file size: 59901 bytes, number of entries: 21
+-rw-r--r--  2.0 unx     1449 b- defN 24-May-03 14:44 teradatasqlalchemy/__init__.py
+-rw-r--r--  2.0 unx     2704 b- defN 24-May-03 14:44 teradatasqlalchemy/base.py
+-rw-r--r--  2.0 unx    23268 b- defN 24-May-03 14:44 teradatasqlalchemy/compiler.py
+-rw-r--r--  2.0 unx    74278 b- defN 24-May-03 14:44 teradatasqlalchemy/dialect.py
+-rw-r--r--  2.0 unx     1576 b- defN 24-May-03 14:44 teradatasqlalchemy/options.py
+-rw-r--r--  2.0 unx      937 b- defN 24-May-03 14:44 teradatasqlalchemy/requirements.py
+-rw-r--r--  2.0 unx     5679 b- defN 24-May-03 14:44 teradatasqlalchemy/resolver.py
+-rw-r--r--  2.0 unx     5135 b- defN 24-May-03 14:44 teradatasqlalchemy/restricted_words.py
+-rw-r--r--  2.0 unx    49810 b- defN 24-May-03 14:44 teradatasqlalchemy/types.py
+-rw-r--r--  2.0 unx      906 b- defN 24-May-03 14:44 teradatasqlalchemy/utils.py
+-rw-r--r--  2.0 unx       92 b- defN 24-May-03 14:44 teradatasqlalchemy/vernumber.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-03 14:44 teradatasqlalchemy/telemetry/__init__.py
+-rw-r--r--  2.0 unx    14901 b- defN 24-May-03 14:44 teradatasqlalchemy/telemetry/queryband.py
+-rw-r--r--  2.0 unx    13766 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.data/data/teradatasqlalchemy/LICENSE
+-rw-r--r--  2.0 unx     9582 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.data/data/teradatasqlalchemy/README.md
+-rw-r--r--  2.0 unx    13766 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    10544 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       80 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       19 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1962 b- defN 24-May-03 14:44 teradatasqlalchemy-20.0.0.1.dist-info/RECORD
+21 files, 230546 bytes uncompressed, 56633 bytes compressed:  75.4%
```

## zipnote {}

```diff
@@ -33,32 +33,32 @@
 
 Filename: teradatasqlalchemy/telemetry/__init__.py
 Comment: 
 
 Filename: teradatasqlalchemy/telemetry/queryband.py
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.data/data/teradatasqlalchemy/LICENSE
+Filename: teradatasqlalchemy-20.0.0.1.data/data/teradatasqlalchemy/LICENSE
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.data/data/teradatasqlalchemy/README.md
+Filename: teradatasqlalchemy-20.0.0.1.data/data/teradatasqlalchemy/README.md
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.dist-info/LICENSE
+Filename: teradatasqlalchemy-20.0.0.1.dist-info/LICENSE
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.dist-info/METADATA
+Filename: teradatasqlalchemy-20.0.0.1.dist-info/METADATA
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.dist-info/WHEEL
+Filename: teradatasqlalchemy-20.0.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.dist-info/entry_points.txt
+Filename: teradatasqlalchemy-20.0.0.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.dist-info/top_level.txt
+Filename: teradatasqlalchemy-20.0.0.1.dist-info/top_level.txt
 Comment: 
 
-Filename: teradatasqlalchemy-20.0.0.0.dist-info/RECORD
+Filename: teradatasqlalchemy-20.0.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## teradatasqlalchemy/__init__.py

 * *Ordering differences only*

```diff
@@ -1,27 +1,27 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-from .types import (INTEGER, SMALLINT, BIGINT, DECIMAL, DATE, TIME,
-                    TIMESTAMP, CHAR, VARCHAR, CLOB, FLOAT, NUMBER, BYTEINT,
-                    BYTE, VARBYTE, BLOB, INTERVAL_YEAR, INTERVAL_YEAR_TO_MONTH,
-                    INTERVAL_MONTH, INTERVAL_DAY, INTERVAL_DAY_TO_HOUR,
-                    INTERVAL_DAY_TO_MINUTE, INTERVAL_DAY_TO_SECOND, INTERVAL_HOUR,
-                    INTERVAL_HOUR_TO_MINUTE, INTERVAL_HOUR_TO_SECOND,
-                    INTERVAL_MINUTE, INTERVAL_MINUTE_TO_SECOND, INTERVAL_SECOND,
-                    PERIOD_DATE, PERIOD_TIME, PERIOD_TIMESTAMP, XML, TDUDT, JSON,
-                    GEOMETRY, MBR, MBB)
-
-from . import vernumber
-
-__version__ = vernumber.sVersionNumber
-
-__all__ = ('INTEGER', 'SMALLINT', 'BIGINT', 'DECIMAL', 'FLOAT', 'DATE', 'TIME',
-           'TIMESTAMP', 'CHAR', 'VARCHAR', 'CLOB', 'NUMBER', 'BYTEINT', 'BYTE',
-           'VARBYTE', 'BLOB', 'INTERVAL_YEAR', 'INTERVAL_YEAR_TO_MONTH',
-           'INTERVAL_MONTH', 'INTERVAL_DAY', 'INTERVAL_DAY_TO_HOUR',
-           'INTERVAL_DAY_TO_MINUTE', 'INTERVAL_DAY_TO_SECOND', 'INTERVAL_HOUR',
-           'INTERVAL_HOUR_TO_MINUTE', 'INTERVAL_HOUR_TO_SECOND', 'INTERVAL_MINUTE',
-           'INTERVAL_MINUTE_TO_SECOND', 'INTERVAL_SECOND', 'PERIOD_DATE',
-           'PERIOD_TIME', 'PERIOD_TIMESTAMP', 'XML', 'TDUDT', 'JSON', 'GEOMETRY',
-           'MBR', 'MBB')
-
-import teradatasql
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+from .types import (INTEGER, SMALLINT, BIGINT, DECIMAL, DATE, TIME,
+                    TIMESTAMP, CHAR, VARCHAR, CLOB, FLOAT, NUMBER, BYTEINT,
+                    BYTE, VARBYTE, BLOB, INTERVAL_YEAR, INTERVAL_YEAR_TO_MONTH,
+                    INTERVAL_MONTH, INTERVAL_DAY, INTERVAL_DAY_TO_HOUR,
+                    INTERVAL_DAY_TO_MINUTE, INTERVAL_DAY_TO_SECOND, INTERVAL_HOUR,
+                    INTERVAL_HOUR_TO_MINUTE, INTERVAL_HOUR_TO_SECOND,
+                    INTERVAL_MINUTE, INTERVAL_MINUTE_TO_SECOND, INTERVAL_SECOND,
+                    PERIOD_DATE, PERIOD_TIME, PERIOD_TIMESTAMP, XML, TDUDT, JSON,
+                    GEOMETRY, MBR, MBB)
+
+from . import vernumber
+
+__version__ = vernumber.sVersionNumber
+
+__all__ = ('INTEGER', 'SMALLINT', 'BIGINT', 'DECIMAL', 'FLOAT', 'DATE', 'TIME',
+           'TIMESTAMP', 'CHAR', 'VARCHAR', 'CLOB', 'NUMBER', 'BYTEINT', 'BYTE',
+           'VARBYTE', 'BLOB', 'INTERVAL_YEAR', 'INTERVAL_YEAR_TO_MONTH',
+           'INTERVAL_MONTH', 'INTERVAL_DAY', 'INTERVAL_DAY_TO_HOUR',
+           'INTERVAL_DAY_TO_MINUTE', 'INTERVAL_DAY_TO_SECOND', 'INTERVAL_HOUR',
+           'INTERVAL_HOUR_TO_MINUTE', 'INTERVAL_HOUR_TO_SECOND', 'INTERVAL_MINUTE',
+           'INTERVAL_MINUTE_TO_SECOND', 'INTERVAL_SECOND', 'PERIOD_DATE',
+           'PERIOD_TIME', 'PERIOD_TIMESTAMP', 'XML', 'TDUDT', 'JSON', 'GEOMETRY',
+           'MBR', 'MBB')
+
+import teradatasql
```

## teradatasqlalchemy/base.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-from sqlalchemy import *
-from sqlalchemy import types as sqltypes
-from sqlalchemy.engine import default
-from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.schema import DDLElement
-from sqlalchemy.sql import compiler
-from sqlalchemy.sql import table
-from sqlalchemy.sql.expression import ClauseElement, Executable
-from .restricted_words import restricted_words
-
-import re
-
-
-AUTOCOMMIT_REGEXP = re.compile(
-            r'\s*(?:UPDATE|INSERT|CREATE|DELETE|DROP|ALTER|MERGE)',
-                re.I | re.UNICODE)
-
-ReservedWords = restricted_words
-
-class TeradataExecutionContext(default.DefaultExecutionContext):
-
-    def __init__(self, dialect, connection, dbapi_connection, compiled_ddl):
-        super(TeradataExecutionContext, self).__init__(dialect, connection, dbapi_connection, compiled_ddl)
-
-    def should_autocommit_text(self, statement):
-        return AUTOCOMMIT_REGEXP.match(statement)
-
-class TeradataIdentifierPreparer(compiler.IdentifierPreparer):
-
-    reserved_words = ReservedWords
-
-    def __init__(self, dialect, initial_quote='"', final_quote=None, escape_quote='"', omit_schema=False):
-
-        super(TeradataIdentifierPreparer, self).__init__(dialect, initial_quote, final_quote,
-                                                         escape_quote, omit_schema)
-
-# Views Recipe from: https://bitbucket.org/zzzeek/sqlalchemy/wiki/UsageRecipes/Views
-class CreateView(DDLElement):
-
-    def __init__(self, name, selectable):
-        self.name = name
-        self.selectable = selectable
-
-class DropView(DDLElement):
-
-    def __init__(self, name):
-        self.name = name
-
-@compiles(CreateView)
-def visit_create_view(element, compiler, **kw):
-    return 'CREATE VIEW {} AS {}'.format(
-        element.name,
-        compiler.sql_compiler.process(element.selectable, literal_binds=True))
-
-@compiles(DropView)
-def visit_drop_view(element, compiler, **kw):
-    return "DROP VIEW {}".format(element.name)
-
-class CreateTableAs(DDLElement):
-
-    def __init__(self, name, selectable, data=False):
-        self.name = name
-        self.selectable = selectable
-        self.data = data
-
-@compiles(CreateTableAs)
-def visit_create_table_as(element, compiler, **kw):
-    return 'CREATE TABLE {} AS ({}) WITH{}DATA;'.format(
-        element.name,
-        compiler.sql_compiler.process(element.selectable, literal_binds=True),
-        ' ' if element.data else ' NO ')
-
-class CreateTableQueue(DDLElement):
-    pass
-
-class CreateTableGlobalTempTrace(DDLElement):
-    pass
-
-class CreateErrorTable(DDLElement):
-    pass
-
-class IdentityColumn(DDLElement):
-    pass
-
-class CreateJoinIndex():
-    pass
-
-class CreateHashIndex():
-    pass
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+from sqlalchemy import *
+from sqlalchemy import types as sqltypes
+from sqlalchemy.engine import default
+from sqlalchemy.ext.compiler import compiles
+from sqlalchemy.schema import DDLElement
+from sqlalchemy.sql import compiler
+from sqlalchemy.sql import table
+from sqlalchemy.sql.expression import ClauseElement, Executable
+from .restricted_words import restricted_words
+
+import re
+
+
+AUTOCOMMIT_REGEXP = re.compile(
+            r'\s*(?:UPDATE|INSERT|CREATE|DELETE|DROP|ALTER|MERGE)',
+                re.I | re.UNICODE)
+
+ReservedWords = restricted_words
+
+class TeradataExecutionContext(default.DefaultExecutionContext):
+
+    def __init__(self, dialect, connection, dbapi_connection, compiled_ddl):
+        super(TeradataExecutionContext, self).__init__(dialect, connection, dbapi_connection, compiled_ddl)
+
+    def should_autocommit_text(self, statement):
+        return AUTOCOMMIT_REGEXP.match(statement)
+
+class TeradataIdentifierPreparer(compiler.IdentifierPreparer):
+
+    reserved_words = ReservedWords
+
+    def __init__(self, dialect, initial_quote='"', final_quote=None, escape_quote='"', omit_schema=False):
+
+        super(TeradataIdentifierPreparer, self).__init__(dialect, initial_quote, final_quote,
+                                                         escape_quote, omit_schema)
+
+# Views Recipe from: https://bitbucket.org/zzzeek/sqlalchemy/wiki/UsageRecipes/Views
+class CreateView(DDLElement):
+
+    def __init__(self, name, selectable):
+        self.name = name
+        self.selectable = selectable
+
+class DropView(DDLElement):
+
+    def __init__(self, name):
+        self.name = name
+
+@compiles(CreateView)
+def visit_create_view(element, compiler, **kw):
+    return 'CREATE VIEW {} AS {}'.format(
+        element.name,
+        compiler.sql_compiler.process(element.selectable, literal_binds=True))
+
+@compiles(DropView)
+def visit_drop_view(element, compiler, **kw):
+    return "DROP VIEW {}".format(element.name)
+
+class CreateTableAs(DDLElement):
+
+    def __init__(self, name, selectable, data=False):
+        self.name = name
+        self.selectable = selectable
+        self.data = data
+
+@compiles(CreateTableAs)
+def visit_create_table_as(element, compiler, **kw):
+    return 'CREATE TABLE {} AS ({}) WITH{}DATA;'.format(
+        element.name,
+        compiler.sql_compiler.process(element.selectable, literal_binds=True),
+        ' ' if element.data else ' NO ')
+
+class CreateTableQueue(DDLElement):
+    pass
+
+class CreateTableGlobalTempTrace(DDLElement):
+    pass
+
+class CreateErrorTable(DDLElement):
+    pass
+
+class IdentityColumn(DDLElement):
+    pass
+
+class CreateJoinIndex():
+    pass
+
+class CreateHashIndex():
+    pass
```

## teradatasqlalchemy/compiler.py

 * *Ordering differences only*

```diff
@@ -1,637 +1,637 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-from sqlalchemy import exc
-from sqlalchemy.sql import compiler
-from teradatasqlalchemy.utils import deprecated
-
-
-@deprecated('16.20.0.5', 'dialect.TeradataCompiler')
-class TeradataCompiler(compiler.SQLCompiler):
-
-    def get_select_precolumns(self, select, **kwargs):
-        """
-        handles the part of the select statement before the columns are specified.
-        Note: Teradata does not allow a 'distinct' to be specified when 'top' is
-              used in the same select statement.
-
-              Instead if a user specifies both in the same select clause,
-              the DISTINCT will be used with a ROW_NUMBER OVER(ORDER BY) subquery.
-        """
-
-        pre = select._distinct and "DISTINCT " or ""
-
-        #TODO: decide whether we can replace this with the recipe...
-        if (select._limit is not None and select._offset is None):
-            pre += "TOP %d " % (select._limit)
-
-        return pre
-
-    def visit_mod_binary(self, binary, operator, **kw):
-        return self.process(binary.left, **kw) + " MOD " + \
-            self.process(binary.right, **kw)
-
-    def visit_ne_binary(self, binary, operator, **kw):
-        return self.process(binary.left, **kw) + " <> " + \
-            self.process(binary.right, **kw)
-
-    def limit_clause(self, select, **kwargs):
-        """Limit after SELECT is implemented in get_select_precolumns"""
-        return ""
-
-@deprecated('16.20.0.5', 'dialect.TeradataDDLCompiler')
-class TeradataDDLCompiler(compiler.DDLCompiler):
-
-    def visit_create_index(self, create, include_schema=False,
-                           include_table_schema=True):
-        index = create.element
-        self._verify_index_table(index)
-        preparer = self.preparer
-        text = "CREATE "
-        if index.unique:
-            text += "UNIQUE "
-        text += "INDEX %s (%s) ON %s" \
-            % (
-                self._prepared_index_name(index,
-                                          include_schema=include_schema),
-                ', '.join(
-                    self.sql_compiler.process(
-                        expr, include_table=False, literal_binds=True) for
-                    expr in index.expressions),
-                preparer.format_table(index.table,
-                                      use_schema=include_table_schema)
-            )
-        return text
-
-    def create_table_suffix(self, table):
-        """
-        This hook processes the optional keyword teradata_suffixes
-        ex.
-        from teradatasqlalchemy.compiler import\
-                        TDCreateTableSuffix as Opts
-        t = Table( 'name', meta,
-                   ...,
-                   teradata_suffixes=Opts.
-                                      fallback().
-                                      log().
-                                      with_journal_table(t2.name)
-
-        CREATE TABLE name, fallback,
-        log,
-        with journal table = [database/user.]table_name(
-          ...
-        )
-
-        teradata_suffixes can also be a list of strings to be appended
-        in the order given.
-        """
-        post=table.dialect_kwargs['teradatasql_suffixes']
-
-        if isinstance(post, TDCreateTableSuffix):
-            if post.opts:
-                return ',\n' + post.compile()
-            else:
-                return post
-        elif post:
-            assert type(post) is list
-            res = ',\n ' + ',\n'.join(post)
-        else:
-            return ''
-
-    def post_create_table(self, table):
-
-        """
-        This hook processes the TDPostCreateTableOpts given by the
-        teradata_post_create dialect kwarg for Table.
-
-        Note that there are other dialect kwargs defined that could possibly
-        be processed here.
-
-        See the kwargs defined in dialect.TeradataDialect
-
-        Ex.
-        from teradatasqlalchemy.compiler import TDCreateTablePost as post
-        Table('t1', meta,
-               ...
-               ,
-               teradata_post_create = post().
-                                        fallback().
-                                        checksum('on').
-                                        mergeblockratio(85)
-
-        creates ddl for a table like so:
-
-        CREATE TABLE "t1" ,
-             checksum=on,
-             fallback,
-             mergeblockratio=85 (
-               ...
-        )
-
-        """
-        kw = table.dialect_kwargs['teradatasql_post_create']
-        if isinstance(kw, TDCreateTablePost):
-            if kw:
-              return '\n' + kw.compile()
-        return ''
-
-    def get_column_specification(self, column, **kwargs):
-
-        if column.table is None:
-            raise exc.CompileError(
-                "Teradata requires Table-bound columns "
-                "in order to generate DDL")
-
-        colspec = (self.preparer.format_column(column) + " " +\
-                        self.dialect.type_compiler.process(
-                          column.type, type_expression=column))
-
-        # Null/NotNull
-        if column.nullable is not None:
-            if not column.nullable or column.primary_key:
-                colspec += " NOT NULL"
-
-        return colspec
-
-class TeradataOptions(object):
-    """
-    An abstract base class for various schema object options
-    """
-    def _append(self, opts, val):
-        _opts=opts.copy()
-        _opts.update(val)
-        return _opts
-
-    @deprecated('16.20.0.5', 'dialect.TeradataOptions')
-    def compile(self):
-        """
-        processes the argument options and returns a string representation
-        """
-        pass
-
-    @deprecated('16.20.0.5', 'dialect.TeradataOptions')
-    def format_cols(self, key, val):
-
-        """
-        key is a string
-        val is a list of strings with an optional dict as the last element
-            the dict values are appended at the end of the col list
-        """
-        res = ''
-        col_expr = ', '.join([x for x in val if type(x) is str])
-
-        res += key + '( ' + col_expr + ' )'
-        if type(val[-1]) is dict:
-            # process syntax elements (dict) after cols
-            res += ' '.join( val[-1]['post'] )
-        return res
-
-@deprecated('16.20.0.5', 'dialect.TDCreateTableSuffix')
-class TDCreateTableSuffix(TeradataOptions):
-    """
-    A generative class for Teradata create table options
-    specified in teradata_suffixes
-    """
-
-    def __init__(self, opts={}):
-        """
-        opts is a dictionary that can be pre-populated with key-value pairs
-        that may be overidden if the keys conflict with those entered
-        in the methods below. See the compile method to see how the dict
-        gets processed.
-        """
-        self.opts = opts
-
-    def compile(self):
-        def process_opts(opts):
-            return [key if opts[key] is None else '{}={}'.\
-                            format(key, opts[key]) for key in opts]
-
-        res = ',\n'.join(process_opts(self.opts))
-        return res
-
-    def fallback(self, enabled=True):
-        res = 'fallback' if enabled else 'no fallback'
-        return self.__class__(self._append(self.opts, {res:None}))
-
-    def log(self, enabled=True):
-        res = 'log' if enabled else 'no log'
-        return self.__class__(self._append(self.opts, {res:None}))
-
-    def with_journal_table(self, tablename=None):
-        """
-        tablename is the schema.tablename of a table.
-        For example, if t1 is a SQLAlchemy:
-                with_journal_table(t1.name)
-        """
-        return self.__class__(self._append(self.opts,\
-                        {'with journal table':tablename}))
-
-    def before_journal(self, prefix='dual'):
-        """
-        prefix is a string taking vaues of 'no' or 'dual'
-        """
-        assert prefix in ('no', 'dual')
-        res = prefix+' '+'before journal'
-        return self.__class__(self._append(self.opts, {res:None}))
-
-    def after_journal(self, prefix='not local'):
-        """
-        prefix is a string taking vaues of 'no', 'dual', 'local',
-        or 'not local'.
-        """
-        assert prefix in ('no', 'dual', 'local', 'not local')
-        res = prefix+' '+'after journal'
-        return self.__class__(self._append(self.opts, {res:None}))
-
-    def checksum(self, integrity_checking='default'):
-        """
-        integrity_checking is a string taking vaues of 'on', 'off',
-        or 'default'.
-        """
-        assert integrity_checking in ('on', 'off', 'default')
-        return self.__class__(self._append(self.opts,\
-                        {'checksum':integrity_checking}))
-
-    def freespace(self, percentage=0):
-        """
-        percentage is an integer taking values from 0 to 75.
-        """
-        return self.__class__(self._append(self.opts,\
-                        {'freespace':percentage}))
-
-    def no_mergeblockratio(self):
-        return self.__class__(self._append(self.opts,\
-                        {'no mergeblockratio':None}))
-
-    def mergeblockratio(self, integer=None):
-        """
-        integer takes values from 0 to 100 inclusive.
-        """
-        res = 'default mergeblockratio' if integer is None\
-                                        else 'mergeblockratio'
-        return self.__class__(self._append(self.opts, {res:integer}))
-
-    def min_datablocksize(self):
-            return self.__class__(self._append(self.opts,\
-                            {'minimum datablocksize':None}))
-
-    def max_datablocksize(self):
-        return self.__class__(self._append(self.opts,\
-                        {'maximum datablocksize':None}))
-
-    def datablocksize(self, data_block_size=None):
-        """
-        data_block_size is an integer specifying the number of bytes
-        """
-        res = 'datablocksize' if data_block_size is not None\
-                              else 'default datablocksize'
-        return self.__class__(self._append(self.opts,\
-                                           {res:data_block_size}))
-
-    def blockcompression(self, opt='default'):
-        """
-        opt is a string that takes values 'autotemp',
-        'default', 'manual', or 'never'
-        """
-        return self.__class__(self._append(self.opts,\
-                        {'blockcompression':opt}))
-
-    def with_no_isolated_loading(self, concurrent=False):
-        res = 'with no ' +\
-            ('concurrent ' if concurrent else '') +\
-            'isolated loading'
-        return self.__class__(self._append(self.opts, {res:None}))
-
-    def with_isolated_loading(self, concurrent=False, opt=None):
-        """
-        opt is a string that takes values 'all', 'insert', 'none',
-        or None
-        """
-        assert opt in ('all', 'insert', 'none', None)
-        for_stmt = ' for ' + opt if opt is not None else ''
-        res = 'with ' +\
-            ('concurrent ' if concurrent else '') +\
-            'isolated loading' + for_stmt
-        return self.__class__(self._append(self.opts, {res:None}))
-
-@deprecated('16.20.0.5', 'dialect.TDCreateTablePost')
-class TDCreateTablePost(TeradataOptions):
-    """
-    A generative class for building post create table options
-    given in the teradata_post_create keyword for Table
-    """
-
-    @deprecated('16.20.0.5', 'dialect.TDCreateTablePost')
-    def __init__(self, opts={}):
-        self.opts = opts
-
-    def compile(self):
-        def process(opts):
-            return [key.upper() if opts[key] is None\
-                       else self.format_cols(key, opts[key])\
-                       for key in opts]
-
-        return ',\n'.join(process(self.opts))
-
-    def no_primary_index(self):
-        return self.__class__(self._append(self.opts, {'no primary index':None}))
-
-    def primary_index(self, name=None, unique=False, cols=[]):
-        """
-        name is a string for the primary index
-        if unique is true then unique primary index is specified
-        cols is a list of column names
-        """
-        res = 'unique primary index' if unique else 'primary index'
-        res += ' ' + name if name is not None else ''
-        return self.__class__(self._append(self.opts, {res:cols}))
-
-
-    def primary_amp(self, name=None, cols=[]):
-
-        """
-        name is an optional string for the name of the amp index
-        cols is a list of column names (strings)
-        """
-        res = 'primary amp index'
-        res += ' ' + name if name is not None else ''
-        return self.__class__(self._append(self.opts, {res:cols}))
-
-    def partition_by_col(self, all_but=False, cols={}, rows={}, const=None):
-
-        """
-        ex:
-
-        Opts.partition_by_col(cols ={'c1': True, 'c2': False, 'c3': None},
-                     rows ={'d1': True, 'd2':False, 'd3': None},
-                     const = 1)
-        will emit:
-
-        partition by(
-          column(
-            column(c1) auto compress,
-            column(c2) no auto compress,
-            column(c3),
-            row(d1) auto compress,
-            row(d2) no auto compress,
-            row(d3))
-            add 1
-            )
-
-        cols is a dictionary whose key is the column name and value True or False
-        specifying AUTO COMPRESS or NO AUTO COMPRESS respectively. The columns
-        are stored with COLUMN format.
-
-        rows is a dictionary similar to cols except the ROW format is used
-
-        const is an unsigned BIGINT
-        """
-        res = 'partition by( column all but' if all_but else\
-                        'partition by( column'
-        c = self._visit_partition_by(cols, rows)
-        c += [{'post': (['add %s' % str(const)]
-            if const is not None
-            else []) + [')']}]
-
-        return self.__class__(self._append(self.opts, {res: c}))
-
-    def _visit_partition_by(self, cols, rows):
-
-        if cols:
-            c = ['column('+ k +') auto compress '\
-                            for k,v in cols.items() if v is True]
-
-            c += ['column('+ k +') no auto compress'\
-                            for k,v in cols.items() if v is False]
-
-            c += ['column('+ k +')' for k,v in cols.items() if v is None]
-
-        if rows:
-            c += ['row('+ k +') auto compress'\
-                            for k,v in rows.items() if v is True]
-
-            c += ['row('+ k +') no auto compress'\
-                            for k,v in rows.items() if v is False]
-
-            c += ['row('+ k +')' for k,v in rows.items() if v is None]
-
-        return c
-
-    def partition_by_col_auto_compress(self, all_but=False, cols={},\
-                                       rows={}, const=None):
-
-        res = 'partition by( column auto compress all but' if all_but else\
-                        'partition by( column auto compress'
-        c = self._visit_partition_by(cols,rows)
-        c += [{'post': (['add %s' % str(const)]
-            if const is not None
-            else []) + [')']}]
-
-        return self.__class__(self._append(self.opts, {res: c}))
-
-
-    def partition_by_col_no_auto_compress(self, all_but=False, cols={},\
-                                          rows={}, const=None):
-
-        res = 'partition by( column no auto compress all but' if all_but else\
-                        'partition by( column no auto compression'
-        c = self._visit_partition_by(cols,rows)
-        c += [{'post': (['add %s' % str(const)]
-            if const is not None
-            else []) + [')']}]
-
-        return self.__class__(self._append(self.opts, {res: c}))
-
-
-    def index(self, index):
-        """
-        Index is created with dialect specific keywords to
-        include loading and ordering syntax elements
-
-        index is a sqlalchemy.sql.schema.Index object.
-        """
-        return self.__class__(self._append(self.opts, {res: c}))
-
-
-    def unique_index(self, name=None, cols=[]):
-        res = 'unique index ' + (name if name is not None else '')
-        return self.__class__(self._append(self.opts, {res:cols}))
-
-@deprecated('16.20.0.5', 'dialect.TeradataTypeCompiler')
-class TeradataTypeCompiler(compiler.GenericTypeCompiler):
-
-    def _get(self, key, type_, kw):
-        return kw.get(key, getattr(type_, key, None))
-
-    def visit_datetime(self, type_, **kw):
-        return self.visit_TIMESTAMP(type_, precision=6, **kw)
-
-    def visit_date(self, type_, **kw):
-        return self.visit_DATE(type_, **kw)
-
-    def visit_text(self, type_, **kw):
-        return self.visit_CLOB(type_, **kw)
-
-    def visit_time(self, type_, **kw):
-        return self.visit_TIME(type_, precision=6, **kw)
-
-    def visit_unicode(self, type_, **kw):
-        return self.visit_VARCHAR(type_, charset='UNICODE', **kw)
-
-    def visit_unicode_text(self, type_, **kw):
-        return self.visit_CLOB(type_, charset='UNICODE', **kw)
-
-    def visit_boolean(self, type_, **kw):
-        return self.visit_BYTEINT(type_, **kw)
-
-    def visit_INTERVAL_YEAR(self, type_, **kw):
-        return 'INTERVAL YEAR{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_YEAR_TO_MONTH(self, type_, **kw):
-        return 'INTERVAL YEAR{} TO MONTH'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_MONTH(self, type_, **kw):
-        return 'INTERVAL MONTH{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY(self, type_, **kw):
-        return 'INTERVAL DAY{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY_TO_HOUR(self, type_, **kw):
-        return 'INTERVAL DAY{} TO HOUR'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY_TO_MINUTE(self, type_, **kw):
-        return 'INTERVAL DAY{} TO MINUTE'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY_TO_SECOND(self, type_, **kw):
-        return 'INTERVAL DAY{} TO SECOND{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '',
-            '('+str(type_.frac_precision)+')' if type_.frac_precision is not None  else '')
-
-    def visit_INTERVAL_HOUR(self, type_, **kw):
-        return 'INTERVAL HOUR{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_HOUR_TO_MINUTE(self, type_, **kw):
-        return 'INTERVAL HOUR{} TO MINUTE'.format(
-            '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_HOUR_TO_SECOND(self, type_, **kw):
-        return 'INTERVAL HOUR{} TO SECOND{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '',
-            '('+str(type_.frac_precision)+')' if type_.frac_precision is not None else '')
-
-    def visit_INTERVAL_MINUTE(self, type_, **kw):
-        return 'INTERVAL MINUTE{}'.format(
-              '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_INTERVAL_MINUTE_TO_SECOND(self, type_, **kw):
-        return 'INTERVAL MINUTE{} TO SECOND{}'.format(
-            '('+str(type_.precision)+')' if type_.precision else '',
-            '('+str(type_.frac_precision)+')' if type_.frac_precision is not None else '')
-
-    def visit_INTERVAL_SECOND(self, type_, **kw):
-        if type_.frac_precision is not None and type_.precision:
-          return 'INTERVAL SECOND{}'.format(
-              '('+str(type_.precision)+', '+str(type_.frac_precision)+')')
-        else:
-          return 'INTERVAL SECOND{}'.format(
-              '('+str(type_.precision)+')' if type_.precision else '')
-
-    def visit_PERIOD_DATE(self, type_, **kw):
-        return 'PERIOD(DATE)' +\
-            (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
-
-    def visit_PERIOD_TIME(self, type_, **kw):
-        return 'PERIOD(TIME{}{})'.format(
-                '(' + str(type_.frac_precision) + ')'
-                    if type_.frac_precision is not None
-                    else '',
-                ' WITH TIME ZONE' if type_.timezone else '') +\
-            (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
-
-    def visit_PERIOD_TIMESTAMP(self, type_, **kw):
-        return 'PERIOD(TIMESTAMP{}{})'.format(
-                '(' + str(type_.frac_precision) + ')'
-                    if type_.frac_precision is not None
-                    else '',
-                ' WITH TIME ZONE' if type_.timezone else '') +\
-            (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
-
-    def visit_TIME(self, type_, **kw):
-        tz = ' WITH TIME ZONE' if type_.timezone else ''
-        prec = self._get('precision', type_, kw)
-        prec = '%s' % '('+str(prec)+')' if prec is not None else ''
-        return 'TIME{}{}'.format(prec, tz)
-
-    def visit_TIMESTAMP(self, type_, **kw):
-        tz = ' WITH TIME ZONE' if type_.timezone else ''
-        prec = self._get('precision', type_, kw)
-        prec = '%s' % '('+str(prec)+')' if prec is not None else ''
-        return 'TIMESTAMP{}{}'.format(prec, tz)
-
-    def _string_process(self, type_, datatype, **kw):
-        length = self._get('length', type_, kw)
-        return 'TIME{}{}'.format(prec, tz)
-
-    def visit_TIMESTAMP(self, type_, **kw):
-        tz = ' WITH TIME ZONE' if type_.timezone else ''
-        prec = self._get('precision', type_, kw)
-        prec = '%s' % '('+str(prec)+')' if prec is not None else ''
-        return 'TIMESTAMP{}{}'.format(prec, tz)
-
-    def _string_process(self, type_, datatype, **kw):
-        length = self._get('length', type_, kw)
-        length = '(%s)' % length if length is not None  else ''
-
-        charset = self._get('charset', type_, kw)
-        charset = ' CHAR SET %s' % charset if charset is not None else ''
-
-        res = '{}{}{}'.format(datatype, length, charset)
-        return res
-
-    def visit_CHAR(self, type_, **kw):
-        return self._string_process(type_, 'CHAR', length=type_.length, **kw)
-
-    def visit_VARCHAR(self, type_, **kw):
-        if type_.length is None:
-            return self._string_process(type_, 'LONG VARCHAR', **kw)
-        else:
-            return self._string_process(type_, 'VARCHAR', length=type_.length, **kw)
-
-    def visit_CLOB(self, type_, **kw):
-        multi = self._get('multiplier', type_, kw)
-        if multi is not None and type_.length is not None:
-            length = str(type_.length) + multi
-            return self._string_process(type_, 'CLOB', length=length, **kw)
-
-        return self._string_process(type_, 'CLOB', **kw)
-
-    def visit_BYTEINT(self, type_, **kw):
-        return 'BYTEINT'
-
-    def visit_BYTE(self, type_, **kw):
-        return 'BYTE{}'.format(
-            '(' + str(type_.length) + ')' if type_.length is not None else '')
-
-    def visit_VARBYTE(self, type_, **kw):
-        return 'VARBYTE{}'.format(
-            '(' + str(type_.length) + ')' if type_.length is not None else '')
-
-    def visit_BLOB(self, type_, **kw):
-        multiplier = self._get('multiplier', type_, kw)
-        return 'BLOB{}'.format(
-            '(' + str(type_.length) + \
-                '{})'.format(multiplier if multiplier is not None else '')
-            if type_.length is not None else '')
-
-    def visit_NUMBER(self, type_, **kw):
-        args = (str(type_.precision), '') if type_.scale is None \
-               else (str(type_.precision), ', ' + str(type_.scale))
-        return 'NUMBER{}'.format(
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+from sqlalchemy import exc
+from sqlalchemy.sql import compiler
+from teradatasqlalchemy.utils import deprecated
+
+
+@deprecated('16.20.0.5', 'dialect.TeradataCompiler')
+class TeradataCompiler(compiler.SQLCompiler):
+
+    def get_select_precolumns(self, select, **kwargs):
+        """
+        handles the part of the select statement before the columns are specified.
+        Note: Teradata does not allow a 'distinct' to be specified when 'top' is
+              used in the same select statement.
+
+              Instead if a user specifies both in the same select clause,
+              the DISTINCT will be used with a ROW_NUMBER OVER(ORDER BY) subquery.
+        """
+
+        pre = select._distinct and "DISTINCT " or ""
+
+        #TODO: decide whether we can replace this with the recipe...
+        if (select._limit is not None and select._offset is None):
+            pre += "TOP %d " % (select._limit)
+
+        return pre
+
+    def visit_mod_binary(self, binary, operator, **kw):
+        return self.process(binary.left, **kw) + " MOD " + \
+            self.process(binary.right, **kw)
+
+    def visit_ne_binary(self, binary, operator, **kw):
+        return self.process(binary.left, **kw) + " <> " + \
+            self.process(binary.right, **kw)
+
+    def limit_clause(self, select, **kwargs):
+        """Limit after SELECT is implemented in get_select_precolumns"""
+        return ""
+
+@deprecated('16.20.0.5', 'dialect.TeradataDDLCompiler')
+class TeradataDDLCompiler(compiler.DDLCompiler):
+
+    def visit_create_index(self, create, include_schema=False,
+                           include_table_schema=True):
+        index = create.element
+        self._verify_index_table(index)
+        preparer = self.preparer
+        text = "CREATE "
+        if index.unique:
+            text += "UNIQUE "
+        text += "INDEX %s (%s) ON %s" \
+            % (
+                self._prepared_index_name(index,
+                                          include_schema=include_schema),
+                ', '.join(
+                    self.sql_compiler.process(
+                        expr, include_table=False, literal_binds=True) for
+                    expr in index.expressions),
+                preparer.format_table(index.table,
+                                      use_schema=include_table_schema)
+            )
+        return text
+
+    def create_table_suffix(self, table):
+        """
+        This hook processes the optional keyword teradata_suffixes
+        ex.
+        from teradatasqlalchemy.compiler import\
+                        TDCreateTableSuffix as Opts
+        t = Table( 'name', meta,
+                   ...,
+                   teradata_suffixes=Opts.
+                                      fallback().
+                                      log().
+                                      with_journal_table(t2.name)
+
+        CREATE TABLE name, fallback,
+        log,
+        with journal table = [database/user.]table_name(
+          ...
+        )
+
+        teradata_suffixes can also be a list of strings to be appended
+        in the order given.
+        """
+        post=table.dialect_kwargs['teradatasql_suffixes']
+
+        if isinstance(post, TDCreateTableSuffix):
+            if post.opts:
+                return ',\n' + post.compile()
+            else:
+                return post
+        elif post:
+            assert type(post) is list
+            res = ',\n ' + ',\n'.join(post)
+        else:
+            return ''
+
+    def post_create_table(self, table):
+
+        """
+        This hook processes the TDPostCreateTableOpts given by the
+        teradata_post_create dialect kwarg for Table.
+
+        Note that there are other dialect kwargs defined that could possibly
+        be processed here.
+
+        See the kwargs defined in dialect.TeradataDialect
+
+        Ex.
+        from teradatasqlalchemy.compiler import TDCreateTablePost as post
+        Table('t1', meta,
+               ...
+               ,
+               teradata_post_create = post().
+                                        fallback().
+                                        checksum('on').
+                                        mergeblockratio(85)
+
+        creates ddl for a table like so:
+
+        CREATE TABLE "t1" ,
+             checksum=on,
+             fallback,
+             mergeblockratio=85 (
+               ...
+        )
+
+        """
+        kw = table.dialect_kwargs['teradatasql_post_create']
+        if isinstance(kw, TDCreateTablePost):
+            if kw:
+              return '\n' + kw.compile()
+        return ''
+
+    def get_column_specification(self, column, **kwargs):
+
+        if column.table is None:
+            raise exc.CompileError(
+                "Teradata requires Table-bound columns "
+                "in order to generate DDL")
+
+        colspec = (self.preparer.format_column(column) + " " +\
+                        self.dialect.type_compiler.process(
+                          column.type, type_expression=column))
+
+        # Null/NotNull
+        if column.nullable is not None:
+            if not column.nullable or column.primary_key:
+                colspec += " NOT NULL"
+
+        return colspec
+
+class TeradataOptions(object):
+    """
+    An abstract base class for various schema object options
+    """
+    def _append(self, opts, val):
+        _opts=opts.copy()
+        _opts.update(val)
+        return _opts
+
+    @deprecated('16.20.0.5', 'dialect.TeradataOptions')
+    def compile(self):
+        """
+        processes the argument options and returns a string representation
+        """
+        pass
+
+    @deprecated('16.20.0.5', 'dialect.TeradataOptions')
+    def format_cols(self, key, val):
+
+        """
+        key is a string
+        val is a list of strings with an optional dict as the last element
+            the dict values are appended at the end of the col list
+        """
+        res = ''
+        col_expr = ', '.join([x for x in val if type(x) is str])
+
+        res += key + '( ' + col_expr + ' )'
+        if type(val[-1]) is dict:
+            # process syntax elements (dict) after cols
+            res += ' '.join( val[-1]['post'] )
+        return res
+
+@deprecated('16.20.0.5', 'dialect.TDCreateTableSuffix')
+class TDCreateTableSuffix(TeradataOptions):
+    """
+    A generative class for Teradata create table options
+    specified in teradata_suffixes
+    """
+
+    def __init__(self, opts={}):
+        """
+        opts is a dictionary that can be pre-populated with key-value pairs
+        that may be overidden if the keys conflict with those entered
+        in the methods below. See the compile method to see how the dict
+        gets processed.
+        """
+        self.opts = opts
+
+    def compile(self):
+        def process_opts(opts):
+            return [key if opts[key] is None else '{}={}'.\
+                            format(key, opts[key]) for key in opts]
+
+        res = ',\n'.join(process_opts(self.opts))
+        return res
+
+    def fallback(self, enabled=True):
+        res = 'fallback' if enabled else 'no fallback'
+        return self.__class__(self._append(self.opts, {res:None}))
+
+    def log(self, enabled=True):
+        res = 'log' if enabled else 'no log'
+        return self.__class__(self._append(self.opts, {res:None}))
+
+    def with_journal_table(self, tablename=None):
+        """
+        tablename is the schema.tablename of a table.
+        For example, if t1 is a SQLAlchemy:
+                with_journal_table(t1.name)
+        """
+        return self.__class__(self._append(self.opts,\
+                        {'with journal table':tablename}))
+
+    def before_journal(self, prefix='dual'):
+        """
+        prefix is a string taking vaues of 'no' or 'dual'
+        """
+        assert prefix in ('no', 'dual')
+        res = prefix+' '+'before journal'
+        return self.__class__(self._append(self.opts, {res:None}))
+
+    def after_journal(self, prefix='not local'):
+        """
+        prefix is a string taking vaues of 'no', 'dual', 'local',
+        or 'not local'.
+        """
+        assert prefix in ('no', 'dual', 'local', 'not local')
+        res = prefix+' '+'after journal'
+        return self.__class__(self._append(self.opts, {res:None}))
+
+    def checksum(self, integrity_checking='default'):
+        """
+        integrity_checking is a string taking vaues of 'on', 'off',
+        or 'default'.
+        """
+        assert integrity_checking in ('on', 'off', 'default')
+        return self.__class__(self._append(self.opts,\
+                        {'checksum':integrity_checking}))
+
+    def freespace(self, percentage=0):
+        """
+        percentage is an integer taking values from 0 to 75.
+        """
+        return self.__class__(self._append(self.opts,\
+                        {'freespace':percentage}))
+
+    def no_mergeblockratio(self):
+        return self.__class__(self._append(self.opts,\
+                        {'no mergeblockratio':None}))
+
+    def mergeblockratio(self, integer=None):
+        """
+        integer takes values from 0 to 100 inclusive.
+        """
+        res = 'default mergeblockratio' if integer is None\
+                                        else 'mergeblockratio'
+        return self.__class__(self._append(self.opts, {res:integer}))
+
+    def min_datablocksize(self):
+            return self.__class__(self._append(self.opts,\
+                            {'minimum datablocksize':None}))
+
+    def max_datablocksize(self):
+        return self.__class__(self._append(self.opts,\
+                        {'maximum datablocksize':None}))
+
+    def datablocksize(self, data_block_size=None):
+        """
+        data_block_size is an integer specifying the number of bytes
+        """
+        res = 'datablocksize' if data_block_size is not None\
+                              else 'default datablocksize'
+        return self.__class__(self._append(self.opts,\
+                                           {res:data_block_size}))
+
+    def blockcompression(self, opt='default'):
+        """
+        opt is a string that takes values 'autotemp',
+        'default', 'manual', or 'never'
+        """
+        return self.__class__(self._append(self.opts,\
+                        {'blockcompression':opt}))
+
+    def with_no_isolated_loading(self, concurrent=False):
+        res = 'with no ' +\
+            ('concurrent ' if concurrent else '') +\
+            'isolated loading'
+        return self.__class__(self._append(self.opts, {res:None}))
+
+    def with_isolated_loading(self, concurrent=False, opt=None):
+        """
+        opt is a string that takes values 'all', 'insert', 'none',
+        or None
+        """
+        assert opt in ('all', 'insert', 'none', None)
+        for_stmt = ' for ' + opt if opt is not None else ''
+        res = 'with ' +\
+            ('concurrent ' if concurrent else '') +\
+            'isolated loading' + for_stmt
+        return self.__class__(self._append(self.opts, {res:None}))
+
+@deprecated('16.20.0.5', 'dialect.TDCreateTablePost')
+class TDCreateTablePost(TeradataOptions):
+    """
+    A generative class for building post create table options
+    given in the teradata_post_create keyword for Table
+    """
+
+    @deprecated('16.20.0.5', 'dialect.TDCreateTablePost')
+    def __init__(self, opts={}):
+        self.opts = opts
+
+    def compile(self):
+        def process(opts):
+            return [key.upper() if opts[key] is None\
+                       else self.format_cols(key, opts[key])\
+                       for key in opts]
+
+        return ',\n'.join(process(self.opts))
+
+    def no_primary_index(self):
+        return self.__class__(self._append(self.opts, {'no primary index':None}))
+
+    def primary_index(self, name=None, unique=False, cols=[]):
+        """
+        name is a string for the primary index
+        if unique is true then unique primary index is specified
+        cols is a list of column names
+        """
+        res = 'unique primary index' if unique else 'primary index'
+        res += ' ' + name if name is not None else ''
+        return self.__class__(self._append(self.opts, {res:cols}))
+
+
+    def primary_amp(self, name=None, cols=[]):
+
+        """
+        name is an optional string for the name of the amp index
+        cols is a list of column names (strings)
+        """
+        res = 'primary amp index'
+        res += ' ' + name if name is not None else ''
+        return self.__class__(self._append(self.opts, {res:cols}))
+
+    def partition_by_col(self, all_but=False, cols={}, rows={}, const=None):
+
+        """
+        ex:
+
+        Opts.partition_by_col(cols ={'c1': True, 'c2': False, 'c3': None},
+                     rows ={'d1': True, 'd2':False, 'd3': None},
+                     const = 1)
+        will emit:
+
+        partition by(
+          column(
+            column(c1) auto compress,
+            column(c2) no auto compress,
+            column(c3),
+            row(d1) auto compress,
+            row(d2) no auto compress,
+            row(d3))
+            add 1
+            )
+
+        cols is a dictionary whose key is the column name and value True or False
+        specifying AUTO COMPRESS or NO AUTO COMPRESS respectively. The columns
+        are stored with COLUMN format.
+
+        rows is a dictionary similar to cols except the ROW format is used
+
+        const is an unsigned BIGINT
+        """
+        res = 'partition by( column all but' if all_but else\
+                        'partition by( column'
+        c = self._visit_partition_by(cols, rows)
+        c += [{'post': (['add %s' % str(const)]
+            if const is not None
+            else []) + [')']}]
+
+        return self.__class__(self._append(self.opts, {res: c}))
+
+    def _visit_partition_by(self, cols, rows):
+
+        if cols:
+            c = ['column('+ k +') auto compress '\
+                            for k,v in cols.items() if v is True]
+
+            c += ['column('+ k +') no auto compress'\
+                            for k,v in cols.items() if v is False]
+
+            c += ['column('+ k +')' for k,v in cols.items() if v is None]
+
+        if rows:
+            c += ['row('+ k +') auto compress'\
+                            for k,v in rows.items() if v is True]
+
+            c += ['row('+ k +') no auto compress'\
+                            for k,v in rows.items() if v is False]
+
+            c += ['row('+ k +')' for k,v in rows.items() if v is None]
+
+        return c
+
+    def partition_by_col_auto_compress(self, all_but=False, cols={},\
+                                       rows={}, const=None):
+
+        res = 'partition by( column auto compress all but' if all_but else\
+                        'partition by( column auto compress'
+        c = self._visit_partition_by(cols,rows)
+        c += [{'post': (['add %s' % str(const)]
+            if const is not None
+            else []) + [')']}]
+
+        return self.__class__(self._append(self.opts, {res: c}))
+
+
+    def partition_by_col_no_auto_compress(self, all_but=False, cols={},\
+                                          rows={}, const=None):
+
+        res = 'partition by( column no auto compress all but' if all_but else\
+                        'partition by( column no auto compression'
+        c = self._visit_partition_by(cols,rows)
+        c += [{'post': (['add %s' % str(const)]
+            if const is not None
+            else []) + [')']}]
+
+        return self.__class__(self._append(self.opts, {res: c}))
+
+
+    def index(self, index):
+        """
+        Index is created with dialect specific keywords to
+        include loading and ordering syntax elements
+
+        index is a sqlalchemy.sql.schema.Index object.
+        """
+        return self.__class__(self._append(self.opts, {res: c}))
+
+
+    def unique_index(self, name=None, cols=[]):
+        res = 'unique index ' + (name if name is not None else '')
+        return self.__class__(self._append(self.opts, {res:cols}))
+
+@deprecated('16.20.0.5', 'dialect.TeradataTypeCompiler')
+class TeradataTypeCompiler(compiler.GenericTypeCompiler):
+
+    def _get(self, key, type_, kw):
+        return kw.get(key, getattr(type_, key, None))
+
+    def visit_datetime(self, type_, **kw):
+        return self.visit_TIMESTAMP(type_, precision=6, **kw)
+
+    def visit_date(self, type_, **kw):
+        return self.visit_DATE(type_, **kw)
+
+    def visit_text(self, type_, **kw):
+        return self.visit_CLOB(type_, **kw)
+
+    def visit_time(self, type_, **kw):
+        return self.visit_TIME(type_, precision=6, **kw)
+
+    def visit_unicode(self, type_, **kw):
+        return self.visit_VARCHAR(type_, charset='UNICODE', **kw)
+
+    def visit_unicode_text(self, type_, **kw):
+        return self.visit_CLOB(type_, charset='UNICODE', **kw)
+
+    def visit_boolean(self, type_, **kw):
+        return self.visit_BYTEINT(type_, **kw)
+
+    def visit_INTERVAL_YEAR(self, type_, **kw):
+        return 'INTERVAL YEAR{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_YEAR_TO_MONTH(self, type_, **kw):
+        return 'INTERVAL YEAR{} TO MONTH'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_MONTH(self, type_, **kw):
+        return 'INTERVAL MONTH{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY(self, type_, **kw):
+        return 'INTERVAL DAY{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY_TO_HOUR(self, type_, **kw):
+        return 'INTERVAL DAY{} TO HOUR'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY_TO_MINUTE(self, type_, **kw):
+        return 'INTERVAL DAY{} TO MINUTE'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY_TO_SECOND(self, type_, **kw):
+        return 'INTERVAL DAY{} TO SECOND{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '',
+            '('+str(type_.frac_precision)+')' if type_.frac_precision is not None  else '')
+
+    def visit_INTERVAL_HOUR(self, type_, **kw):
+        return 'INTERVAL HOUR{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_HOUR_TO_MINUTE(self, type_, **kw):
+        return 'INTERVAL HOUR{} TO MINUTE'.format(
+            '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_HOUR_TO_SECOND(self, type_, **kw):
+        return 'INTERVAL HOUR{} TO SECOND{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '',
+            '('+str(type_.frac_precision)+')' if type_.frac_precision is not None else '')
+
+    def visit_INTERVAL_MINUTE(self, type_, **kw):
+        return 'INTERVAL MINUTE{}'.format(
+              '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_INTERVAL_MINUTE_TO_SECOND(self, type_, **kw):
+        return 'INTERVAL MINUTE{} TO SECOND{}'.format(
+            '('+str(type_.precision)+')' if type_.precision else '',
+            '('+str(type_.frac_precision)+')' if type_.frac_precision is not None else '')
+
+    def visit_INTERVAL_SECOND(self, type_, **kw):
+        if type_.frac_precision is not None and type_.precision:
+          return 'INTERVAL SECOND{}'.format(
+              '('+str(type_.precision)+', '+str(type_.frac_precision)+')')
+        else:
+          return 'INTERVAL SECOND{}'.format(
+              '('+str(type_.precision)+')' if type_.precision else '')
+
+    def visit_PERIOD_DATE(self, type_, **kw):
+        return 'PERIOD(DATE)' +\
+            (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
+
+    def visit_PERIOD_TIME(self, type_, **kw):
+        return 'PERIOD(TIME{}{})'.format(
+                '(' + str(type_.frac_precision) + ')'
+                    if type_.frac_precision is not None
+                    else '',
+                ' WITH TIME ZONE' if type_.timezone else '') +\
+            (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
+
+    def visit_PERIOD_TIMESTAMP(self, type_, **kw):
+        return 'PERIOD(TIMESTAMP{}{})'.format(
+                '(' + str(type_.frac_precision) + ')'
+                    if type_.frac_precision is not None
+                    else '',
+                ' WITH TIME ZONE' if type_.timezone else '') +\
+            (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
+
+    def visit_TIME(self, type_, **kw):
+        tz = ' WITH TIME ZONE' if type_.timezone else ''
+        prec = self._get('precision', type_, kw)
+        prec = '%s' % '('+str(prec)+')' if prec is not None else ''
+        return 'TIME{}{}'.format(prec, tz)
+
+    def visit_TIMESTAMP(self, type_, **kw):
+        tz = ' WITH TIME ZONE' if type_.timezone else ''
+        prec = self._get('precision', type_, kw)
+        prec = '%s' % '('+str(prec)+')' if prec is not None else ''
+        return 'TIMESTAMP{}{}'.format(prec, tz)
+
+    def _string_process(self, type_, datatype, **kw):
+        length = self._get('length', type_, kw)
+        return 'TIME{}{}'.format(prec, tz)
+
+    def visit_TIMESTAMP(self, type_, **kw):
+        tz = ' WITH TIME ZONE' if type_.timezone else ''
+        prec = self._get('precision', type_, kw)
+        prec = '%s' % '('+str(prec)+')' if prec is not None else ''
+        return 'TIMESTAMP{}{}'.format(prec, tz)
+
+    def _string_process(self, type_, datatype, **kw):
+        length = self._get('length', type_, kw)
+        length = '(%s)' % length if length is not None  else ''
+
+        charset = self._get('charset', type_, kw)
+        charset = ' CHAR SET %s' % charset if charset is not None else ''
+
+        res = '{}{}{}'.format(datatype, length, charset)
+        return res
+
+    def visit_CHAR(self, type_, **kw):
+        return self._string_process(type_, 'CHAR', length=type_.length, **kw)
+
+    def visit_VARCHAR(self, type_, **kw):
+        if type_.length is None:
+            return self._string_process(type_, 'LONG VARCHAR', **kw)
+        else:
+            return self._string_process(type_, 'VARCHAR', length=type_.length, **kw)
+
+    def visit_CLOB(self, type_, **kw):
+        multi = self._get('multiplier', type_, kw)
+        if multi is not None and type_.length is not None:
+            length = str(type_.length) + multi
+            return self._string_process(type_, 'CLOB', length=length, **kw)
+
+        return self._string_process(type_, 'CLOB', **kw)
+
+    def visit_BYTEINT(self, type_, **kw):
+        return 'BYTEINT'
+
+    def visit_BYTE(self, type_, **kw):
+        return 'BYTE{}'.format(
+            '(' + str(type_.length) + ')' if type_.length is not None else '')
+
+    def visit_VARBYTE(self, type_, **kw):
+        return 'VARBYTE{}'.format(
+            '(' + str(type_.length) + ')' if type_.length is not None else '')
+
+    def visit_BLOB(self, type_, **kw):
+        multiplier = self._get('multiplier', type_, kw)
+        return 'BLOB{}'.format(
+            '(' + str(type_.length) + \
+                '{})'.format(multiplier if multiplier is not None else '')
+            if type_.length is not None else '')
+
+    def visit_NUMBER(self, type_, **kw):
+        args = (str(type_.precision), '') if type_.scale is None \
+               else (str(type_.precision), ', ' + str(type_.scale))
+        return 'NUMBER{}'.format(
             '' if type_.precision is None else '({}{})'.format(*args))
```

## teradatasqlalchemy/dialect.py

 * *Ordering differences only*

```diff
@@ -1,1896 +1,1896 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-from itertools import groupby
-from sqlalchemy import pool, String, Numeric
-from sqlalchemy import Table, Column, Index
-from sqlalchemy.engine import default
-from sqlalchemy.sql import select, and_, or_, elements
-from sqlalchemy.sql.expression import text, table, column, asc, bindparam
-from teradatasqlalchemy.base import TeradataIdentifierPreparer, TeradataExecutionContext
-from teradatasqlalchemy.resolver import TeradataTypeResolver
-from teradatasqlalchemy import TIMESTAMP, DATE
-from teradatasqlalchemy.options import configure
-
-from sqlalchemy import exc
-from sqlalchemy.sql import compiler
-
-import sqlalchemy.types as sqltypes
-import teradatasqlalchemy.types as tdtypes
-from sqlalchemy.exc import OperationalError
-from sqlalchemy.sql.functions import func
-
-import json
-import re
-import datetime
-
-# ischema names is used for reflecting columns (see get_columns in the dialect)
-ischema_names = {
-
-    # SQL standard types (modified only to extend _TDComparable)
-    'I': tdtypes.INTEGER,
-    'I2': tdtypes.SMALLINT,
-    'I8': tdtypes.BIGINT,
-    'D': tdtypes.DECIMAL,
-    'DA': tdtypes.DATE,
-
-    # Numeric types
-    'I1': tdtypes.BYTEINT,
-    'F': tdtypes.FLOAT,
-    'N': tdtypes.NUMBER,
-
-    # Character types
-    'CF': tdtypes.CHAR,
-    'CV': tdtypes.VARCHAR,
-    'CO': tdtypes.CLOB,
-
-    # Datetime types
-    'TS': tdtypes.TIMESTAMP,
-    'SZ': tdtypes.TIMESTAMP,  # Timestamp with timezone
-    'AT': tdtypes.TIME,
-    'TZ': tdtypes.TIME,  # Time with timezone
-
-    # Binary types
-    'BF': tdtypes.BYTE,
-    'BV': tdtypes.VARBYTE,
-    'BO': tdtypes.BLOB,
-
-    # Interval types
-    'DH': tdtypes.INTERVAL_DAY_TO_HOUR,
-    'DM': tdtypes.INTERVAL_DAY_TO_MINUTE,
-    'DS': tdtypes.INTERVAL_DAY_TO_SECOND,
-    'DY': tdtypes.INTERVAL_DAY,
-    'HM': tdtypes.INTERVAL_HOUR_TO_MINUTE,
-    'HR': tdtypes.INTERVAL_HOUR,
-    'HS': tdtypes.INTERVAL_HOUR_TO_SECOND,
-    'MI': tdtypes.INTERVAL_MINUTE,
-    'MO': tdtypes.INTERVAL_MONTH,
-    'MS': tdtypes.INTERVAL_MINUTE_TO_SECOND,
-    'SC': tdtypes.INTERVAL_SECOND,
-    'YM': tdtypes.INTERVAL_YEAR_TO_MONTH,
-    'YR': tdtypes.INTERVAL_YEAR,
-
-    # Period types
-    'PD': tdtypes.PERIOD_DATE,
-    'PT': tdtypes.PERIOD_TIME,
-    'PZ': tdtypes.PERIOD_TIME,
-    'PS': tdtypes.PERIOD_TIMESTAMP,
-    'PM': tdtypes.PERIOD_TIMESTAMP,
-
-    # XML type
-    'XM': tdtypes.XML,
-
-    # JSON type
-    'JN': tdtypes.JSON,
-
-    # UDT
-    'UT': tdtypes.TDUDT
-
-}
-
-stringtypes = [t for t in ischema_names if issubclass(ischema_names[t], sqltypes.String)]
-# JSON data type in Teradata also has CharacterSet argument.
-stringtypes.append('JN')
-
-
-class TeradataCompiler(compiler.SQLCompiler):
-
-    def __init__(self, dialect, statement, **kwargs):
-        super(TeradataCompiler, self).__init__(dialect=dialect,
-                                               statement=statement,
-                                               **kwargs)
-
-    def get_select_precolumns(self, select, **kwargs):
-        """
-        handles the part of the select statement before the columns are specified.
-        Note: Teradata does not allow a 'distinct' to be specified when 'top' is
-              used in the same select statement.
-
-              Instead if a user specifies both in the same select clause,
-              the DISTINCT will be used with a ROW_NUMBER OVER(ORDER BY) subquery.
-        """
-
-        pre = select._distinct and "DISTINCT " or ""
-
-        # TODO: decide whether we can replace this with the recipe...
-        if (select._limit is not None and select._offset is None):
-            pre += "TOP %d " % (select._limit)
-
-        return pre
-
-    def visit_mod_binary(self, binary, operator, **kw):
-        return self.process(binary.left, **kw) + " MOD " + \
-               self.process(binary.right, **kw)
-
-    def visit_ne_binary(self, binary, operator, **kw):
-        return self.process(binary.left, **kw) + " <> " + \
-               self.process(binary.right, **kw)
-
-    def limit_clause(self, select, **kwargs):
-        """Limit after SELECT is implemented in get_select_precolumns"""
-        return ""
-
-    def visit_truediv_binary(self, binary, operator, **kw):
-        return (
-            self.process(binary.left, **kw)
-            + " / "
-            # TODO: would need a fast cast again here,
-            # unless we want to use an implicit cast like "+ 0.0"
-            + self.process(
-                elements.Cast(
-                    binary.right,
-                    binary.right.type
-                    if binary.right.type._type_affinity is sqltypes.Numeric
-                    else tdtypes.NUMBER(),
-                ),
-                **kw,
-            )
-        )
-
-    def visit_floordiv_binary(self, binary, operator, **kw):
-        return "FLOOR(%s)" % (
-                self.process(binary.left, **kw)
-                + " / "
-                + self.process(binary.right, **kw)
-            )
-
-
-class TeradataDDLCompiler(compiler.DDLCompiler):
-
-    def visit_create_index(self, create, include_schema=False,
-                           include_table_schema=True):
-        index = create.element
-        self._verify_index_table(index)
-        preparer = self.preparer
-        text = "CREATE "
-        if index.unique:
-            text += "UNIQUE "
-        text += "INDEX %s (%s) ON %s" \
-                % (
-                    self._prepared_index_name(index,
-                                              include_schema=include_schema),
-                    ', '.join(
-                        self.sql_compiler.process(
-                            expr, include_table=False, literal_binds=True) for
-                        expr in index.expressions),
-                    preparer.format_table(index.table,
-                                          use_schema=include_table_schema)
-                )
-        return text
-
-    def create_table_suffix(self, table):
-        """
-        This hook processes the optional keyword teradata_suffixes
-        ex.
-        from teradatasqlalchemy.compiler import\
-                        TDCreateTableSuffix as Opts
-        t = Table( 'name', meta,
-                   ...,
-                   teradata_suffixes=Opts.
-                                      fallback().
-                                      log().
-                                      with_journal_table(t2.name)
-
-        CREATE TABLE name, fallback,
-        log,
-        with journal table = [database/user.]table_name(
-          ...
-        )
-
-        teradata_suffixes can also be a list of strings to be appended
-        in the order given.
-        """
-        post = table.dialect_kwargs['teradatasql_suffixes']
-
-        if isinstance(post, TDCreateTableSuffix):
-            if post.opts:
-                return ',\n' + post.compile()
-            else:
-                return post
-        elif post:
-            assert type(post) is list
-            res = ',\n ' + ',\n'.join(post)
-        else:
-            return ''
-
-    def post_create_table(self, table):
-
-        """
-        This hook processes the TDPostCreateTableOpts given by the
-        teradata_post_create dialect kwarg for Table.
-
-        Note that there are other dialect kwargs defined that could possibly
-        be processed here.
-
-        See the kwargs defined in dialect.TeradataDialect
-
-        Ex.
-        from teradatasqlalchemy.compiler import TDCreateTablePost as post
-        Table('t1', meta,
-               ...
-               ,
-               teradata_post_create = post().
-                                        fallback().
-                                        checksum('on').
-                                        mergeblockratio(85)
-
-        creates ddl for a table like so:
-
-        CREATE TABLE "t1" ,
-             checksum=on,
-             fallback,
-             mergeblockratio=85 (
-               ...
-        )
-
-        """
-        kw = table.dialect_kwargs['teradatasql_post_create']
-        if isinstance(kw, TDCreateTablePost):
-            if kw:
-                return '\n' + kw.compile()
-        return ''
-
-    def get_column_specification(self, column, **kwargs):
-
-        if column.table is None:
-            raise exc.CompileError(
-                "Teradata requires Table-bound columns "
-                "in order to generate DDL")
-
-        colspec = (self.preparer.format_column(column) + " " + \
-                   self.dialect.type_compiler.process(
-                       column.type, type_expression=column))
-
-        # Null/NotNull
-        if column.nullable is not None:
-            if not column.nullable or column.primary_key:
-                colspec += " NOT NULL"
-
-        return colspec
-
-
-class TeradataTypeCompiler(compiler.GenericTypeCompiler):
-
-    def _get(self, key, type_, kw):
-        return kw.get(key, getattr(type_, key, None))
-
-    def visit_datetime(self, type_, **kw):
-        return self.visit_TIMESTAMP(type_, precision=6, **kw)
-
-    def visit_date(self, type_, **kw):
-        return self.visit_DATE(type_, **kw)
-
-    def visit_text(self, type_, **kw):
-        return self.visit_CLOB(type_, **kw)
-
-    def visit_time(self, type_, **kw):
-        return self.visit_TIME(type_, precision=6, **kw)
-
-    def visit_unicode(self, type_, **kw):
-        return self.visit_VARCHAR(type_, charset='UNICODE', **kw)
-
-    def visit_unicode_text(self, type_, **kw):
-        return self.visit_CLOB(type_, charset='UNICODE', **kw)
-
-    def visit_boolean(self, type_, **kw):
-        return self.visit_BYTEINT(type_, **kw)
-
-    def visit_INTERVAL_YEAR(self, type_, **kw):
-        return 'INTERVAL YEAR{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_YEAR_TO_MONTH(self, type_, **kw):
-        return 'INTERVAL YEAR{} TO MONTH'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_MONTH(self, type_, **kw):
-        return 'INTERVAL MONTH{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY(self, type_, **kw):
-        return 'INTERVAL DAY{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY_TO_HOUR(self, type_, **kw):
-        return 'INTERVAL DAY{} TO HOUR'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY_TO_MINUTE(self, type_, **kw):
-        return 'INTERVAL DAY{} TO MINUTE'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_DAY_TO_SECOND(self, type_, **kw):
-        return 'INTERVAL DAY{} TO SECOND{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '',
-            '(' + str(type_.frac_precision) + ')' if type_.frac_precision is not None else '')
-
-    def visit_INTERVAL_HOUR(self, type_, **kw):
-        return 'INTERVAL HOUR{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_HOUR_TO_MINUTE(self, type_, **kw):
-        return 'INTERVAL HOUR{} TO MINUTE'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_HOUR_TO_SECOND(self, type_, **kw):
-        return 'INTERVAL HOUR{} TO SECOND{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '',
-            '(' + str(type_.frac_precision) + ')' if type_.frac_precision is not None else '')
-
-    def visit_INTERVAL_MINUTE(self, type_, **kw):
-        return 'INTERVAL MINUTE{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_INTERVAL_MINUTE_TO_SECOND(self, type_, **kw):
-        return 'INTERVAL MINUTE{} TO SECOND{}'.format(
-            '(' + str(type_.precision) + ')' if type_.precision else '',
-            '(' + str(type_.frac_precision) + ')' if type_.frac_precision is not None else '')
-
-    def visit_INTERVAL_SECOND(self, type_, **kw):
-        if type_.frac_precision is not None and type_.precision:
-            return 'INTERVAL SECOND{}'.format(
-                '(' + str(type_.precision) + ', ' + str(type_.frac_precision) + ')')
-        else:
-            return 'INTERVAL SECOND{}'.format(
-                '(' + str(type_.precision) + ')' if type_.precision else '')
-
-    def visit_PERIOD_DATE(self, type_, **kw):
-        return 'PERIOD(DATE)' + \
-               (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
-
-    def visit_PERIOD_TIME(self, type_, **kw):
-        return 'PERIOD(TIME{}{})'.format(
-            '(' + str(type_.frac_precision) + ')'
-            if type_.frac_precision is not None
-            else '',
-            ' WITH TIME ZONE' if type_.timezone else '') + \
-               (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
-
-    def visit_PERIOD_TIMESTAMP(self, type_, **kw):
-        return 'PERIOD(TIMESTAMP{}{})'.format(
-            '(' + str(type_.frac_precision) + ')'
-            if type_.frac_precision is not None
-            else '',
-            ' WITH TIME ZONE' if type_.timezone else '') + \
-               (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
-
-    def visit_TIME(self, type_, **kw):
-        tz = ' WITH TIME ZONE' if type_.timezone else ''
-        prec = self._get('precision', type_, kw)
-        prec = '%s' % '(' + str(prec) + ')' if prec is not None else ''
-        return 'TIME{}{}'.format(prec, tz)
-
-    def visit_TIMESTAMP(self, type_, **kw):
-        tz = ' WITH TIME ZONE' if type_.timezone else ''
-        prec = self._get('precision', type_, kw)
-        prec = '%s' % '(' + str(prec) + ')' if prec is not None else ''
-        return 'TIMESTAMP{}{}'.format(prec, tz)
-
-    def _string_process(self, type_, datatype, **kw):
-        length = self._get('length', type_, kw)
-        length = '(%s)' % length if length is not None else ''
-
-        charset = self._get('charset', type_, kw)
-        charset = ' CHAR SET %s' % charset if charset is not None else ''
-
-        res = '{}{}{}'.format(datatype, length, charset)
-        return res
-
-    def visit_CHAR(self, type_, **kw):
-        return self._string_process(type_, 'CHAR', length=type_.length, **kw)
-
-    def visit_VARCHAR(self, type_, **kw):
-        if type_.length is None:
-            return self._string_process(type_, 'LONG VARCHAR', **kw)
-        else:
-            return self._string_process(type_, 'VARCHAR', length=type_.length, **kw)
-
-    def visit_CLOB(self, type_, **kw):
-        multi = self._get('multiplier', type_, kw)
-        if multi is not None and type_.length is not None:
-            length = str(type_.length) + multi
-            return self._string_process(type_, 'CLOB', length=length, **kw)
-
-        return self._string_process(type_, 'CLOB', **kw)
-
-    def visit_BYTEINT(self, type_, **kw):
-        return 'BYTEINT'
-
-    def visit_BYTE(self, type_, **kw):
-        return 'BYTE{}'.format(
-            '(' + str(type_.length) + ')' if type_.length is not None else '')
-
-    def visit_VARBYTE(self, type_, **kw):
-        return 'VARBYTE{}'.format(
-            '(' + str(type_.length) + ')' if type_.length is not None else '')
-
-    def visit_BLOB(self, type_, **kw):
-        multiplier = self._get('multiplier', type_, kw)
-        return 'BLOB{}'.format(
-            '(' + str(type_.length) + \
-            '{})'.format(multiplier if multiplier is not None else '')
-            if type_.length is not None else '')
-
-    def visit_NUMBER(self, type_, **kw):
-        args = (str(type_.precision), '') if type_.scale is None \
-            else (str(type_.precision), ', ' + str(type_.scale))
-        return 'NUMBER{}'.format(
-            '' if type_.precision is None else '({}{})'.format(*args))
-
-    def visit_XML(self, type_, **kw):
-        return 'XML({0}) INLINE LENGTH {1}'.format(str(type_.maximum_length),
-                                                   str(type_.inline_length))
-
-    def visit_JSON(self, type_, **kw):
-        sec_part = ''
-        # Adding charset if it is not None, storage_format is only being added
-        # if it doesn't hold its default value: 'TEXT'. Reason for this being,
-        # 'TEXT' is not a valid type name for STORAGE FORMAT.
-        if type_.charset is not None:
-            sec_part = 'CHARACTER SET {}'.format(type_.charset)
-        if type_.storage_format is not None and type_.storage_format != 'TEXT':
-            sec_part = '{0} STORAGE FORMAT {1}'.format(sec_part, type_.storage_format)
-
-        return 'JSON({0}) INLINE LENGTH {1} {2}'.format(str(type_.max_length),
-                                                        str(type_.inline_length),
-                                                        str(sec_part))
-
-    def visit_TDUDT(self, type_, **kw):
-        return 'TDUDT{}'.format('' if type_.type_name is None else '(UDTName: {0})'.format(str(type_.type_name)))
-
-    def visit_GEOMETRY(self, type_, **kw):
-        return 'ST_GEOMETRY({0}) INLINE LENGTH {1}'.format(str(type_.max_length),
-                                                           str(type_.inline_length))
-
-    def visit_MBR(self, type_, **kw):
-        return 'MBR'
-
-    def visit_MBB(self, type_, **kw):
-        return 'MBB'
-
-
-class TeradataDialect(default.DefaultDialect):
-    name = 'teradatasql'
-    driver = 'teradatasql'
-    paramstyle = 'qmark'
-    default_paramstyle = 'qmark'
-    poolclass = pool.SingletonThreadPool
-
-    statement_compiler = TeradataCompiler
-    ddl_compiler = TeradataDDLCompiler
-    type_compiler = TeradataTypeCompiler
-    preparer = TeradataIdentifierPreparer
-    execution_ctx_cls = TeradataExecutionContext
-
-    supports_native_boolean = False
-    supports_native_decimal = True
-    supports_unicode_statements = True
-    supports_unicode_binds = True
-    postfetch_lastrowid = False
-    implicit_returning = False
-    preexecute_autoincrement_sequences = False
-    case_sensitive = False
-    supports_statement_cache = False
-
-    construct_arguments = [
-        (Table, {
-            "post_create": None,
-            "suffixes": None
-        }),
-
-        (Index, {
-            "order_by": None,
-            "loading": None
-        }),
-
-        (Column, {
-            "compress": None,
-            "identity": None
-        })
-    ]
-
-    def __init__(self, **kwargs):
-        super(TeradataDialect, self).__init__(**kwargs)
-
-        # Method to append 'X' at the end of string when "usexviews" is set to 'True'.
-        self.__get_xviews_obj = lambda db_obj: db_obj + 'X' if configure.usexviews else db_obj
-
-    def create_connect_args(self, url):
-
-        params = super(TeradataDialect, self).create_connect_args(url)[1]
-
-        if 'username' in params:
-            sUserName = params.pop('username')
-            if 'user' not in params:  # user URL parameter has higher priority than username prefix before host
-                params['user'] = sUserName
-
-        if 'port' in params:
-            params['dbs_port'] = str(params['port'])
-            del params['port']
-
-        args = json.dumps(params),  # single-element tuple
-        kwargs = {}
-        return (args, kwargs)
-
-    @classmethod
-    def dbapi(cls):
-
-        """ Hook to the dbapi2.0 implementation's module"""
-        import teradatasql
-        return teradatasql
-
-    @classmethod
-    def import_dbapi(cls):
-
-        """ Hook to the dbapi2.0 implementation's module"""
-        import teradatasql
-        return teradatasql
-
-    def normalize_name(self, name, **kw):
-        if name is not None:
-            return name.strip()
-        return name
-
-    def _is_table_volatile(self, connection, table_name, schema=None):
-        """ Internal function to check if the table is a volatile table or not """
-
-        isVolatile = False
-        # Volatile tables are always created in users login space.
-        # This means that the schema either has to be None or the same as the login user space (the login user name).
-        if schema is not None and schema.lower() != self._get_login_user_space(connection).lower():
-            return isVolatile
-
-        res = self._get_volatile_tables_list(connection, table_name)
-        for r in res:
-            if r.lower() == table_name.lower():
-                isVolatile = True
-                break
-
-        return isVolatile
-
-    def _get_volatile_tables_list(self, connection, table_name=None):
-        """ Internal function to get a list of all volatile tables in the current session """
-        # TODO: The check can be made convenient by allowing the option to check for only a single table when the
-        #       command is modified to 'help volatile table <tablename>' which is currently failing.
-
-        stmt = 'help volatile table'
-        res = connection.execute(text(stmt))
-
-        return [row['Table Dictionary Name'] for row in res.mappings()]
-
-    def has_table(self, connection, table_name, schema=None, **kw):
-        """
-        DESCRIPTION:
-            Function to check for the presence of a table in the schema provided, if any.
-            Note:
-                By default presence of a table is checked irrespective of whether
-                user has access to the table or not. This provides faster lookup.
-                When option 'configure.usexviews' is set to True, then search for
-                the table happens in only user accessible tables.
-
-        PARAMETERS:
-            connection:
-                Required argument.
-                A SQLAlchemy connection object.
-
-            table_name:
-                Required argument.
-                The name of the table to search for.
-
-            schema:
-                Optional argument.
-                The schema to search the table in.
-                By default, the default schema will be searched.
-
-        RETURNS:
-            A Boolean value indicating whether a table named table_name is present or not.
-
-        RAISES:
-            None.
-
-        EXAMPLES:
-            # Example 1 - Check table 'mytable' exists.
-            >>> table_exists = has_table(conn, 'mytable', schema='myschema')
-
-            # Example 2 - Check if table 'mytable' exists and user has access.
-            >>> from teradatasqlalchemy.options import configure
-            >>> configure.usexviews=True
-            >>> table_exists = has_table(conn, 'mytable', schema='myschema')
-
-        """
-        schema_name = self.default_schema_name if schema is None else schema
-
-        # Default value for 'usexviews' is False so use dbc.tablesV by default
-        # which is faster.
-        dbc_tables = self.__get_xviews_obj("tablesV")
-
-        # Permanent tables (TableKind in 'O', 'Q', 'T')
-        table_obj = table(dbc_tables, column('DatabaseName'), column('TableName'),
-                          column('TableKind'), schema='dbc')
-        stmt = select(table_obj.c.TableName) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
-                        text('TableName=:table_name'),
-                        text("TableKind IN ('O', 'Q', 'T')")))
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema=schema_name, table_name=table_name)
-        res = connection.execute(stmt).fetchone()
-        table_present = res is not None
-
-        # Volatile tables
-        if not table_present:
-            table_present = self._is_table_volatile(connection, table_name, schema)
-
-        return table_present
-
-    def has_view(self, connection, view_name, schema=None):
-        """
-        DESCRIPTION:
-            Function to check for the presence of a view in the schema provided, if any.
-            Note:
-                By default presence of a view is checked irrespective of whether
-                user has access to the view or not. This provides faster lookup.
-                When option 'configure.usexviews' is set to True, then search for
-                the view happens in only user accessible views.
-
-        PARAMETERS:
-            connection:
-                Required argument.
-                A SQLAlchemy connection object.
-
-            view_name:
-                Required argument.
-                The name of the view to search for.
-
-            schema:
-                Optional argument.
-                The schema to search the view in.
-                By default, the default schema will be searched.
-
-        RETURNS:
-            A Boolean value indicating whether a view named view_name is present or not.
-
-        RAISES:
-            None.
-
-        EXAMPLES:
-            # Example 1 - Check view 'myview' exists.
-            >>> view_exists = has_view(conn, 'myview', schema='myschema')
-
-            # Example 2 - Check if view 'myview' exists and user has access.
-            >>> from teradatasqlalchemy.options import configure
-            >>> usexviews=True
-            >>> view_exists = has_view(conn, 'myview', schema='myschema')
-        """
-        schema_name = self.default_schema_name if schema is None else schema
-
-        # Default value for 'usexviews' is False so use dbc.tablesV by default
-        # which is faster.
-        dbc_tables = self.__get_xviews_obj("tablesV")
-
-        table_obj = table(dbc_tables, column('DatabaseName'), column('TableName'),
-                          column('TableKind'), schema='dbc')
-        # Views (TableKind = 'V')
-        stmt = select(table_obj.c.TableName) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
-                        text('TableName=:view_name'),
-                        text("TableKind (NOT CASESPECIFIC) = 'V' (NOT CASESPECIFIC)")))
-
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema=schema_name, view_name=view_name)
-        res = connection.execute(stmt).fetchone()
-        view_present = res is not None
-
-        return view_present
-
-    def _resolve_udt_type(self, **kw):
-        """
-        Resolves the types for UDT columns.
-        """
-        col_udt = kw["col_udt_name"]
-        if col_udt == 'ST_GEOMETRY':
-            return tdtypes.GEOMETRY
-        elif col_udt == 'MBR':
-            return tdtypes.MBR
-        elif col_udt == 'MBB':
-            return tdtypes.MBB
-        else:
-            return tdtypes.TDUDT
-
-    def _resolve_type(self, t, **kw):
-        """
-        Resolves the types for String, Numeric, Date/Time, etc. columns.
-        """
-        tc = self.normalize_name(t)
-        if tc in ischema_names:
-            if tc == 'UT':
-                type_ = self._resolve_udt_type(**kw)
-            else:
-                type_ = ischema_names[tc]
-            return TeradataTypeResolver().process(type_, typecode=tc, **kw)
-
-        return sqltypes.NullType
-
-    def _get_column_info(self, row):
-        """
-        Resolves the column information for get_columns given a row.
-        """
-        chartype = {
-            0: None,
-            1: 'LATIN',
-            2: 'UNICODE',
-            3: 'KANJISJIS',
-            4: 'GRAPHIC'
-        }
-
-        # Handle unspecified characterset and disregard chartypes specified for
-        # non-character types (e.g. binary)
-        character_set = row['CharType'] if self.normalize_name(row['ColumnType']) in stringtypes else 0
-
-        try:
-            inline_length = row['InlineLength']
-        except KeyError:
-            inline_length = 0
-
-        try:
-            storage_format = row['StorageFormat']
-        except KeyError:
-            storage_format = ""
-
-        try:
-            col_udt_name = row['ColumnUDTName']
-        except KeyError:
-            col_udt_name = ""
-
-        typ = self._resolve_type(row['ColumnType'],
-                                 length=int(row['ColumnLength'] or 0),
-                                 chartype=chartype[character_set],
-                                 prec=int(row['DecimalTotalDigits'] or 0),
-                                 scale=int(row['DecimalFractionalDigits'] or 0),
-                                 fmt=row['ColumnFormat'],
-                                 inline_length=inline_length,
-                                 storage_format=storage_format,
-                                 col_udt_name=col_udt_name)
-
-        autoinc = row['IdColType'] in ('GA', 'GD')
-
-        # attrs contains all the attributes queried from DBC.ColumnsV
-        attrs = {self.normalize_name(k): row[k] for k in row.keys()}
-        col_info = {
-            'name': self.normalize_name(row['ColumnName']),
-            'type': typ,
-            'nullable': row['Nullable'] == u'Y',
-            'default': row['DefaultValue'],
-            'autoincrement': autoinc
-        }
-
-        return dict(attrs, **col_info)
-
-    def get_columns(self, connection, table_name, schema=None, **kw):
-
-        # Check if table is a volatile table before the schema is set to the default schema.
-        isVolatile = self._is_table_volatile(connection, table_name, schema)
-
-        if schema is None:
-            schema = self.default_schema_name
-
-        # Using 'help schema.table.*' statements has been considered.
-        # The DBC.ColumnsV provides the default value which is not available
-        # with the 'help column' commands result.
-
-        # Check if the object is a view
-        schema_name = ':schema'
-        # Default value for 'usexviews' is False so use dbc.tablesV by default
-        dbc_tables = self.__get_xviews_obj("tablesV")
-        table_obj = table(dbc_tables, column('DatabaseName'), column('TableName'),
-                          column('tablekind'), column('TVMFlavor'), schema='dbc')
-
-        is_view_c = "CASE WHEN {} = 'V' THEN 1 ELSE 0 END as is_view".format(table_obj.c.tablekind)
-        is_art_c = "CASE WHEN {} = 'A' THEN 1 ELSE 0 END as is_art_table".format(table_obj.c.TVMFlavor)
-
-        stmt = select(text(is_view_c + ', ' + is_art_c)) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + schema_name + ' (NOT CASESPECIFIC)'),
-                        text('TableName=:table_name')))
-
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema=schema, table_name=table_name)
-        res = connection.execute(stmt)
-        isView, is_art_table = next(res) if res.rowcount > 0 else (False, False)
-
-        if isView or isVolatile:
-            # Volatile table definition is not stored in the dictionary.
-            # We use the 'help schema.table.*' command instead to get information for all columns.
-            # We have to do the same for views since we need the type information
-            # which is not available in dbc.ColumnsV.
-            res = self._get_column_help(connection, schema, table_name, column_name=None)
-
-            # If this is a view, get types for individual columns (dbc.ColumnsV won't have types for view columns).
-            # For a view or a volatile table, we have to set the default values as the 'help' command does not have it.
-            col_info_list = []
-            for r in res:
-                updated_column_info_dict = self._update_column_help_info(r._mapping)
-                col_info_list.append(dict(r._mapping, **(updated_column_info_dict)))
-            res = col_info_list
-        else:
-            # Default value for 'usexviews' is False so use dbc.ColumnsV by default
-            dbc_columns = self.__get_xviews_obj("ColumnsV")
-            # For permanent tables.
-            # ORDER BY ColumnId added to make sure the columns are retrieved in order
-            # of their appearance in the table. HELP COLUMN already maintains that order.
-            table_obj = table(dbc_columns, schema='dbc')
-            stmt = select(text('*')) \
-                .select_from(table_obj) \
-                .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + schema_name + ' (NOT CASESPECIFIC)'),
-                            text('TableName=:table_name'))) \
-                .order_by(text("ColumnId"))
-
-            stmt = text(str(stmt))
-            stmt = stmt.bindparams(schema=schema, table_name=table_name)
-            res = connection.execute(stmt).fetchall()
-            res =[res_row._mapping for res_row in res]
-
-        final_column_info = []
-        # For ART Tables, every layer columns appear in output table. Columns which do not belongs to
-        # primary layer will have TSColumnType as 'AS'. Also, some columns will be duplicated so remove
-        # duplicates.
-        if is_art_table:
-            _col_names = set()
-            for row in res:
-                col_info = self._get_column_info(row)
-                if col_info['TSColumnType'].strip() == 'AS' or col_info['ColumnName'] in _col_names:
-                    continue
-                final_column_info.append(col_info)
-                _col_names.add(col_info['ColumnName'])
-        else:
-            # Ignore the non-functional column in a PTI table
-            for row in res:
-                col_info = self._get_column_info(row)
-                if 'TSColumnType' in col_info and col_info['TSColumnType'] is not None:
-                    if col_info['ColumnName'] == 'TD_TIMEBUCKET' and col_info['TSColumnType'].strip() == 'TB':
-                        continue
-                final_column_info.append(col_info)
-
-        return final_column_info
-
-    def _get_default_schema_name(self, connection):
-        res = self.normalize_name(
-            connection.execute(text('select database')).scalar())
-        return res
-
-    def _get_login_user_space(self, connection):
-        """
-        Internal function to get the current users login user space.
-        This is currently used for volatile table checks.
-        """
-        res = self.normalize_name(
-            connection.execute(text('select user')).scalar())
-        return res
-
-    def _get_column_help(self, connection, schema, table_name, column_name):
-        """
-        Internal function to get the help on a column, when provided, else all columns
-        for a table provided using the HELP COLUMN command.
-
-        :param connection:  SQLAlchemy connection object.
-        :param schema:      Schema name of the table/view to run 'HELP COLUMN' command for.
-        :param table_name:  The name of the table/view to run 'HELP COLUMN' command for.
-        :param column_name: Optional column name to get information only about the column.
-
-        :return: The result of the HELP COLUMN command:
-                 * A list of dictionaries when column_name is not provided.
-                 * A dictionary when column_name is provided.
-        """
-        prepared = preparer(dialect())
-        stmt = 'help column ' + prepared.quote(schema) + '.' + prepared.quote(table_name) + '.' \
-               + (prepared.quote(column_name) if column_name else '*')
-
-        result_set = connection.execute(text(stmt)).fetchall()
-        return result_set[0] if column_name else result_set
-
-    def _update_column_help_info(self, res):
-        """
-        Internal function to create new fields in the result dictionary to have the field names
-        similar to those for tables.
-        """
-        return {
-            'ColumnName': res['Column Dictionary Name'],
-            'ColumnType': res['Type'],
-            'ColumnLength': res['Max Length'],
-            'CharType': res['Char Type'],
-            'DecimalTotalDigits': res['Decimal Total Digits'],
-            'DecimalFractionalDigits': res['Decimal Fractional Digits'],
-            'ColumnFormat': res['Format'],
-            'Nullable': res['Nullable'],
-            'DefaultValue': None,
-            'IdColType': res['IdCol Type'],
-            'TSColumnType': res['Time Series Column Type'] if 'Time Series Column Type' in res else None,
-            'ColumnUDTName': res['UDT Dictionary Name'] if 'UDT Dictionary Name' in res else None,
-            'InlineLength': res['Inline Length'] if 'Inline Length' in res else None
-        }
-
-    def get_table_names(self, connection, schema=None, **kw):
-        """
-        DESCRIPTION:
-            Function to get all the available table names in the schema provided, if any.
-            Note:
-                By default function returns all tables available irrespective of whether
-                user has access to the table or not. This provides faster lookup. In
-                case user wants to list down all available tables which user has
-                access to, then set option 'configure.usexviews' to True.
-
-        PARAMETERS:
-            connection:
-                Required argument.
-                A SQLAlchemy connection object.
-
-            schema:
-                Optional argument.
-                The schema to search the table in.
-                By default, the default schema will be searched.
-
-        RETURNS:
-            A list of table names.
-
-        RAISES:
-            None.
-
-        EXAMPLES:
-            # Example 1 - Get all the available tables.
-            >>> tables = get_table_names(conn, schema='myschema')
-
-            # Example 2 - Get all the user accessible tables.
-            >>> from teradatasqlalchemy.options import configure
-            >>> configure.usexviews=True
-            >>> tables = get_table_names(conn, schema='myschema')
-        """
-        if schema is None:
-            schema = self.default_schema_name
-
-        # Default value for 'usexviews' is False so use dbc.tablesV by default
-        dbc_tables = self.__get_xviews_obj("tablesV")
-
-        table_obj = table(dbc_tables, column('tablename'), schema='dbc')
-        stmt = select(table_obj.c.tablename) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
-                        or_(text('tablekind=\'T\''),
-                            text('tablekind=\'O\''),
-                            text('tablekind=\'Q\''))))
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema=schema)
-        res = connection.execute(stmt).fetchall()
-        return [self.normalize_name(name._mapping['TableName']) for name in res]
-
-    def get_schema_names(self, connection, **kw):
-        """Retrieves names of Databases/Schemas on the system"""
-
-        # Default value for 'usexviews' is False so use dbc.DatabasesV by default
-        dbc_schemas = self.__get_xviews_obj("DatabasesV")
-        table_obj = table(dbc_schemas, column('DatabaseName'), schema='dbc')
-        stmt = select(table_obj.c.DatabaseName) \
-            .select_from(table_obj) \
-            .order_by(table_obj.c.DatabaseName)
-        res = connection.execute(stmt).fetchall()
-        return [self.normalize_name(name._mapping['DatabaseName']) for name in res]
-
-    def get_view_definition(self, connection, view_name, schema=None, **kw):
-
-        if schema is None:
-            schema = self.default_schema_name
-
-        res = connection.execute(text('show view {}.{}'.format(schema, view_name))).scalar()
-        return self.normalize_name(res)
-
-    def get_view_names(self, connection, schema=None, **kw):
-        """
-        DESCRIPTION:
-            Function to get all the available view names in the schema provided, if any.
-            Note:
-                By default function returns all views available irrespective of whether
-                user has access to the view or not. This provides faster lookup. In
-                case user wants to list down all available views which user has
-                access to, then set option 'configure.usexviews' to True.
-
-        PARAMETERS:
-            connection:
-                Required argument.
-                A SQLAlchemy connection object.
-
-            schema:
-                Optional argument.
-                The schema to search the view in.
-                By default, the default schema will be searched.
-
-        RETURNS:
-            A list of view names.
-
-        RAISES:
-            None.
-
-        EXAMPLES:
-            # Example 1 - Get all the available views.
-            >>> views = get_view_names(conn, schema='myschema')
-
-            # Example 2 - Get all the user accessible views.
-            >>> from teradatasqlalchemy.options import configure
-            >>> configure.usexviews=True
-            >>> tables = get_view_names(conn, schema='myschema')
-        """
-        if schema is None:
-            schema = self.default_schema_name
-
-        # Default value for 'usexviews' is False so use dbc.tablesV by default
-        # which is faster.
-        dbc_tables = self.__get_xviews_obj("tablesV")
-
-        table_obj = table(dbc_tables, column('tablename'), schema='dbc')
-        stmt = select(table_obj.c.tablename) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
-                        text('tablekind=\'V\'')))
-
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema=schema)
-        res = connection.execute(stmt).fetchall()
-        return [self.normalize_name(name._mapping['TableName']) for name in res]
-
-    def get_pk_constraint(self, connection, table_name, schema=None, **kw):
-        """
-        Override
-        TODO: Check if we need PRIMARY Indices or PRIMARY KEY Indices
-        TODO: Check for border cases (No PK Indices)
-        """
-
-        if schema is None:
-            schema = self.default_schema_name
-
-        # Default value for 'usexviews' is False so use dbc.IndicesV by default
-        dbc_indices = self.__get_xviews_obj("IndicesV")
-
-        table_obj = table(dbc_indices, column('ColumnName'), column('IndexName'),
-                          schema='dbc')
-        stmt = select(table_obj.c.ColumnName, table_obj.c.IndexName) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
-                        text('TableName=:table'), text('IndexType=:indextype'))) \
-            .order_by(asc(column('IndexNumber')))
-        stmt = text(str(stmt))
-        # K for Primary Key.
-        stmt = stmt.bindparams(schema=schema, table=table_name, indextype='K')
-        res = connection.execute(stmt).fetchall()
-
-        index_columns = list()
-        index_name = None
-
-        for index_column in res:
-            index_columns.append(self.normalize_name(index_column._mapping['ColumnName']))
-            index_name = self.normalize_name(index_column._mapping['IndexName'])  # There should be just one IndexName
-
-        return {
-            "constrained_columns": index_columns,
-            "name": index_name
-        }
-
-    def get_unique_constraints(self, connection, table_name, schema=None, **kw):
-        """
-        Overrides base class method
-        """
-        if schema is None:
-            schema = self.default_schema_name
-
-        # Default value for 'usexviews' is False so use dbc.IndicesV by default
-        dbc_indices = self.__get_xviews_obj("IndicesV")
-
-        table_obj = table(dbc_indices, column('ColumnName'), column('IndexName'), schema='dbc')
-        stmt = select(table_obj.c.ColumnName, table_obj.c.IndexName) \
-            .select_from(table_obj) \
-            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
-                        text('TableName=:table'),
-                        text('IndexType=:indextype'))) \
-            .order_by(asc(column('IndexName')))
-        stmt = text(str(stmt))
-        # U for Unique Key.
-        stmt = stmt.bindparams(schema=schema, table=table_name, indextype='U')
-        res = connection.execute(stmt).fetchall()
-
-        def grouper(fk_row):
-            return {
-                'name': self.normalize_name(fk_row.IndexName),
-            }
-
-        unique_constraints = list()
-        for constraint_info, constraint_cols in groupby(res, grouper):
-            unique_constraint = {
-                'name': self.normalize_name(constraint_info['name']),
-                'column_names': list()
-            }
-
-            for constraint_col in constraint_cols:
-                unique_constraint['column_names'].append(self.normalize_name(constraint_col._mapping['ColumnName']))
-
-            unique_constraints.append(unique_constraint)
-
-        return unique_constraints
-
-    def get_foreign_keys(self, connection, table_name, schema=None, **kw):
-        """
-        Overrides base class method
-        """
-
-        if schema is None:
-            schema = self.default_schema_name
-        # Default value for 'usexviews' is False so use DBC.All_RI_ChildrenV by default
-        dbc_child_parent_table = self.__get_xviews_obj("All_RI_ChildrenV")
-
-        table_obj = table(dbc_child_parent_table, column('IndexID'), column('IndexName'),
-                          column('ChildKeyColumn'), column('ParentDB'),
-                          column('ParentTable'), column('ParentKeyColumn'),
-                          schema='dbc')
-        stmt = select(table_obj.c.IndexID, table_obj.c.IndexName,
-                      table_obj.c.ChildKeyColumn, table_obj.c.ParentDB,
-                      table_obj.c.ParentTable, table_obj.c.ParentKeyColumn) \
-            .select_from(table_obj) \
-            .where(and_(text('ChildTable = :table'), text('ChildDB = :schema'))) \
-            .order_by(asc(column('IndexID')))
-
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema=schema, table=table_name)
-        res = connection.execute(stmt).fetchall()
-
-        def grouper(fk_row):
-            return {
-                'name': fk_row.IndexName or fk_row.IndexID,  # ID if IndexName is None
-                'schema': fk_row.ParentDB,
-                'table': fk_row.ParentTable
-            }
-
-        # TODO: Check if there's a better way
-        fk_dicts = list()
-        for constraint_info, constraint_cols in groupby(res, grouper):
-            fk_dict = {
-                'name': constraint_info['name'],
-                'constrained_columns': list(),
-                'referred_table': constraint_info['table'],
-                'referred_schema': constraint_info['schema'],
-                'referred_columns': list()
-            }
-
-            for constraint_col in constraint_cols:
-                fk_dict['constrained_columns'].append(self.normalize_name(constraint_col.ChildKeyColumn))
-                fk_dict['referred_columns'].append(self.normalize_name(constraint_col.ParentKeyColumn))
-
-            fk_dicts.append(fk_dict)
-
-        return fk_dicts
-
-    def get_indexes(self, connection, table_name, schema=None, **kw):
-        """
-        Overrides base class method
-        """
-
-        if schema is None:
-            schema = self.default_schema_name
-
-        indices = list()
-        prepared = preparer(dialect())
-        stmt = 'help index ' + prepared.quote(schema) + '.' + prepared.quote(table_name)
-        try:
-            res = connection.execute(text(stmt)).fetchall()
-            res_set = [row._mapping for row in res]
-            for row in res_set:
-                index_dict = {
-                    'name': row['Index Dictionary Name'],
-                    'column_names': list(),
-                    'unique': True if row['Unique?'] == 'Y' else False
-                }
-
-                if "," in row['Column Names']:
-                    index_cols = row['Column Names'].split(',')
-                else:
-                    index_cols = [row['Column Names']]
-                for index_col in index_cols:
-                    if index_col == 'TD_TIMEBUCKET' and 'Timebucket' in row and row['Timebucket'] is not None:
-                        continue
-                    else:
-                        index_dict['column_names'].append(self.normalize_name(index_col))
-
-                if len(index_dict['column_names']) > 0:
-                    indices.append(index_dict)
-        except OperationalError as err:
-            # This is to catch the following error when the object may be a view and not a table, Like:
-            # 1. [Error 3720] This view does not contain any complete index columns of the underlying table.
-            # 2. [Error 3823] VIEW 'xxx' may not be used for Help Index/Constraint/Statistics, Update, Delete or Insert.
-            if ('3720' in str(err) and 'This view does not contain any complete index columns of the underlying table'
-                in str(err)) or \
-                    ('3823' in str(
-                        err) and 'may not be used for Help Index/Constraint/Statistics, Update, Delete or Insert'
-                     in str(err)):
-                pass
-            else:
-                raise
-
-        return indices
-
-    def get_transaction_mode(self, connection, **kw):
-        """
-        Returns the transaction mode set for the current session.
-        T = TDBS
-        A = ANSI
-        """
-        # Default value for 'usexviews' is False so use dbc.sessioninfoV by default
-        dbc_sessions = self.__get_xviews_obj("sessioninfoV")
-
-        table_obj = table(dbc_sessions, column('transaction_mode'), column('sessionno'), schema='dbc')
-        stmt = select(table_obj.c.transaction_mode) \
-            .select_from(table_obj) \
-            .where(table_obj.c.sessionno == text('SESSION'))
-        res = connection.execute(stmt).scalar()
-        return res
-
-    def _get_server_version_info(self, connection, **kw):
-        """
-        Returns the Teradata Database software version.
-        """
-        table_obj = table('dbcinfov', column('InfoData'), column('InfoKey'), schema='dbc')
-        stmt = select(table_obj.c.InfoData) \
-            .select_from(table_obj) \
-            .where(table_obj.c.InfoKey == 'VERSION')
-        res = connection.execute(stmt).scalar()
-        return res
-
-    def conn_supports_autocommit(self, connection, **kw):
-        """
-        Returns True if autocommit is used for this connection (underlying Teradata session)
-        else False
-        """
-        return self.get_transaction_mode(connection) == 'T'
-
-    def _get_database_names(self, connection, schema_name):
-        """
-        Function to return a list valid of database names for a given sqlalchemy connection.
-        """
-        table_obj = table('databasesV', column('databasename'), schema='dbc')
-        stmt = select(text(str(func.lower(table_obj.c.databasename)) + ' as databasename')) \
-            .select_from(table_obj) \
-            .where(text('databasename (NOT CASESPECIFIC) = {} (NOT CASESPECIFIC)'.format(':schema_name')))
-        stmt = text(str(stmt))
-        stmt = stmt.bindparams(schema_name=schema_name)
-
-        res = connection.execute(stmt).fetchall()
-        return [name.databasename for name in res]
-
-
-dialect = TeradataDialect
-preparer = TeradataIdentifierPreparer
-compiler = TeradataCompiler
-
-
-class TeradataOptions(object):
-    """
-    An abstract base class for various schema object options
-    """
-    _prepare = preparer(dialect())
-
-    def _append(self, opts, val):
-        _opts = opts.copy()
-        _opts.update(val)
-        return _opts
-
-    def compile(self, **kw):
-        """
-        processes the argument options and returns a string representation
-        """
-        pass
-
-    def format_cols(self, key, val, **kw):
-        """
-        key is a string
-        val is a list of strings with an optional dict as the last element
-            the dict values are appended at the end of the col list
-        """
-        res = ''
-        col_expr = ', '.join([x for x in val if type(x) is str])
-
-        res += key + '( ' + col_expr + ' )'
-        if type(val[-1]) is dict:
-            # process syntax elements (dict) after cols
-            res += ' '.join(val[-1]['post'])
-        return res
-
-
-class TDCreateTableSuffix(TeradataOptions):
-    """
-    A generative class for Teradata create table options
-    specified in teradata_suffixes
-    """
-
-    def __init__(self, opts={}, **kw):
-        """
-        opts is a dictionary that can be pre-populated with key-value pairs
-        that may be overidden if the keys conflict with those entered
-        in the methods below. See the compile method to see how the dict
-        gets processed.
-        """
-        self.opts = opts
-
-    def compile(self):
-        def process_opts(opts):
-            return [key if opts[key] is None else '{}={}'. \
-                format(key, opts[key]) for key in opts]
-
-        res = ',\n'.join(process_opts(self.opts))
-        return res
-
-    def fallback(self, enabled=True):
-        res = 'fallback' if enabled else 'no fallback'
-        return self.__class__(self._append(self.opts, {res: None}))
-
-    def log(self, enabled=True):
-        res = 'log' if enabled else 'no log'
-        return self.__class__(self._append(self.opts, {res: None}))
-
-    def with_journal_table(self, tablename=None):
-        """
-        tablename is the schema.tablename of a table.
-        For example, if t1 is a SQLAlchemy:
-                with_journal_table(t1.name)
-        """
-        return self.__class__(self._append(self.opts, \
-                                           {'with journal table': tablename}))
-
-    def before_journal(self, prefix='dual'):
-        """
-        prefix is a string taking vaues of 'no' or 'dual'
-        """
-        assert prefix in ('no', 'dual')
-        res = prefix + ' ' + 'before journal'
-        return self.__class__(self._append(self.opts, {res: None}))
-
-    def after_journal(self, prefix='not local'):
-        """
-        prefix is a string taking vaues of 'no', 'dual', 'local',
-        or 'not local'.
-        """
-        assert prefix in ('no', 'dual', 'local', 'not local')
-        res = prefix + ' ' + 'after journal'
-        return self.__class__(self._append(self.opts, {res: None}))
-
-    def checksum(self, integrity_checking='default'):
-        """
-        integrity_checking is a string taking vaues of 'on', 'off',
-        or 'default'.
-        """
-        assert integrity_checking in ('on', 'off', 'default')
-        return self.__class__(self._append(self.opts, \
-                                           {'checksum': integrity_checking}))
-
-    def freespace(self, percentage=0):
-        """
-        percentage is an integer taking values from 0 to 75.
-        """
-        return self.__class__(self._append(self.opts, \
-                                           {'freespace': percentage}))
-
-    def no_mergeblockratio(self):
-        return self.__class__(self._append(self.opts, \
-                                           {'no mergeblockratio': None}))
-
-    def mergeblockratio(self, integer=None):
-        """
-        integer takes values from 0 to 100 inclusive.
-        """
-        res = 'default mergeblockratio' if integer is None \
-            else 'mergeblockratio'
-        return self.__class__(self._append(self.opts, {res: integer}))
-
-    def min_datablocksize(self):
-        return self.__class__(self._append(self.opts, \
-                                           {'minimum datablocksize': None}))
-
-    def max_datablocksize(self):
-        return self.__class__(self._append(self.opts, \
-                                           {'maximum datablocksize': None}))
-
-    def datablocksize(self, data_block_size=None):
-        """
-        data_block_size is an integer specifying the number of bytes
-        """
-        res = 'datablocksize' if data_block_size is not None \
-            else 'default datablocksize'
-        return self.__class__(self._append(self.opts, \
-                                           {res: data_block_size}))
-
-    def blockcompression(self, opt='default'):
-        """
-        opt is a string that takes values 'autotemp',
-        'default', 'manual', or 'never'
-        """
-        return self.__class__(self._append(self.opts, \
-                                           {'blockcompression': opt}))
-
-    def with_no_isolated_loading(self, concurrent=False):
-        res = 'with no ' + \
-              ('concurrent ' if concurrent else '') + \
-              'isolated loading'
-        return self.__class__(self._append(self.opts, {res: None}))
-
-    def with_isolated_loading(self, concurrent=False, opt=None):
-        """
-        opt is a string that takes values 'all', 'insert', 'none',
-        or None
-        """
-        assert opt in ('all', 'insert', 'none', None)
-        for_stmt = ' for ' + opt if opt is not None else ''
-        res = 'with ' + \
-              ('concurrent ' if concurrent else '') + \
-              'isolated loading' + for_stmt
-        return self.__class__(self._append(self.opts, {res: None}))
-
-
-class TDCreateTablePost(TeradataOptions):
-    """
-    A generative class for building post create table options
-    given in the teradata_post_create keyword for Table
-    """
-
-    def __init__(self, opts={}, **kw):
-        self.opts = opts
-
-    def compile(self, **kw):
-        def process(opts):
-            return [key.upper() if opts[key] is None \
-                        else self.format_cols(key, opts[key], **kw) \
-                    for key in opts if key != 'on commit']
-
-        def process_last(opts):
-            if 'on commit' in opts:
-                return '\n' + 'on commit {} rows'.format(opts['on commit'])
-            else:
-                return ''
-
-        return ',\n'.join(process(self.opts)) + process_last(self.opts)
-
-    def on_commit(self, option='delete'):
-        assert type(option) is str and option.lower() in ('preserve', 'delete')
-        return self.__class__(
-            self._append(self.opts, {'on commit': option}))
-
-    def no_primary_index(self):
-        return self.__class__(self._append(self.opts, {'no primary index': None}))
-
-    def primary_index(self, name=None, unique=False, cols=[]):
-        """
-        name is a string for the primary index
-        if unique is true then unique primary index is specified
-        cols is a list of column names
-        """
-        res = 'unique primary index' if unique else 'primary index'
-        res += ' ' + name if name is not None else ''
-        return self.__class__(self._append(self.opts, {res: [self._prepare.quote(c) for c in cols if c is not None]}))
-
-    def __validate_timecode_datatype(self, timecode_datatype):
-        """
-        Internal function to validate timecode_datatype specified when creating a
-        Primary Time Index (PTI) table.
-
-        :param timecode_datatype: The timecode_datatype passed to primary_time_index().
-
-        :return: Boolean value indicating whether the argument is valid (True),
-         or raise ValueError/TypeError when invalid.
-        """
-        # Raise a ValueError is argument is None or not specified - it is required
-        if timecode_datatype is None:
-            raise ValueError("'timecode_datatype' is a required argument and must not be None.")
-
-        valid_timecode_datatypes = [TIMESTAMP, DATE]
-        if type(timecode_datatype) not in valid_timecode_datatypes:
-            raise TypeError("timecode_datatype must be of one of the following types: {}. Found {}".
-                            format(valid_timecode_datatypes,
-                                   type(timecode_datatype)))
-
-        # Looks like the value is valid
-        return True
-
-    def __validate_timezero_date(self, timezero_date):
-        """
-        Internal function to validate timezero_date specified when creating a
-        Primary Time Index (PTI) table.
-
-        :param timezero_date: The timezero_date passed to primary_time_index().
-
-        :return: Boolean value indicating whether the argument is valid (True),
-         or raise ValueError when invalid.
-        """
-        # Return True is it is not specified or is None since it is optional
-        if timezero_date is None:
-            return True
-
-        pattern = re.compile(r"^DATE\s+'(.*)'$")
-        match = pattern.match(timezero_date)
-
-        err_msg = "Date format must be: DATE 'YYYY-MM-DD'. Found value with incorrect format: {}".format(timezero_date)
-        if match is not None:
-            try:
-                datetime.datetime.strptime(match.group(1), '%Y-%m-%d')
-            except ValueError:
-                raise ValueError(err_msg)
-        else:
-            raise ValueError(err_msg)
-
-        # Looks like the value is valid
-        return True
-
-    def __validate_columns_list(self, cols):
-        """
-        Internal function to validate columns list specified when creating a
-        Primary Time Index (PTI) table.
-
-        :param cols: The columns list (cols) passed to primary_time_index().
-
-        :return: Validated column list (list of strings).
-         Raise a ValueError/TypeError on validation failure.
-        """
-
-        err_msg = "'cols' must be a of type str or list of values of type str. Found: {}"
-        col_length_err = "Column name cannot be an empty string. Found: '{}'"
-
-        if cols is None:
-            return []
-
-        # Single column (string) specified
-        if isinstance(cols, str):
-            if len(cols) == 0:
-                raise ValueError(col_length_err.format(cols))
-            return [cols]
-        # list
-        elif isinstance(cols, list):
-            for col in cols:
-                # Must be a list of strings only
-                if not isinstance(col, str):
-                    raise TypeError(err_msg.format(list(map(type, cols))))
-                if len(col) == 0:
-                    raise ValueError(col_length_err.format(col))
-
-            # all columns in list validated to be strings
-            return cols
-        # neither a single string, nor a list
-        else:
-            raise TypeError(err_msg.format(type(cols)))
-
-    def __validate_timebucket_duration(self, timebucket_duration):
-        """
-        Internal function to validate timeduration_bucket specified when creating a
-        Primary Time Index (PTI) table.
-
-        :param timebucket_duration: The timebucket_duration passed to the primary_time_index().
-
-        :return: Boolean value indicating whether the argument is valid (True).
-         or raise ValueError when invalid.
-        """
-        # Return True is it is not specified or is None since it is optional
-        if timebucket_duration is None:
-            return True
-
-        if len(timebucket_duration) == 0:
-            raise ValueError("timebucket_duration cannot be an empty string.")
-
-        valid_timebucket_durations_formal = ['CAL_YEARS', 'CAL_MONTHS', 'WEEKS', 'DAYS', 'HOURS', 'MINUTES', 'SECONDS',
-                                             'MILLISECONDS', 'MICROSECONDS']
-        valid_timebucket_durations_shorthand = ['cy', 'cyear', 'cyears',
-                                                'cm', 'cmonth', 'cmonths',
-                                                'cd', 'cday', 'cdays',
-                                                'w', 'week', 'weeks',
-                                                'd', 'day', 'days',
-                                                'h', 'hr', 'hrs', 'hour', 'hours',
-                                                'm', 'mins', 'minute', 'minutes',
-                                                's', 'sec', 'secs', 'second', 'seconds',
-                                                'ms', 'msec', 'msecs', 'millisecond', 'milliseconds',
-                                                'us', 'usec', 'usecs', 'microsecond', 'microseconds']
-
-        # Message for error to be raise when n is invalid
-        n_err_msg = "'n' must be a positive integer. Found: {}"
-
-        # Check if notation if formal or shorthand (beginning with a digit)
-        if timebucket_duration[0].isdigit():
-            for short_notation in valid_timebucket_durations_shorthand:
-                pattern = re.compile("^([0-9]+){}$".format(short_notation))
-                match = pattern.match(timebucket_duration.lower())
-                if match is not None:
-                    try:
-                        n = int(match.group(1))
-                        if n < 0:
-                            raise ValueError(n_err_msg.format(n))
-
-                        # Looks like the value is valid
-                        return True
-                    except ValueError:
-                        raise ValueError(n_err_msg.format(match.group(1)))
-        else:
-            for formal_notation in valid_timebucket_durations_formal:
-                pattern = re.compile(r"^{}\(([0-9]+)\)$".format(formal_notation))
-                match = pattern.match(timebucket_duration.upper())
-                if match is not None:
-                    try:
-                        n = int(match.group(1))
-                        if n < 0:
-                            raise ValueError(n_err_msg.format(n))
-
-                        # Looks like the value is valid
-                        return True
-                    except ValueError:
-                        raise ValueError(n_err_msg.format(match.group(1)))
-
-        # Match not found
-        raise ValueError("Invalid timebucket_duration: {}".format(timebucket_duration))
-
-    def primary_time_index(self,
-                           timecode_datatype,
-                           name=None,
-                           timezero_date=None,
-                           timebucket_duration=None,
-                           sequenced=None,
-                           seq_max=None,
-                           cols=[]):
-        """
-        timecode_datatype:
-            Required Argument.
-            Reflection of the form of the timestamp data in the time series.
-            Permitted values:
-                A teradatasqlalchemy type representing either
-                * TIMESTAMP(n),
-                * TIMESTAMP(n) WITH TIME ZONE, or
-                * DATE.
-
-        name:
-            Optional Argument.
-            A name for the Primary Time Index (PTI).
-
-        timezero_date:
-            Optional Argument.
-            Specifies the earliest time series data that the PTI table will accept;
-            a date that precedes the earliest date in the time series data.
-            Value specified must be of the following format: DATE 'YYYY-MM-DD'
-            Default Value: DATE '1970-01-01'.
-
-        timebucket_duration:
-            Optional Argument.
-            Required if cols is not specified or is empty.
-            A duration that serves to break up the time continuum in
-            the time series data into discrete groups or buckets.
-            Specified using the formal form time_unit(n), where n is a positive
-            integer, and time_unit can be any of the following:
-            CAL_YEARS, CAL_MONTHS, CAL_DAYS, WEEKS, DAYS, HOURS, MINUTES,
-            SECONDS, MILLISECONDS, or MICROSECONDS.
-
-        sequenced:
-            Optional Argument.
-            Specifies whether the time series data readings are unique in time or not.
-            * True implies SEQUENCED, meaning more than one reading from the same
-              sensor may have the same timestamp.
-            * False implies NONSEQUENCED, meaning there is only one sensor reading
-              per timestamp.
-              This is the default.
-
-        seq_max:
-            Optional Argument.
-            Specifies the maximum number of sensor data rows that can have the
-            same timestamp. Can be used when 'sequenced' is True.
-            Accepted range:  1 - 2147483647.
-            Default Value: 20000.
-
-        cols:
-            Optional Argument.
-            Required if timebucket_duration is not specified.
-            A list of one or more PTI table column names.
-        """
-        # Validate timecode_datatype
-        self.__validate_timecode_datatype(timecode_datatype)
-
-        # Validate timebucket_duration
-        self.__validate_timebucket_duration(timebucket_duration)
-
-        # Validate timezero_date
-        self.__validate_timezero_date(timezero_date)
-
-        # Validate sequenced
-        if sequenced is not None:
-            if not isinstance(sequenced, bool):
-                raise TypeError("'sequenced', when specified, must be of type 'bool'. Found type: {}".
-                                format(type(sequenced)))
-
-        # Validate seq_max
-        if seq_max is not None:
-            if not isinstance(seq_max, int) or \
-                    (seq_max < 1 or seq_max > 2147483647):
-                raise ValueError("'seq_max' must be a positive integer in the range 1 through 2147483647. Found: {}".
-                                 format(seq_max))
-
-        # Validate cols
-        cols = self.__validate_columns_list(cols)
-
-        if timebucket_duration is None and len(cols) == 0:
-            raise SyntaxError("At least one of 'cols' or 'timebucket_duration' must be specified.")
-
-        res = 'PRIMARY TIME INDEX'
-        res += ' ' + name if name is not None else ''
-
-        val = [timecode_datatype.compile(dialect())]
-        if timezero_date is not None:
-            val.append(timezero_date)
-        if timebucket_duration is not None:
-            val.append(timebucket_duration)
-        if len(cols) > 0:
-            val.append('COLUMNS({})'.format(','.join([self._prepare.quote(c) for c in cols if c is not None])))
-        if sequenced is not None:
-            if sequenced:
-                val.append('SEQUENCED{}'.format('(' + str(seq_max) + ')' if seq_max is not None else ''))
-            else:
-                val.append('NONSEQUENCED')
-
-        return self.__class__(self._append(self.opts, {res: val}))
-
-    def primary_amp(self, name=None, cols=[]):
-
-        """
-        name is an optional string for the name of the amp index
-        cols is a list of column names (strings)
-        """
-        res = 'primary amp index'
-        res += ' ' + name if name is not None else ''
-        return self.__class__(self._append(self.opts, {res: [self._prepare.quote(c) for c in cols if c is not None]}))
-
-
-    def partition_by_col(self, all_but=False, cols={}, rows={}, const=None):
-
-        """
-        ex:
-
-        Opts.partition_by_col(cols ={'c1': True, 'c2': False, 'c3': None},
-                     rows ={'d1': True, 'd2':False, 'd3': None},
-                     const = 1)
-        will emit:
-
-        partition by(
-          column(
-            column(c1) auto compress,
-            column(c2) no auto compress,
-            column(c3),
-            row(d1) auto compress,
-            row(d2) no auto compress,
-            row(d3))
-            add 1
-            )
-
-        cols is a dictionary whose key is the column name and value True or False
-        specifying AUTO COMPRESS or NO AUTO COMPRESS respectively. The columns
-        are stored with COLUMN format.
-
-        rows is a dictionary similar to cols except the ROW format is used
-
-        const is an unsigned BIGINT
-        """
-        res = 'partition by( column all but' if all_but else \
-            'partition by( column'
-        c = self._visit_partition_by(cols, rows)
-        c += [{'post': (['add %s' % str(const)]
-                        if const is not None
-                        else []) + [')']}]
-
-        return self.__class__(self._append(self.opts, {res: c}))
-
-    def _visit_partition_by(self, cols, rows):
-
-        if cols:
-            c = ['column(' + self._prepare.quote(k) + ') auto compress ' \
-                 for k, v in cols.items() if v is True]
-
-            c += ['column(' + self._prepare.quote(k) + ') no auto compress' \
-                  for k, v in cols.items() if v is False]
-
-            c += ['column(' + self._prepare.quote(k) + ')' for k, v in cols.items() if v is None]
-
-        if rows:
-            c += ['row(' + k + ') auto compress' \
-                  for k, v in rows.items() if v is True]
-
-            c += ['row(' + k + ') no auto compress' \
-                  for k, v in rows.items() if v is False]
-
-            c += ['row(' + k + ')' for k, v in rows.items() if v is None]
-
-        return c
-
-    def partition_by_col_auto_compress(self, all_but=False, cols={}, \
-                                       rows={}, const=None):
-
-        res = 'partition by( column auto compress all but' if all_but else \
-            'partition by( column auto compress'
-        c = self._visit_partition_by(cols, rows)
-        c += [{'post': (['add %s' % str(const)]
-                        if const is not None
-                        else []) + [')']}]
-
-        return self.__class__(self._append(self.opts, {res: c}))
-
-    def partition_by_col_no_auto_compress(self, all_but=False, cols={}, \
-                                          rows={}, const=None):
-
-        res = 'partition by( column no auto compress all but' if all_but else \
-            'partition by( column no auto compression'
-        c = self._visit_partition_by(cols, rows)
-        c += [{'post': (['add %s' % str(const)]
-                        if const is not None
-                        else []) + [')']}]
-
-        return self.__class__(self._append(self.opts, {res: c}))
-
-    def index(self, index):
-        """
-        Index is created with dialect specific keywords to
-        include loading and ordering syntax elements
-
-        index is a sqlalchemy.sql.schema.Index object.
-        """
-        return self.__class__(self._append(self.opts, {res: c}))
-
-    def unique_index(self, name=None, cols=[]):
-        res = 'unique index ' + (name if name is not None else '')
-        return self.__class__(self._append(self.opts, {res: [self._prepare.quote(c) for c in cols if c is not None]}))
-
-# @compiles(Select, 'teradata')
-# def compile_select(element, compiler, **kw):
-#    """
-#    """
-#
-#    if not getattr(element, '_window_visit', None):
-#      if element._limit is not None or element._offset is not None:
-#          limit, offset = element._limit, element._offset
-#
-#          orderby=compiler.process(element._order_by_clause)
-#          if orderby:
-#            element = element._generate()
-#            element._window_visit=True
-#            #element._limit = None
-#            #element._offset = None  cant set to none...
-#
-#            # add a ROW NUMBER() OVER(ORDER BY) column
-#            element = element.column(sql.literal_column('ROW NUMBER() OVER (ORDER BY %s)' % orderby).label('rownum')).order_by(None)
-#
-#            # wrap into a subquery
-#            limitselect = sql.select([c for c in element.alias().c if c.key != 'rownum'])
-#
-#            limitselect._window_visit=True
-#            limitselect._is_wrapper=True
-#
-#            if offset is not None:
-#              limitselect.append_whereclause(sql.column('rownum') > offset)
-#              if limit is not None:
-#                  limitselect.append_whereclause(sql.column('rownum') <= (limit + offset))
-#            else:
-#              limitselect.append_whereclause(sql.column("rownum") <= limit)
-#
-#            element = limitselect
-#
-#    kw['iswrapper'] = getattr(element, '_is_wrapper', False)
-#    return compiler.visit_select(element, **kw)
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+from itertools import groupby
+from sqlalchemy import pool, String, Numeric
+from sqlalchemy import Table, Column, Index
+from sqlalchemy.engine import default
+from sqlalchemy.sql import select, and_, or_, elements
+from sqlalchemy.sql.expression import text, table, column, asc, bindparam
+from teradatasqlalchemy.base import TeradataIdentifierPreparer, TeradataExecutionContext
+from teradatasqlalchemy.resolver import TeradataTypeResolver
+from teradatasqlalchemy import TIMESTAMP, DATE
+from teradatasqlalchemy.options import configure
+
+from sqlalchemy import exc
+from sqlalchemy.sql import compiler
+
+import sqlalchemy.types as sqltypes
+import teradatasqlalchemy.types as tdtypes
+from sqlalchemy.exc import OperationalError
+from sqlalchemy.sql.functions import func
+
+import json
+import re
+import datetime
+
+# ischema names is used for reflecting columns (see get_columns in the dialect)
+ischema_names = {
+
+    # SQL standard types (modified only to extend _TDComparable)
+    'I': tdtypes.INTEGER,
+    'I2': tdtypes.SMALLINT,
+    'I8': tdtypes.BIGINT,
+    'D': tdtypes.DECIMAL,
+    'DA': tdtypes.DATE,
+
+    # Numeric types
+    'I1': tdtypes.BYTEINT,
+    'F': tdtypes.FLOAT,
+    'N': tdtypes.NUMBER,
+
+    # Character types
+    'CF': tdtypes.CHAR,
+    'CV': tdtypes.VARCHAR,
+    'CO': tdtypes.CLOB,
+
+    # Datetime types
+    'TS': tdtypes.TIMESTAMP,
+    'SZ': tdtypes.TIMESTAMP,  # Timestamp with timezone
+    'AT': tdtypes.TIME,
+    'TZ': tdtypes.TIME,  # Time with timezone
+
+    # Binary types
+    'BF': tdtypes.BYTE,
+    'BV': tdtypes.VARBYTE,
+    'BO': tdtypes.BLOB,
+
+    # Interval types
+    'DH': tdtypes.INTERVAL_DAY_TO_HOUR,
+    'DM': tdtypes.INTERVAL_DAY_TO_MINUTE,
+    'DS': tdtypes.INTERVAL_DAY_TO_SECOND,
+    'DY': tdtypes.INTERVAL_DAY,
+    'HM': tdtypes.INTERVAL_HOUR_TO_MINUTE,
+    'HR': tdtypes.INTERVAL_HOUR,
+    'HS': tdtypes.INTERVAL_HOUR_TO_SECOND,
+    'MI': tdtypes.INTERVAL_MINUTE,
+    'MO': tdtypes.INTERVAL_MONTH,
+    'MS': tdtypes.INTERVAL_MINUTE_TO_SECOND,
+    'SC': tdtypes.INTERVAL_SECOND,
+    'YM': tdtypes.INTERVAL_YEAR_TO_MONTH,
+    'YR': tdtypes.INTERVAL_YEAR,
+
+    # Period types
+    'PD': tdtypes.PERIOD_DATE,
+    'PT': tdtypes.PERIOD_TIME,
+    'PZ': tdtypes.PERIOD_TIME,
+    'PS': tdtypes.PERIOD_TIMESTAMP,
+    'PM': tdtypes.PERIOD_TIMESTAMP,
+
+    # XML type
+    'XM': tdtypes.XML,
+
+    # JSON type
+    'JN': tdtypes.JSON,
+
+    # UDT
+    'UT': tdtypes.TDUDT
+
+}
+
+stringtypes = [t for t in ischema_names if issubclass(ischema_names[t], sqltypes.String)]
+# JSON data type in Teradata also has CharacterSet argument.
+stringtypes.append('JN')
+
+
+class TeradataCompiler(compiler.SQLCompiler):
+
+    def __init__(self, dialect, statement, **kwargs):
+        super(TeradataCompiler, self).__init__(dialect=dialect,
+                                               statement=statement,
+                                               **kwargs)
+
+    def get_select_precolumns(self, select, **kwargs):
+        """
+        handles the part of the select statement before the columns are specified.
+        Note: Teradata does not allow a 'distinct' to be specified when 'top' is
+              used in the same select statement.
+
+              Instead if a user specifies both in the same select clause,
+              the DISTINCT will be used with a ROW_NUMBER OVER(ORDER BY) subquery.
+        """
+
+        pre = select._distinct and "DISTINCT " or ""
+
+        # TODO: decide whether we can replace this with the recipe...
+        if (select._limit is not None and select._offset is None):
+            pre += "TOP %d " % (select._limit)
+
+        return pre
+
+    def visit_mod_binary(self, binary, operator, **kw):
+        return self.process(binary.left, **kw) + " MOD " + \
+               self.process(binary.right, **kw)
+
+    def visit_ne_binary(self, binary, operator, **kw):
+        return self.process(binary.left, **kw) + " <> " + \
+               self.process(binary.right, **kw)
+
+    def limit_clause(self, select, **kwargs):
+        """Limit after SELECT is implemented in get_select_precolumns"""
+        return ""
+
+    def visit_truediv_binary(self, binary, operator, **kw):
+        return (
+            self.process(binary.left, **kw)
+            + " / "
+            # TODO: would need a fast cast again here,
+            # unless we want to use an implicit cast like "+ 0.0"
+            + self.process(
+                elements.Cast(
+                    binary.right,
+                    binary.right.type
+                    if binary.right.type._type_affinity is sqltypes.Numeric
+                    else tdtypes.NUMBER(),
+                ),
+                **kw,
+            )
+        )
+
+    def visit_floordiv_binary(self, binary, operator, **kw):
+        return "FLOOR(%s)" % (
+                self.process(binary.left, **kw)
+                + " / "
+                + self.process(binary.right, **kw)
+            )
+
+
+class TeradataDDLCompiler(compiler.DDLCompiler):
+
+    def visit_create_index(self, create, include_schema=False,
+                           include_table_schema=True):
+        index = create.element
+        self._verify_index_table(index)
+        preparer = self.preparer
+        text = "CREATE "
+        if index.unique:
+            text += "UNIQUE "
+        text += "INDEX %s (%s) ON %s" \
+                % (
+                    self._prepared_index_name(index,
+                                              include_schema=include_schema),
+                    ', '.join(
+                        self.sql_compiler.process(
+                            expr, include_table=False, literal_binds=True) for
+                        expr in index.expressions),
+                    preparer.format_table(index.table,
+                                          use_schema=include_table_schema)
+                )
+        return text
+
+    def create_table_suffix(self, table):
+        """
+        This hook processes the optional keyword teradata_suffixes
+        ex.
+        from teradatasqlalchemy.compiler import\
+                        TDCreateTableSuffix as Opts
+        t = Table( 'name', meta,
+                   ...,
+                   teradata_suffixes=Opts.
+                                      fallback().
+                                      log().
+                                      with_journal_table(t2.name)
+
+        CREATE TABLE name, fallback,
+        log,
+        with journal table = [database/user.]table_name(
+          ...
+        )
+
+        teradata_suffixes can also be a list of strings to be appended
+        in the order given.
+        """
+        post = table.dialect_kwargs['teradatasql_suffixes']
+
+        if isinstance(post, TDCreateTableSuffix):
+            if post.opts:
+                return ',\n' + post.compile()
+            else:
+                return post
+        elif post:
+            assert type(post) is list
+            res = ',\n ' + ',\n'.join(post)
+        else:
+            return ''
+
+    def post_create_table(self, table):
+
+        """
+        This hook processes the TDPostCreateTableOpts given by the
+        teradata_post_create dialect kwarg for Table.
+
+        Note that there are other dialect kwargs defined that could possibly
+        be processed here.
+
+        See the kwargs defined in dialect.TeradataDialect
+
+        Ex.
+        from teradatasqlalchemy.compiler import TDCreateTablePost as post
+        Table('t1', meta,
+               ...
+               ,
+               teradata_post_create = post().
+                                        fallback().
+                                        checksum('on').
+                                        mergeblockratio(85)
+
+        creates ddl for a table like so:
+
+        CREATE TABLE "t1" ,
+             checksum=on,
+             fallback,
+             mergeblockratio=85 (
+               ...
+        )
+
+        """
+        kw = table.dialect_kwargs['teradatasql_post_create']
+        if isinstance(kw, TDCreateTablePost):
+            if kw:
+                return '\n' + kw.compile()
+        return ''
+
+    def get_column_specification(self, column, **kwargs):
+
+        if column.table is None:
+            raise exc.CompileError(
+                "Teradata requires Table-bound columns "
+                "in order to generate DDL")
+
+        colspec = (self.preparer.format_column(column) + " " + \
+                   self.dialect.type_compiler.process(
+                       column.type, type_expression=column))
+
+        # Null/NotNull
+        if column.nullable is not None:
+            if not column.nullable or column.primary_key:
+                colspec += " NOT NULL"
+
+        return colspec
+
+
+class TeradataTypeCompiler(compiler.GenericTypeCompiler):
+
+    def _get(self, key, type_, kw):
+        return kw.get(key, getattr(type_, key, None))
+
+    def visit_datetime(self, type_, **kw):
+        return self.visit_TIMESTAMP(type_, precision=6, **kw)
+
+    def visit_date(self, type_, **kw):
+        return self.visit_DATE(type_, **kw)
+
+    def visit_text(self, type_, **kw):
+        return self.visit_CLOB(type_, **kw)
+
+    def visit_time(self, type_, **kw):
+        return self.visit_TIME(type_, precision=6, **kw)
+
+    def visit_unicode(self, type_, **kw):
+        return self.visit_VARCHAR(type_, charset='UNICODE', **kw)
+
+    def visit_unicode_text(self, type_, **kw):
+        return self.visit_CLOB(type_, charset='UNICODE', **kw)
+
+    def visit_boolean(self, type_, **kw):
+        return self.visit_BYTEINT(type_, **kw)
+
+    def visit_INTERVAL_YEAR(self, type_, **kw):
+        return 'INTERVAL YEAR{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_YEAR_TO_MONTH(self, type_, **kw):
+        return 'INTERVAL YEAR{} TO MONTH'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_MONTH(self, type_, **kw):
+        return 'INTERVAL MONTH{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY(self, type_, **kw):
+        return 'INTERVAL DAY{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY_TO_HOUR(self, type_, **kw):
+        return 'INTERVAL DAY{} TO HOUR'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY_TO_MINUTE(self, type_, **kw):
+        return 'INTERVAL DAY{} TO MINUTE'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_DAY_TO_SECOND(self, type_, **kw):
+        return 'INTERVAL DAY{} TO SECOND{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '',
+            '(' + str(type_.frac_precision) + ')' if type_.frac_precision is not None else '')
+
+    def visit_INTERVAL_HOUR(self, type_, **kw):
+        return 'INTERVAL HOUR{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_HOUR_TO_MINUTE(self, type_, **kw):
+        return 'INTERVAL HOUR{} TO MINUTE'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_HOUR_TO_SECOND(self, type_, **kw):
+        return 'INTERVAL HOUR{} TO SECOND{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '',
+            '(' + str(type_.frac_precision) + ')' if type_.frac_precision is not None else '')
+
+    def visit_INTERVAL_MINUTE(self, type_, **kw):
+        return 'INTERVAL MINUTE{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_INTERVAL_MINUTE_TO_SECOND(self, type_, **kw):
+        return 'INTERVAL MINUTE{} TO SECOND{}'.format(
+            '(' + str(type_.precision) + ')' if type_.precision else '',
+            '(' + str(type_.frac_precision) + ')' if type_.frac_precision is not None else '')
+
+    def visit_INTERVAL_SECOND(self, type_, **kw):
+        if type_.frac_precision is not None and type_.precision:
+            return 'INTERVAL SECOND{}'.format(
+                '(' + str(type_.precision) + ', ' + str(type_.frac_precision) + ')')
+        else:
+            return 'INTERVAL SECOND{}'.format(
+                '(' + str(type_.precision) + ')' if type_.precision else '')
+
+    def visit_PERIOD_DATE(self, type_, **kw):
+        return 'PERIOD(DATE)' + \
+               (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
+
+    def visit_PERIOD_TIME(self, type_, **kw):
+        return 'PERIOD(TIME{}{})'.format(
+            '(' + str(type_.frac_precision) + ')'
+            if type_.frac_precision is not None
+            else '',
+            ' WITH TIME ZONE' if type_.timezone else '') + \
+               (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
+
+    def visit_PERIOD_TIMESTAMP(self, type_, **kw):
+        return 'PERIOD(TIMESTAMP{}{})'.format(
+            '(' + str(type_.frac_precision) + ')'
+            if type_.frac_precision is not None
+            else '',
+            ' WITH TIME ZONE' if type_.timezone else '') + \
+               (" FORMAT '" + type_.format + "'" if type_.format is not None else '')
+
+    def visit_TIME(self, type_, **kw):
+        tz = ' WITH TIME ZONE' if type_.timezone else ''
+        prec = self._get('precision', type_, kw)
+        prec = '%s' % '(' + str(prec) + ')' if prec is not None else ''
+        return 'TIME{}{}'.format(prec, tz)
+
+    def visit_TIMESTAMP(self, type_, **kw):
+        tz = ' WITH TIME ZONE' if type_.timezone else ''
+        prec = self._get('precision', type_, kw)
+        prec = '%s' % '(' + str(prec) + ')' if prec is not None else ''
+        return 'TIMESTAMP{}{}'.format(prec, tz)
+
+    def _string_process(self, type_, datatype, **kw):
+        length = self._get('length', type_, kw)
+        length = '(%s)' % length if length is not None else ''
+
+        charset = self._get('charset', type_, kw)
+        charset = ' CHAR SET %s' % charset if charset is not None else ''
+
+        res = '{}{}{}'.format(datatype, length, charset)
+        return res
+
+    def visit_CHAR(self, type_, **kw):
+        return self._string_process(type_, 'CHAR', length=type_.length, **kw)
+
+    def visit_VARCHAR(self, type_, **kw):
+        if type_.length is None:
+            return self._string_process(type_, 'LONG VARCHAR', **kw)
+        else:
+            return self._string_process(type_, 'VARCHAR', length=type_.length, **kw)
+
+    def visit_CLOB(self, type_, **kw):
+        multi = self._get('multiplier', type_, kw)
+        if multi is not None and type_.length is not None:
+            length = str(type_.length) + multi
+            return self._string_process(type_, 'CLOB', length=length, **kw)
+
+        return self._string_process(type_, 'CLOB', **kw)
+
+    def visit_BYTEINT(self, type_, **kw):
+        return 'BYTEINT'
+
+    def visit_BYTE(self, type_, **kw):
+        return 'BYTE{}'.format(
+            '(' + str(type_.length) + ')' if type_.length is not None else '')
+
+    def visit_VARBYTE(self, type_, **kw):
+        return 'VARBYTE{}'.format(
+            '(' + str(type_.length) + ')' if type_.length is not None else '')
+
+    def visit_BLOB(self, type_, **kw):
+        multiplier = self._get('multiplier', type_, kw)
+        return 'BLOB{}'.format(
+            '(' + str(type_.length) + \
+            '{})'.format(multiplier if multiplier is not None else '')
+            if type_.length is not None else '')
+
+    def visit_NUMBER(self, type_, **kw):
+        args = (str(type_.precision), '') if type_.scale is None \
+            else (str(type_.precision), ', ' + str(type_.scale))
+        return 'NUMBER{}'.format(
+            '' if type_.precision is None else '({}{})'.format(*args))
+
+    def visit_XML(self, type_, **kw):
+        return 'XML({0}) INLINE LENGTH {1}'.format(str(type_.maximum_length),
+                                                   str(type_.inline_length))
+
+    def visit_JSON(self, type_, **kw):
+        sec_part = ''
+        # Adding charset if it is not None, storage_format is only being added
+        # if it doesn't hold its default value: 'TEXT'. Reason for this being,
+        # 'TEXT' is not a valid type name for STORAGE FORMAT.
+        if type_.charset is not None:
+            sec_part = 'CHARACTER SET {}'.format(type_.charset)
+        if type_.storage_format is not None and type_.storage_format != 'TEXT':
+            sec_part = '{0} STORAGE FORMAT {1}'.format(sec_part, type_.storage_format)
+
+        return 'JSON({0}) INLINE LENGTH {1} {2}'.format(str(type_.max_length),
+                                                        str(type_.inline_length),
+                                                        str(sec_part))
+
+    def visit_TDUDT(self, type_, **kw):
+        return 'TDUDT{}'.format('' if type_.type_name is None else '(UDTName: {0})'.format(str(type_.type_name)))
+
+    def visit_GEOMETRY(self, type_, **kw):
+        return 'ST_GEOMETRY({0}) INLINE LENGTH {1}'.format(str(type_.max_length),
+                                                           str(type_.inline_length))
+
+    def visit_MBR(self, type_, **kw):
+        return 'MBR'
+
+    def visit_MBB(self, type_, **kw):
+        return 'MBB'
+
+
+class TeradataDialect(default.DefaultDialect):
+    name = 'teradatasql'
+    driver = 'teradatasql'
+    paramstyle = 'qmark'
+    default_paramstyle = 'qmark'
+    poolclass = pool.SingletonThreadPool
+
+    statement_compiler = TeradataCompiler
+    ddl_compiler = TeradataDDLCompiler
+    type_compiler = TeradataTypeCompiler
+    preparer = TeradataIdentifierPreparer
+    execution_ctx_cls = TeradataExecutionContext
+
+    supports_native_boolean = False
+    supports_native_decimal = True
+    supports_unicode_statements = True
+    supports_unicode_binds = True
+    postfetch_lastrowid = False
+    implicit_returning = False
+    preexecute_autoincrement_sequences = False
+    case_sensitive = False
+    supports_statement_cache = False
+
+    construct_arguments = [
+        (Table, {
+            "post_create": None,
+            "suffixes": None
+        }),
+
+        (Index, {
+            "order_by": None,
+            "loading": None
+        }),
+
+        (Column, {
+            "compress": None,
+            "identity": None
+        })
+    ]
+
+    def __init__(self, **kwargs):
+        super(TeradataDialect, self).__init__(**kwargs)
+
+        # Method to append 'X' at the end of string when "usexviews" is set to 'True'.
+        self.__get_xviews_obj = lambda db_obj: db_obj + 'X' if configure.usexviews else db_obj
+
+    def create_connect_args(self, url):
+
+        params = super(TeradataDialect, self).create_connect_args(url)[1]
+
+        if 'username' in params:
+            sUserName = params.pop('username')
+            if 'user' not in params:  # user URL parameter has higher priority than username prefix before host
+                params['user'] = sUserName
+
+        if 'port' in params:
+            params['dbs_port'] = str(params['port'])
+            del params['port']
+
+        args = json.dumps(params),  # single-element tuple
+        kwargs = {}
+        return (args, kwargs)
+
+    @classmethod
+    def dbapi(cls):
+
+        """ Hook to the dbapi2.0 implementation's module"""
+        import teradatasql
+        return teradatasql
+
+    @classmethod
+    def import_dbapi(cls):
+
+        """ Hook to the dbapi2.0 implementation's module"""
+        import teradatasql
+        return teradatasql
+
+    def normalize_name(self, name, **kw):
+        if name is not None:
+            return name.strip()
+        return name
+
+    def _is_table_volatile(self, connection, table_name, schema=None):
+        """ Internal function to check if the table is a volatile table or not """
+
+        isVolatile = False
+        # Volatile tables are always created in users login space.
+        # This means that the schema either has to be None or the same as the login user space (the login user name).
+        if schema is not None and schema.lower() != self._get_login_user_space(connection).lower():
+            return isVolatile
+
+        res = self._get_volatile_tables_list(connection, table_name)
+        for r in res:
+            if r.lower() == table_name.lower():
+                isVolatile = True
+                break
+
+        return isVolatile
+
+    def _get_volatile_tables_list(self, connection, table_name=None):
+        """ Internal function to get a list of all volatile tables in the current session """
+        # TODO: The check can be made convenient by allowing the option to check for only a single table when the
+        #       command is modified to 'help volatile table <tablename>' which is currently failing.
+
+        stmt = 'help volatile table'
+        res = connection.execute(text(stmt))
+
+        return [row['Table Dictionary Name'] for row in res.mappings()]
+
+    def has_table(self, connection, table_name, schema=None, **kw):
+        """
+        DESCRIPTION:
+            Function to check for the presence of a table in the schema provided, if any.
+            Note:
+                By default presence of a table is checked irrespective of whether
+                user has access to the table or not. This provides faster lookup.
+                When option 'configure.usexviews' is set to True, then search for
+                the table happens in only user accessible tables.
+
+        PARAMETERS:
+            connection:
+                Required argument.
+                A SQLAlchemy connection object.
+
+            table_name:
+                Required argument.
+                The name of the table to search for.
+
+            schema:
+                Optional argument.
+                The schema to search the table in.
+                By default, the default schema will be searched.
+
+        RETURNS:
+            A Boolean value indicating whether a table named table_name is present or not.
+
+        RAISES:
+            None.
+
+        EXAMPLES:
+            # Example 1 - Check table 'mytable' exists.
+            >>> table_exists = has_table(conn, 'mytable', schema='myschema')
+
+            # Example 2 - Check if table 'mytable' exists and user has access.
+            >>> from teradatasqlalchemy.options import configure
+            >>> configure.usexviews=True
+            >>> table_exists = has_table(conn, 'mytable', schema='myschema')
+
+        """
+        schema_name = self.default_schema_name if schema is None else schema
+
+        # Default value for 'usexviews' is False so use dbc.tablesV by default
+        # which is faster.
+        dbc_tables = self.__get_xviews_obj("tablesV")
+
+        # Permanent tables (TableKind in 'O', 'Q', 'T')
+        table_obj = table(dbc_tables, column('DatabaseName'), column('TableName'),
+                          column('TableKind'), schema='dbc')
+        stmt = select(table_obj.c.TableName) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
+                        text('TableName=:table_name'),
+                        text("TableKind IN ('O', 'Q', 'T')")))
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema=schema_name, table_name=table_name)
+        res = connection.execute(stmt).fetchone()
+        table_present = res is not None
+
+        # Volatile tables
+        if not table_present:
+            table_present = self._is_table_volatile(connection, table_name, schema)
+
+        return table_present
+
+    def has_view(self, connection, view_name, schema=None):
+        """
+        DESCRIPTION:
+            Function to check for the presence of a view in the schema provided, if any.
+            Note:
+                By default presence of a view is checked irrespective of whether
+                user has access to the view or not. This provides faster lookup.
+                When option 'configure.usexviews' is set to True, then search for
+                the view happens in only user accessible views.
+
+        PARAMETERS:
+            connection:
+                Required argument.
+                A SQLAlchemy connection object.
+
+            view_name:
+                Required argument.
+                The name of the view to search for.
+
+            schema:
+                Optional argument.
+                The schema to search the view in.
+                By default, the default schema will be searched.
+
+        RETURNS:
+            A Boolean value indicating whether a view named view_name is present or not.
+
+        RAISES:
+            None.
+
+        EXAMPLES:
+            # Example 1 - Check view 'myview' exists.
+            >>> view_exists = has_view(conn, 'myview', schema='myschema')
+
+            # Example 2 - Check if view 'myview' exists and user has access.
+            >>> from teradatasqlalchemy.options import configure
+            >>> usexviews=True
+            >>> view_exists = has_view(conn, 'myview', schema='myschema')
+        """
+        schema_name = self.default_schema_name if schema is None else schema
+
+        # Default value for 'usexviews' is False so use dbc.tablesV by default
+        # which is faster.
+        dbc_tables = self.__get_xviews_obj("tablesV")
+
+        table_obj = table(dbc_tables, column('DatabaseName'), column('TableName'),
+                          column('TableKind'), schema='dbc')
+        # Views (TableKind = 'V')
+        stmt = select(table_obj.c.TableName) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
+                        text('TableName=:view_name'),
+                        text("TableKind (NOT CASESPECIFIC) = 'V' (NOT CASESPECIFIC)")))
+
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema=schema_name, view_name=view_name)
+        res = connection.execute(stmt).fetchone()
+        view_present = res is not None
+
+        return view_present
+
+    def _resolve_udt_type(self, **kw):
+        """
+        Resolves the types for UDT columns.
+        """
+        col_udt = kw["col_udt_name"]
+        if col_udt == 'ST_GEOMETRY':
+            return tdtypes.GEOMETRY
+        elif col_udt == 'MBR':
+            return tdtypes.MBR
+        elif col_udt == 'MBB':
+            return tdtypes.MBB
+        else:
+            return tdtypes.TDUDT
+
+    def _resolve_type(self, t, **kw):
+        """
+        Resolves the types for String, Numeric, Date/Time, etc. columns.
+        """
+        tc = self.normalize_name(t)
+        if tc in ischema_names:
+            if tc == 'UT':
+                type_ = self._resolve_udt_type(**kw)
+            else:
+                type_ = ischema_names[tc]
+            return TeradataTypeResolver().process(type_, typecode=tc, **kw)
+
+        return sqltypes.NullType
+
+    def _get_column_info(self, row):
+        """
+        Resolves the column information for get_columns given a row.
+        """
+        chartype = {
+            0: None,
+            1: 'LATIN',
+            2: 'UNICODE',
+            3: 'KANJISJIS',
+            4: 'GRAPHIC'
+        }
+
+        # Handle unspecified characterset and disregard chartypes specified for
+        # non-character types (e.g. binary)
+        character_set = row['CharType'] if self.normalize_name(row['ColumnType']) in stringtypes else 0
+
+        try:
+            inline_length = row['InlineLength']
+        except KeyError:
+            inline_length = 0
+
+        try:
+            storage_format = row['StorageFormat']
+        except KeyError:
+            storage_format = ""
+
+        try:
+            col_udt_name = row['ColumnUDTName']
+        except KeyError:
+            col_udt_name = ""
+
+        typ = self._resolve_type(row['ColumnType'],
+                                 length=int(row['ColumnLength'] or 0),
+                                 chartype=chartype[character_set],
+                                 prec=int(row['DecimalTotalDigits'] or 0),
+                                 scale=int(row['DecimalFractionalDigits'] or 0),
+                                 fmt=row['ColumnFormat'],
+                                 inline_length=inline_length,
+                                 storage_format=storage_format,
+                                 col_udt_name=col_udt_name)
+
+        autoinc = row['IdColType'] in ('GA', 'GD')
+
+        # attrs contains all the attributes queried from DBC.ColumnsV
+        attrs = {self.normalize_name(k): row[k] for k in row.keys()}
+        col_info = {
+            'name': self.normalize_name(row['ColumnName']),
+            'type': typ,
+            'nullable': row['Nullable'] == u'Y',
+            'default': row['DefaultValue'],
+            'autoincrement': autoinc
+        }
+
+        return dict(attrs, **col_info)
+
+    def get_columns(self, connection, table_name, schema=None, **kw):
+
+        # Check if table is a volatile table before the schema is set to the default schema.
+        isVolatile = self._is_table_volatile(connection, table_name, schema)
+
+        if schema is None:
+            schema = self.default_schema_name
+
+        # Using 'help schema.table.*' statements has been considered.
+        # The DBC.ColumnsV provides the default value which is not available
+        # with the 'help column' commands result.
+
+        # Check if the object is a view
+        schema_name = ':schema'
+        # Default value for 'usexviews' is False so use dbc.tablesV by default
+        dbc_tables = self.__get_xviews_obj("tablesV")
+        table_obj = table(dbc_tables, column('DatabaseName'), column('TableName'),
+                          column('tablekind'), column('TVMFlavor'), schema='dbc')
+
+        is_view_c = "CASE WHEN {} = 'V' THEN 1 ELSE 0 END as is_view".format(table_obj.c.tablekind)
+        is_art_c = "CASE WHEN {} = 'A' THEN 1 ELSE 0 END as is_art_table".format(table_obj.c.TVMFlavor)
+
+        stmt = select(text(is_view_c + ', ' + is_art_c)) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + schema_name + ' (NOT CASESPECIFIC)'),
+                        text('TableName=:table_name')))
+
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema=schema, table_name=table_name)
+        res = connection.execute(stmt)
+        isView, is_art_table = next(res) if res.rowcount > 0 else (False, False)
+
+        if isView or isVolatile:
+            # Volatile table definition is not stored in the dictionary.
+            # We use the 'help schema.table.*' command instead to get information for all columns.
+            # We have to do the same for views since we need the type information
+            # which is not available in dbc.ColumnsV.
+            res = self._get_column_help(connection, schema, table_name, column_name=None)
+
+            # If this is a view, get types for individual columns (dbc.ColumnsV won't have types for view columns).
+            # For a view or a volatile table, we have to set the default values as the 'help' command does not have it.
+            col_info_list = []
+            for r in res:
+                updated_column_info_dict = self._update_column_help_info(r._mapping)
+                col_info_list.append(dict(r._mapping, **(updated_column_info_dict)))
+            res = col_info_list
+        else:
+            # Default value for 'usexviews' is False so use dbc.ColumnsV by default
+            dbc_columns = self.__get_xviews_obj("ColumnsV")
+            # For permanent tables.
+            # ORDER BY ColumnId added to make sure the columns are retrieved in order
+            # of their appearance in the table. HELP COLUMN already maintains that order.
+            table_obj = table(dbc_columns, schema='dbc')
+            stmt = select(text('*')) \
+                .select_from(table_obj) \
+                .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + schema_name + ' (NOT CASESPECIFIC)'),
+                            text('TableName=:table_name'))) \
+                .order_by(text("ColumnId"))
+
+            stmt = text(str(stmt))
+            stmt = stmt.bindparams(schema=schema, table_name=table_name)
+            res = connection.execute(stmt).fetchall()
+            res =[res_row._mapping for res_row in res]
+
+        final_column_info = []
+        # For ART Tables, every layer columns appear in output table. Columns which do not belongs to
+        # primary layer will have TSColumnType as 'AS'. Also, some columns will be duplicated so remove
+        # duplicates.
+        if is_art_table:
+            _col_names = set()
+            for row in res:
+                col_info = self._get_column_info(row)
+                if col_info['TSColumnType'].strip() == 'AS' or col_info['ColumnName'] in _col_names:
+                    continue
+                final_column_info.append(col_info)
+                _col_names.add(col_info['ColumnName'])
+        else:
+            # Ignore the non-functional column in a PTI table
+            for row in res:
+                col_info = self._get_column_info(row)
+                if 'TSColumnType' in col_info and col_info['TSColumnType'] is not None:
+                    if col_info['ColumnName'] == 'TD_TIMEBUCKET' and col_info['TSColumnType'].strip() == 'TB':
+                        continue
+                final_column_info.append(col_info)
+
+        return final_column_info
+
+    def _get_default_schema_name(self, connection):
+        res = self.normalize_name(
+            connection.execute(text('select database')).scalar())
+        return res
+
+    def _get_login_user_space(self, connection):
+        """
+        Internal function to get the current users login user space.
+        This is currently used for volatile table checks.
+        """
+        res = self.normalize_name(
+            connection.execute(text('select user')).scalar())
+        return res
+
+    def _get_column_help(self, connection, schema, table_name, column_name):
+        """
+        Internal function to get the help on a column, when provided, else all columns
+        for a table provided using the HELP COLUMN command.
+
+        :param connection:  SQLAlchemy connection object.
+        :param schema:      Schema name of the table/view to run 'HELP COLUMN' command for.
+        :param table_name:  The name of the table/view to run 'HELP COLUMN' command for.
+        :param column_name: Optional column name to get information only about the column.
+
+        :return: The result of the HELP COLUMN command:
+                 * A list of dictionaries when column_name is not provided.
+                 * A dictionary when column_name is provided.
+        """
+        prepared = preparer(dialect())
+        stmt = 'help column ' + prepared.quote(schema) + '.' + prepared.quote(table_name) + '.' \
+               + (prepared.quote(column_name) if column_name else '*')
+
+        result_set = connection.execute(text(stmt)).fetchall()
+        return result_set[0] if column_name else result_set
+
+    def _update_column_help_info(self, res):
+        """
+        Internal function to create new fields in the result dictionary to have the field names
+        similar to those for tables.
+        """
+        return {
+            'ColumnName': res['Column Dictionary Name'],
+            'ColumnType': res['Type'],
+            'ColumnLength': res['Max Length'],
+            'CharType': res['Char Type'],
+            'DecimalTotalDigits': res['Decimal Total Digits'],
+            'DecimalFractionalDigits': res['Decimal Fractional Digits'],
+            'ColumnFormat': res['Format'],
+            'Nullable': res['Nullable'],
+            'DefaultValue': None,
+            'IdColType': res['IdCol Type'],
+            'TSColumnType': res['Time Series Column Type'] if 'Time Series Column Type' in res else None,
+            'ColumnUDTName': res['UDT Dictionary Name'] if 'UDT Dictionary Name' in res else None,
+            'InlineLength': res['Inline Length'] if 'Inline Length' in res else None
+        }
+
+    def get_table_names(self, connection, schema=None, **kw):
+        """
+        DESCRIPTION:
+            Function to get all the available table names in the schema provided, if any.
+            Note:
+                By default function returns all tables available irrespective of whether
+                user has access to the table or not. This provides faster lookup. In
+                case user wants to list down all available tables which user has
+                access to, then set option 'configure.usexviews' to True.
+
+        PARAMETERS:
+            connection:
+                Required argument.
+                A SQLAlchemy connection object.
+
+            schema:
+                Optional argument.
+                The schema to search the table in.
+                By default, the default schema will be searched.
+
+        RETURNS:
+            A list of table names.
+
+        RAISES:
+            None.
+
+        EXAMPLES:
+            # Example 1 - Get all the available tables.
+            >>> tables = get_table_names(conn, schema='myschema')
+
+            # Example 2 - Get all the user accessible tables.
+            >>> from teradatasqlalchemy.options import configure
+            >>> configure.usexviews=True
+            >>> tables = get_table_names(conn, schema='myschema')
+        """
+        if schema is None:
+            schema = self.default_schema_name
+
+        # Default value for 'usexviews' is False so use dbc.tablesV by default
+        dbc_tables = self.__get_xviews_obj("tablesV")
+
+        table_obj = table(dbc_tables, column('tablename'), schema='dbc')
+        stmt = select(table_obj.c.tablename) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
+                        or_(text('tablekind=\'T\''),
+                            text('tablekind=\'O\''),
+                            text('tablekind=\'Q\''))))
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema=schema)
+        res = connection.execute(stmt).fetchall()
+        return [self.normalize_name(name._mapping['TableName']) for name in res]
+
+    def get_schema_names(self, connection, **kw):
+        """Retrieves names of Databases/Schemas on the system"""
+
+        # Default value for 'usexviews' is False so use dbc.DatabasesV by default
+        dbc_schemas = self.__get_xviews_obj("DatabasesV")
+        table_obj = table(dbc_schemas, column('DatabaseName'), schema='dbc')
+        stmt = select(table_obj.c.DatabaseName) \
+            .select_from(table_obj) \
+            .order_by(table_obj.c.DatabaseName)
+        res = connection.execute(stmt).fetchall()
+        return [self.normalize_name(name._mapping['DatabaseName']) for name in res]
+
+    def get_view_definition(self, connection, view_name, schema=None, **kw):
+
+        if schema is None:
+            schema = self.default_schema_name
+
+        res = connection.execute(text('show view {}.{}'.format(schema, view_name))).scalar()
+        return self.normalize_name(res)
+
+    def get_view_names(self, connection, schema=None, **kw):
+        """
+        DESCRIPTION:
+            Function to get all the available view names in the schema provided, if any.
+            Note:
+                By default function returns all views available irrespective of whether
+                user has access to the view or not. This provides faster lookup. In
+                case user wants to list down all available views which user has
+                access to, then set option 'configure.usexviews' to True.
+
+        PARAMETERS:
+            connection:
+                Required argument.
+                A SQLAlchemy connection object.
+
+            schema:
+                Optional argument.
+                The schema to search the view in.
+                By default, the default schema will be searched.
+
+        RETURNS:
+            A list of view names.
+
+        RAISES:
+            None.
+
+        EXAMPLES:
+            # Example 1 - Get all the available views.
+            >>> views = get_view_names(conn, schema='myschema')
+
+            # Example 2 - Get all the user accessible views.
+            >>> from teradatasqlalchemy.options import configure
+            >>> configure.usexviews=True
+            >>> tables = get_view_names(conn, schema='myschema')
+        """
+        if schema is None:
+            schema = self.default_schema_name
+
+        # Default value for 'usexviews' is False so use dbc.tablesV by default
+        # which is faster.
+        dbc_tables = self.__get_xviews_obj("tablesV")
+
+        table_obj = table(dbc_tables, column('tablename'), schema='dbc')
+        stmt = select(table_obj.c.tablename) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
+                        text('tablekind=\'V\'')))
+
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema=schema)
+        res = connection.execute(stmt).fetchall()
+        return [self.normalize_name(name._mapping['TableName']) for name in res]
+
+    def get_pk_constraint(self, connection, table_name, schema=None, **kw):
+        """
+        Override
+        TODO: Check if we need PRIMARY Indices or PRIMARY KEY Indices
+        TODO: Check for border cases (No PK Indices)
+        """
+
+        if schema is None:
+            schema = self.default_schema_name
+
+        # Default value for 'usexviews' is False so use dbc.IndicesV by default
+        dbc_indices = self.__get_xviews_obj("IndicesV")
+
+        table_obj = table(dbc_indices, column('ColumnName'), column('IndexName'),
+                          schema='dbc')
+        stmt = select(table_obj.c.ColumnName, table_obj.c.IndexName) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
+                        text('TableName=:table'), text('IndexType=:indextype'))) \
+            .order_by(asc(column('IndexNumber')))
+        stmt = text(str(stmt))
+        # K for Primary Key.
+        stmt = stmt.bindparams(schema=schema, table=table_name, indextype='K')
+        res = connection.execute(stmt).fetchall()
+
+        index_columns = list()
+        index_name = None
+
+        for index_column in res:
+            index_columns.append(self.normalize_name(index_column._mapping['ColumnName']))
+            index_name = self.normalize_name(index_column._mapping['IndexName'])  # There should be just one IndexName
+
+        return {
+            "constrained_columns": index_columns,
+            "name": index_name
+        }
+
+    def get_unique_constraints(self, connection, table_name, schema=None, **kw):
+        """
+        Overrides base class method
+        """
+        if schema is None:
+            schema = self.default_schema_name
+
+        # Default value for 'usexviews' is False so use dbc.IndicesV by default
+        dbc_indices = self.__get_xviews_obj("IndicesV")
+
+        table_obj = table(dbc_indices, column('ColumnName'), column('IndexName'), schema='dbc')
+        stmt = select(table_obj.c.ColumnName, table_obj.c.IndexName) \
+            .select_from(table_obj) \
+            .where(and_(text('DatabaseName (NOT CASESPECIFIC) = ' + ':schema' + ' (NOT CASESPECIFIC)'),
+                        text('TableName=:table'),
+                        text('IndexType=:indextype'))) \
+            .order_by(asc(column('IndexName')))
+        stmt = text(str(stmt))
+        # U for Unique Key.
+        stmt = stmt.bindparams(schema=schema, table=table_name, indextype='U')
+        res = connection.execute(stmt).fetchall()
+
+        def grouper(fk_row):
+            return {
+                'name': self.normalize_name(fk_row.IndexName),
+            }
+
+        unique_constraints = list()
+        for constraint_info, constraint_cols in groupby(res, grouper):
+            unique_constraint = {
+                'name': self.normalize_name(constraint_info['name']),
+                'column_names': list()
+            }
+
+            for constraint_col in constraint_cols:
+                unique_constraint['column_names'].append(self.normalize_name(constraint_col._mapping['ColumnName']))
+
+            unique_constraints.append(unique_constraint)
+
+        return unique_constraints
+
+    def get_foreign_keys(self, connection, table_name, schema=None, **kw):
+        """
+        Overrides base class method
+        """
+
+        if schema is None:
+            schema = self.default_schema_name
+        # Default value for 'usexviews' is False so use DBC.All_RI_ChildrenV by default
+        dbc_child_parent_table = self.__get_xviews_obj("All_RI_ChildrenV")
+
+        table_obj = table(dbc_child_parent_table, column('IndexID'), column('IndexName'),
+                          column('ChildKeyColumn'), column('ParentDB'),
+                          column('ParentTable'), column('ParentKeyColumn'),
+                          schema='dbc')
+        stmt = select(table_obj.c.IndexID, table_obj.c.IndexName,
+                      table_obj.c.ChildKeyColumn, table_obj.c.ParentDB,
+                      table_obj.c.ParentTable, table_obj.c.ParentKeyColumn) \
+            .select_from(table_obj) \
+            .where(and_(text('ChildTable = :table'), text('ChildDB = :schema'))) \
+            .order_by(asc(column('IndexID')))
+
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema=schema, table=table_name)
+        res = connection.execute(stmt).fetchall()
+
+        def grouper(fk_row):
+            return {
+                'name': fk_row.IndexName or fk_row.IndexID,  # ID if IndexName is None
+                'schema': fk_row.ParentDB,
+                'table': fk_row.ParentTable
+            }
+
+        # TODO: Check if there's a better way
+        fk_dicts = list()
+        for constraint_info, constraint_cols in groupby(res, grouper):
+            fk_dict = {
+                'name': constraint_info['name'],
+                'constrained_columns': list(),
+                'referred_table': constraint_info['table'],
+                'referred_schema': constraint_info['schema'],
+                'referred_columns': list()
+            }
+
+            for constraint_col in constraint_cols:
+                fk_dict['constrained_columns'].append(self.normalize_name(constraint_col.ChildKeyColumn))
+                fk_dict['referred_columns'].append(self.normalize_name(constraint_col.ParentKeyColumn))
+
+            fk_dicts.append(fk_dict)
+
+        return fk_dicts
+
+    def get_indexes(self, connection, table_name, schema=None, **kw):
+        """
+        Overrides base class method
+        """
+
+        if schema is None:
+            schema = self.default_schema_name
+
+        indices = list()
+        prepared = preparer(dialect())
+        stmt = 'help index ' + prepared.quote(schema) + '.' + prepared.quote(table_name)
+        try:
+            res = connection.execute(text(stmt)).fetchall()
+            res_set = [row._mapping for row in res]
+            for row in res_set:
+                index_dict = {
+                    'name': row['Index Dictionary Name'],
+                    'column_names': list(),
+                    'unique': True if row['Unique?'] == 'Y' else False
+                }
+
+                if "," in row['Column Names']:
+                    index_cols = row['Column Names'].split(',')
+                else:
+                    index_cols = [row['Column Names']]
+                for index_col in index_cols:
+                    if index_col == 'TD_TIMEBUCKET' and 'Timebucket' in row and row['Timebucket'] is not None:
+                        continue
+                    else:
+                        index_dict['column_names'].append(self.normalize_name(index_col))
+
+                if len(index_dict['column_names']) > 0:
+                    indices.append(index_dict)
+        except OperationalError as err:
+            # This is to catch the following error when the object may be a view and not a table, Like:
+            # 1. [Error 3720] This view does not contain any complete index columns of the underlying table.
+            # 2. [Error 3823] VIEW 'xxx' may not be used for Help Index/Constraint/Statistics, Update, Delete or Insert.
+            if ('3720' in str(err) and 'This view does not contain any complete index columns of the underlying table'
+                in str(err)) or \
+                    ('3823' in str(
+                        err) and 'may not be used for Help Index/Constraint/Statistics, Update, Delete or Insert'
+                     in str(err)):
+                pass
+            else:
+                raise
+
+        return indices
+
+    def get_transaction_mode(self, connection, **kw):
+        """
+        Returns the transaction mode set for the current session.
+        T = TDBS
+        A = ANSI
+        """
+        # Default value for 'usexviews' is False so use dbc.sessioninfoV by default
+        dbc_sessions = self.__get_xviews_obj("sessioninfoV")
+
+        table_obj = table(dbc_sessions, column('transaction_mode'), column('sessionno'), schema='dbc')
+        stmt = select(table_obj.c.transaction_mode) \
+            .select_from(table_obj) \
+            .where(table_obj.c.sessionno == text('SESSION'))
+        res = connection.execute(stmt).scalar()
+        return res
+
+    def _get_server_version_info(self, connection, **kw):
+        """
+        Returns the Teradata Database software version.
+        """
+        table_obj = table('dbcinfov', column('InfoData'), column('InfoKey'), schema='dbc')
+        stmt = select(table_obj.c.InfoData) \
+            .select_from(table_obj) \
+            .where(table_obj.c.InfoKey == 'VERSION')
+        res = connection.execute(stmt).scalar()
+        return res
+
+    def conn_supports_autocommit(self, connection, **kw):
+        """
+        Returns True if autocommit is used for this connection (underlying Teradata session)
+        else False
+        """
+        return self.get_transaction_mode(connection) == 'T'
+
+    def _get_database_names(self, connection, schema_name):
+        """
+        Function to return a list valid of database names for a given sqlalchemy connection.
+        """
+        table_obj = table('databasesV', column('databasename'), schema='dbc')
+        stmt = select(text(str(func.lower(table_obj.c.databasename)) + ' as databasename')) \
+            .select_from(table_obj) \
+            .where(text('databasename (NOT CASESPECIFIC) = {} (NOT CASESPECIFIC)'.format(':schema_name')))
+        stmt = text(str(stmt))
+        stmt = stmt.bindparams(schema_name=schema_name)
+
+        res = connection.execute(stmt).fetchall()
+        return [name.databasename for name in res]
+
+
+dialect = TeradataDialect
+preparer = TeradataIdentifierPreparer
+compiler = TeradataCompiler
+
+
+class TeradataOptions(object):
+    """
+    An abstract base class for various schema object options
+    """
+    _prepare = preparer(dialect())
+
+    def _append(self, opts, val):
+        _opts = opts.copy()
+        _opts.update(val)
+        return _opts
+
+    def compile(self, **kw):
+        """
+        processes the argument options and returns a string representation
+        """
+        pass
+
+    def format_cols(self, key, val, **kw):
+        """
+        key is a string
+        val is a list of strings with an optional dict as the last element
+            the dict values are appended at the end of the col list
+        """
+        res = ''
+        col_expr = ', '.join([x for x in val if type(x) is str])
+
+        res += key + '( ' + col_expr + ' )'
+        if type(val[-1]) is dict:
+            # process syntax elements (dict) after cols
+            res += ' '.join(val[-1]['post'])
+        return res
+
+
+class TDCreateTableSuffix(TeradataOptions):
+    """
+    A generative class for Teradata create table options
+    specified in teradata_suffixes
+    """
+
+    def __init__(self, opts={}, **kw):
+        """
+        opts is a dictionary that can be pre-populated with key-value pairs
+        that may be overidden if the keys conflict with those entered
+        in the methods below. See the compile method to see how the dict
+        gets processed.
+        """
+        self.opts = opts
+
+    def compile(self):
+        def process_opts(opts):
+            return [key if opts[key] is None else '{}={}'. \
+                format(key, opts[key]) for key in opts]
+
+        res = ',\n'.join(process_opts(self.opts))
+        return res
+
+    def fallback(self, enabled=True):
+        res = 'fallback' if enabled else 'no fallback'
+        return self.__class__(self._append(self.opts, {res: None}))
+
+    def log(self, enabled=True):
+        res = 'log' if enabled else 'no log'
+        return self.__class__(self._append(self.opts, {res: None}))
+
+    def with_journal_table(self, tablename=None):
+        """
+        tablename is the schema.tablename of a table.
+        For example, if t1 is a SQLAlchemy:
+                with_journal_table(t1.name)
+        """
+        return self.__class__(self._append(self.opts, \
+                                           {'with journal table': tablename}))
+
+    def before_journal(self, prefix='dual'):
+        """
+        prefix is a string taking vaues of 'no' or 'dual'
+        """
+        assert prefix in ('no', 'dual')
+        res = prefix + ' ' + 'before journal'
+        return self.__class__(self._append(self.opts, {res: None}))
+
+    def after_journal(self, prefix='not local'):
+        """
+        prefix is a string taking vaues of 'no', 'dual', 'local',
+        or 'not local'.
+        """
+        assert prefix in ('no', 'dual', 'local', 'not local')
+        res = prefix + ' ' + 'after journal'
+        return self.__class__(self._append(self.opts, {res: None}))
+
+    def checksum(self, integrity_checking='default'):
+        """
+        integrity_checking is a string taking vaues of 'on', 'off',
+        or 'default'.
+        """
+        assert integrity_checking in ('on', 'off', 'default')
+        return self.__class__(self._append(self.opts, \
+                                           {'checksum': integrity_checking}))
+
+    def freespace(self, percentage=0):
+        """
+        percentage is an integer taking values from 0 to 75.
+        """
+        return self.__class__(self._append(self.opts, \
+                                           {'freespace': percentage}))
+
+    def no_mergeblockratio(self):
+        return self.__class__(self._append(self.opts, \
+                                           {'no mergeblockratio': None}))
+
+    def mergeblockratio(self, integer=None):
+        """
+        integer takes values from 0 to 100 inclusive.
+        """
+        res = 'default mergeblockratio' if integer is None \
+            else 'mergeblockratio'
+        return self.__class__(self._append(self.opts, {res: integer}))
+
+    def min_datablocksize(self):
+        return self.__class__(self._append(self.opts, \
+                                           {'minimum datablocksize': None}))
+
+    def max_datablocksize(self):
+        return self.__class__(self._append(self.opts, \
+                                           {'maximum datablocksize': None}))
+
+    def datablocksize(self, data_block_size=None):
+        """
+        data_block_size is an integer specifying the number of bytes
+        """
+        res = 'datablocksize' if data_block_size is not None \
+            else 'default datablocksize'
+        return self.__class__(self._append(self.opts, \
+                                           {res: data_block_size}))
+
+    def blockcompression(self, opt='default'):
+        """
+        opt is a string that takes values 'autotemp',
+        'default', 'manual', or 'never'
+        """
+        return self.__class__(self._append(self.opts, \
+                                           {'blockcompression': opt}))
+
+    def with_no_isolated_loading(self, concurrent=False):
+        res = 'with no ' + \
+              ('concurrent ' if concurrent else '') + \
+              'isolated loading'
+        return self.__class__(self._append(self.opts, {res: None}))
+
+    def with_isolated_loading(self, concurrent=False, opt=None):
+        """
+        opt is a string that takes values 'all', 'insert', 'none',
+        or None
+        """
+        assert opt in ('all', 'insert', 'none', None)
+        for_stmt = ' for ' + opt if opt is not None else ''
+        res = 'with ' + \
+              ('concurrent ' if concurrent else '') + \
+              'isolated loading' + for_stmt
+        return self.__class__(self._append(self.opts, {res: None}))
+
+
+class TDCreateTablePost(TeradataOptions):
+    """
+    A generative class for building post create table options
+    given in the teradata_post_create keyword for Table
+    """
+
+    def __init__(self, opts={}, **kw):
+        self.opts = opts
+
+    def compile(self, **kw):
+        def process(opts):
+            return [key.upper() if opts[key] is None \
+                        else self.format_cols(key, opts[key], **kw) \
+                    for key in opts if key != 'on commit']
+
+        def process_last(opts):
+            if 'on commit' in opts:
+                return '\n' + 'on commit {} rows'.format(opts['on commit'])
+            else:
+                return ''
+
+        return ',\n'.join(process(self.opts)) + process_last(self.opts)
+
+    def on_commit(self, option='delete'):
+        assert type(option) is str and option.lower() in ('preserve', 'delete')
+        return self.__class__(
+            self._append(self.opts, {'on commit': option}))
+
+    def no_primary_index(self):
+        return self.__class__(self._append(self.opts, {'no primary index': None}))
+
+    def primary_index(self, name=None, unique=False, cols=[]):
+        """
+        name is a string for the primary index
+        if unique is true then unique primary index is specified
+        cols is a list of column names
+        """
+        res = 'unique primary index' if unique else 'primary index'
+        res += ' ' + name if name is not None else ''
+        return self.__class__(self._append(self.opts, {res: [self._prepare.quote(c) for c in cols if c is not None]}))
+
+    def __validate_timecode_datatype(self, timecode_datatype):
+        """
+        Internal function to validate timecode_datatype specified when creating a
+        Primary Time Index (PTI) table.
+
+        :param timecode_datatype: The timecode_datatype passed to primary_time_index().
+
+        :return: Boolean value indicating whether the argument is valid (True),
+         or raise ValueError/TypeError when invalid.
+        """
+        # Raise a ValueError is argument is None or not specified - it is required
+        if timecode_datatype is None:
+            raise ValueError("'timecode_datatype' is a required argument and must not be None.")
+
+        valid_timecode_datatypes = [TIMESTAMP, DATE]
+        if type(timecode_datatype) not in valid_timecode_datatypes:
+            raise TypeError("timecode_datatype must be of one of the following types: {}. Found {}".
+                            format(valid_timecode_datatypes,
+                                   type(timecode_datatype)))
+
+        # Looks like the value is valid
+        return True
+
+    def __validate_timezero_date(self, timezero_date):
+        """
+        Internal function to validate timezero_date specified when creating a
+        Primary Time Index (PTI) table.
+
+        :param timezero_date: The timezero_date passed to primary_time_index().
+
+        :return: Boolean value indicating whether the argument is valid (True),
+         or raise ValueError when invalid.
+        """
+        # Return True is it is not specified or is None since it is optional
+        if timezero_date is None:
+            return True
+
+        pattern = re.compile(r"^DATE\s+'(.*)'$")
+        match = pattern.match(timezero_date)
+
+        err_msg = "Date format must be: DATE 'YYYY-MM-DD'. Found value with incorrect format: {}".format(timezero_date)
+        if match is not None:
+            try:
+                datetime.datetime.strptime(match.group(1), '%Y-%m-%d')
+            except ValueError:
+                raise ValueError(err_msg)
+        else:
+            raise ValueError(err_msg)
+
+        # Looks like the value is valid
+        return True
+
+    def __validate_columns_list(self, cols):
+        """
+        Internal function to validate columns list specified when creating a
+        Primary Time Index (PTI) table.
+
+        :param cols: The columns list (cols) passed to primary_time_index().
+
+        :return: Validated column list (list of strings).
+         Raise a ValueError/TypeError on validation failure.
+        """
+
+        err_msg = "'cols' must be a of type str or list of values of type str. Found: {}"
+        col_length_err = "Column name cannot be an empty string. Found: '{}'"
+
+        if cols is None:
+            return []
+
+        # Single column (string) specified
+        if isinstance(cols, str):
+            if len(cols) == 0:
+                raise ValueError(col_length_err.format(cols))
+            return [cols]
+        # list
+        elif isinstance(cols, list):
+            for col in cols:
+                # Must be a list of strings only
+                if not isinstance(col, str):
+                    raise TypeError(err_msg.format(list(map(type, cols))))
+                if len(col) == 0:
+                    raise ValueError(col_length_err.format(col))
+
+            # all columns in list validated to be strings
+            return cols
+        # neither a single string, nor a list
+        else:
+            raise TypeError(err_msg.format(type(cols)))
+
+    def __validate_timebucket_duration(self, timebucket_duration):
+        """
+        Internal function to validate timeduration_bucket specified when creating a
+        Primary Time Index (PTI) table.
+
+        :param timebucket_duration: The timebucket_duration passed to the primary_time_index().
+
+        :return: Boolean value indicating whether the argument is valid (True).
+         or raise ValueError when invalid.
+        """
+        # Return True is it is not specified or is None since it is optional
+        if timebucket_duration is None:
+            return True
+
+        if len(timebucket_duration) == 0:
+            raise ValueError("timebucket_duration cannot be an empty string.")
+
+        valid_timebucket_durations_formal = ['CAL_YEARS', 'CAL_MONTHS', 'WEEKS', 'DAYS', 'HOURS', 'MINUTES', 'SECONDS',
+                                             'MILLISECONDS', 'MICROSECONDS']
+        valid_timebucket_durations_shorthand = ['cy', 'cyear', 'cyears',
+                                                'cm', 'cmonth', 'cmonths',
+                                                'cd', 'cday', 'cdays',
+                                                'w', 'week', 'weeks',
+                                                'd', 'day', 'days',
+                                                'h', 'hr', 'hrs', 'hour', 'hours',
+                                                'm', 'mins', 'minute', 'minutes',
+                                                's', 'sec', 'secs', 'second', 'seconds',
+                                                'ms', 'msec', 'msecs', 'millisecond', 'milliseconds',
+                                                'us', 'usec', 'usecs', 'microsecond', 'microseconds']
+
+        # Message for error to be raise when n is invalid
+        n_err_msg = "'n' must be a positive integer. Found: {}"
+
+        # Check if notation if formal or shorthand (beginning with a digit)
+        if timebucket_duration[0].isdigit():
+            for short_notation in valid_timebucket_durations_shorthand:
+                pattern = re.compile("^([0-9]+){}$".format(short_notation))
+                match = pattern.match(timebucket_duration.lower())
+                if match is not None:
+                    try:
+                        n = int(match.group(1))
+                        if n < 0:
+                            raise ValueError(n_err_msg.format(n))
+
+                        # Looks like the value is valid
+                        return True
+                    except ValueError:
+                        raise ValueError(n_err_msg.format(match.group(1)))
+        else:
+            for formal_notation in valid_timebucket_durations_formal:
+                pattern = re.compile(r"^{}\(([0-9]+)\)$".format(formal_notation))
+                match = pattern.match(timebucket_duration.upper())
+                if match is not None:
+                    try:
+                        n = int(match.group(1))
+                        if n < 0:
+                            raise ValueError(n_err_msg.format(n))
+
+                        # Looks like the value is valid
+                        return True
+                    except ValueError:
+                        raise ValueError(n_err_msg.format(match.group(1)))
+
+        # Match not found
+        raise ValueError("Invalid timebucket_duration: {}".format(timebucket_duration))
+
+    def primary_time_index(self,
+                           timecode_datatype,
+                           name=None,
+                           timezero_date=None,
+                           timebucket_duration=None,
+                           sequenced=None,
+                           seq_max=None,
+                           cols=[]):
+        """
+        timecode_datatype:
+            Required Argument.
+            Reflection of the form of the timestamp data in the time series.
+            Permitted values:
+                A teradatasqlalchemy type representing either
+                * TIMESTAMP(n),
+                * TIMESTAMP(n) WITH TIME ZONE, or
+                * DATE.
+
+        name:
+            Optional Argument.
+            A name for the Primary Time Index (PTI).
+
+        timezero_date:
+            Optional Argument.
+            Specifies the earliest time series data that the PTI table will accept;
+            a date that precedes the earliest date in the time series data.
+            Value specified must be of the following format: DATE 'YYYY-MM-DD'
+            Default Value: DATE '1970-01-01'.
+
+        timebucket_duration:
+            Optional Argument.
+            Required if cols is not specified or is empty.
+            A duration that serves to break up the time continuum in
+            the time series data into discrete groups or buckets.
+            Specified using the formal form time_unit(n), where n is a positive
+            integer, and time_unit can be any of the following:
+            CAL_YEARS, CAL_MONTHS, CAL_DAYS, WEEKS, DAYS, HOURS, MINUTES,
+            SECONDS, MILLISECONDS, or MICROSECONDS.
+
+        sequenced:
+            Optional Argument.
+            Specifies whether the time series data readings are unique in time or not.
+            * True implies SEQUENCED, meaning more than one reading from the same
+              sensor may have the same timestamp.
+            * False implies NONSEQUENCED, meaning there is only one sensor reading
+              per timestamp.
+              This is the default.
+
+        seq_max:
+            Optional Argument.
+            Specifies the maximum number of sensor data rows that can have the
+            same timestamp. Can be used when 'sequenced' is True.
+            Accepted range:  1 - 2147483647.
+            Default Value: 20000.
+
+        cols:
+            Optional Argument.
+            Required if timebucket_duration is not specified.
+            A list of one or more PTI table column names.
+        """
+        # Validate timecode_datatype
+        self.__validate_timecode_datatype(timecode_datatype)
+
+        # Validate timebucket_duration
+        self.__validate_timebucket_duration(timebucket_duration)
+
+        # Validate timezero_date
+        self.__validate_timezero_date(timezero_date)
+
+        # Validate sequenced
+        if sequenced is not None:
+            if not isinstance(sequenced, bool):
+                raise TypeError("'sequenced', when specified, must be of type 'bool'. Found type: {}".
+                                format(type(sequenced)))
+
+        # Validate seq_max
+        if seq_max is not None:
+            if not isinstance(seq_max, int) or \
+                    (seq_max < 1 or seq_max > 2147483647):
+                raise ValueError("'seq_max' must be a positive integer in the range 1 through 2147483647. Found: {}".
+                                 format(seq_max))
+
+        # Validate cols
+        cols = self.__validate_columns_list(cols)
+
+        if timebucket_duration is None and len(cols) == 0:
+            raise SyntaxError("At least one of 'cols' or 'timebucket_duration' must be specified.")
+
+        res = 'PRIMARY TIME INDEX'
+        res += ' ' + name if name is not None else ''
+
+        val = [timecode_datatype.compile(dialect())]
+        if timezero_date is not None:
+            val.append(timezero_date)
+        if timebucket_duration is not None:
+            val.append(timebucket_duration)
+        if len(cols) > 0:
+            val.append('COLUMNS({})'.format(','.join([self._prepare.quote(c) for c in cols if c is not None])))
+        if sequenced is not None:
+            if sequenced:
+                val.append('SEQUENCED{}'.format('(' + str(seq_max) + ')' if seq_max is not None else ''))
+            else:
+                val.append('NONSEQUENCED')
+
+        return self.__class__(self._append(self.opts, {res: val}))
+
+    def primary_amp(self, name=None, cols=[]):
+
+        """
+        name is an optional string for the name of the amp index
+        cols is a list of column names (strings)
+        """
+        res = 'primary amp index'
+        res += ' ' + name if name is not None else ''
+        return self.__class__(self._append(self.opts, {res: [self._prepare.quote(c) for c in cols if c is not None]}))
+
+
+    def partition_by_col(self, all_but=False, cols={}, rows={}, const=None):
+
+        """
+        ex:
+
+        Opts.partition_by_col(cols ={'c1': True, 'c2': False, 'c3': None},
+                     rows ={'d1': True, 'd2':False, 'd3': None},
+                     const = 1)
+        will emit:
+
+        partition by(
+          column(
+            column(c1) auto compress,
+            column(c2) no auto compress,
+            column(c3),
+            row(d1) auto compress,
+            row(d2) no auto compress,
+            row(d3))
+            add 1
+            )
+
+        cols is a dictionary whose key is the column name and value True or False
+        specifying AUTO COMPRESS or NO AUTO COMPRESS respectively. The columns
+        are stored with COLUMN format.
+
+        rows is a dictionary similar to cols except the ROW format is used
+
+        const is an unsigned BIGINT
+        """
+        res = 'partition by( column all but' if all_but else \
+            'partition by( column'
+        c = self._visit_partition_by(cols, rows)
+        c += [{'post': (['add %s' % str(const)]
+                        if const is not None
+                        else []) + [')']}]
+
+        return self.__class__(self._append(self.opts, {res: c}))
+
+    def _visit_partition_by(self, cols, rows):
+
+        if cols:
+            c = ['column(' + self._prepare.quote(k) + ') auto compress ' \
+                 for k, v in cols.items() if v is True]
+
+            c += ['column(' + self._prepare.quote(k) + ') no auto compress' \
+                  for k, v in cols.items() if v is False]
+
+            c += ['column(' + self._prepare.quote(k) + ')' for k, v in cols.items() if v is None]
+
+        if rows:
+            c += ['row(' + k + ') auto compress' \
+                  for k, v in rows.items() if v is True]
+
+            c += ['row(' + k + ') no auto compress' \
+                  for k, v in rows.items() if v is False]
+
+            c += ['row(' + k + ')' for k, v in rows.items() if v is None]
+
+        return c
+
+    def partition_by_col_auto_compress(self, all_but=False, cols={}, \
+                                       rows={}, const=None):
+
+        res = 'partition by( column auto compress all but' if all_but else \
+            'partition by( column auto compress'
+        c = self._visit_partition_by(cols, rows)
+        c += [{'post': (['add %s' % str(const)]
+                        if const is not None
+                        else []) + [')']}]
+
+        return self.__class__(self._append(self.opts, {res: c}))
+
+    def partition_by_col_no_auto_compress(self, all_but=False, cols={}, \
+                                          rows={}, const=None):
+
+        res = 'partition by( column no auto compress all but' if all_but else \
+            'partition by( column no auto compression'
+        c = self._visit_partition_by(cols, rows)
+        c += [{'post': (['add %s' % str(const)]
+                        if const is not None
+                        else []) + [')']}]
+
+        return self.__class__(self._append(self.opts, {res: c}))
+
+    def index(self, index):
+        """
+        Index is created with dialect specific keywords to
+        include loading and ordering syntax elements
+
+        index is a sqlalchemy.sql.schema.Index object.
+        """
+        return self.__class__(self._append(self.opts, {res: c}))
+
+    def unique_index(self, name=None, cols=[]):
+        res = 'unique index ' + (name if name is not None else '')
+        return self.__class__(self._append(self.opts, {res: [self._prepare.quote(c) for c in cols if c is not None]}))
+
+# @compiles(Select, 'teradata')
+# def compile_select(element, compiler, **kw):
+#    """
+#    """
+#
+#    if not getattr(element, '_window_visit', None):
+#      if element._limit is not None or element._offset is not None:
+#          limit, offset = element._limit, element._offset
+#
+#          orderby=compiler.process(element._order_by_clause)
+#          if orderby:
+#            element = element._generate()
+#            element._window_visit=True
+#            #element._limit = None
+#            #element._offset = None  cant set to none...
+#
+#            # add a ROW NUMBER() OVER(ORDER BY) column
+#            element = element.column(sql.literal_column('ROW NUMBER() OVER (ORDER BY %s)' % orderby).label('rownum')).order_by(None)
+#
+#            # wrap into a subquery
+#            limitselect = sql.select([c for c in element.alias().c if c.key != 'rownum'])
+#
+#            limitselect._window_visit=True
+#            limitselect._is_wrapper=True
+#
+#            if offset is not None:
+#              limitselect.append_whereclause(sql.column('rownum') > offset)
+#              if limit is not None:
+#                  limitselect.append_whereclause(sql.column('rownum') <= (limit + offset))
+#            else:
+#              limitselect.append_whereclause(sql.column("rownum") <= limit)
+#
+#            element = limitselect
+#
+#    kw['iswrapper'] = getattr(element, '_is_wrapper', False)
+#    return compiler.visit_select(element, **kw)
```

## teradatasqlalchemy/options.py

 * *Ordering differences only*

```diff
@@ -1,60 +1,60 @@
-# ##################################################################
-#
-# Copyright 2021 Teradata. All rights reserved.
-# TERADATA CONFIDENTIAL AND TRADE SECRET
-#
-# Primary Owner: Pradeep Garre (pradeep.garre@teradata.com)
-# Secondary Owner: Pankaj Purandare (pankajvinod.purandare@teradata.com)
-#
-#
-# ##################################################################
-
-
-class _ConfigureSuper(object):
-
-    def __init__(self):
-        pass
-
-    def _SetKeyValue(self, name, value):
-        super().__setattr__(name, value)
-
-    def _GetValue(self, name):
-        return super().__getattribute__(name)
-
-
-def _create_property(name):
-    storage_name = '_' + name
-
-    @property
-    def prop(self):
-        return self._GetValue(storage_name)
-
-    @prop.setter
-    def prop(self, value):
-        self._SetKeyValue(storage_name, value)
-
-    return prop
-
-
-class _Configure(_ConfigureSuper):
-    """
-    Options to configure global parameters.
-    """
-
-    usexviews = _create_property('usexviews')
-
-    def __init__(self, usexviews=False):
-        super().__init__()
-        super().__setattr__('usexviews', usexviews)
-
-    def __setattr__(self, name, value):
-        if hasattr(self, name):
-            if name == 'usexviews':
-                if not isinstance(value, bool):
-                    raise TypeError("Invalid type passed to argument '{}', should be: {}.".format(name, "bool"))
-            super().__setattr__(name, value)
-        else:
-            raise AttributeError("'{}' object has no attribute '{}'".format(self.__class__.__name__, name))
-
-
+# ##################################################################
+#
+# Copyright 2021 Teradata. All rights reserved.
+# TERADATA CONFIDENTIAL AND TRADE SECRET
+#
+# Primary Owner: Pradeep Garre (pradeep.garre@teradata.com)
+# Secondary Owner: Pankaj Purandare (pankajvinod.purandare@teradata.com)
+#
+#
+# ##################################################################
+
+
+class _ConfigureSuper(object):
+
+    def __init__(self):
+        pass
+
+    def _SetKeyValue(self, name, value):
+        super().__setattr__(name, value)
+
+    def _GetValue(self, name):
+        return super().__getattribute__(name)
+
+
+def _create_property(name):
+    storage_name = '_' + name
+
+    @property
+    def prop(self):
+        return self._GetValue(storage_name)
+
+    @prop.setter
+    def prop(self, value):
+        self._SetKeyValue(storage_name, value)
+
+    return prop
+
+
+class _Configure(_ConfigureSuper):
+    """
+    Options to configure global parameters.
+    """
+
+    usexviews = _create_property('usexviews')
+
+    def __init__(self, usexviews=False):
+        super().__init__()
+        super().__setattr__('usexviews', usexviews)
+
+    def __setattr__(self, name, value):
+        if hasattr(self, name):
+            if name == 'usexviews':
+                if not isinstance(value, bool):
+                    raise TypeError("Invalid type passed to argument '{}', should be: {}.".format(name, "bool"))
+            super().__setattr__(name, value)
+        else:
+            raise AttributeError("'{}' object has no attribute '{}'".format(self.__class__.__name__, name))
+
+
 configure = _Configure()
```

## teradatasqlalchemy/requirements.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-from sqlalchemy.testing.requirements import SuiteRequirements
-from sqlalchemy.testing import exclusions
-
-# Requirements specifies the features this dialect does/does not support for testing purposes
-# see: https://github.com/zzzeek/sqlalchemy/blob/master/README.dialects.rst
-
-
-class Requirements(SuiteRequirements):
-    @property
-    def datetime_microseconds(self):
-        """target dialect supports representation of Python
-        datetime.datetime() with microsecond objects."""
-        return exclusions.open()
-
-    @property
-    def offset(self):
-        """target database can render OFFSET, or an equivalent, in a
-        SELECT.
-        """
-        return exclusions.closed()
-
-    @property
-    def bound_limit_offset(self):
-        """target database can render LIMIT and/or OFFSET using a bound
-        parameter
-        """
-        return exclusions.closed()
-
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+from sqlalchemy.testing.requirements import SuiteRequirements
+from sqlalchemy.testing import exclusions
+
+# Requirements specifies the features this dialect does/does not support for testing purposes
+# see: https://github.com/zzzeek/sqlalchemy/blob/master/README.dialects.rst
+
+
+class Requirements(SuiteRequirements):
+    @property
+    def datetime_microseconds(self):
+        """target dialect supports representation of Python
+        datetime.datetime() with microsecond objects."""
+        return exclusions.open()
+
+    @property
+    def offset(self):
+        """target database can render OFFSET, or an equivalent, in a
+        SELECT.
+        """
+        return exclusions.closed()
+
+    @property
+    def bound_limit_offset(self):
+        """target database can render LIMIT and/or OFFSET using a bound
+        parameter
+        """
+        return exclusions.closed()
+
```

## teradatasqlalchemy/resolver.py

 * *Ordering differences only*

```diff
@@ -1,167 +1,167 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-class TeradataTypeResolver:
-    """Type Resolver for Teradata Data Types.
-
-    For dynamically instantiating instances of TypeEngine (subclasses).
-    This class mimics the design of SQLAlchemy's TypeCompiler and in fact
-    takes advantage of the compiler's visitor double-dispatch mechanism.
-    This is accomplished by having the main process method redirect to the
-    passed in type_'s corresponding visit method defined by the TypeResolver
-    below.
-    """
-
-    def process(self, type_, **kw):
-        """Resolves the type.
-
-        Instantiate the type and populate its relevant attributes with the
-        appropriate keyword arguments.
-
-        Args:
-            type_: The type to be resolved (instantiated).
-
-            **kw:  Keyword arguments used for populating the attributes of the
-                   type being resolved.
-
-        Returns:
-            An instance of type_ correctly populated with the appropriate
-            keyword arguments.
-        """
-
-        return getattr(self, 'visit_' + type_.__visit_name__)(type_, **kw)
-
-    def visit_INTEGER(self, type_, **kw):
-        return type_()
-
-    def visit_SMALLINT(self, type_, **kw):
-        return type_()
-
-    def visit_BIGINT(self, type_, **kw):
-        return type_()
-
-    def visit_DECIMAL(self, type_, **kw):
-        return type_(precision=kw['prec'], scale=kw['scale'])
-
-    def visit_DATE(self, type_, **kw):
-        return type_()
-
-    def _resolve_type_interval(self, type_, **kw):
-        return type_(precision=kw['prec'], frac_precision=kw['scale'])
-
-    def visit_INTERVAL_YEAR(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_YEAR_TO_MONTH(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_MONTH(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_DAY(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_DAY_TO_HOUR(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_DAY_TO_MINUTE(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_DAY_TO_SECOND(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_HOUR(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_HOUR_TO_MINUTE(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_HOUR_TO_SECOND(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_MINUTE(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_MINUTE_TO_SECOND(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_INTERVAL_SECOND(self, type_, **kw):
-        return self._resolve_type_interval(type_, **kw)
-
-    def visit_PERIOD_DATE(self, type_, **kw):
-        return type_(format=kw['fmt'])
-
-    def visit_PERIOD_TIME(self, type_, **kw):
-        tz = kw['typecode'] == 'PZ'
-        return type_(format=kw['fmt'], frac_precision=kw['scale'], timezone=tz)
-
-    def visit_PERIOD_TIMESTAMP(self, type_, **kw):
-        tz = kw['typecode'] == 'PM'
-        return type_(format=kw['fmt'], frac_precision=kw['scale'], timezone=tz)
-
-    def visit_TIME(self, type_, **kw):
-        tz = kw['typecode'] == 'TZ'
-        return type_(precision=kw['scale'], timezone=tz)
-
-    def visit_TIMESTAMP(self, type_, **kw):
-        tz = kw['typecode'] == 'SZ'
-        return type_(precision=kw['scale'], timezone=tz)
-
-    def _resolve_type_string(self, type_, **kw):
-        return type_(
-            length=int(kw['length'] / 2) if
-                   (kw['chartype'] == 'UNICODE' or kw['chartype'] == 'GRAPHIC')
-                    else kw['length'],
-            charset=kw['chartype'])
-
-    def visit_CHAR(self, type_, **kw):
-        return self._resolve_type_string(type_, **kw)
-
-    def visit_VARCHAR(self, type_, **kw):
-        return self._resolve_type_string(type_, **kw)
-
-    def visit_CLOB(self, type_, **kw):
-        return self._resolve_type_string(type_, **kw)
-
-    def visit_BYTEINT(self, type_, **kw):
-        return type_()
-
-    def visit_FLOAT(self, type_, **kw):
-        return type_()
-
-    def _resolve_type_binary(self, type_, **kw):
-        return type_(length=kw['length'])
-
-    def visit_BYTE(self, type_, **kw):
-        return self._resolve_type_binary(type_, **kw)
-
-    def visit_VARBYTE(self, type_, **kw):
-        return self._resolve_type_binary(type_, **kw)
-
-    def visit_BLOB(self, type_, **kw):
-        # TODO Multiplier of BLOB currently not recovered when reflected
-        return self._resolve_type_binary(type_, **kw)
-
-    def visit_NUMBER(self, type_, **kw):
-        return type_(precision=kw['prec'], scale=kw['scale'])
-
-    def visit_XML(self, type_, **kw):
-        return type_(maximum_length=kw['length'], inline_length=kw['inline_length'])
-
-    def visit_JSON(self, type_, **kw):
-        return type_(max_length=kw['length'],
-                     inline_length=kw['inline_length'], charset=kw['chartype'],
-                     storage_format=kw['storage_format'])
-
-    def visit_TDUDT(self, type_, **kw):
-        return type_(type_name=kw['col_udt_name'])
-
-    def visit_GEOMETRY(self, type_, **kw):
-        return type_(max_length=kw['length'],
-                     inline_length=kw['inline_length'])
-
-    def visit_MBR(self, type_, **kw):
-        return type_()
-
-    def visit_MBB(self, type_, **kw):
-        return type_()
-
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+class TeradataTypeResolver:
+    """Type Resolver for Teradata Data Types.
+
+    For dynamically instantiating instances of TypeEngine (subclasses).
+    This class mimics the design of SQLAlchemy's TypeCompiler and in fact
+    takes advantage of the compiler's visitor double-dispatch mechanism.
+    This is accomplished by having the main process method redirect to the
+    passed in type_'s corresponding visit method defined by the TypeResolver
+    below.
+    """
+
+    def process(self, type_, **kw):
+        """Resolves the type.
+
+        Instantiate the type and populate its relevant attributes with the
+        appropriate keyword arguments.
+
+        Args:
+            type_: The type to be resolved (instantiated).
+
+            **kw:  Keyword arguments used for populating the attributes of the
+                   type being resolved.
+
+        Returns:
+            An instance of type_ correctly populated with the appropriate
+            keyword arguments.
+        """
+
+        return getattr(self, 'visit_' + type_.__visit_name__)(type_, **kw)
+
+    def visit_INTEGER(self, type_, **kw):
+        return type_()
+
+    def visit_SMALLINT(self, type_, **kw):
+        return type_()
+
+    def visit_BIGINT(self, type_, **kw):
+        return type_()
+
+    def visit_DECIMAL(self, type_, **kw):
+        return type_(precision=kw['prec'], scale=kw['scale'])
+
+    def visit_DATE(self, type_, **kw):
+        return type_()
+
+    def _resolve_type_interval(self, type_, **kw):
+        return type_(precision=kw['prec'], frac_precision=kw['scale'])
+
+    def visit_INTERVAL_YEAR(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_YEAR_TO_MONTH(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_MONTH(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_DAY(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_DAY_TO_HOUR(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_DAY_TO_MINUTE(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_DAY_TO_SECOND(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_HOUR(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_HOUR_TO_MINUTE(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_HOUR_TO_SECOND(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_MINUTE(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_MINUTE_TO_SECOND(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_INTERVAL_SECOND(self, type_, **kw):
+        return self._resolve_type_interval(type_, **kw)
+
+    def visit_PERIOD_DATE(self, type_, **kw):
+        return type_(format=kw['fmt'])
+
+    def visit_PERIOD_TIME(self, type_, **kw):
+        tz = kw['typecode'] == 'PZ'
+        return type_(format=kw['fmt'], frac_precision=kw['scale'], timezone=tz)
+
+    def visit_PERIOD_TIMESTAMP(self, type_, **kw):
+        tz = kw['typecode'] == 'PM'
+        return type_(format=kw['fmt'], frac_precision=kw['scale'], timezone=tz)
+
+    def visit_TIME(self, type_, **kw):
+        tz = kw['typecode'] == 'TZ'
+        return type_(precision=kw['scale'], timezone=tz)
+
+    def visit_TIMESTAMP(self, type_, **kw):
+        tz = kw['typecode'] == 'SZ'
+        return type_(precision=kw['scale'], timezone=tz)
+
+    def _resolve_type_string(self, type_, **kw):
+        return type_(
+            length=int(kw['length'] / 2) if
+                   (kw['chartype'] == 'UNICODE' or kw['chartype'] == 'GRAPHIC')
+                    else kw['length'],
+            charset=kw['chartype'])
+
+    def visit_CHAR(self, type_, **kw):
+        return self._resolve_type_string(type_, **kw)
+
+    def visit_VARCHAR(self, type_, **kw):
+        return self._resolve_type_string(type_, **kw)
+
+    def visit_CLOB(self, type_, **kw):
+        return self._resolve_type_string(type_, **kw)
+
+    def visit_BYTEINT(self, type_, **kw):
+        return type_()
+
+    def visit_FLOAT(self, type_, **kw):
+        return type_()
+
+    def _resolve_type_binary(self, type_, **kw):
+        return type_(length=kw['length'])
+
+    def visit_BYTE(self, type_, **kw):
+        return self._resolve_type_binary(type_, **kw)
+
+    def visit_VARBYTE(self, type_, **kw):
+        return self._resolve_type_binary(type_, **kw)
+
+    def visit_BLOB(self, type_, **kw):
+        # TODO Multiplier of BLOB currently not recovered when reflected
+        return self._resolve_type_binary(type_, **kw)
+
+    def visit_NUMBER(self, type_, **kw):
+        return type_(precision=kw['prec'], scale=kw['scale'])
+
+    def visit_XML(self, type_, **kw):
+        return type_(maximum_length=kw['length'], inline_length=kw['inline_length'])
+
+    def visit_JSON(self, type_, **kw):
+        return type_(max_length=kw['length'],
+                     inline_length=kw['inline_length'], charset=kw['chartype'],
+                     storage_format=kw['storage_format'])
+
+    def visit_TDUDT(self, type_, **kw):
+        return type_(type_name=kw['col_udt_name'])
+
+    def visit_GEOMETRY(self, type_, **kw):
+        return type_(max_length=kw['length'],
+                     inline_length=kw['inline_length'])
+
+    def visit_MBR(self, type_, **kw):
+        return type_()
+
+    def visit_MBB(self, type_, **kw):
+        return type_()
+
```

## teradatasqlalchemy/restricted_words.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-restricted_words = {'corr', 'check', 'sqlexception', 'year', 'int', 'td_host', 'substr', 'count', 'monitor', 'add_months', 'natural', 'blob', 'convert_table_header', 'hashamp', 'transactiontime', 'desc', 'objects', 'max', 'like', 'referencing', 'iterate', 'disabled', 'locking', 'escape', 'execute', 'deferred', 'statistics', 'minimum', 'var_pop', 'before', 'replication', 'ins', 'named', 'lt', 'errortables', 'number', 'time', 'regr_avgy', 'specific', 'graphic', 'mindex', 'result', 'trim', 'leading', 'regr_avgx', 'char2hexint', 'clob', 'double', 'except', 'extract', 'trigger', 'acosh', 'sql', 'jar', 'current_user', 'password', 'work', 'min', 'regr_slope', 'union', 'compress', 'returns', 'logging', 'or', 'override', 'ansidate', 'with', 'until_changed', 'var_samp', 'local', 'parameter', 'rollup', 'protection', 'sqrt', 'exp', 'mode', 'minus', 'relative', 'bt', 'leave', 'mavg', 'regr_r2', 'rights', 'database', 'varbyte', 'private', 'some', 'et', 'loading', 'expand', 'between', 'upd', 'arglparen', 'continue', 'overlaps', 'uc', 'dynamic', 'cd', 'byteint', 'begin', 'release', 'cross', 'identity', 'if', 'explain', 'open', 'in', 'id2bigint', 'handler', 'retrieve', 'elseif', 'position', 'fallback', 'startup', 'varying', 'tan', 'hashbakamp', 'covar_pop', 'amp', 'at', 'td_rowloadid', 'update', 'lower', 'macro', 'but', 'first', 'public', 'log', 'initiate', 'degrees', 'abort', 'object', 'asin', 'trailing', 'format', 'long', 'case_n', 'until_closed', 'authorization', 'modifies', 'large', 'statement', 'instance', 'while', 'mlinreg', 'hashrow', 'modify', 'hour', 'terminate', 'real', 'start', 'mod', 'stddev_pop', 'atan2', 'role', 'bytes', 'descriptor', 'top', 'day', 'integer', 'from', 'inout', 'join', 'wait', 'dateform', 'maximum', 'null', 'when', 'end', 'integerdate', 'mload', 'new_table', 'then', 'dump', 'insert', 'numeric', 'method', 'ct', 'ctcontrol', 'session', 'request', 'revalidate', 'references', 'datablocksize', 'msum', 'del', 'replace', 'default', 'rank', 'values', 'order', 'no', 'sel', 'row', 'checkpoint', 'collation', 'else', 'sqlwarning', 'csum', 'ln', 'row_number', 'rowid', 'zeroifnull', 'transform', 'character', 'each', 'gt', 'char', 'nontemporal', 'uescape', 'skew', 'td_valist', 'dec', 'decimal', 'soundex', 'exec', 'spool', 'ret', 'get', 'percent', 'validtime', 'instead', 'timezone_minute', 'cos', 'set', 'current', 'uppercase', 'translate', 'regr_sxy', 'account', 'sinh', 'return', 'rename', 'full', 'external', 'udtcastas', 'out', 'all', 'regr_sxx', 'nullif', 'case', 'chars', 'td_authid', 'perm', 'exists', 'xmlplan', 'sample', 'tbl_cs', 'threshold', 'udtcastlparen', 'udttype', 'primary', 'characters', 'smallint', 'indicator', 'minute', 'constructor', 'cluster', 'char_length', 'abortsession', 'float', 'fastexport', 'only', 'intersect', 'old_table', 'setresrate', 'having', 'radians', 'rollforward', 'consume', 'is', 'not', 'percent_rank', 'resume', 'using', 'resignal', 'where', 'character_length', 'deallocate', 'comment', 'domain', 'monsession', 'atan', 'grouping', 'help', 'off', 'index', 'asc', 'current_role', 'old', 'prepare', 'expanding', 'generated', 'collect', 'ge', 'echo', 'connect', 'table', 'restart', 'cycle', 'ne', 'sets', 'width_bucket', 'undo', 'second', 'into', 'procedure', 'timestamp', 'user', 'current_timestamp', 'subscriber', 'next', 'drop', 'none', 'qualified', 'goto', 'covar_samp', 'acos', 'deterministic', 'queue', 'timezone_hour', 'cv', 'preserve', 'setsessrate', 'inconsistent', 'cursor', 'casespecific', 'function', 'for', 'volatile', 'cosh', 'hash', 'multiset', 'regr_count', 'column', 'tanh', 'trace', 'ave', 'vargraphic', 'equals', 'zone', 'add', 'over', 'substring', 'recursive', 'inner', 'errorfiles', 'range_n', 'byte', 'locator', 'create', 'access_lock', 'current_date', 'suspend', 'alias', 'on', 'journal', 'nowait', 'exit', 'binary', 'varchar', 'foreign', 'right', 'input', 'nullifzero', 'kurtosis', 'view', 'average', 'le', 'class', 'td_anytype', 'new', 'unique', 'commit', 'option', 'precision', 'by', 'temporary', 'translate_chk', 'restore', 'lock', 'dual', 'replcontrol', 'normalize', 'aggregate', 'found', 'delete', 'diagnostic', 'regr_intercept', 'atanh', 'alter', 'bigint', 'logon', 'select', 'signal', 'mdiff', 'undefined', 'any', 'month', 'both', 'revoke', 'to', 'cast', 'declare', 'show', 'value', 'mcharacters', 'group', 'abs', 'go', 'ordering', 'udtusage', 'distinct', 'call', 'do', 'type', 'transaction', 'limit', 'give', 'rollback', 'ss', 'summary', 'date', 'cm', 'without', 'qualify', 'variant_type', 'cs', 'error', 'rows', 'flush', 'close', 'freespace', 'eq', 'upper', 'title', 'string_cs', 'fetch', 'hashbucket', 'msubstr', 'until', 'sin', 'repeat', 'key', 'avg', 'loop', 'sqltext', 'merge', 'coalesce', 'immediate', 'permanent', 'language', 'profile', 'atomic', 'asinh', 'random', 'sum', 'scroll', 'as', 'octet_length', 'contains', 'sampleid', 'after', 'stddev_samp', 'interval', 'grant', 'of', 'and', 'outer', 'map', 'reads', 'udtmethod', 'left', 'constraint', 'admin', 'monresource', 'stepinfo', 'regr_syy', 'current_time', 'cube', 'privileges', 'enabled', 'quantile'}
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+restricted_words = {'corr', 'check', 'sqlexception', 'year', 'int', 'td_host', 'substr', 'count', 'monitor', 'add_months', 'natural', 'blob', 'convert_table_header', 'hashamp', 'transactiontime', 'desc', 'objects', 'max', 'like', 'referencing', 'iterate', 'disabled', 'locking', 'escape', 'execute', 'deferred', 'statistics', 'minimum', 'var_pop', 'before', 'replication', 'ins', 'named', 'lt', 'errortables', 'number', 'time', 'regr_avgy', 'specific', 'graphic', 'mindex', 'result', 'trim', 'leading', 'regr_avgx', 'char2hexint', 'clob', 'double', 'except', 'extract', 'trigger', 'acosh', 'sql', 'jar', 'current_user', 'password', 'work', 'min', 'regr_slope', 'union', 'compress', 'returns', 'logging', 'or', 'override', 'ansidate', 'with', 'until_changed', 'var_samp', 'local', 'parameter', 'rollup', 'protection', 'sqrt', 'exp', 'mode', 'minus', 'relative', 'bt', 'leave', 'mavg', 'regr_r2', 'rights', 'database', 'varbyte', 'private', 'some', 'et', 'loading', 'expand', 'between', 'upd', 'arglparen', 'continue', 'overlaps', 'uc', 'dynamic', 'cd', 'byteint', 'begin', 'release', 'cross', 'identity', 'if', 'explain', 'open', 'in', 'id2bigint', 'handler', 'retrieve', 'elseif', 'position', 'fallback', 'startup', 'varying', 'tan', 'hashbakamp', 'covar_pop', 'amp', 'at', 'td_rowloadid', 'update', 'lower', 'macro', 'but', 'first', 'public', 'log', 'initiate', 'degrees', 'abort', 'object', 'asin', 'trailing', 'format', 'long', 'case_n', 'until_closed', 'authorization', 'modifies', 'large', 'statement', 'instance', 'while', 'mlinreg', 'hashrow', 'modify', 'hour', 'terminate', 'real', 'start', 'mod', 'stddev_pop', 'atan2', 'role', 'bytes', 'descriptor', 'top', 'day', 'integer', 'from', 'inout', 'join', 'wait', 'dateform', 'maximum', 'null', 'when', 'end', 'integerdate', 'mload', 'new_table', 'then', 'dump', 'insert', 'numeric', 'method', 'ct', 'ctcontrol', 'session', 'request', 'revalidate', 'references', 'datablocksize', 'msum', 'del', 'replace', 'default', 'rank', 'values', 'order', 'no', 'sel', 'row', 'checkpoint', 'collation', 'else', 'sqlwarning', 'csum', 'ln', 'row_number', 'rowid', 'zeroifnull', 'transform', 'character', 'each', 'gt', 'char', 'nontemporal', 'uescape', 'skew', 'td_valist', 'dec', 'decimal', 'soundex', 'exec', 'spool', 'ret', 'get', 'percent', 'validtime', 'instead', 'timezone_minute', 'cos', 'set', 'current', 'uppercase', 'translate', 'regr_sxy', 'account', 'sinh', 'return', 'rename', 'full', 'external', 'udtcastas', 'out', 'all', 'regr_sxx', 'nullif', 'case', 'chars', 'td_authid', 'perm', 'exists', 'xmlplan', 'sample', 'tbl_cs', 'threshold', 'udtcastlparen', 'udttype', 'primary', 'characters', 'smallint', 'indicator', 'minute', 'constructor', 'cluster', 'char_length', 'abortsession', 'float', 'fastexport', 'only', 'intersect', 'old_table', 'setresrate', 'having', 'radians', 'rollforward', 'consume', 'is', 'not', 'percent_rank', 'resume', 'using', 'resignal', 'where', 'character_length', 'deallocate', 'comment', 'domain', 'monsession', 'atan', 'grouping', 'help', 'off', 'index', 'asc', 'current_role', 'old', 'prepare', 'expanding', 'generated', 'collect', 'ge', 'echo', 'connect', 'table', 'restart', 'cycle', 'ne', 'sets', 'width_bucket', 'undo', 'second', 'into', 'procedure', 'timestamp', 'user', 'current_timestamp', 'subscriber', 'next', 'drop', 'none', 'qualified', 'goto', 'covar_samp', 'acos', 'deterministic', 'queue', 'timezone_hour', 'cv', 'preserve', 'setsessrate', 'inconsistent', 'cursor', 'casespecific', 'function', 'for', 'volatile', 'cosh', 'hash', 'multiset', 'regr_count', 'column', 'tanh', 'trace', 'ave', 'vargraphic', 'equals', 'zone', 'add', 'over', 'substring', 'recursive', 'inner', 'errorfiles', 'range_n', 'byte', 'locator', 'create', 'access_lock', 'current_date', 'suspend', 'alias', 'on', 'journal', 'nowait', 'exit', 'binary', 'varchar', 'foreign', 'right', 'input', 'nullifzero', 'kurtosis', 'view', 'average', 'le', 'class', 'td_anytype', 'new', 'unique', 'commit', 'option', 'precision', 'by', 'temporary', 'translate_chk', 'restore', 'lock', 'dual', 'replcontrol', 'normalize', 'aggregate', 'found', 'delete', 'diagnostic', 'regr_intercept', 'atanh', 'alter', 'bigint', 'logon', 'select', 'signal', 'mdiff', 'undefined', 'any', 'month', 'both', 'revoke', 'to', 'cast', 'declare', 'show', 'value', 'mcharacters', 'group', 'abs', 'go', 'ordering', 'udtusage', 'distinct', 'call', 'do', 'type', 'transaction', 'limit', 'give', 'rollback', 'ss', 'summary', 'date', 'cm', 'without', 'qualify', 'variant_type', 'cs', 'error', 'rows', 'flush', 'close', 'freespace', 'eq', 'upper', 'title', 'string_cs', 'fetch', 'hashbucket', 'msubstr', 'until', 'sin', 'repeat', 'key', 'avg', 'loop', 'sqltext', 'merge', 'coalesce', 'immediate', 'permanent', 'language', 'profile', 'atomic', 'asinh', 'random', 'sum', 'scroll', 'as', 'octet_length', 'contains', 'sampleid', 'after', 'stddev_samp', 'interval', 'grant', 'of', 'and', 'outer', 'map', 'reads', 'udtmethod', 'left', 'constraint', 'admin', 'monresource', 'stepinfo', 'regr_syy', 'current_time', 'cube', 'privileges', 'enabled', 'quantile'}
```

## teradatasqlalchemy/types.py

 * *Ordering differences only*

```diff
@@ -1,1669 +1,1669 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-from sqlalchemy import util
-from sqlalchemy import types
-from sqlalchemy.sql import sqltypes, operators
-
-import datetime, decimal
-import warnings, sys
-#import teradata.datatypes as td_dtypes
-
-
-class _TDComparable:
-    """Teradata Comparable Data Type."""
-
-    class Comparator(types.TypeEngine.Comparator):
-        """Comparator for expression adaptation.
-
-        Use the TeradataExpressionAdapter to process the resulting types
-        for binary operations over Teradata types.
-        """
-
-        def _adapt_expression(self, op, other_comparator):
-            expr_type = TeradataExpressionAdapter().process(
-                self.type, op=op, other=other_comparator.type)
-            return op, expr_type()
-
-    comparator_factory = Comparator
-
-
-class _TDConcatenable:
-    """Teradata Concatenable Data Type.
-
-    This family of types currently encompasses the binary types
-    (BYTE, VARBYTE, BLOB) and the character types (CHAR, VARCHAR, CLOB).
-    """
-
-    class Comparator(_TDComparable.Comparator):
-        """Comparator for expression adaptation.
-
-        Overloads the addition (+) operator over concatenable Teradata types
-        to use concat_op. Note that this overloading only occurs between types
-        within the same type_affinity.
-        """
-
-        def _adapt_expression(self, op, other_comparator):
-            return super(_TDConcatenable.Comparator, self)._adapt_expression(
-                operators.concat_op if op is operators.add and
-                    isinstance(other_comparator.type, self.type._type_affinity)
-                else op, other_comparator)
-
-    comparator_factory = Comparator
-
-
-class _TDLiteralCoercer:
-    """Mixin for literal type processing against Teradata data types."""
-
-    def coerce_compared_value(self, op, value):
-        type_ = type(value)
-
-        if type_ == int:
-            return INTEGER()
-        elif type_ == float:
-            return FLOAT()
-        elif type_ == bytes:
-            return BYTE()
-        elif type_ == str:
-            return VARCHAR()
-        elif type_ == decimal.Decimal:
-            return DECIMAL()
-        elif type_ == datetime.date:
-            return DATE()
-        elif type_ == datetime.datetime:
-            return TIMESTAMP()
-        elif type_ == datetime.time:
-            return TIME()
-       # elif type_ == td_dtypes.Interval:
-       #     return getattr(sys.modules[__name__],
-       #         'INTERVAL_' + value.type.replace(' ', '_'),
-       #         sqltypes.NullType)()
-        # TODO PERIOD
-
-        return sqltypes.NullType()
-
-
-class _TDType(_TDLiteralCoercer, _TDComparable):
-
-    """ Teradata Data Type
-
-    Identifies a Teradata data type. Currently used to override __str__
-    behavior such that the type will get printed without being compiled by the
-    GenericTypeCompiler (which would otherwise result in an exception).
-    """
-
-    def _parse_name(self, name):
-        return name.replace('_', ' ')
-
-    def __str__(self):
-        return self._parse_name(self.__class__.__name__)
-
-
-class INTEGER(_TDType, sqltypes.INTEGER):
-
-    """ Teradata INTEGER type
-
-    Represents a signed, binary integer value from -2,147,483,648 to
-    2,147,483,647.
-
-    """
-
-    def __init__(self, **kwargs):
-
-        """ Construct a INTEGER Object """
-        super(INTEGER, self).__init__(**kwargs)
-
-
-class SMALLINT(_TDType, sqltypes.SMALLINT):
-
-    """ Teradata SMALLINT type
-
-    Represents a signed binary integer value in the range -32768 to 32767.
-
-    """
-
-    def __init__(self, **kwargs):
-
-        """ Construct a SMALLINT Object """
-        super(SMALLINT, self).__init__(**kwargs)
-
-
-class BIGINT(_TDType, sqltypes.BIGINT):
-
-    """ Teradata BIGINT type
-
-    Represents a signed, binary integer value from -9,223,372,036,854,775,808
-    to 9,223,372,036,854,775,807.
-
-    """
-
-    def __init__(self, **kwargs):
-
-        """ Construct a BIGINT Object """
-        super(BIGINT, self).__init__(**kwargs)
-
-
-class DECIMAL(_TDType, sqltypes.DECIMAL):
-
-    """ Teradata DECIMAL type
-
-    Represents a decimal number of n digits, with m of those n digits to the
-    right of the decimal point.
-
-    """
-
-    def __init__(self, precision = 38, scale = 19, **kwargs):
-
-        """ Construct a DECIMAL Object """
-        super(DECIMAL, self).__init__(precision = precision, scale = scale, **kwargs)
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-            return str(value) + ('' if value.as_tuple()[2] < 0 else '.')
-        return process
-
-
-class BYTEINT(_TDType, sqltypes.Integer):
-
-    """ Teradata BYTEINT type
-
-    This type represents a one byte signed integer.
-
-    """
-
-    __visit_name__ = 'BYTEINT'
-
-    def __init__(self, **kwargs):
-
-        """ Construct a BYTEINT Object """
-        super(BYTEINT, self).__init__(**kwargs)
-
-
-class _TDBinary(_TDConcatenable, _TDType, sqltypes._Binary):
-
-    """ Teradata Binary Types
-
-    This type represents a Teradata binary string. Warns users when
-    data may get truncated upon insertion.
-
-    """
-
-    class TruncationWarning(UserWarning):
-        pass
-
-    def _length(self):
-        """Compute the length allocated to this binary column."""
-
-        multiplier_map = {
-            'K': 1024,
-            'M': 1048576,
-            'G': 1073741824
-        }
-        if hasattr(self, 'multiplier') and self.multiplier in multiplier_map:
-            return self.length * multiplier_map[self.multiplier]
-
-        return self.length
-
-    def bind_processor(self, dialect):
-        if dialect.dbapi is None:
-            return None
-
-        def process(value):
-            bin_length = self._length()
-            if value is not None and bin_length is not None:
-                if len(value) > bin_length:
-                    warnings.warn(
-                        'Attempting to insert an item that is larger than the '
-                        'space allocated for this column. Data may get truncated.',
-                        self.TruncationWarning)
-                return value
-            else:
-                return None
-        return process
-
-
-class BYTE(_TDBinary, sqltypes.BINARY):
-
-    """ Teradata BYTE type
-
-    This type represents a fixed-length binary string and is equivalent to
-    the BINARY SQL standard type.
-
-    """
-
-    __visit_name__ = 'BYTE'
-
-    def __init__(self, length=None, **kwargs):
-
-        """ Construct a BYTE object
-
-        :param length: Optional 1 to n. Specifies the number of bytes in the
-        fixed-length binary string. The maximum value for n is 64000.
-
-        """
-        super(BYTE, self).__init__(length=length, **kwargs)
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-
-            try:
-
-              # Python 3.5+
-              return "'%s'XB" % value.hex()
-
-            except AttributeError:
-
-              # try it with codecs
-              import codecs
-              return "'%s'XB" % codecs.encode(value, 'hex').decode('utf-8')
-
-        return process
-
-
-class VARBYTE(_TDBinary, sqltypes.VARBINARY):
-
-    """ Teradata VARBYTE type
-
-    This type represents a variable-length binary string and is equivalent to
-    the VARBINARY SQL standard type.
-
-    """
-
-    __visit_name__ = 'VARBYTE'
-
-    def __init__(self, length=None, **kwargs):
-
-        """ Construct a VARBYTE object
-
-        :param length: Optional 1 to n. Specifies the number of bytes in the
-        fixed-length binary string. The maximum value for n is 64000.
-
-        """
-        super(VARBYTE, self).__init__(length=length, **kwargs)
-
-
-class BLOB(_TDBinary, sqltypes.BLOB):
-
-    """ Teradata BLOB type
-
-    This type represents a large binary string of raw bytes. A binary large
-    object (BLOB) column can store binary objects, such as graphics, video
-    clips, files, and documents.
-
-    """
-
-    def __init__(self, length=None, multiplier=None, **kwargs):
-
-        """ Construct a BLOB object
-
-        :param length: Optional 1 to n. Specifies the number of bytes allocated
-        for the BLOB column. The maximum number of bytes is 2097088000, which
-        is the default if n is not specified.
-
-        :param multiplier: Optional value in ('K', 'M', 'G'). Indicates that the
-        length parameter n is specified in kilobytes (KB), megabytes (Mb),
-        or gigabytes (GB) respectively. Note the following constraints on n
-        hold for each of the allowable multiplier:
-
-            'K' is specified, n cannot exceed 2047937.
-            'M' is specified, n cannot exceed 1999.
-            'G' is specified, n must be 1.
-
-        If multiplier is None, the length is interepreted as bytes (B).
-
-        Note: If you specify a multiplier without specifying the length, the
-              multiplier argument will simply get ignored. On the other hand,
-              specifying a length without a multiplier will implicitly indicate
-              that the length value should be interpreted as bytes (B).
-
-        """
-        super(BLOB, self).__init__(length=length, **kwargs)
-        self.multiplier = multiplier
-
-
-class FLOAT(_TDType, sqltypes.FLOAT):
-
-    """ Teradata FLOAT type
-
-    This type represent values in sign/magnitude form ranging from
-    2.226 x 10^-308 to 1.797 x 10^308.
-
-    """
-
-    def __init__(self, **kwargs):
-
-        """ Construct a FLOAT object """
-        super(FLOAT, self).__init__(**kwargs)
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-            return 'CAST(%s as FLOAT)' % value
-        return process
-
-
-class NUMBER(_TDType, sqltypes.NUMERIC):
-
-    """ Teradata NUMBER type
-
-    This type represents a numeric value with optional precision and scale
-    limitations.
-
-    """
-
-    __visit_name__ = 'NUMBER'
-
-    def __init__(self, precision=None, scale=None, **kwargs):
-
-        """ Construct a NUMBER object
-
-        :param precision: max number of digits that can be stored. Valid values
-        range from 1 to 38.
-
-        :param scale: number of fractional digits of :param precision: to the
-        right of the decimal point. Valid values range from 0 to
-        :param precision:.
-
-        Note: Both parameters are optional. When both are left unspecified,
-              defaults to NUMBER with the system limits for precision and scale.
-
-        """
-        prec = None if precision is not None and precision < 0 else precision
-        scale = None if scale is not None and scale < 0 else scale
-        super(NUMBER, self).__init__(precision=prec, scale=scale, **kwargs)
-
-
-class DATE(_TDType, sqltypes.DATE):
-
-    """ Teradata DATE type
-
-    Identifies a field as a DATE value and simplifies handling and formatting
-    of date variables.
-
-    """
-
-    def __init__(self, **kwargs):
-
-        """ Construct a DATE Object """
-        super(DATE, self).__init__(**kwargs)
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-            return "DATE '%s'" % value
-        return process
-
-
-class TIME(_TDType, sqltypes.TIME):
-
-    """ Teradata TIME type
-
-    This type identifies a field as a TIME value.
-
-    """
-
-    def __init__(self, precision=6, timezone=False, **kwargs):
-
-        """ Construct a TIME stored as UTC in Teradata
-
-        :param precision: optional fractional seconds precision. A single digit
-        representing the number of significant digits in the fractional
-        portion of the SECOND field. Valid values range from 0 to 6 inclusive.
-        The default precision is 6.
-
-        :param timezone: If set to True creates a Time WITH TIME ZONE type
-
-        """
-        super(TIME, self).__init__(timezone=timezone, **kwargs)
-        self.precision = precision
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-            return "TIME '%s'" % value
-        return process
-
-
-class TIMESTAMP(_TDType, sqltypes.TIMESTAMP):
-
-    """ Teradata TIMESTAMP type
-
-    This type identifies a field as a TIMESTAMP value.
-
-    """
-
-    def __init__(self, precision=6, timezone=False, **kwargs):
-        """ Construct a TIMESTAMP stored as UTC in Teradata
-
-        :param precision: optional fractional seconds precision. A single digit
-        representing the number of significant digits in the fractional
-        portion of the SECOND field. Valid values range from 0 to 6 inclusive.
-        The default precision is 6.
-
-        :param timezone: If set to True creates a TIMESTAMP WITH TIME ZONE type
-
-        """
-        super(TIMESTAMP, self).__init__(timezone=timezone, **kwargs)
-        self.precision = precision
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-            return "TIMESTAMP '%s'" % value
-        return process
-
-    def get_dbapi_type(self, dbapi):
-      return dbapi.DATETIME
-
-
-class _TDInterval(_TDType, types.UserDefinedType):
-
-    """ Base class for the Teradata INTERVAL sqltypes """
-
-    def __init__(self, precision=None, frac_precision=None, **kwargs):
-        self.precision      = precision
-        self.frac_precision = frac_precision
-
-    def bind_processor(self, dialect):
-
-        """
-        Processes the Interval value from SQLAlchemy to DB
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-
-        """
-        Processes the Interval value from DB to SQLAlchemy
-        """
-        def process(value):
-            return value
-        return process
-
-    def literal_processor(self, dialect):
-
-        def process(value):
-            return "INTERVAL '%s' %s" % (value, value.type)
-        return process
-
-class INTERVAL_YEAR(_TDInterval):
-
-    """ Teradata INTERVAL YEAR type
-
-    This type identifies a field defining a period of time in years.
-
-    """
-    __visit_name__ = 'INTERVAL_YEAR'
-
-    def __init__(self, precision=None, **kwargs):
-
-       """ Construct an INTERVAL_YEAR object
-
-       :param precision: permitted range of digits for year ranging from 1 to 4
-
-       """
-       super(INTERVAL_YEAR, self).__init__(precision=precision)
-
-class INTERVAL_YEAR_TO_MONTH(_TDInterval):
-
-    """ Teradata INTERVAL YEAR TO MONTH type
-
-    This type identifies a field defining a period of time in years and months.
-
-    """
-
-    __visit_name__ = 'INTERVAL_YEAR_TO_MONTH'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_YEAR_TO_MONTH object
-
-        :param precision: permitted range of digits for year ranging from 1 to 4
-
-        """
-        super(INTERVAL_YEAR_TO_MONTH, self).__init__(precision=precision)
-
-class INTERVAL_MONTH(_TDInterval):
-
-    """ Teradata INTERVAL MONTH type
-
-    This type identifies a field defining a period of time in months.
-
-    """
-
-    __visit_name__ = 'INTERVAL_MONTH'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_MONTH object
-
-        :param precision: permitted range of digits for month ranging from 1 to 4
-
-        """
-        super(INTERVAL_MONTH, self).__init__(precision=precision)
-
-class INTERVAL_DAY(_TDInterval):
-
-    """ Teradata INTERVAL DAY type
-
-    This type identifies a field defining a period of time in days.
-
-    """
-
-    __visit_name__ = 'INTERVAL_DAY'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_DAY object
-
-        :param precision: permitted range of digits for day ranging from 1 to 4
-
-        """
-        super(INTERVAL_DAY, self).__init__(precision=precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL DAY
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                value = td_dtypes.Interval(days=value.days)
-#            return value
-#        return process
-
-class INTERVAL_DAY_TO_HOUR(_TDInterval):
-
-    """ Teradata INTERVAL DAY TO HOUR type
-
-    This type identifies a field defining a period of time in days and hours.
-
-    """
-
-    __visit_name__ = 'INTERVAL_DAY_TO_HOUR'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_DAY_TO_HOUR object
-
-        :param precision: permitted range of digits for day ranging from 1 to 4
-
-        """
-        super(INTERVAL_DAY_TO_HOUR, self).__init__(precision=precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL DAY
-#        TO HOUR
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                hours = int(value.seconds / 3600)
-#                value = td_dtypes.Interval(days=value.days, hours=hours)
-#            return value
-#        return process
-
-class INTERVAL_DAY_TO_MINUTE(_TDInterval):
-
-    """ Teradata INTERVAL DAY TO MINUTE type
-
-    This type identifies a field defining a period of time in days, hours,
-    and minutes.
-
-    """
-
-    __visit_name__ = 'INTERVAL_DAY_TO_MINUTE'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_DAY_TO_MINUTE object
-
-        :param precision: permitted range of digits for day ranging from 1 to 4
-
-        """
-        super(INTERVAL_DAY_TO_MINUTE, self).__init__(precision=precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL DAY
-#        TO MINUTE
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                minutes = int(value.seconds / 60)
-#                value   = td_dtypes.Interval(days=value.days, minutes=minutes)
-#            return value
-#        return process
-
-class INTERVAL_DAY_TO_SECOND(_TDInterval):
-
-    """ Teradata INTERVAL DAY TO SECOND type
-
-    This type identifies a field during a period of time in days, hours, minutes,
-    and seconds.
-
-    """
-
-    __visit_name__ = 'INTERVAL_DAY_TO_SECOND'
-
-    def __init__(self, precision=None, frac_precision=None, **kwargs):
-
-        """ Construct an INTERVAL_DAY_TO_SECOND object
-
-        :param precision: permitted range of digits for day ranging from 1 to 4
-
-        :param frac_precision: fracional_seconds_precision ranging from 0 to 6
-
-        """
-        super(INTERVAL_DAY_TO_SECOND, self).__init__(precision=precision,
-                                                     frac_precision=frac_precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL DAY
-#        TO SECOND
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                seconds = value.seconds + value.microseconds / 1000000
-#                value   = td_dtypes.Interval(days=value.days, seconds=seconds)
-#            return value
-#        return process
-
-class INTERVAL_HOUR(_TDInterval):
-
-    """ Teradata INTERVAL HOUR type
-
-    This type identifies a field defining a period of time in hours.
-
-    """
-
-    __visit_name__ = 'INTERVAL_HOUR'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_HOUR object
-
-        :param precision: permitted range of digits for hour ranging from 1 to 4
-
-        """
-        super(INTERVAL_HOUR, self).__init__(precision=precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL HOUR
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                hours = int(value.total_seconds() / 3600)
-#                value = td_dtypes.Interval(hours=hours)
-#            return value
-#        return process
-
-class INTERVAL_HOUR_TO_MINUTE(_TDInterval):
-
-    """ Teradata INTERVAL HOUR TO MINUTE type
-
-    This type identifies a field defining a period of time in hours and minutes.
-
-    """
-
-    __visit_name__ = 'INTERVAL_HOUR_TO_MINUTE'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_HOUR_TO_MINUTE object
-
-        :param precision: permitted range of digits for hour ranging from 1 to 4
-
-        """
-        super(INTERVAL_HOUR_TO_MINUTE, self).__init__(precision=precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL HOUR
-#        TO MINUTE
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                hours, seconds = divmod(value.total_seconds(), 3600)
-#                hours   = int(hours)
-#                minutes = int(seconds / 60)
-#                value   = td_dtypes.Interval(hours=hours, minutes=minutes)
-#            return value
-#        return process
-
-class INTERVAL_HOUR_TO_SECOND(_TDInterval):
-
-    """ Teradata INTERVAL HOUR TO SECOND type
-
-    This type identifies a field defining a period of time in hours, minutes,
-    and seconds.
-
-    """
-
-    __visit_name__ = 'INTERVAL_HOUR_TO_SECOND'
-
-    def __init__(self, precision=None, frac_precision=None, **kwargs):
-
-        """ Construct an INTERVAL_HOUR_TO_SECOND object
-
-        :param precision: permitted range of digits for hour ranging from 1 to 4
-
-        :param frac_precision: fracional_seconds_precision ranging from 0 to 6
-
-        """
-        super(INTERVAL_HOUR_TO_SECOND, self).__init__(precision=precision,
-                                                      frac_precision=frac_precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL HOUR
-#        TO SECOND
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                hours, seconds = divmod(value.total_seconds(), 3600)
-#                hours   = int(hours)
-#                seconds = int(seconds) + value.microseconds / 1000000
-#                value   = td_dtypes.Interval(hours=hours, seconds=seconds)
-#            return value
-#        return process
-
-class INTERVAL_MINUTE(_TDInterval):
-
-    """ Teradata INTERVAL MINUTE type
-
-    This type identifies a field defining a period of time in minutes.
-
-    """
-
-    __visit_name__ = 'INTERVAL_MINUTE'
-
-    def __init__(self, precision=None, **kwargs):
-
-        """ Construct an INTERVAL_MINUTE object
-
-        :param precision: permitted range of digits for minute ranging from 1 to 4
-
-        """
-        super(INTERVAL_MINUTE, self).__init__(precision=precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL MINUTE
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                minutes = int(value.total_seconds() / 60)
-#                value = td_dtypes.Interval(minutes=minutes)
-#            return value
-#        return process
-
-class INTERVAL_MINUTE_TO_SECOND(_TDInterval):
-
-    """ Teradata INTERVAL MINUTE TO SECOND type
-
-    This type identifies a field defining a period of time in minutes and seconds.
-
-    """
-
-    __visit_name__ = 'INTERVAL_MINUTE_TO_SECOND'
-
-    def __init__(self, precision=None, frac_precision=None, **kwargs):
-
-        """ Construct an INTERVAL_MINUTE_TO_SECOND object
-
-        :param precision: permitted range of digits for minute ranging from 1 to 4
-
-        :param frac_precision: fracional_seconds_precision ranging from 0 to 6
-
-        """
-        super(INTERVAL_MINUTE_TO_SECOND, self).__init__(precision=precision,
-                                                        frac_precision=frac_precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL MINUTE
-#        TO SECOND
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                minutes, seconds = divmod(value.total_seconds(), 60)
-#                minutes = int(minutes)
-#                seconds = int(seconds) + value.microseconds / 1000000
-#                value   = td_dtypes.Interval(minutes=minutes, seconds=seconds)
-#            return value
-#        return process
-
-class INTERVAL_SECOND(_TDInterval):
-
-    """ Teradata INTERVAL SECOND type
-
-    This type identifies a field defining a period of time in seconds.
-
-    """
-
-    __visit_name__ = 'INTERVAL_SECOND'
-
-    def __init__(self, precision=None, frac_precision=None, **kwargs):
-
-        """ Construct an INTERVAL_SECOND object
-
-        :param precision: permitted range of digits for second ranging from 1 to 4
-
-        :param frac_precision: fractional_seconds_precision ranging from 0 to 6
-
-        """
-        super(INTERVAL_SECOND, self).__init__(precision=precision,
-                                              frac_precision=frac_precision)
-
-#    def bind_processor(self, dialect):
-#
-#        """
-#        Handles the conversion from a datetime.timedelta object to an Interval
-#        object appropriate for inserting into a column with type INTERVAL SECOND
-#
-#        """
-#        def process(value):
-#            if isinstance(value, datetime.timedelta):
-#                seconds = value.total_seconds()
-#                value = td_dtypes.Interval(seconds=seconds)
-#            return value
-#        return process
-
-
-class _TDPeriod(_TDType, types.UserDefinedType):
-
-    """ Base class for the Teradata Period sqltypes """
-
-    def __init__(self, format=None, **kwargs):
-        self.format = format
-
-    def bind_processor(self, dialect):
-
-        """
-        Processes the Period value from SQLAlchemy to DB
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-
-        """
-        Processes the Period value from DB to SQLAlchemy
-        """
-        def process(value):
-            return value
-        return process
-
-class PERIOD_DATE(_TDPeriod):
-
-    """ Teradata PERIOD DATE type
-
-    This type identifies a field defining a duration with a beginning and end date.
-
-    """
-
-    __visit_name__ = 'PERIOD_DATE'
-
-    def __init__(self, format=None, **kwargs):
-
-        """ Construct a PERIOD_DATE object
-
-        :param format: format of the date, e.g. 'yyyy-mm-dd'
-
-        """
-        super(PERIOD_DATE, self).__init__(format=format, **kwargs)
-
-class PERIOD_TIME(_TDPeriod):
-
-    """ Teradata PERIOD TIME type
-
-    This type identifies a field defining a duration with a beginning and end time.
-
-    """
-
-    __visit_name__ = 'PERIOD_TIME'
-
-    def __init__(self, format=None, frac_precision=None, timezone=False, **kwargs):
-
-        """ Construct a PERIOD_TIME object
-
-        :param format: format of the time, e.g. 'HH:MI:SS.S(6)' and
-        'HH:MI:SS.S(6)Z' (with timezone)
-
-        :param frac_precision: fractional_seconds_precision ranging from 0 to 6
-
-        :param timezone: true if WITH TIME ZONE, false otherwise
-
-        """
-        super(PERIOD_TIME, self).__init__(format=format, **kwargs)
-        self.frac_precision = frac_precision
-        self.timezone       = timezone
-
-class PERIOD_TIMESTAMP(_TDPeriod):
-
-    """ Teradata PERIOD TIMESTAMP type
-
-    This type identifies a field defining a duration with a beginning and end timestamp.
-
-    """
-
-    __visit_name__ = 'PERIOD_TIMESTAMP'
-
-    def __init__(self, format=None, frac_precision=None, timezone=False, **kwargs):
-
-        """ Construct a PERIOD_TIMESTAMP object
-
-        :param format: format of the timestamp, e.g. 'YYYY-MM-DDBHH:MI:SS.S(6)'
-        and 'YYYY-MM-DDBHH:MI:SS.S(6)Z' (with timezone)
-
-        :param frac_precision: fractional_seconds_precision ranging from 0 to 6
-
-        :param timezone: true if WITH TIME ZONE, false otherwise
-
-        """
-        super(PERIOD_TIMESTAMP, self).__init__(format=format, **kwargs)
-        self.frac_precision = frac_precision
-        self.timezone       = timezone
-
-
-class CHAR(_TDConcatenable, _TDType, sqltypes.CHAR):
-
-    """ Teradata CHAR type
-
-    This type represents a fixed-length character string for Teradata Database
-    internal character storage.
-
-    """
-
-    def __init__(self, length=1, charset=None, **kwargs):
-
-        """ Construct a CHAR object
-
-        :param length: number of characters or bytes allocated. Maximum value
-        for n depends on the character set. For LATIN - 64000 characters,
-        For UNICODE - 32000 characters, For KANJISJIS - 32000 bytes. If a value
-        for n is not specified, the default is 1.
-
-        :param charset: Server character set for the character column.
-        Supported values:
-            'LATIN': fixed 8-bit characters from the ASCII ISO 8859 Latin1
-            or ISO 8859 Latin9.
-            'UNICODE': fixed 16-bit characters from the UNICODE 6.0 standard.
-            'GRAPHIC': fixed 16-bit UNICODE characters defined by IBM for DB2.
-            'KANJISJIS': mixed single byte/multibyte characters intended for
-            Japanese applications that rely on KanjiShiftJIS characteristics.
-        Note: GRAPHIC(n) is equivalent to CHAR(n) CHARACTER SET GRAPHIC
-
-        """
-        super(CHAR, self).__init__(length=length, **kwargs)
-        self.charset = charset
-
-
-class VARCHAR(_TDConcatenable, _TDType, sqltypes.VARCHAR):
-
-    """ Teradata VARCHAR type
-
-    This type represents a variable length character string of length 0 to n
-    for Teradata Database internal character storage. LONG VARCHAR specifies
-    the longest permissible variable length character string for Teradata
-    Database internal character storage.
-
-    """
-
-    def __init__(self, length=None, charset=None, **kwargs):
-
-        """ Construct a VARCHAR object
-
-        :param length: Optional 0 to n. If None, LONG is used
-        (the longest permissible variable length character string)
-
-        :param charset: optional character set for varchar.
-
-        Note: VARGRAPHIC(n) is equivalent to VARCHAR(n) CHARACTER SET GRAPHIC
-
-        """
-        super(VARCHAR, self).__init__(length=length, **kwargs)
-        self.charset = charset
-
-
-class CLOB(_TDConcatenable, _TDType, sqltypes.CLOB):
-
-    """ Teradata CLOB type
-
-    This type represents a large character string. A character large object
-    (CLOB) column can store character data, such as simple text or HTML.
-
-    """
-
-    def __init__(self, length=None, charset=None, multiplier=None, **kwargs):
-
-        """ Construct a CLOB object
-
-        :param length: Optional length for clob. For Latin server character set,
-        length cannot exceed 2097088000. For Unicode server character set,
-        length cannot exceed 1048544000.
-        If no length is specified then the maximum is used.
-
-        :param multiplier: Either 'K', 'M', or 'G'.
-        K specifies number of characters to allocate as nK, where K=1024
-        (For Latin char sets, n < 2047937 and For Unicode char sets, n < 1023968)
-        M specifies nM, where M=1024K
-        (For Latin char sets, n < 1999 and For Unicode char sets, n < 999)
-        G specifies nG, where G=1024M
-        (For Latin char sets, n must be 1 and char set must be LATIN)
-
-        :param charset: LATIN (fixed 8-bit characters ASCII ISO 8859 Latin1 or ISO 8859 Latin9)
-        or UNICODE (fixed 16-bit characters from the UNICODE 6.0 standard)
-
-        """
-        super(CLOB, self).__init__(length=length, **kwargs)
-        self.charset    = charset
-        self.multiplier = multiplier
-
-class XML(_TDType, types.UserDefinedType):
-
-    """Class for the Teradata datatype XML """
-
-    #maximum_length = 2097088000, inline_length = 4046
-
-    __visit_name__ = "XML"
-
-    def __init__(self, maximum_length=2097088000, inline_length=4046,
-                 **kwargs):
-        self.maximum_length = maximum_length
-        self.inline_length = inline_length
-
-    def bind_processor(self, dialect):
-
-        """
-        Processes the XML value from SQLAlchemy to DB
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-
-        """
-        Processes the XML value from DB to SQLAlchemy
-        """
-        def process(value):
-            return value
-        return process
-
-
-class JSON(_TDType, types.UserDefinedType):
-
-    """ Class for the Teradata JSON type """
-
-    __visit_name__ = "JSON"
-
-    def __init__(self, max_length=16776192, inline_length=64000, charset=None,
-                 storage_format=None, **kwargs):
-        """
-        Constructor for JSON Data Type.
-
-        PARAMETERS:
-            max_length:
-                Optional Argument.
-                Specifies the maximum length of JSON type.
-                Default Value: 16776192
-                Type: int
-
-            inline_length:
-                Optional Argument.
-                Specifies the inline storage size of JSON type.
-                Default Value: 64000
-                Type: int
-
-            charset:
-                Optional Argument.
-                Specifies the character set for JSON type.
-                Note:
-                    This argument cannot be specified with storage_format. Teradata
-                    databases throws error in that case.
-                Default Value: None
-                Type: str
-
-            storage_format:
-                Optional Argument.
-                Specifies storage format for the JSON type.
-                Note:
-                    This argument cannot be specified with storage_format, Teradata
-                    databases throws error in that case.
-                Default Value: None
-                Type: str
-
-        RETURNS:
-            Object of JSON() type.
-
-        RAISES:
-            None.
-        """
-        self.max_length = max_length
-        self.inline_length = inline_length
-        self.charset = charset
-        self.storage_format = storage_format
-        
-    def bind_processor(self, dialect):
-        """
-        Processes the value from SQLAlchemy to database.
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-        """
-        Processes the XML value from database to SQLAlchemy.
-        """
-        def process(value):
-            return value
-        return process
-
-class TDUDT(_TDType, types.UserDefinedType):
-    """Class for the Teradata User Defined types """
-
-    __visit_name__ = "TDUDT"
-
-    def __init__(self, type_name=None,  **kwargs):
-        """
-        Constructor for User Defined Types
-
-        PARAMETERS:
-            type_name:
-                Optional Argument.
-                Specifies the name of User Defined Type.
-                Type: str
-
-        RETURNS:
-            Object of TDUDT().
-
-        RAISES:
-            None.
-        """
-        self.type_name = type_name
-
-    def bind_processor(self, dialect):
-        """
-        Processes the  value from SQLAlchemy to DB
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-        """
-        Processes the XML value from DB to SQLAlchemy
-        """
-        def process(value):
-            return value
-        return process
-
-class GEOMETRY(_TDType, types.UserDefinedType):
-
-    """ Class for the Teradata datatype ST_GEOMETRY """
-
-    __visit_name__ = "GEOMETRY"
-
-    def __init__(self, max_length=16776192, inline_length=9920,
-                 **kwargs):
-        """
-        Constructor for GEOMETRY Data Type.
-
-        PARAMETERS:
-           maximum_length:
-               Optional Argument.
-               Specifies the maximum length of GEOMETRY type.
-               Default Value: 16776192
-               Type: int
-
-           inline_length:
-               Optional Argument.
-               Specifies the inline storage size of GEOMETRY type.
-               Default Value: 9920
-               Type: int
-
-        RETURNS:
-           Object of GEOMETRY() type.
-
-        RAISES:
-           None.
-        """
-        self.max_length = max_length
-        self.inline_length = inline_length
-
-    def bind_processor(self, dialect):
-        """
-        Processes the GEOMETRY value from SQLAlchemy to Database.
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-        """
-        Processes the GEOMETRY value from Database to SQLAlchemy.
-        """
-        def process(value):
-            return value
-        return process
-
-class MBR(_TDType, types.UserDefinedType):
-
-    """ Class for the Teradata datatype MBR """
-
-    __visit_name__ = "MBR"
-
-    def bind_processor(self, dialect):
-        """
-        Processes the MBR value from SQLAlchemy to Database.
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-        """
-        Processes the MBR value from Database to SQLAlchemy.
-        """
-        def process(value):
-            return value
-        return process
-
-class MBB(_TDType, types.UserDefinedType):
-
-    """ Class for the Teradata datatype MBB """
-
-    __visit_name__ = "MBB"
-
-    def bind_processor(self, dialect):
-        """
-        Processes the MBB value from SQLAlchemy to Database.
-        """
-        def process(value):
-            return value
-        return process
-
-    def result_processor(self, dialect, coltype):
-        """
-        Processes the MBB value from Database to SQLAlchemy.
-        """
-        def process(value):
-            return value
-        return process
-
-class TeradataExpressionAdapter:
-    """Expression Adapter for Teradata Data Types.
-
-    For inferring the resulting type of a BinaryExpression whose operation
-    involves operands that are of Teradata types.
-    """
-
-    def process(self, type_, op=None, other=None, **kw):
-        """Adapts the expression.
-
-        Infer the type of the resultant BinaryExpression defined by the passed
-        in operator and operands. This resulting type should be consistent with
-        the Teradata database when the operation is defined.
-
-        Args:
-            type_: The type instance of the left operand.
-
-            op:    The operator of the BinaryExpression.
-
-            other: The type instance of the right operand.
-
-        Returns:
-            The type to adapt the BinaryExpression to.
-        """
-
-        if isinstance(type_, _TDInterval) or isinstance(other, _TDInterval):
-            adapt_strategy = _IntervalRuleStrategy()
-        else:
-            adapt_strategy = _LookupStrategy()
-
-        return adapt_strategy.adapt(type_, op, other, **kw)
-
-
-class _AdaptStrategy:
-    """Interface for expression adaptation strategies."""
-
-    def adapt(self, type_, op, other, **kw):
-        """Adapt the expression according to some strategy.
-
-        Given the type of the left and right operand, and the operator, produce
-        a resulting type class for the BinaryExpression.
-        """
-
-        raise NotImplementedError()
-
-class _IntervalRuleStrategy(_AdaptStrategy):
-    """Expression adaptation strategy which follows a set of rules for inferring
-    Teradata Interval types.
-    """
-
-    ordering = ('YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND')
-
-    def adapt(self, type_, op, other, **kw):
-        """Adapt the expression by a set of predefined rules over the Teradata
-        Interval types.
-        """
-
-        # If the (Interval) types are equal, simply return the class of
-        # those types
-        if type_.__class__ == other.__class__:
-            return type_.__class__
-
-        # If the (Interval) types are not equal, return the valid Interval type
-        # with the greatest range.
-        #
-        # E.g. INTERVAL YEAR TO MONTH and INTERVAL DAY TO HOUR -->
-        #      INTERVAL YEAR TO HOUR.
-        #
-        # Otherwise if the resulting Interval type is invalid, return NullType.
-        #
-        # E.g. INTERVAL YEAR TO MONTH and INTERVAL MINUTE TO SECOND -->
-        #      INTERVAL YEAR TO SECOND (invalid) -->
-        #      NullType
-        elif isinstance(type_, _TDInterval) and isinstance(other, _TDInterval):
-            tokens = self._tokenize_name(type_.__class__.__name__) + \
-                     self._tokenize_name(other.__class__.__name__)
-            tokens.sort(key=lambda tok: self.ordering.index(tok))
-
-            return getattr(sys.modules[__name__],
-                self._combine_tokens(tokens[0], tokens[-1]),
-                sqltypes.NullType)()
-
-        # Else the binary expression has an Interval and non-Interval operand.
-        # If the non-Interval operand is a Date, Time, or Datetime, return that
-        # type, otherwise return the Interval type.
-        else:
-            interval, non_interval = (type_, other) if \
-                    isinstance(type_, _TDInterval) \
-                else (other, type_)
-
-            return non_interval.__class__ if \
-                    isinstance(non_interval, (sqltypes.Date,
-                                              sqltypes.Time,
-                                              sqltypes.DateTime)) \
-                else interval.__class__
-
-    def _tokenize_name(self, interval_name):
-        """Tokenize the name of Interval types.
-
-        Returns a list of (str) tokens of the corresponding Interval type name.
-
-        E.g. 'INTERVAL_DAY_TO_HOUR' --> ['DAY', 'HOUR'].
-        """
-
-        return list(filter(lambda tok: tok not in ('INTERVAL', 'TO'),
-                           interval_name.split('_')))
-
-    def _combine_tokens(self, tok_l, tok_r):
-        """Combine the tokens of an Interval type to form its name.
-
-        Returns a string for the name of the Interval type corresponding to the
-        tokens passed in.
-
-        E.g. tok_l='DAY' and tok_r='HOUR' --> 'INTERVAL_DAY_TO_HOUR'
-        """
-
-        return 'INTERVAL_%s_TO_%s' % (tok_l, tok_r)
-
-class _LookupStrategy(_AdaptStrategy):
-    """Expression adaptation strategy which employs a general lookup table."""
-
-    def adapt(self, type_, op, other, **kw):
-        """Adapt the expression by looking up a hardcoded table.
-
-        The lookup table is defined as `visit_` methods below. Each method
-        returns a nested dictionary which is keyed by the operator and the other
-        operand's type.
-        """
-
-        return getattr(self, self._process_visit_name(type_.__visit_name__),
-                   lambda *args, **kw: {})(type_, other, **kw) \
-            .get(op, util.immutabledict()) \
-            .get(other.__class__, type_.__class__)
-
-    def _process_visit_name(self, visit_name):
-        """Generate the corresponding visit function name from a type's
-        __visit_name__ field.
-        """
-
-        prefix = 'visit_'
-        return prefix + visit_name
-
-    def _flatten_tuple_keyed_dict(self, tuple_dict):
-        """Recursively flatten a dictionary with (many-to-one) tuple keys to a
-        standard one.
-        """
-
-        flat_dict = {}
-        for ks, v in tuple_dict.items():
-            v = self._flatten_tuple_keyed_dict(v) if isinstance(v, dict) else v
-            if isinstance(ks, tuple):
-                for k in ks:
-                    flat_dict[k] = v
-            else:
-                flat_dict[ks] = v
-        return flat_dict
-
-    def visit_INTEGER(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.add: {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                BIGINT:  BIGINT,
-                DECIMAL: DECIMAL,
-                FLOAT:   FLOAT,
-                NUMBER:  NUMBER,
-                DATE:    DATE
-            },
-            (operators.sub, operators.mul, operators.truediv,
-             operators.mod): {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                BIGINT:  BIGINT,
-                DECIMAL: DECIMAL,
-                FLOAT:   FLOAT,
-                NUMBER:  NUMBER
-            }
-        })
-
-    def visit_SMALLINT(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.add: {
-                (INTEGER, SMALLINT, BYTEINT): INTEGER,
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                BIGINT:  BIGINT,
-                DECIMAL: DECIMAL,
-                FLOAT:   FLOAT,
-                NUMBER:  NUMBER,
-                DATE:    DATE
-            },
-            (operators.sub, operators.mul, operators.truediv,
-             operators.mod): {
-                (INTEGER, SMALLINT, BYTEINT, DATE): INTEGER,
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                BIGINT:  BIGINT,
-                DECIMAL: DECIMAL,
-                FLOAT:   FLOAT,
-                NUMBER:  NUMBER,
-            }
-        })
-
-    def visit_BIGINT(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.add: {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                DECIMAL: DECIMAL,
-                FLOAT:   FLOAT,
-                NUMBER:  NUMBER,
-                DATE:    DATE
-            },
-            (operators.sub, operators.mul, operators.truediv,
-             operators.mod): {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                DECIMAL: DECIMAL,
-                FLOAT:   FLOAT,
-                NUMBER:  NUMBER
-            }
-        })
-
-    def visit_DECIMAL(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.add: {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                FLOAT:  FLOAT,
-                NUMBER: NUMBER,
-                DATE:   DATE
-            },
-            (operators.sub, operators.mul, operators.truediv,
-             operators.mod): {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                FLOAT:  FLOAT,
-                NUMBER: NUMBER
-            }
-        })
-
-    def visit_DATE(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.add: {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                DATE:  INTEGER,
-                FLOAT: FLOAT
-            },
-            operators.sub: {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                DATE:  INTEGER,
-                FLOAT: FLOAT
-            },
-            (operators.mul, operators.truediv, operators.mod): {
-                (DATE, INTEGER, SMALLINT, BYTEINT): INTEGER,
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                (FLOAT, TIME): FLOAT,
-                BIGINT:  BIGINT,
-                DECIMAL: DECIMAL,
-                NUMBER:  NUMBER,
-            }
-        })
-
-    def visit_TIME(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            (operators.add, operators.mul, operators.truediv,
-             operators.mod): {
-                DATE: FLOAT
-            }
-        })
-
-    def visit_CHAR(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.concat_op: {
-                CHAR:    VARCHAR if hasattr(other, 'charset') and \
-                            ((type_.charset == 'unicode') !=
-                             (other.charset == 'unicode'))
-                         else CHAR,
-                VARCHAR: VARCHAR,
-                CLOB:    CLOB
-            },
-            (operators.add, operators.sub, operators.mul,
-             operators.truediv, operators.mod): {
-                (INTEGER, SMALLINT, BIGINT, BYTEINT, NUMBER, FLOAT, DECIMAL,
-                 DATE, CHAR, VARCHAR): FLOAT
-            }
-        })
-
-    def visit_VARCHAR(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.concat_op: {
-                CLOB: CLOB
-            },
-            (operators.add, operators.sub, operators.mul,
-             operators.truediv, operators.mod): {
-                (INTEGER, SMALLINT, BIGINT, BYTEINT, NUMBER, FLOAT, DECIMAL,
-                 DATE, CHAR, VARCHAR): FLOAT
-            }
-        })
-
-    def visit_BYTEINT(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            (operators.add, operators.sub): {
-                (INTEGER, SMALLINT, BYTEINT): INTEGER,
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                BIGINT:   BIGINT,
-                DECIMAL:  DECIMAL,
-                FLOAT:    FLOAT,
-                NUMBER:   NUMBER,
-                DATE:     DATE
-            },
-            (operators.mul, operators.truediv, operators.mod): {
-                (INTEGER, SMALLINT, BYTEINT, DATE): INTEGER,
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                BIGINT:   BIGINT,
-                DECIMAL:  DECIMAL,
-                FLOAT:    FLOAT,
-                NUMBER:   NUMBER
-            }
-        })
-
-    def visit_BYTE(self, type_, other, **kw):
-        return {
-            operators.concat_op: {
-                VARBYTE: VARBYTE,
-                BLOB:    BLOB
-            }
-        }
-
-    def visit_VARBYTE(self, type_, other, **kw):
-        return {
-            operators.concat_op: {
-                BLOB: BLOB
-            }
-        }
-
-    def visit_NUMBER(self, type_, other, **kw):
-        return self._flatten_tuple_keyed_dict({
-            operators.add: {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                FLOAT: FLOAT,
-                DATE:  DATE
-            },
-            (operators.sub, operators.mul, operators.truediv,
-             operators.mod): {
-                (CHAR, VARCHAR, BLOB): FLOAT,
-                FLOAT: FLOAT
-            }
-        })
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+from sqlalchemy import util
+from sqlalchemy import types
+from sqlalchemy.sql import sqltypes, operators
+
+import datetime, decimal
+import warnings, sys
+#import teradata.datatypes as td_dtypes
+
+
+class _TDComparable:
+    """Teradata Comparable Data Type."""
+
+    class Comparator(types.TypeEngine.Comparator):
+        """Comparator for expression adaptation.
+
+        Use the TeradataExpressionAdapter to process the resulting types
+        for binary operations over Teradata types.
+        """
+
+        def _adapt_expression(self, op, other_comparator):
+            expr_type = TeradataExpressionAdapter().process(
+                self.type, op=op, other=other_comparator.type)
+            return op, expr_type()
+
+    comparator_factory = Comparator
+
+
+class _TDConcatenable:
+    """Teradata Concatenable Data Type.
+
+    This family of types currently encompasses the binary types
+    (BYTE, VARBYTE, BLOB) and the character types (CHAR, VARCHAR, CLOB).
+    """
+
+    class Comparator(_TDComparable.Comparator):
+        """Comparator for expression adaptation.
+
+        Overloads the addition (+) operator over concatenable Teradata types
+        to use concat_op. Note that this overloading only occurs between types
+        within the same type_affinity.
+        """
+
+        def _adapt_expression(self, op, other_comparator):
+            return super(_TDConcatenable.Comparator, self)._adapt_expression(
+                operators.concat_op if op is operators.add and
+                    isinstance(other_comparator.type, self.type._type_affinity)
+                else op, other_comparator)
+
+    comparator_factory = Comparator
+
+
+class _TDLiteralCoercer:
+    """Mixin for literal type processing against Teradata data types."""
+
+    def coerce_compared_value(self, op, value):
+        type_ = type(value)
+
+        if type_ == int:
+            return INTEGER()
+        elif type_ == float:
+            return FLOAT()
+        elif type_ == bytes:
+            return BYTE()
+        elif type_ == str:
+            return VARCHAR()
+        elif type_ == decimal.Decimal:
+            return DECIMAL()
+        elif type_ == datetime.date:
+            return DATE()
+        elif type_ == datetime.datetime:
+            return TIMESTAMP()
+        elif type_ == datetime.time:
+            return TIME()
+       # elif type_ == td_dtypes.Interval:
+       #     return getattr(sys.modules[__name__],
+       #         'INTERVAL_' + value.type.replace(' ', '_'),
+       #         sqltypes.NullType)()
+        # TODO PERIOD
+
+        return sqltypes.NullType()
+
+
+class _TDType(_TDLiteralCoercer, _TDComparable):
+
+    """ Teradata Data Type
+
+    Identifies a Teradata data type. Currently used to override __str__
+    behavior such that the type will get printed without being compiled by the
+    GenericTypeCompiler (which would otherwise result in an exception).
+    """
+
+    def _parse_name(self, name):
+        return name.replace('_', ' ')
+
+    def __str__(self):
+        return self._parse_name(self.__class__.__name__)
+
+
+class INTEGER(_TDType, sqltypes.INTEGER):
+
+    """ Teradata INTEGER type
+
+    Represents a signed, binary integer value from -2,147,483,648 to
+    2,147,483,647.
+
+    """
+
+    def __init__(self, **kwargs):
+
+        """ Construct a INTEGER Object """
+        super(INTEGER, self).__init__(**kwargs)
+
+
+class SMALLINT(_TDType, sqltypes.SMALLINT):
+
+    """ Teradata SMALLINT type
+
+    Represents a signed binary integer value in the range -32768 to 32767.
+
+    """
+
+    def __init__(self, **kwargs):
+
+        """ Construct a SMALLINT Object """
+        super(SMALLINT, self).__init__(**kwargs)
+
+
+class BIGINT(_TDType, sqltypes.BIGINT):
+
+    """ Teradata BIGINT type
+
+    Represents a signed, binary integer value from -9,223,372,036,854,775,808
+    to 9,223,372,036,854,775,807.
+
+    """
+
+    def __init__(self, **kwargs):
+
+        """ Construct a BIGINT Object """
+        super(BIGINT, self).__init__(**kwargs)
+
+
+class DECIMAL(_TDType, sqltypes.DECIMAL):
+
+    """ Teradata DECIMAL type
+
+    Represents a decimal number of n digits, with m of those n digits to the
+    right of the decimal point.
+
+    """
+
+    def __init__(self, precision = 38, scale = 19, **kwargs):
+
+        """ Construct a DECIMAL Object """
+        super(DECIMAL, self).__init__(precision = precision, scale = scale, **kwargs)
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+            return str(value) + ('' if value.as_tuple()[2] < 0 else '.')
+        return process
+
+
+class BYTEINT(_TDType, sqltypes.Integer):
+
+    """ Teradata BYTEINT type
+
+    This type represents a one byte signed integer.
+
+    """
+
+    __visit_name__ = 'BYTEINT'
+
+    def __init__(self, **kwargs):
+
+        """ Construct a BYTEINT Object """
+        super(BYTEINT, self).__init__(**kwargs)
+
+
+class _TDBinary(_TDConcatenable, _TDType, sqltypes._Binary):
+
+    """ Teradata Binary Types
+
+    This type represents a Teradata binary string. Warns users when
+    data may get truncated upon insertion.
+
+    """
+
+    class TruncationWarning(UserWarning):
+        pass
+
+    def _length(self):
+        """Compute the length allocated to this binary column."""
+
+        multiplier_map = {
+            'K': 1024,
+            'M': 1048576,
+            'G': 1073741824
+        }
+        if hasattr(self, 'multiplier') and self.multiplier in multiplier_map:
+            return self.length * multiplier_map[self.multiplier]
+
+        return self.length
+
+    def bind_processor(self, dialect):
+        if dialect.dbapi is None:
+            return None
+
+        def process(value):
+            bin_length = self._length()
+            if value is not None and bin_length is not None:
+                if len(value) > bin_length:
+                    warnings.warn(
+                        'Attempting to insert an item that is larger than the '
+                        'space allocated for this column. Data may get truncated.',
+                        self.TruncationWarning)
+                return value
+            else:
+                return None
+        return process
+
+
+class BYTE(_TDBinary, sqltypes.BINARY):
+
+    """ Teradata BYTE type
+
+    This type represents a fixed-length binary string and is equivalent to
+    the BINARY SQL standard type.
+
+    """
+
+    __visit_name__ = 'BYTE'
+
+    def __init__(self, length=None, **kwargs):
+
+        """ Construct a BYTE object
+
+        :param length: Optional 1 to n. Specifies the number of bytes in the
+        fixed-length binary string. The maximum value for n is 64000.
+
+        """
+        super(BYTE, self).__init__(length=length, **kwargs)
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+
+            try:
+
+              # Python 3.5+
+              return "'%s'XB" % value.hex()
+
+            except AttributeError:
+
+              # try it with codecs
+              import codecs
+              return "'%s'XB" % codecs.encode(value, 'hex').decode('utf-8')
+
+        return process
+
+
+class VARBYTE(_TDBinary, sqltypes.VARBINARY):
+
+    """ Teradata VARBYTE type
+
+    This type represents a variable-length binary string and is equivalent to
+    the VARBINARY SQL standard type.
+
+    """
+
+    __visit_name__ = 'VARBYTE'
+
+    def __init__(self, length=None, **kwargs):
+
+        """ Construct a VARBYTE object
+
+        :param length: Optional 1 to n. Specifies the number of bytes in the
+        fixed-length binary string. The maximum value for n is 64000.
+
+        """
+        super(VARBYTE, self).__init__(length=length, **kwargs)
+
+
+class BLOB(_TDBinary, sqltypes.BLOB):
+
+    """ Teradata BLOB type
+
+    This type represents a large binary string of raw bytes. A binary large
+    object (BLOB) column can store binary objects, such as graphics, video
+    clips, files, and documents.
+
+    """
+
+    def __init__(self, length=None, multiplier=None, **kwargs):
+
+        """ Construct a BLOB object
+
+        :param length: Optional 1 to n. Specifies the number of bytes allocated
+        for the BLOB column. The maximum number of bytes is 2097088000, which
+        is the default if n is not specified.
+
+        :param multiplier: Optional value in ('K', 'M', 'G'). Indicates that the
+        length parameter n is specified in kilobytes (KB), megabytes (Mb),
+        or gigabytes (GB) respectively. Note the following constraints on n
+        hold for each of the allowable multiplier:
+
+            'K' is specified, n cannot exceed 2047937.
+            'M' is specified, n cannot exceed 1999.
+            'G' is specified, n must be 1.
+
+        If multiplier is None, the length is interepreted as bytes (B).
+
+        Note: If you specify a multiplier without specifying the length, the
+              multiplier argument will simply get ignored. On the other hand,
+              specifying a length without a multiplier will implicitly indicate
+              that the length value should be interpreted as bytes (B).
+
+        """
+        super(BLOB, self).__init__(length=length, **kwargs)
+        self.multiplier = multiplier
+
+
+class FLOAT(_TDType, sqltypes.FLOAT):
+
+    """ Teradata FLOAT type
+
+    This type represent values in sign/magnitude form ranging from
+    2.226 x 10^-308 to 1.797 x 10^308.
+
+    """
+
+    def __init__(self, **kwargs):
+
+        """ Construct a FLOAT object """
+        super(FLOAT, self).__init__(**kwargs)
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+            return 'CAST(%s as FLOAT)' % value
+        return process
+
+
+class NUMBER(_TDType, sqltypes.NUMERIC):
+
+    """ Teradata NUMBER type
+
+    This type represents a numeric value with optional precision and scale
+    limitations.
+
+    """
+
+    __visit_name__ = 'NUMBER'
+
+    def __init__(self, precision=None, scale=None, **kwargs):
+
+        """ Construct a NUMBER object
+
+        :param precision: max number of digits that can be stored. Valid values
+        range from 1 to 38.
+
+        :param scale: number of fractional digits of :param precision: to the
+        right of the decimal point. Valid values range from 0 to
+        :param precision:.
+
+        Note: Both parameters are optional. When both are left unspecified,
+              defaults to NUMBER with the system limits for precision and scale.
+
+        """
+        prec = None if precision is not None and precision < 0 else precision
+        scale = None if scale is not None and scale < 0 else scale
+        super(NUMBER, self).__init__(precision=prec, scale=scale, **kwargs)
+
+
+class DATE(_TDType, sqltypes.DATE):
+
+    """ Teradata DATE type
+
+    Identifies a field as a DATE value and simplifies handling and formatting
+    of date variables.
+
+    """
+
+    def __init__(self, **kwargs):
+
+        """ Construct a DATE Object """
+        super(DATE, self).__init__(**kwargs)
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+            return "DATE '%s'" % value
+        return process
+
+
+class TIME(_TDType, sqltypes.TIME):
+
+    """ Teradata TIME type
+
+    This type identifies a field as a TIME value.
+
+    """
+
+    def __init__(self, precision=6, timezone=False, **kwargs):
+
+        """ Construct a TIME stored as UTC in Teradata
+
+        :param precision: optional fractional seconds precision. A single digit
+        representing the number of significant digits in the fractional
+        portion of the SECOND field. Valid values range from 0 to 6 inclusive.
+        The default precision is 6.
+
+        :param timezone: If set to True creates a Time WITH TIME ZONE type
+
+        """
+        super(TIME, self).__init__(timezone=timezone, **kwargs)
+        self.precision = precision
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+            return "TIME '%s'" % value
+        return process
+
+
+class TIMESTAMP(_TDType, sqltypes.TIMESTAMP):
+
+    """ Teradata TIMESTAMP type
+
+    This type identifies a field as a TIMESTAMP value.
+
+    """
+
+    def __init__(self, precision=6, timezone=False, **kwargs):
+        """ Construct a TIMESTAMP stored as UTC in Teradata
+
+        :param precision: optional fractional seconds precision. A single digit
+        representing the number of significant digits in the fractional
+        portion of the SECOND field. Valid values range from 0 to 6 inclusive.
+        The default precision is 6.
+
+        :param timezone: If set to True creates a TIMESTAMP WITH TIME ZONE type
+
+        """
+        super(TIMESTAMP, self).__init__(timezone=timezone, **kwargs)
+        self.precision = precision
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+            return "TIMESTAMP '%s'" % value
+        return process
+
+    def get_dbapi_type(self, dbapi):
+      return dbapi.DATETIME
+
+
+class _TDInterval(_TDType, types.UserDefinedType):
+
+    """ Base class for the Teradata INTERVAL sqltypes """
+
+    def __init__(self, precision=None, frac_precision=None, **kwargs):
+        self.precision      = precision
+        self.frac_precision = frac_precision
+
+    def bind_processor(self, dialect):
+
+        """
+        Processes the Interval value from SQLAlchemy to DB
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+
+        """
+        Processes the Interval value from DB to SQLAlchemy
+        """
+        def process(value):
+            return value
+        return process
+
+    def literal_processor(self, dialect):
+
+        def process(value):
+            return "INTERVAL '%s' %s" % (value, value.type)
+        return process
+
+class INTERVAL_YEAR(_TDInterval):
+
+    """ Teradata INTERVAL YEAR type
+
+    This type identifies a field defining a period of time in years.
+
+    """
+    __visit_name__ = 'INTERVAL_YEAR'
+
+    def __init__(self, precision=None, **kwargs):
+
+       """ Construct an INTERVAL_YEAR object
+
+       :param precision: permitted range of digits for year ranging from 1 to 4
+
+       """
+       super(INTERVAL_YEAR, self).__init__(precision=precision)
+
+class INTERVAL_YEAR_TO_MONTH(_TDInterval):
+
+    """ Teradata INTERVAL YEAR TO MONTH type
+
+    This type identifies a field defining a period of time in years and months.
+
+    """
+
+    __visit_name__ = 'INTERVAL_YEAR_TO_MONTH'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_YEAR_TO_MONTH object
+
+        :param precision: permitted range of digits for year ranging from 1 to 4
+
+        """
+        super(INTERVAL_YEAR_TO_MONTH, self).__init__(precision=precision)
+
+class INTERVAL_MONTH(_TDInterval):
+
+    """ Teradata INTERVAL MONTH type
+
+    This type identifies a field defining a period of time in months.
+
+    """
+
+    __visit_name__ = 'INTERVAL_MONTH'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_MONTH object
+
+        :param precision: permitted range of digits for month ranging from 1 to 4
+
+        """
+        super(INTERVAL_MONTH, self).__init__(precision=precision)
+
+class INTERVAL_DAY(_TDInterval):
+
+    """ Teradata INTERVAL DAY type
+
+    This type identifies a field defining a period of time in days.
+
+    """
+
+    __visit_name__ = 'INTERVAL_DAY'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_DAY object
+
+        :param precision: permitted range of digits for day ranging from 1 to 4
+
+        """
+        super(INTERVAL_DAY, self).__init__(precision=precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL DAY
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                value = td_dtypes.Interval(days=value.days)
+#            return value
+#        return process
+
+class INTERVAL_DAY_TO_HOUR(_TDInterval):
+
+    """ Teradata INTERVAL DAY TO HOUR type
+
+    This type identifies a field defining a period of time in days and hours.
+
+    """
+
+    __visit_name__ = 'INTERVAL_DAY_TO_HOUR'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_DAY_TO_HOUR object
+
+        :param precision: permitted range of digits for day ranging from 1 to 4
+
+        """
+        super(INTERVAL_DAY_TO_HOUR, self).__init__(precision=precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL DAY
+#        TO HOUR
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                hours = int(value.seconds / 3600)
+#                value = td_dtypes.Interval(days=value.days, hours=hours)
+#            return value
+#        return process
+
+class INTERVAL_DAY_TO_MINUTE(_TDInterval):
+
+    """ Teradata INTERVAL DAY TO MINUTE type
+
+    This type identifies a field defining a period of time in days, hours,
+    and minutes.
+
+    """
+
+    __visit_name__ = 'INTERVAL_DAY_TO_MINUTE'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_DAY_TO_MINUTE object
+
+        :param precision: permitted range of digits for day ranging from 1 to 4
+
+        """
+        super(INTERVAL_DAY_TO_MINUTE, self).__init__(precision=precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL DAY
+#        TO MINUTE
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                minutes = int(value.seconds / 60)
+#                value   = td_dtypes.Interval(days=value.days, minutes=minutes)
+#            return value
+#        return process
+
+class INTERVAL_DAY_TO_SECOND(_TDInterval):
+
+    """ Teradata INTERVAL DAY TO SECOND type
+
+    This type identifies a field during a period of time in days, hours, minutes,
+    and seconds.
+
+    """
+
+    __visit_name__ = 'INTERVAL_DAY_TO_SECOND'
+
+    def __init__(self, precision=None, frac_precision=None, **kwargs):
+
+        """ Construct an INTERVAL_DAY_TO_SECOND object
+
+        :param precision: permitted range of digits for day ranging from 1 to 4
+
+        :param frac_precision: fracional_seconds_precision ranging from 0 to 6
+
+        """
+        super(INTERVAL_DAY_TO_SECOND, self).__init__(precision=precision,
+                                                     frac_precision=frac_precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL DAY
+#        TO SECOND
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                seconds = value.seconds + value.microseconds / 1000000
+#                value   = td_dtypes.Interval(days=value.days, seconds=seconds)
+#            return value
+#        return process
+
+class INTERVAL_HOUR(_TDInterval):
+
+    """ Teradata INTERVAL HOUR type
+
+    This type identifies a field defining a period of time in hours.
+
+    """
+
+    __visit_name__ = 'INTERVAL_HOUR'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_HOUR object
+
+        :param precision: permitted range of digits for hour ranging from 1 to 4
+
+        """
+        super(INTERVAL_HOUR, self).__init__(precision=precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL HOUR
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                hours = int(value.total_seconds() / 3600)
+#                value = td_dtypes.Interval(hours=hours)
+#            return value
+#        return process
+
+class INTERVAL_HOUR_TO_MINUTE(_TDInterval):
+
+    """ Teradata INTERVAL HOUR TO MINUTE type
+
+    This type identifies a field defining a period of time in hours and minutes.
+
+    """
+
+    __visit_name__ = 'INTERVAL_HOUR_TO_MINUTE'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_HOUR_TO_MINUTE object
+
+        :param precision: permitted range of digits for hour ranging from 1 to 4
+
+        """
+        super(INTERVAL_HOUR_TO_MINUTE, self).__init__(precision=precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL HOUR
+#        TO MINUTE
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                hours, seconds = divmod(value.total_seconds(), 3600)
+#                hours   = int(hours)
+#                minutes = int(seconds / 60)
+#                value   = td_dtypes.Interval(hours=hours, minutes=minutes)
+#            return value
+#        return process
+
+class INTERVAL_HOUR_TO_SECOND(_TDInterval):
+
+    """ Teradata INTERVAL HOUR TO SECOND type
+
+    This type identifies a field defining a period of time in hours, minutes,
+    and seconds.
+
+    """
+
+    __visit_name__ = 'INTERVAL_HOUR_TO_SECOND'
+
+    def __init__(self, precision=None, frac_precision=None, **kwargs):
+
+        """ Construct an INTERVAL_HOUR_TO_SECOND object
+
+        :param precision: permitted range of digits for hour ranging from 1 to 4
+
+        :param frac_precision: fracional_seconds_precision ranging from 0 to 6
+
+        """
+        super(INTERVAL_HOUR_TO_SECOND, self).__init__(precision=precision,
+                                                      frac_precision=frac_precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL HOUR
+#        TO SECOND
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                hours, seconds = divmod(value.total_seconds(), 3600)
+#                hours   = int(hours)
+#                seconds = int(seconds) + value.microseconds / 1000000
+#                value   = td_dtypes.Interval(hours=hours, seconds=seconds)
+#            return value
+#        return process
+
+class INTERVAL_MINUTE(_TDInterval):
+
+    """ Teradata INTERVAL MINUTE type
+
+    This type identifies a field defining a period of time in minutes.
+
+    """
+
+    __visit_name__ = 'INTERVAL_MINUTE'
+
+    def __init__(self, precision=None, **kwargs):
+
+        """ Construct an INTERVAL_MINUTE object
+
+        :param precision: permitted range of digits for minute ranging from 1 to 4
+
+        """
+        super(INTERVAL_MINUTE, self).__init__(precision=precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL MINUTE
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                minutes = int(value.total_seconds() / 60)
+#                value = td_dtypes.Interval(minutes=minutes)
+#            return value
+#        return process
+
+class INTERVAL_MINUTE_TO_SECOND(_TDInterval):
+
+    """ Teradata INTERVAL MINUTE TO SECOND type
+
+    This type identifies a field defining a period of time in minutes and seconds.
+
+    """
+
+    __visit_name__ = 'INTERVAL_MINUTE_TO_SECOND'
+
+    def __init__(self, precision=None, frac_precision=None, **kwargs):
+
+        """ Construct an INTERVAL_MINUTE_TO_SECOND object
+
+        :param precision: permitted range of digits for minute ranging from 1 to 4
+
+        :param frac_precision: fracional_seconds_precision ranging from 0 to 6
+
+        """
+        super(INTERVAL_MINUTE_TO_SECOND, self).__init__(precision=precision,
+                                                        frac_precision=frac_precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL MINUTE
+#        TO SECOND
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                minutes, seconds = divmod(value.total_seconds(), 60)
+#                minutes = int(minutes)
+#                seconds = int(seconds) + value.microseconds / 1000000
+#                value   = td_dtypes.Interval(minutes=minutes, seconds=seconds)
+#            return value
+#        return process
+
+class INTERVAL_SECOND(_TDInterval):
+
+    """ Teradata INTERVAL SECOND type
+
+    This type identifies a field defining a period of time in seconds.
+
+    """
+
+    __visit_name__ = 'INTERVAL_SECOND'
+
+    def __init__(self, precision=None, frac_precision=None, **kwargs):
+
+        """ Construct an INTERVAL_SECOND object
+
+        :param precision: permitted range of digits for second ranging from 1 to 4
+
+        :param frac_precision: fractional_seconds_precision ranging from 0 to 6
+
+        """
+        super(INTERVAL_SECOND, self).__init__(precision=precision,
+                                              frac_precision=frac_precision)
+
+#    def bind_processor(self, dialect):
+#
+#        """
+#        Handles the conversion from a datetime.timedelta object to an Interval
+#        object appropriate for inserting into a column with type INTERVAL SECOND
+#
+#        """
+#        def process(value):
+#            if isinstance(value, datetime.timedelta):
+#                seconds = value.total_seconds()
+#                value = td_dtypes.Interval(seconds=seconds)
+#            return value
+#        return process
+
+
+class _TDPeriod(_TDType, types.UserDefinedType):
+
+    """ Base class for the Teradata Period sqltypes """
+
+    def __init__(self, format=None, **kwargs):
+        self.format = format
+
+    def bind_processor(self, dialect):
+
+        """
+        Processes the Period value from SQLAlchemy to DB
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+
+        """
+        Processes the Period value from DB to SQLAlchemy
+        """
+        def process(value):
+            return value
+        return process
+
+class PERIOD_DATE(_TDPeriod):
+
+    """ Teradata PERIOD DATE type
+
+    This type identifies a field defining a duration with a beginning and end date.
+
+    """
+
+    __visit_name__ = 'PERIOD_DATE'
+
+    def __init__(self, format=None, **kwargs):
+
+        """ Construct a PERIOD_DATE object
+
+        :param format: format of the date, e.g. 'yyyy-mm-dd'
+
+        """
+        super(PERIOD_DATE, self).__init__(format=format, **kwargs)
+
+class PERIOD_TIME(_TDPeriod):
+
+    """ Teradata PERIOD TIME type
+
+    This type identifies a field defining a duration with a beginning and end time.
+
+    """
+
+    __visit_name__ = 'PERIOD_TIME'
+
+    def __init__(self, format=None, frac_precision=None, timezone=False, **kwargs):
+
+        """ Construct a PERIOD_TIME object
+
+        :param format: format of the time, e.g. 'HH:MI:SS.S(6)' and
+        'HH:MI:SS.S(6)Z' (with timezone)
+
+        :param frac_precision: fractional_seconds_precision ranging from 0 to 6
+
+        :param timezone: true if WITH TIME ZONE, false otherwise
+
+        """
+        super(PERIOD_TIME, self).__init__(format=format, **kwargs)
+        self.frac_precision = frac_precision
+        self.timezone       = timezone
+
+class PERIOD_TIMESTAMP(_TDPeriod):
+
+    """ Teradata PERIOD TIMESTAMP type
+
+    This type identifies a field defining a duration with a beginning and end timestamp.
+
+    """
+
+    __visit_name__ = 'PERIOD_TIMESTAMP'
+
+    def __init__(self, format=None, frac_precision=None, timezone=False, **kwargs):
+
+        """ Construct a PERIOD_TIMESTAMP object
+
+        :param format: format of the timestamp, e.g. 'YYYY-MM-DDBHH:MI:SS.S(6)'
+        and 'YYYY-MM-DDBHH:MI:SS.S(6)Z' (with timezone)
+
+        :param frac_precision: fractional_seconds_precision ranging from 0 to 6
+
+        :param timezone: true if WITH TIME ZONE, false otherwise
+
+        """
+        super(PERIOD_TIMESTAMP, self).__init__(format=format, **kwargs)
+        self.frac_precision = frac_precision
+        self.timezone       = timezone
+
+
+class CHAR(_TDConcatenable, _TDType, sqltypes.CHAR):
+
+    """ Teradata CHAR type
+
+    This type represents a fixed-length character string for Teradata Database
+    internal character storage.
+
+    """
+
+    def __init__(self, length=1, charset=None, **kwargs):
+
+        """ Construct a CHAR object
+
+        :param length: number of characters or bytes allocated. Maximum value
+        for n depends on the character set. For LATIN - 64000 characters,
+        For UNICODE - 32000 characters, For KANJISJIS - 32000 bytes. If a value
+        for n is not specified, the default is 1.
+
+        :param charset: Server character set for the character column.
+        Supported values:
+            'LATIN': fixed 8-bit characters from the ASCII ISO 8859 Latin1
+            or ISO 8859 Latin9.
+            'UNICODE': fixed 16-bit characters from the UNICODE 6.0 standard.
+            'GRAPHIC': fixed 16-bit UNICODE characters defined by IBM for DB2.
+            'KANJISJIS': mixed single byte/multibyte characters intended for
+            Japanese applications that rely on KanjiShiftJIS characteristics.
+        Note: GRAPHIC(n) is equivalent to CHAR(n) CHARACTER SET GRAPHIC
+
+        """
+        super(CHAR, self).__init__(length=length, **kwargs)
+        self.charset = charset
+
+
+class VARCHAR(_TDConcatenable, _TDType, sqltypes.VARCHAR):
+
+    """ Teradata VARCHAR type
+
+    This type represents a variable length character string of length 0 to n
+    for Teradata Database internal character storage. LONG VARCHAR specifies
+    the longest permissible variable length character string for Teradata
+    Database internal character storage.
+
+    """
+
+    def __init__(self, length=None, charset=None, **kwargs):
+
+        """ Construct a VARCHAR object
+
+        :param length: Optional 0 to n. If None, LONG is used
+        (the longest permissible variable length character string)
+
+        :param charset: optional character set for varchar.
+
+        Note: VARGRAPHIC(n) is equivalent to VARCHAR(n) CHARACTER SET GRAPHIC
+
+        """
+        super(VARCHAR, self).__init__(length=length, **kwargs)
+        self.charset = charset
+
+
+class CLOB(_TDConcatenable, _TDType, sqltypes.CLOB):
+
+    """ Teradata CLOB type
+
+    This type represents a large character string. A character large object
+    (CLOB) column can store character data, such as simple text or HTML.
+
+    """
+
+    def __init__(self, length=None, charset=None, multiplier=None, **kwargs):
+
+        """ Construct a CLOB object
+
+        :param length: Optional length for clob. For Latin server character set,
+        length cannot exceed 2097088000. For Unicode server character set,
+        length cannot exceed 1048544000.
+        If no length is specified then the maximum is used.
+
+        :param multiplier: Either 'K', 'M', or 'G'.
+        K specifies number of characters to allocate as nK, where K=1024
+        (For Latin char sets, n < 2047937 and For Unicode char sets, n < 1023968)
+        M specifies nM, where M=1024K
+        (For Latin char sets, n < 1999 and For Unicode char sets, n < 999)
+        G specifies nG, where G=1024M
+        (For Latin char sets, n must be 1 and char set must be LATIN)
+
+        :param charset: LATIN (fixed 8-bit characters ASCII ISO 8859 Latin1 or ISO 8859 Latin9)
+        or UNICODE (fixed 16-bit characters from the UNICODE 6.0 standard)
+
+        """
+        super(CLOB, self).__init__(length=length, **kwargs)
+        self.charset    = charset
+        self.multiplier = multiplier
+
+class XML(_TDType, types.UserDefinedType):
+
+    """Class for the Teradata datatype XML """
+
+    #maximum_length = 2097088000, inline_length = 4046
+
+    __visit_name__ = "XML"
+
+    def __init__(self, maximum_length=2097088000, inline_length=4046,
+                 **kwargs):
+        self.maximum_length = maximum_length
+        self.inline_length = inline_length
+
+    def bind_processor(self, dialect):
+
+        """
+        Processes the XML value from SQLAlchemy to DB
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+
+        """
+        Processes the XML value from DB to SQLAlchemy
+        """
+        def process(value):
+            return value
+        return process
+
+
+class JSON(_TDType, types.UserDefinedType):
+
+    """ Class for the Teradata JSON type """
+
+    __visit_name__ = "JSON"
+
+    def __init__(self, max_length=16776192, inline_length=64000, charset=None,
+                 storage_format=None, **kwargs):
+        """
+        Constructor for JSON Data Type.
+
+        PARAMETERS:
+            max_length:
+                Optional Argument.
+                Specifies the maximum length of JSON type.
+                Default Value: 16776192
+                Type: int
+
+            inline_length:
+                Optional Argument.
+                Specifies the inline storage size of JSON type.
+                Default Value: 64000
+                Type: int
+
+            charset:
+                Optional Argument.
+                Specifies the character set for JSON type.
+                Note:
+                    This argument cannot be specified with storage_format. Teradata
+                    databases throws error in that case.
+                Default Value: None
+                Type: str
+
+            storage_format:
+                Optional Argument.
+                Specifies storage format for the JSON type.
+                Note:
+                    This argument cannot be specified with storage_format, Teradata
+                    databases throws error in that case.
+                Default Value: None
+                Type: str
+
+        RETURNS:
+            Object of JSON() type.
+
+        RAISES:
+            None.
+        """
+        self.max_length = max_length
+        self.inline_length = inline_length
+        self.charset = charset
+        self.storage_format = storage_format
+        
+    def bind_processor(self, dialect):
+        """
+        Processes the value from SQLAlchemy to database.
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+        """
+        Processes the XML value from database to SQLAlchemy.
+        """
+        def process(value):
+            return value
+        return process
+
+class TDUDT(_TDType, types.UserDefinedType):
+    """Class for the Teradata User Defined types """
+
+    __visit_name__ = "TDUDT"
+
+    def __init__(self, type_name=None,  **kwargs):
+        """
+        Constructor for User Defined Types
+
+        PARAMETERS:
+            type_name:
+                Optional Argument.
+                Specifies the name of User Defined Type.
+                Type: str
+
+        RETURNS:
+            Object of TDUDT().
+
+        RAISES:
+            None.
+        """
+        self.type_name = type_name
+
+    def bind_processor(self, dialect):
+        """
+        Processes the  value from SQLAlchemy to DB
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+        """
+        Processes the XML value from DB to SQLAlchemy
+        """
+        def process(value):
+            return value
+        return process
+
+class GEOMETRY(_TDType, types.UserDefinedType):
+
+    """ Class for the Teradata datatype ST_GEOMETRY """
+
+    __visit_name__ = "GEOMETRY"
+
+    def __init__(self, max_length=16776192, inline_length=9920,
+                 **kwargs):
+        """
+        Constructor for GEOMETRY Data Type.
+
+        PARAMETERS:
+           maximum_length:
+               Optional Argument.
+               Specifies the maximum length of GEOMETRY type.
+               Default Value: 16776192
+               Type: int
+
+           inline_length:
+               Optional Argument.
+               Specifies the inline storage size of GEOMETRY type.
+               Default Value: 9920
+               Type: int
+
+        RETURNS:
+           Object of GEOMETRY() type.
+
+        RAISES:
+           None.
+        """
+        self.max_length = max_length
+        self.inline_length = inline_length
+
+    def bind_processor(self, dialect):
+        """
+        Processes the GEOMETRY value from SQLAlchemy to Database.
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+        """
+        Processes the GEOMETRY value from Database to SQLAlchemy.
+        """
+        def process(value):
+            return value
+        return process
+
+class MBR(_TDType, types.UserDefinedType):
+
+    """ Class for the Teradata datatype MBR """
+
+    __visit_name__ = "MBR"
+
+    def bind_processor(self, dialect):
+        """
+        Processes the MBR value from SQLAlchemy to Database.
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+        """
+        Processes the MBR value from Database to SQLAlchemy.
+        """
+        def process(value):
+            return value
+        return process
+
+class MBB(_TDType, types.UserDefinedType):
+
+    """ Class for the Teradata datatype MBB """
+
+    __visit_name__ = "MBB"
+
+    def bind_processor(self, dialect):
+        """
+        Processes the MBB value from SQLAlchemy to Database.
+        """
+        def process(value):
+            return value
+        return process
+
+    def result_processor(self, dialect, coltype):
+        """
+        Processes the MBB value from Database to SQLAlchemy.
+        """
+        def process(value):
+            return value
+        return process
+
+class TeradataExpressionAdapter:
+    """Expression Adapter for Teradata Data Types.
+
+    For inferring the resulting type of a BinaryExpression whose operation
+    involves operands that are of Teradata types.
+    """
+
+    def process(self, type_, op=None, other=None, **kw):
+        """Adapts the expression.
+
+        Infer the type of the resultant BinaryExpression defined by the passed
+        in operator and operands. This resulting type should be consistent with
+        the Teradata database when the operation is defined.
+
+        Args:
+            type_: The type instance of the left operand.
+
+            op:    The operator of the BinaryExpression.
+
+            other: The type instance of the right operand.
+
+        Returns:
+            The type to adapt the BinaryExpression to.
+        """
+
+        if isinstance(type_, _TDInterval) or isinstance(other, _TDInterval):
+            adapt_strategy = _IntervalRuleStrategy()
+        else:
+            adapt_strategy = _LookupStrategy()
+
+        return adapt_strategy.adapt(type_, op, other, **kw)
+
+
+class _AdaptStrategy:
+    """Interface for expression adaptation strategies."""
+
+    def adapt(self, type_, op, other, **kw):
+        """Adapt the expression according to some strategy.
+
+        Given the type of the left and right operand, and the operator, produce
+        a resulting type class for the BinaryExpression.
+        """
+
+        raise NotImplementedError()
+
+class _IntervalRuleStrategy(_AdaptStrategy):
+    """Expression adaptation strategy which follows a set of rules for inferring
+    Teradata Interval types.
+    """
+
+    ordering = ('YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND')
+
+    def adapt(self, type_, op, other, **kw):
+        """Adapt the expression by a set of predefined rules over the Teradata
+        Interval types.
+        """
+
+        # If the (Interval) types are equal, simply return the class of
+        # those types
+        if type_.__class__ == other.__class__:
+            return type_.__class__
+
+        # If the (Interval) types are not equal, return the valid Interval type
+        # with the greatest range.
+        #
+        # E.g. INTERVAL YEAR TO MONTH and INTERVAL DAY TO HOUR -->
+        #      INTERVAL YEAR TO HOUR.
+        #
+        # Otherwise if the resulting Interval type is invalid, return NullType.
+        #
+        # E.g. INTERVAL YEAR TO MONTH and INTERVAL MINUTE TO SECOND -->
+        #      INTERVAL YEAR TO SECOND (invalid) -->
+        #      NullType
+        elif isinstance(type_, _TDInterval) and isinstance(other, _TDInterval):
+            tokens = self._tokenize_name(type_.__class__.__name__) + \
+                     self._tokenize_name(other.__class__.__name__)
+            tokens.sort(key=lambda tok: self.ordering.index(tok))
+
+            return getattr(sys.modules[__name__],
+                self._combine_tokens(tokens[0], tokens[-1]),
+                sqltypes.NullType)()
+
+        # Else the binary expression has an Interval and non-Interval operand.
+        # If the non-Interval operand is a Date, Time, or Datetime, return that
+        # type, otherwise return the Interval type.
+        else:
+            interval, non_interval = (type_, other) if \
+                    isinstance(type_, _TDInterval) \
+                else (other, type_)
+
+            return non_interval.__class__ if \
+                    isinstance(non_interval, (sqltypes.Date,
+                                              sqltypes.Time,
+                                              sqltypes.DateTime)) \
+                else interval.__class__
+
+    def _tokenize_name(self, interval_name):
+        """Tokenize the name of Interval types.
+
+        Returns a list of (str) tokens of the corresponding Interval type name.
+
+        E.g. 'INTERVAL_DAY_TO_HOUR' --> ['DAY', 'HOUR'].
+        """
+
+        return list(filter(lambda tok: tok not in ('INTERVAL', 'TO'),
+                           interval_name.split('_')))
+
+    def _combine_tokens(self, tok_l, tok_r):
+        """Combine the tokens of an Interval type to form its name.
+
+        Returns a string for the name of the Interval type corresponding to the
+        tokens passed in.
+
+        E.g. tok_l='DAY' and tok_r='HOUR' --> 'INTERVAL_DAY_TO_HOUR'
+        """
+
+        return 'INTERVAL_%s_TO_%s' % (tok_l, tok_r)
+
+class _LookupStrategy(_AdaptStrategy):
+    """Expression adaptation strategy which employs a general lookup table."""
+
+    def adapt(self, type_, op, other, **kw):
+        """Adapt the expression by looking up a hardcoded table.
+
+        The lookup table is defined as `visit_` methods below. Each method
+        returns a nested dictionary which is keyed by the operator and the other
+        operand's type.
+        """
+
+        return getattr(self, self._process_visit_name(type_.__visit_name__),
+                   lambda *args, **kw: {})(type_, other, **kw) \
+            .get(op, util.immutabledict()) \
+            .get(other.__class__, type_.__class__)
+
+    def _process_visit_name(self, visit_name):
+        """Generate the corresponding visit function name from a type's
+        __visit_name__ field.
+        """
+
+        prefix = 'visit_'
+        return prefix + visit_name
+
+    def _flatten_tuple_keyed_dict(self, tuple_dict):
+        """Recursively flatten a dictionary with (many-to-one) tuple keys to a
+        standard one.
+        """
+
+        flat_dict = {}
+        for ks, v in tuple_dict.items():
+            v = self._flatten_tuple_keyed_dict(v) if isinstance(v, dict) else v
+            if isinstance(ks, tuple):
+                for k in ks:
+                    flat_dict[k] = v
+            else:
+                flat_dict[ks] = v
+        return flat_dict
+
+    def visit_INTEGER(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.add: {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                BIGINT:  BIGINT,
+                DECIMAL: DECIMAL,
+                FLOAT:   FLOAT,
+                NUMBER:  NUMBER,
+                DATE:    DATE
+            },
+            (operators.sub, operators.mul, operators.truediv,
+             operators.mod): {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                BIGINT:  BIGINT,
+                DECIMAL: DECIMAL,
+                FLOAT:   FLOAT,
+                NUMBER:  NUMBER
+            }
+        })
+
+    def visit_SMALLINT(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.add: {
+                (INTEGER, SMALLINT, BYTEINT): INTEGER,
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                BIGINT:  BIGINT,
+                DECIMAL: DECIMAL,
+                FLOAT:   FLOAT,
+                NUMBER:  NUMBER,
+                DATE:    DATE
+            },
+            (operators.sub, operators.mul, operators.truediv,
+             operators.mod): {
+                (INTEGER, SMALLINT, BYTEINT, DATE): INTEGER,
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                BIGINT:  BIGINT,
+                DECIMAL: DECIMAL,
+                FLOAT:   FLOAT,
+                NUMBER:  NUMBER,
+            }
+        })
+
+    def visit_BIGINT(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.add: {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                DECIMAL: DECIMAL,
+                FLOAT:   FLOAT,
+                NUMBER:  NUMBER,
+                DATE:    DATE
+            },
+            (operators.sub, operators.mul, operators.truediv,
+             operators.mod): {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                DECIMAL: DECIMAL,
+                FLOAT:   FLOAT,
+                NUMBER:  NUMBER
+            }
+        })
+
+    def visit_DECIMAL(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.add: {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                FLOAT:  FLOAT,
+                NUMBER: NUMBER,
+                DATE:   DATE
+            },
+            (operators.sub, operators.mul, operators.truediv,
+             operators.mod): {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                FLOAT:  FLOAT,
+                NUMBER: NUMBER
+            }
+        })
+
+    def visit_DATE(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.add: {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                DATE:  INTEGER,
+                FLOAT: FLOAT
+            },
+            operators.sub: {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                DATE:  INTEGER,
+                FLOAT: FLOAT
+            },
+            (operators.mul, operators.truediv, operators.mod): {
+                (DATE, INTEGER, SMALLINT, BYTEINT): INTEGER,
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                (FLOAT, TIME): FLOAT,
+                BIGINT:  BIGINT,
+                DECIMAL: DECIMAL,
+                NUMBER:  NUMBER,
+            }
+        })
+
+    def visit_TIME(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            (operators.add, operators.mul, operators.truediv,
+             operators.mod): {
+                DATE: FLOAT
+            }
+        })
+
+    def visit_CHAR(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.concat_op: {
+                CHAR:    VARCHAR if hasattr(other, 'charset') and \
+                            ((type_.charset == 'unicode') !=
+                             (other.charset == 'unicode'))
+                         else CHAR,
+                VARCHAR: VARCHAR,
+                CLOB:    CLOB
+            },
+            (operators.add, operators.sub, operators.mul,
+             operators.truediv, operators.mod): {
+                (INTEGER, SMALLINT, BIGINT, BYTEINT, NUMBER, FLOAT, DECIMAL,
+                 DATE, CHAR, VARCHAR): FLOAT
+            }
+        })
+
+    def visit_VARCHAR(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.concat_op: {
+                CLOB: CLOB
+            },
+            (operators.add, operators.sub, operators.mul,
+             operators.truediv, operators.mod): {
+                (INTEGER, SMALLINT, BIGINT, BYTEINT, NUMBER, FLOAT, DECIMAL,
+                 DATE, CHAR, VARCHAR): FLOAT
+            }
+        })
+
+    def visit_BYTEINT(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            (operators.add, operators.sub): {
+                (INTEGER, SMALLINT, BYTEINT): INTEGER,
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                BIGINT:   BIGINT,
+                DECIMAL:  DECIMAL,
+                FLOAT:    FLOAT,
+                NUMBER:   NUMBER,
+                DATE:     DATE
+            },
+            (operators.mul, operators.truediv, operators.mod): {
+                (INTEGER, SMALLINT, BYTEINT, DATE): INTEGER,
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                BIGINT:   BIGINT,
+                DECIMAL:  DECIMAL,
+                FLOAT:    FLOAT,
+                NUMBER:   NUMBER
+            }
+        })
+
+    def visit_BYTE(self, type_, other, **kw):
+        return {
+            operators.concat_op: {
+                VARBYTE: VARBYTE,
+                BLOB:    BLOB
+            }
+        }
+
+    def visit_VARBYTE(self, type_, other, **kw):
+        return {
+            operators.concat_op: {
+                BLOB: BLOB
+            }
+        }
+
+    def visit_NUMBER(self, type_, other, **kw):
+        return self._flatten_tuple_keyed_dict({
+            operators.add: {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                FLOAT: FLOAT,
+                DATE:  DATE
+            },
+            (operators.sub, operators.mul, operators.truediv,
+             operators.mod): {
+                (CHAR, VARCHAR, BLOB): FLOAT,
+                FLOAT: FLOAT
+            }
+        })
```

## teradatasqlalchemy/utils.py

 * *Ordering differences only*

```diff
@@ -1,32 +1,32 @@
-# Copyright 2018 by Teradata Corporation. All rights reserved.
-
-import warnings
-from functools import wraps
-
-
-def deprecated(version, replacement=None):
-    """
-    Define a deprecation decorator.
-    An optional `replacement` should refer to the new API to be used instead.
-
-    Example:
-    -------
-
-      @deprecated('1.1')
-      def old_func(): ...
-
-      @deprecated('1.1', 'new_func')
-      def old_func(): ..."""
-
-    def decorator(func):
-        def wrapper(*args, **kwargs):
-            msg = "\"{}\" has been deprecated in version {} and will be removed in a future version."
-            if replacement:
-                msg += "\n Use \"{}\" instead."
-            warnings.warn(msg.format(func.__name__, version, replacement),
-                          category=DeprecationWarning, stacklevel=2)
-            return func(*args, **kwargs)
-
-        return wraps(func)(wrapper)
-
-    return decorator
+# Copyright 2018 by Teradata Corporation. All rights reserved.
+
+import warnings
+from functools import wraps
+
+
+def deprecated(version, replacement=None):
+    """
+    Define a deprecation decorator.
+    An optional `replacement` should refer to the new API to be used instead.
+
+    Example:
+    -------
+
+      @deprecated('1.1')
+      def old_func(): ...
+
+      @deprecated('1.1', 'new_func')
+      def old_func(): ..."""
+
+    def decorator(func):
+        def wrapper(*args, **kwargs):
+            msg = "\"{}\" has been deprecated in version {} and will be removed in a future version."
+            if replacement:
+                msg += "\n Use \"{}\" instead."
+            warnings.warn(msg.format(func.__name__, version, replacement),
+                          category=DeprecationWarning, stacklevel=2)
+            return func(*args, **kwargs)
+
+        return wraps(func)(wrapper)
+
+    return decorator
```

## teradatasqlalchemy/vernumber.py

```diff
@@ -1,3 +1,3 @@
-# Copyright 2024 by Teradata Corporation. All rights reserved.
-
-sVersionNumber = "20.0.0.0"
+# Copyright 2024 by Teradata Corporation. All rights reserved.
+
+sVersionNumber = "20.0.0.1"
```

## teradatasqlalchemy/telemetry/queryband.py

```diff
@@ -1,442 +1,445 @@
-import inspect
-from teradatasqlalchemy import vernumber
-import re
-
-
-class _QueryBand:
-    """
-    Class to hold the common attributes required for queryband enabling.
-    """
-    def __init__(self):
-        self._qb_buffer = []
-        self._org = "TERADATA-INTERNAL-TELEM"
-        self._app_name = "TDSQLMY"
-        self._app_version = vernumber.sVersionNumber
-        self._qb_template = "QUERY_BAND='ORG={org};APPNAME={app_name};APPVERSION={app_version};{client_qb};'"
-        self._set_qb_query_template = "SET {query_band} FOR TRANSACTION;"
-        self._prev_qb_str = None
-        self._prev_qb_str_freq = 0
-        self._qb_regex = r'^[a-zA-Z0-9_-]+$'
-        self.verbose = False
-
-    @property
-    def qb_buffer(self):
-        """
-        RETURNS:
-            Queryband string.
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.qb_buffer
-        """
-        return self._qb_buffer
-
-    @qb_buffer.setter
-    def qb_buffer(self, query_band):
-        """
-        Creates query band buffer if it doesn't exist else appends query band
-        to existing query band buffer self._qb_buffer.
-
-        PARAMETERS:
-            query_band
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.qb_buffer('ORG=TERADATA-INTERNAL-TELEM')
-        """
-        if not self._qb_buffer:
-            self._qb_buffer = []
-        self._qb_buffer.append(query_band)
-
-    @property
-    def qb_regex(self):
-        """
-        RETURNS:
-            Regular expression which validates queryband string.
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.qb_regex
-        """
-        return self._qb_regex
-
-    @property
-    def verbose(self):
-        """
-        RETURNS:
-            Configuration option which decides whether to print
-            error logs on console or not.
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.verbose
-        """
-        return self._verbose
-
-    @verbose.setter
-    def verbose(self, verbose):
-        """
-        Sets configuration option which decides whether to print
-        error logs on console or not.
-
-        PARAMETERS:
-            verbose
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.verbose(True)
-        """
-        self._verbose = verbose
-
-    def append_qb(self, query_band):
-        """
-        Creates query band buffer if it doesn't exist else appends query band
-        to existing query band buffer.
-
-        PARAMETERS:
-            query_band
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.append_qb('ORG=TERADATA-INTERNAL-TELEM')
-
-        """
-        # When new buffer is created.
-        if not self._prev_qb_str:
-            self._prev_qb_str = query_band
-            self._prev_qb_str_freq = 1
-            return
-
-        # check if queryband string is repeated.
-        # If not, append previous queryband to buffer and update _prev_qb_str
-        # and _prev_qb_str_freq else just increase the frequency of previous queryband.
-        if query_band != self._prev_qb_str:
-            # If _prev_qb_str is having frequency more than 1, then append queryband
-            # string with frequency, else append without frequency.
-            self.qb_buffer.append(self._prev_qb_str + "_" + str(self._prev_qb_str_freq)
-                                  if self._prev_qb_str_freq > 1 else self._prev_qb_str)
-            self._prev_qb_str = query_band
-            self._prev_qb_str_freq = 1
-        else:
-            self._prev_qb_str_freq = self._prev_qb_str_freq + 1
-
-    def pop_qb(self):
-        """
-        Removes last added queryband from query band buffer list.
-
-        PARAMETERS:
-            None
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.append_qb('ORG=TERADATA-INTERNAL-TELEM')
-            >>> qb.pop_qb()
-
-        """
-        try:
-            del self._qb_buffer[-1]
-        except IndexError:
-            self._qb_buffer = []
-
-    def reset_qb(self):
-        """
-        Removes all querybands from query band buffer list.
-
-        PARAMETERS:
-            None
-
-        EXAMPLES:
-            >>> qb = _QueryBand()
-            >>> qb.append_qb('ORG=TERADATA-INTERNAL-TELEM')
-            >>> qb.reset_qb()
-        """
-        self._prev_qb_str = None
-        self._prev_qb_str_freq = 0
-        self._qb_buffer = []
-
-    def configure_queryband_parameters(self, app_name, app_version):
-        """
-        DESCRIPTION:
-            Configures application name and application version which
-            uses queryband utility.
-
-        PARAMETERS:
-            app_name:
-                Required Argument:
-                Specifies name of the application which uses queryband utility.
-                Types: str
-
-            app_version:
-                Required Argument:
-                Specifies version of the application which uses queryband utility.
-                Types: str
-
-        RETURNS:
-            None
-
-        RAISES:
-            None.
-
-        EXAMPLES:
-            >>> session_qb = _QueryBand()
-            >>> session_qb.configure_queryband_parameters(app_name="TDML", app_version="20.00.00.00")
-        """
-        self._app_name = app_name
-        self._app_version = app_version
-
-    def generate_set_queryband_query(self):
-        """
-        DESCRIPTION:
-            Generates a SQL query to be used while setting transaction level
-            queryband for an application. Application specific data and querybands
-            collected during execution of application's APIs are used while genearting
-            final queryband string. Finally, cleans queryband buffer to start with
-            new workflow.
-
-        PARAMETERS:
-            None
-
-        RETURNS:
-            str
-
-        RAISES:
-            None.
-
-        EXAMPLES:
-            >>> session_qb = _QueryBand()
-            >>> session_qb.generate_set_queryband_query()
-        """
-
-        try:
-            # Before utilizing buffer, append lazy entries
-            # in _prev_qb_str to _qb_buffer.
-            if self._prev_qb_str:
-                # If _prev_qb_str is having frequency more than 1, then append queryband
-                # string with frequency, else append without frequency.
-                self.qb_buffer.append(self._prev_qb_str + "_" + str(self._prev_qb_str_freq)
-                                      if self._prev_qb_str_freq > 1 else self._prev_qb_str)
-            return self._set_qb_query_template.format(
-                query_band=self._qb_template.format(org=self._org,
-                                                    app_name=self._app_name,
-                                                    app_version=self._app_version,
-                                                    client_qb="FLOW={}".format("-".join(self._qb_buffer))))
-        except Exception as append_err:
-            log("Failed to generate SET QB query: ", append_err)
-        finally:
-            self.reset_qb()
-
-
-session_queryband = _QueryBand()
-
-
-def collect_queryband(queryband=None, attr=None, method=None,
-                      arg_name=None, prefix=None, suffix=None):
-    """
-    DESCRIPTION:
-        Decorator for collecting queryband string in queryband buffer.
-
-    PARAMETERS:
-        queryband:
-            Optional Argument:
-            Specifies queryband string.
-            Types: str
-
-        attr:
-            Optional Argument:
-            Specifies name of a class attribute whose value is to be used as
-            queryband string.
-            Types: str
-
-        method:
-            Optional Argument:
-            Specifies name of a class method which returns string to be used as
-            queryband string.
-            Note:
-                This method of class is expected to be a no-arg utility method and
-                should return an expected queryband string for some processing done
-                by a class/class method which needs to be tracked by queryband.
-            Types: str
-
-        arg_name:
-            Optional Argument:
-            Specifies name of an argument of a decorated function/method, whose value
-            is to be used as queryband string.
-            Types: str
-
-        prefix:
-            Optional Argument:
-            Specifies prefix to be applied to queryband string.
-            Types: str
-
-        suffix:
-            Optional Argument:
-            Specifies suffix to be applied to queryband string.
-            Types: str
-
-    EXAMPLES:
-        >>> from teradatasqlalchemy.telemetry import collect_queryband
-        # Example 1: Collect queryband for a standalone function.
-        @collect_queryband(queryband="CreateContext")
-        def create_context(host = None, username ...): ...
-
-        # Example 2: Collect queryband for a class method and use
-        #            class attribute to retrive queryband string.
-        @collect_queryband(attr="func_name")
-        def _execute_query(self, persist=False, volatile=False):...
-
-        # Example 3: Collect queryband for a class method and use
-        #            method of same class to retrive queryband string.
-        @collect_queryband(method="get_class_specific_queryband")
-        def _execute_query(self, persist=False, volatile=False):...
-    """
-    def qb_decorator(exposed_func):
-        def wrapper(*args, **kwargs):
-            qb_str = queryband
-            # If queryband string is not provided by client while calling decorator,
-            # it can be devised using following ways.
-            if not qb_str:
-                # Approach 1:
-                # Extract queryband from value of argument passed
-                # to decorated function/method.
-                if arg_name:
-                    # Extract value from Keyword arguments.
-                    if arg_name in kwargs:
-                        qb_str = kwargs[arg_name]
-
-                    # Extract value from positional arguments.
-                    # Also consider default values.
-                    else:
-                        # Generate a dictionary containing mapping between
-                        # argument names and their run time values.
-                        signature = inspect.signature(exposed_func)
-                        bound_args = signature.bind(*args, **kwargs)
-                        bound_args.apply_defaults()
-
-                        qb_str = bound_args.arguments[arg_name]
-
-                # Approach 2:
-                # Extract queryband from an attribute/method associated
-                # with class object.
-                is_instance_method = args and ('.' in exposed_func.__qualname__)
-                if is_instance_method:
-                    try:
-                        if attr:
-                            qb_str = getattr(args[0], attr)
-                        elif method:
-                            qb_str = getattr(args[0], method)()
-                    except Exception as stat_method_err:
-                        log("Failed to collect queryband for static class method.", stat_method_err)
-                        return exposed_func(*args, **kwargs)
-                else:
-                    log("Failed to collect queryband for standalone function.")
-                    return exposed_func(*args, **kwargs)
-
-            if qb_str:
-                # Validate queryband for string type.
-                if not isinstance(qb_str, str):
-                    log("Failed to collect queryband. Queryband must be of type str not {}".format(type(qb_str)))
-                    return exposed_func(*args, **kwargs)
-
-                # Process suffix and prefix.
-                if suffix and isinstance(suffix, str):
-                    qb_str = qb_str + "_" + suffix
-                if prefix and isinstance(prefix, str):
-                    qb_str = prefix + "_" + qb_str
-
-                # Validate queryband for allowed characters.
-                if not re.match(session_queryband.qb_regex, qb_str):
-                    log("Failed to collect queryband. Queryband string: '{}' contains invalid characters. Allowed characters are [a-z, A-Z, 0-9, '_', '-']".format(qb_str))
-                    return exposed_func(*args, **kwargs)
-
-                # Append queryband to buffer.
-                session_queryband.append_qb(qb_str)
-
-            return exposed_func(*args, **kwargs)
-
-        return wrapper
-    return qb_decorator
-
-
-def set_queryband(con_obj):
-    """
-    DESCRIPTION:
-        Decorator for executing set queryband SQL request using connection object from application
-        and then clearing queryband buffer for next workflow.
-
-    PARAMETERS:
-        con_obj:
-            Required Argument:
-            Specifies connection object to execute string.
-            Types: Sqlalchemy connection
-
-
-    EXAMPLES:
-        Setting queryband before execution of application's SQL request.
-        >>> from teradatasqlalchemy.telemetry import set_queryband
-        @set_queryband(con_obj=get_connection())
-        def _execute_ddl_statement(ddl_statement):...
-    """
-    def qb_decorator(execute_func):
-        def wrapper(*args, **kwargs):
-            # Execute set queryband SQL request.
-            try:
-                con_obj.exec_driver_sql(session_queryband.generate_set_queryband_query())
-            except Exception as qb_err:
-                log("Failed to set QB!!!", qb_err)
-            # Execute application's SQL request and after successful execution
-            # clean queryband buffer.
-            try:
-                ret_val = execute_func(*args, **kwargs)
-            except Exception as exec_err:
-                raise
-            else:
-                session_queryband.reset_qb()
-            return ret_val
-        return wrapper
-    return qb_decorator
-
-
-def get_qb_query():
-    """
-    DESCRIPTION:
-        Returns a SET queryband SQL query to be used while setting transaction level
-        queryband for an application using _QueryBand object.
-
-    PARAMETERS:
-        None
-
-    RETURNS:
-        str
-
-    RAISES:
-        None.
-
-    EXAMPLES:
-        >>> from teradatasqlalchemy.telemetry.queryband import get_qb_query
-        >>> set_qb_query = get_qb_query()
-    """
-    return session_queryband.generate_set_queryband_query()
-
-
-def log(*args):
-    """
-    DESCRIPTION:
-        Prints error message on console if configuration option is enabled.
-
-    PARAMETERS:
-        Variable number of arguments each of string type.
-
-    RETURNS:
-        None
-
-    EXAMPLES:
-        >>> from teradatasqlalchemy.telemetry.queryband import log
-        >>> err_str = "SOME_ERR_IN_QUERYBAND"
-        >>> log("Failed to collect queryband.", err_str)
-    """
-    try:
-        if session_queryband.verbose:
-            print(*args)
-    except Exception as log_err:
-        if session_queryband.verbose:
-            print("Failed to log error in queryband:", log_err)
+from functools import wraps
+import inspect
+from teradatasqlalchemy import vernumber
+import re
+
+
+class _QueryBand:
+    """
+    Class to hold the common attributes required for queryband enabling.
+    """
+    def __init__(self):
+        self._qb_buffer = []
+        self._org = "TERADATA-INTERNAL-TELEM"
+        self._app_name = "TDSQLMY"
+        self._app_version = vernumber.sVersionNumber
+        self._qb_template = "QUERY_BAND='ORG={org};APPNAME={app_name};APPVERSION={app_version};{client_qb};'"
+        self._set_qb_query_template = "SET {query_band} FOR TRANSACTION;"
+        self._prev_qb_str = None
+        self._prev_qb_str_freq = 0
+        self._qb_regex = r'^[a-zA-Z0-9_-]+$'
+        self._verbose = False
+
+    @property
+    def qb_buffer(self):
+        """
+        RETURNS:
+            Queryband string.
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.qb_buffer
+        """
+        return self._qb_buffer
+
+    @qb_buffer.setter
+    def qb_buffer(self, query_band):
+        """
+        Creates query band buffer if it doesn't exist else appends query band
+        to existing query band buffer self._qb_buffer.
+
+        PARAMETERS:
+            query_band
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.qb_buffer('ORG=TERADATA-INTERNAL-TELEM')
+        """
+        if not self._qb_buffer:
+            self._qb_buffer = []
+        self._qb_buffer.append(query_band)
+
+    @property
+    def qb_regex(self):
+        """
+        RETURNS:
+            Regular expression which validates queryband string.
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.qb_regex
+        """
+        return self._qb_regex
+
+    @property
+    def verbose(self):
+        """
+        RETURNS:
+            Configuration option which decides whether to print
+            error logs on console or not.
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.verbose
+        """
+        return self._verbose
+
+    @verbose.setter
+    def verbose(self, verbose):
+        """
+        Sets configuration option which decides whether to print
+        error logs on console or not.
+
+        PARAMETERS:
+            verbose
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.verbose(True)
+        """
+        self._verbose = verbose
+
+    def append_qb(self, query_band):
+        """
+        Creates query band buffer if it doesn't exist else appends query band
+        to existing query band buffer.
+
+        PARAMETERS:
+            query_band
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.append_qb('ORG=TERADATA-INTERNAL-TELEM')
+
+        """
+        # When new buffer is created.
+        if not self._prev_qb_str:
+            self._prev_qb_str = query_band
+            self._prev_qb_str_freq = 1
+            return
+
+        # check if queryband string is repeated.
+        # If not, append previous queryband to buffer and update _prev_qb_str
+        # and _prev_qb_str_freq else just increase the frequency of previous queryband.
+        if query_band != self._prev_qb_str:
+            # If _prev_qb_str is having frequency more than 1, then append queryband
+            # string with frequency, else append without frequency.
+            self.qb_buffer.append(self._prev_qb_str + "_" + str(self._prev_qb_str_freq)
+                                  if self._prev_qb_str_freq > 1 else self._prev_qb_str)
+            self._prev_qb_str = query_band
+            self._prev_qb_str_freq = 1
+        else:
+            self._prev_qb_str_freq = self._prev_qb_str_freq + 1
+
+    def pop_qb(self):
+        """
+        Removes last added queryband from query band buffer list.
+
+        PARAMETERS:
+            None
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.append_qb('ORG=TERADATA-INTERNAL-TELEM')
+            >>> qb.pop_qb()
+
+        """
+        try:
+            del self._qb_buffer[-1]
+        except IndexError:
+            self._qb_buffer = []
+
+    def reset_qb(self):
+        """
+        Removes all querybands from query band buffer list.
+
+        PARAMETERS:
+            None
+
+        EXAMPLES:
+            >>> qb = _QueryBand()
+            >>> qb.append_qb('ORG=TERADATA-INTERNAL-TELEM')
+            >>> qb.reset_qb()
+        """
+        self._prev_qb_str = None
+        self._prev_qb_str_freq = 0
+        self._qb_buffer = []
+
+    def configure_queryband_parameters(self, app_name, app_version):
+        """
+        DESCRIPTION:
+            Configures application name and application version which
+            uses queryband utility.
+
+        PARAMETERS:
+            app_name:
+                Required Argument:
+                Specifies name of the application which uses queryband utility.
+                Types: str
+
+            app_version:
+                Required Argument:
+                Specifies version of the application which uses queryband utility.
+                Types: str
+
+        RETURNS:
+            None
+
+        RAISES:
+            None.
+
+        EXAMPLES:
+            >>> session_qb = _QueryBand()
+            >>> session_qb.configure_queryband_parameters(app_name="TDML", app_version="20.00.00.00")
+        """
+        self._app_name = app_name
+        self._app_version = app_version
+
+    def generate_set_queryband_query(self):
+        """
+        DESCRIPTION:
+            Generates a SQL query to be used while setting transaction level
+            queryband for an application. Application specific data and querybands
+            collected during execution of application's APIs are used while genearting
+            final queryband string. Finally, cleans queryband buffer to start with
+            new workflow.
+
+        PARAMETERS:
+            None
+
+        RETURNS:
+            str
+
+        RAISES:
+            None.
+
+        EXAMPLES:
+            >>> session_qb = _QueryBand()
+            >>> session_qb.generate_set_queryband_query()
+        """
+
+        try:
+            # Before utilizing buffer, append lazy entries
+            # in _prev_qb_str to _qb_buffer.
+            if self._prev_qb_str:
+                # If _prev_qb_str is having frequency more than 1, then append queryband
+                # string with frequency, else append without frequency.
+                self.qb_buffer.append(self._prev_qb_str + "_" + str(self._prev_qb_str_freq)
+                                      if self._prev_qb_str_freq > 1 else self._prev_qb_str)
+            return self._set_qb_query_template.format(
+                query_band=self._qb_template.format(org=self._org,
+                                                    app_name=self._app_name,
+                                                    app_version=self._app_version,
+                                                    client_qb="APPFUNC={}".format("-".join(self._qb_buffer))))
+        except Exception as append_err:
+            log("Failed to generate SET QB query: ", append_err)
+        finally:
+            self.reset_qb()
+
+
+session_queryband = _QueryBand()
+
+
+def collect_queryband(queryband=None, attr=None, method=None,
+                      arg_name=None, prefix=None, suffix=None):
+    """
+    DESCRIPTION:
+        Decorator for collecting queryband string in queryband buffer.
+
+    PARAMETERS:
+        queryband:
+            Optional Argument:
+            Specifies queryband string.
+            Types: str
+
+        attr:
+            Optional Argument:
+            Specifies name of a class attribute whose value is to be used as
+            queryband string.
+            Types: str
+
+        method:
+            Optional Argument:
+            Specifies name of a class method which returns string to be used as
+            queryband string.
+            Note:
+                This method of class is expected to be a no-arg utility method and
+                should return an expected queryband string for some processing done
+                by a class/class method which needs to be tracked by queryband.
+            Types: str
+
+        arg_name:
+            Optional Argument:
+            Specifies name of an argument of a decorated function/method, whose value
+            is to be used as queryband string.
+            Types: str
+
+        prefix:
+            Optional Argument:
+            Specifies prefix to be applied to queryband string.
+            Types: str
+
+        suffix:
+            Optional Argument:
+            Specifies suffix to be applied to queryband string.
+            Types: str
+
+    EXAMPLES:
+        >>> from teradatasqlalchemy.telemetry import collect_queryband
+        # Example 1: Collect queryband for a standalone function.
+        @collect_queryband(queryband="CreateContext")
+        def create_context(host = None, username ...): ...
+
+        # Example 2: Collect queryband for a class method and use
+        #            class attribute to retrive queryband string.
+        @collect_queryband(attr="func_name")
+        def _execute_query(self, persist=False, volatile=False):...
+
+        # Example 3: Collect queryband for a class method and use
+        #            method of same class to retrive queryband string.
+        @collect_queryband(method="get_class_specific_queryband")
+        def _execute_query(self, persist=False, volatile=False):...
+    """
+    def qb_decorator(exposed_func):
+        # This is needed to preserve the docstring of decorated function.
+        @wraps(exposed_func)
+        def wrapper(*args, **kwargs):
+            qb_str = queryband
+            # If queryband string is not provided by client while calling decorator,
+            # it can be devised using following ways.
+            if not qb_str:
+                # Approach 1:
+                # Extract queryband from value of argument passed
+                # to decorated function/method.
+                if arg_name:
+                    # Extract value from Keyword arguments.
+                    if arg_name in kwargs:
+                        qb_str = kwargs[arg_name]
+
+                    # Extract value from positional arguments.
+                    # Also consider default values.
+                    else:
+                        # Generate a dictionary containing mapping between
+                        # argument names and their run time values.
+                        signature = inspect.signature(exposed_func)
+                        bound_args = signature.bind(*args, **kwargs)
+                        bound_args.apply_defaults()
+
+                        qb_str = bound_args.arguments[arg_name]
+
+                # Approach 2:
+                # Extract queryband from an attribute/method associated
+                # with class object.
+                is_instance_method = args and ('.' in exposed_func.__qualname__)
+                if is_instance_method:
+                    try:
+                        if attr:
+                            qb_str = getattr(args[0], attr)
+                        elif method:
+                            qb_str = getattr(args[0], method)()
+                    except Exception as stat_method_err:
+                        log("Failed to collect queryband for static class method.", stat_method_err)
+                        return exposed_func(*args, **kwargs)
+                else:
+                    log("Failed to collect queryband for standalone function.")
+                    return exposed_func(*args, **kwargs)
+
+            if qb_str:
+                # Validate queryband for string type.
+                if not isinstance(qb_str, str):
+                    log("Failed to collect queryband. Queryband must be of type str not {}".format(type(qb_str)))
+                    return exposed_func(*args, **kwargs)
+
+                # Process suffix and prefix.
+                if suffix and isinstance(suffix, str):
+                    qb_str = qb_str + "_" + suffix
+                if prefix and isinstance(prefix, str):
+                    qb_str = prefix + "_" + qb_str
+
+                # Validate queryband for allowed characters.
+                if not re.match(session_queryband.qb_regex, qb_str):
+                    log("Failed to collect queryband. Queryband string: '{}' contains invalid characters. Allowed characters are [a-z, A-Z, 0-9, '_', '-']".format(qb_str))
+                    return exposed_func(*args, **kwargs)
+
+                # Append queryband to buffer.
+                session_queryband.append_qb(qb_str)
+
+            return exposed_func(*args, **kwargs)
+
+        return wrapper
+    return qb_decorator
+
+
+def set_queryband(con_obj):
+    """
+    DESCRIPTION:
+        Decorator for executing set queryband SQL request using connection object from application
+        and then clearing queryband buffer for next workflow.
+
+    PARAMETERS:
+        con_obj:
+            Required Argument:
+            Specifies connection object to execute string.
+            Types: Sqlalchemy connection
+
+
+    EXAMPLES:
+        Setting queryband before execution of application's SQL request.
+        >>> from teradatasqlalchemy.telemetry import set_queryband
+        @set_queryband(con_obj=get_connection())
+        def _execute_ddl_statement(ddl_statement):...
+    """
+    def qb_decorator(execute_func):
+        def wrapper(*args, **kwargs):
+            # Execute set queryband SQL request.
+            try:
+                con_obj.exec_driver_sql(session_queryband.generate_set_queryband_query())
+            except Exception as qb_err:
+                log("Failed to set QB!!!", qb_err)
+            # Execute application's SQL request and after successful execution
+            # clean queryband buffer.
+            try:
+                ret_val = execute_func(*args, **kwargs)
+            except Exception as exec_err:
+                raise
+            else:
+                session_queryband.reset_qb()
+            return ret_val
+        return wrapper
+    return qb_decorator
+
+
+def get_qb_query():
+    """
+    DESCRIPTION:
+        Returns a SET queryband SQL query to be used while setting transaction level
+        queryband for an application using _QueryBand object.
+
+    PARAMETERS:
+        None
+
+    RETURNS:
+        str
+
+    RAISES:
+        None.
+
+    EXAMPLES:
+        >>> from teradatasqlalchemy.telemetry.queryband import get_qb_query
+        >>> set_qb_query = get_qb_query()
+    """
+    return session_queryband.generate_set_queryband_query()
+
+
+def log(*args):
+    """
+    DESCRIPTION:
+        Prints error message on console if configuration option is enabled.
+
+    PARAMETERS:
+        Variable number of arguments each of string type.
+
+    RETURNS:
+        None
+
+    EXAMPLES:
+        >>> from teradatasqlalchemy.telemetry.queryband import log
+        >>> err_str = "SOME_ERR_IN_QUERYBAND"
+        >>> log("Failed to collect queryband.", err_str)
+    """
+    try:
+        if session_queryband.verbose:
+            print(*args)
+    except Exception as log_err:
+        if session_queryband.verbose:
+            print("Failed to log error in queryband:", log_err)
```

## Comparing `teradatasqlalchemy-20.0.0.0.data/data/teradatasqlalchemy/LICENSE` & `teradatasqlalchemy-20.0.0.1.data/data/teradatasqlalchemy/LICENSE`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-Teradata SQL Driver Dialect for SQLAlchemy
-Copyright 2023 Teradata.  All rights reserved.
-
-LICENSE AGREEMENT
-
-PRODUCT: Teradata SQL Driver Dialect for SQLAlchemy
-
-IMPORTANT - READ THIS AGREEMENT CAREFULLY BEFORE INSTALLING OR USING THE SOFTWARE.
-TERADATA WILL LICENSE THE SOFTWARE TO YOU ONLY IF YOU ACCEPT THE TERMS AND CONDITIONS OF
-THIS AGREEMENT AND MEET THE CONDITIONS FOR USING THE SOFTWARE DESCRIBED BELOW.  BY
-DOWNLOADING, INSTALLING OR USING THE SOFTWARE, YOU (1) AGREE TO THE TERMS AND CONDITIONS
-OF THIS AGREEMENT, AND (2) REPRESENT AND WARRANT THAT YOU POSSESS THE AUTHORITY TO ENTER
-INTO THIS AGREEMENT ON BEHALF OF YOU, YOUR EMPLOYER (WHEN ACTING ON BEHALF OF YOUR EMPLOYER),
-AND/OR A TERADATA-AUTHORIZED LICENSEE (WHEN YOU AND YOUR EMPLOYER ARE ACTING ON BEHALF OF A
-TERADATA-AUTHORIZED LICENSEE).  IF YOU DO NOT ACCEPT THE TERMS AND CONDITIONS OF THIS
-AGREEMENT, DO NOT DOWNLOAD, INSTALL OR USE THE SOFTWARE.
-
-IMPORTANT - BY DOWNLOADING THE SOFTWARE:
-* YOU ACKNOWLEDGE THAT THE SOFTWARE YOU ARE DOWNLOADING FROM TERADATA IS SUBJECT
-  TO THE RESTRICTIONS AND CONTROLS IMPOSED BY UNITED STATES EXPORT REGULATIONS.
-* YOU CERTIFY THAT:
-** YOU DO NOT INTEND TO USE THE SOFTWARE FOR ANY PURPOSE PROHIBITED BY UNITED STATES EXPORT
-   REGULATIONS, INCLUDING, WITHOUT LIMITATION, TERRORISM, CYBER-ATTACKS, CYBER-CRIMES,
-   MONEY-LAUNDERING, INDUSTRIAL ESPIONAGE, OR NUCLEAR, CHEMICAL OR BIOLOGICAL WEAPONS
-   PROLIFERATION.
-** YOU ARE NOT LISTED AS A DENIED PARTY ON ANY LIST GOVERNING UNITED STATES EXPORTS.
-** YOU ARE NOT A NATIONAL OF ANY COUNTRY THAT IS NOT APPROVED FOR EXPORT OF THE SOFTWARE.
-   AS OF 2017, THESE COUNTRIES ARE CUBA, IRAN, NORTH KOREA, AND SYRIA.
-
-This License Agreement ("Agreement") is a legal contract between you (as defined below) and
-Teradata (as defined below) regarding the Software (as defined below).  The terms "you",
-"your" and "yours" collectively and individually refer to you as an individual and to any
-company for which you are acting.  The term "Teradata" refers to either Teradata U.S., Inc.
-for Software deliveries in the US or Teradata Ireland Ltd. for Software deliveries outside
-the United States.  "Software" refers to the software product identified above, which
-consists of computer software code in object code form only, as well as associated
-documentation that Teradata may elect in its sole discretion to provide you.  "Software"
-also includes any and all error corrections, bug fixes, updates, upgrades, or new versions
-or releases of the Software (collectively and individually, "Enhancements") that Teradata
-may elect in its sole discretion to provide you.
-
-1.  Term.  This Agreement commences on the earliest date of the first download, first
-    copying, first installation, or first use of the Software (the "Effective Date").
-    Unless terminated earlier as provided herein, this agreement, including your license
-    to the Software, will expire or terminate on the same date that your Teradata-authorized
-    license to use the Teradata Analytics Platform product expires or terminates (whichever
-    occurs first).
-
-2.  License.
-
-(a) Subject to your compliance with all of the terms and conditions of this Agreement
-    and only during the term of this Agreement, Teradata grants you a nonexclusive,
-    nontransferable, paid up license to install and use the Software on your computer
-    solely for purposes of facilitating your Teradata-authorized license to use the
-    Teradata Analytics Platform.  You may make reasonable archival backup copies of the
-    Software, but may only use an archival copy in lieu of your primary copy and subject
-    to the same restrictions as your primary copy.
-
-(b) The term Third Party Software means computer programs or modules (including their
-    documentation) that bear the logo, copyright and/or trademark of a third party
-    (including open source software that are contained in files marked as "open source"
-    or the like) or are otherwise subject to the written license terms of a third party.
-    The term "Software" does not include Third Party Software.  Third Party Software is
-    licensed to you subject to the applicable license terms accompanying it, included
-    in/with it, referenced in it, or otherwise entered into by you with respect to it.
-    Third Party Software license terms include those found in the FOSS licensing zip file
-    accompanying the Software.  Teradata provides source code to certain Third Party
-    Software for certain periods of time in compliance with certain applicable licenses.
-    To request such source code, visit http://developer.teradata.com/download/license/oss-request.
-
-(c) You will not sell, copy, rent, loan, modify, transfer, disclose, embed, sublicense,
-    create derivative works of or distribute the Software, in whole or in part, without
-    Teradata's prior written consent.  You are granted no rights to obtain or use the
-    Software's source code.  You will not reverse-assemble, reverse compile or reverse-
-    engineer the Software, except as expressly permitted by applicable law without the
-    possibility of contractual waiver.  Notwithstanding anything to the contrary, you do
-    not have any license, right, or authority to subject the Software, in whole or in part
-    or as part of a larger work, to any terms of any other agreement, including GNU Public
-    Licenses.
-
-(d) No license rights to the Software will be implied.  The Software, which includes all
-    copies thereof (whether in whole or in part), is and remains the exclusive property of
-    Teradata.  You will ensure that all copies of the Software contain Teradata's copyright
-    notices, as well as all other proprietary legends.  Teradata reserves the right to
-    inspect your use of the Software for purposes of verifying your compliance with the
-    terms and conditions of this Agreement.
-
-3.  Responsibilities.  You are responsible for the installation of the Software, as
-    well as for providing data security and backup operations.  This Agreement does not
-    require Teradata to provide you with any Enhancements, consulting services, technical
-    assistance, installation, training, support, or maintenance of any kind (collectively
-    and individually, "Services").  To the extent that Teradata elects to provide you
-    with any Services, such Services are provided to you at Teradata's sole discretion
-    and may be modified or discontinued at any time for any reason.
-
-4.  DISCLAIMER OF WARRANTY.  TERADATA: (a) PROVIDES SERVICES (IF ANY), (b) LICENSES THE
-    SOFTWARE, AND (c) PROVIDES THIRD PARTY SOFTWARE TO YOU ON AN "AS-IS" BASIS WITHOUT
-    WARRANTIES OF ANY KIND (ORAL OR WRITTEN, EXPRESS OR IMPLIED, OR STATUTORY). WITHOUT
-    LIMITATION TO THE FOREGOING, THERE ARE NO IMPLIED WARRANTIES OF MERCHANTABILITY,
-    FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT.  TERADATA DOES NOT WARRANT
-    THAT THE SOFTWARE, THIRD PARTY SOFTWARE, OR SERVICES WILL MEET YOUR REQUIREMENTS
-    OR CONFORM TO ANY SPECIFICATIONS, OR THAT THE OPERATION OF THE SOFTWARE OR THIRD
-    PARTY SOFTWARE WILL BE UNINTERRUPTED OR ERROR FREE.  YOU BEAR THE ENTIRE RISK AS
-    TO SATISFACTORY QUALITY, PERFORMANCE, ACCURACY, AND RESULTS OBTAINED FROM THE
-    SOFTWARE, THIRD PARTY SOFTWARE, AND SERVICES.
-
-    SOME JURISDICTIONS RESTRICT DISCLAIMERS OF WARRANTY, SO THE ABOVE DISCLAIMERS MAY
-    NOT FULLY APPLY TO YOU.
-
-5.  LIMITATIONS ON LIABILITY: UNDER NO CIRCUMSTANCES WILL TERADATA'S AND ITS LICENSORS'
-    TOTAL CUMULATIVE LIABILITY FOR CLAIMS RELATING TO THIS AGREEMENT, THE SERVICES,
-    THE SOFTWARE, AND/OR THIRD PARTY SOFTWARE (WHETHER BASED IN CONTRACT, STATUTE,
-    TORT (INCLUDING NEGLIGENCE) OR OTHERWISE) EXCEED US$1,000; PROVIDED, HOWEVER,
-    THAT THE FOREGOING WILL NOT APPLY TO CLAIMS FOR (A) PERSONAL INJURY, INCLUDING
-    DEATH, TO THE EXTENT CAUSED BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT; OR
-    (B) PHYSICAL DAMAGE TO TANGIBLE REAL OR PERSONAL PROPERTY TO THE EXTENT CAUSED
-    BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT EQUAL TO THE AMOUNT OF DIRECT
-    DAMAGES UP TO ONE MILLION DOLLARS PER OCCURRENCE.  IN NO EVENT WILL TERADATA
-    OR ITS LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, PUNITIVE, INCIDENTAL OR
-    CONSEQUENTIAL DAMAGES, OR FOR LOSS OF PROFITS, REVENUE, TIME, OPPORTUNITY OR
-    DATA, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-    SOME JURISDICTIONS RESTRICT LIMITATIONS OF LIABILITY, SO THE ABOVE LIMITATIONS MAY
-    NOT FULLY APPLY TO YOU.
-
-6.  Government Restrictions. You agree that you will not, directly or indirectly, export
-    or transmit any Software without obtaining Teradata's prior written authorization,
-    as well as appropriate governmental approvals, including those required by the
-    U.S. Government. Use and or distribution of this software is subject to export laws
-    and regulations of the United States and other jurisdictions. The links below connect
-    you to applicable U.S. government agencies, and their regulations, that have
-    jurisdiction over this transaction.
-    http://www.bis.doc.gov/
-    http://www.treas.gov/offices/enforcement/ofac/
-
-    In downloading this product, you acknowledge that this transaction is subject to
-    applicable export control laws and you certify that your download, use and/or subsequent
-    distribution of this product is not prohibited under applicable laws and regulations.
-
-    The Government's use, duplication, or disclosure of Teradata's commercial computer
-    software and commercial computer software documentation is subject to: (a) the
-    Restricted Rights Notice set forth in 48 C.F.R. � 52.227-14 (Rights In Data - General);
-    (b) Teradata's standard commercial license rights supplemented by 48 C.F.R. � 52.227-19
-    (Commercial Computer Software - Restricted Rights); and/or (c) the limited rights and
-    license set forth 48 CFR � 252.227-7015 (Technical Data�Commercial Items), as applicable.
-
-7.  Termination and Expiration. A party may terminate this Agreement with or without
-    cause, upon providing written notice to the other parties.  When this Agreement
-    terminates or expires, you will immediately cease all use of the Software, permanently
-    remove the Software from all computers, destroy all copies of the Software, and (upon
-    receipt of Teradata's request) provide a signed written certification that the
-    foregoing has occurred. Sections 4, 5, 6, 7, 8, 9, 10, and 11 will survive expiration
-    or termination of this Agreement.
-
-8.  Choice of Law and Dispute Resolution. The parties will attempt in good faith to resolve
-    any controversy or claim by negotiation or mediation.  If they are unable to do so, and
-    regardless of the causes of action alleged and whether they arise under this Agreement
-    or otherwise, the claim will be resolved by arbitration before a sole arbitrator in
-    Dayton, Ohio pursuant to the then-current Commercial Rules of the American Arbitration
-    Association and the federal substantive and procedural law of arbitration.  The
-    arbitrator's award will be final and binding, and may be entered in any court having
-    jurisdiction thereof, but may include only damages consistent with the limitations in
-    this Agreement.  Each party will bear its own attorney's fees and costs related to the
-    arbitration. The obligations to negotiate, mediate and arbitrate shall not apply to
-    claims for misuse or infringement of a party's intellectual property rights.  Any
-    claim or action must be brought within two years after the claimant knows or should
-    have known of the claim.  New York law will govern the interpretation and enforcement
-    of this Agreement, except that the Federal Arbitration Act will govern the
-    interpretation and enforcement of the arbitrability of claims under this Section.
-
-9.  Feedback.  Notwithstanding anything to the contrary: (a) Teradata will have no
-    obligation of any kind with respect to any Software-related comments, suggestions,
-    design changes or improvements that you elect to provide to Teradata in either verbal
-    or written form (collectively, "Software Feedback"), and (b) Teradata and its
-    affiliates are hereby free to use any ideas, concepts, know-how or techniques, in
-    whole or in part, contained in Software Feedback: (i) for any purpose whatsoever,
-    including developing, manufacturing, and/or marketing products and/or services
-    incorporating Software Feedback in whole or in part, and (ii) without any
-    restrictions or limitations, including requiring the payment of any license fees,
-    royalties, or other consideration.
-
-10. Confidentiality.  You will not disclose the results of any testing or evaluations,
-    including any benchmarks, insofar as it relates to the Software without Teradata's
-    prior written consent.
-
-11. Entire Agreement. This Agreement constitutes the entire understanding of the parties
-    with respect to the Software and Services, and supersede all other prior agreements
-    and understandings whether oral or written.  No oral representation or change to this
-    Agreement will be binding upon either party unless agreed to in writing and signed by
-    authorized representatives of all parties. You will not assign this Agreement or your
-    rights, nor will you delegate your obligations under this Agreement.  Failure by
-    either party to enforce any term or condition of this Agreement will not be deemed
-    a waiver of future enforcement of that or any other term or condition. The provisions
-    of this Agreement are severable. "Include", "includes", and "including" shall be
-    interpreted as introducing a list of examples which do not limit the generality of
-    any preceding words or any words in the list of examples.
+Teradata SQL Driver Dialect for SQLAlchemy
+Copyright 2023 Teradata.  All rights reserved.
+
+LICENSE AGREEMENT
+
+PRODUCT: Teradata SQL Driver Dialect for SQLAlchemy
+
+IMPORTANT - READ THIS AGREEMENT CAREFULLY BEFORE INSTALLING OR USING THE SOFTWARE.
+TERADATA WILL LICENSE THE SOFTWARE TO YOU ONLY IF YOU ACCEPT THE TERMS AND CONDITIONS OF
+THIS AGREEMENT AND MEET THE CONDITIONS FOR USING THE SOFTWARE DESCRIBED BELOW.  BY
+DOWNLOADING, INSTALLING OR USING THE SOFTWARE, YOU (1) AGREE TO THE TERMS AND CONDITIONS
+OF THIS AGREEMENT, AND (2) REPRESENT AND WARRANT THAT YOU POSSESS THE AUTHORITY TO ENTER
+INTO THIS AGREEMENT ON BEHALF OF YOU, YOUR EMPLOYER (WHEN ACTING ON BEHALF OF YOUR EMPLOYER),
+AND/OR A TERADATA-AUTHORIZED LICENSEE (WHEN YOU AND YOUR EMPLOYER ARE ACTING ON BEHALF OF A
+TERADATA-AUTHORIZED LICENSEE).  IF YOU DO NOT ACCEPT THE TERMS AND CONDITIONS OF THIS
+AGREEMENT, DO NOT DOWNLOAD, INSTALL OR USE THE SOFTWARE.
+
+IMPORTANT - BY DOWNLOADING THE SOFTWARE:
+* YOU ACKNOWLEDGE THAT THE SOFTWARE YOU ARE DOWNLOADING FROM TERADATA IS SUBJECT
+  TO THE RESTRICTIONS AND CONTROLS IMPOSED BY UNITED STATES EXPORT REGULATIONS.
+* YOU CERTIFY THAT:
+** YOU DO NOT INTEND TO USE THE SOFTWARE FOR ANY PURPOSE PROHIBITED BY UNITED STATES EXPORT
+   REGULATIONS, INCLUDING, WITHOUT LIMITATION, TERRORISM, CYBER-ATTACKS, CYBER-CRIMES,
+   MONEY-LAUNDERING, INDUSTRIAL ESPIONAGE, OR NUCLEAR, CHEMICAL OR BIOLOGICAL WEAPONS
+   PROLIFERATION.
+** YOU ARE NOT LISTED AS A DENIED PARTY ON ANY LIST GOVERNING UNITED STATES EXPORTS.
+** YOU ARE NOT A NATIONAL OF ANY COUNTRY THAT IS NOT APPROVED FOR EXPORT OF THE SOFTWARE.
+   AS OF 2017, THESE COUNTRIES ARE CUBA, IRAN, NORTH KOREA, AND SYRIA.
+
+This License Agreement ("Agreement") is a legal contract between you (as defined below) and
+Teradata (as defined below) regarding the Software (as defined below).  The terms "you",
+"your" and "yours" collectively and individually refer to you as an individual and to any
+company for which you are acting.  The term "Teradata" refers to either Teradata U.S., Inc.
+for Software deliveries in the US or Teradata Ireland Ltd. for Software deliveries outside
+the United States.  "Software" refers to the software product identified above, which
+consists of computer software code in object code form only, as well as associated
+documentation that Teradata may elect in its sole discretion to provide you.  "Software"
+also includes any and all error corrections, bug fixes, updates, upgrades, or new versions
+or releases of the Software (collectively and individually, "Enhancements") that Teradata
+may elect in its sole discretion to provide you.
+
+1.  Term.  This Agreement commences on the earliest date of the first download, first
+    copying, first installation, or first use of the Software (the "Effective Date").
+    Unless terminated earlier as provided herein, this agreement, including your license
+    to the Software, will expire or terminate on the same date that your Teradata-authorized
+    license to use the Teradata Analytics Platform product expires or terminates (whichever
+    occurs first).
+
+2.  License.
+
+(a) Subject to your compliance with all of the terms and conditions of this Agreement
+    and only during the term of this Agreement, Teradata grants you a nonexclusive,
+    nontransferable, paid up license to install and use the Software on your computer
+    solely for purposes of facilitating your Teradata-authorized license to use the
+    Teradata Analytics Platform.  You may make reasonable archival backup copies of the
+    Software, but may only use an archival copy in lieu of your primary copy and subject
+    to the same restrictions as your primary copy.
+
+(b) The term Third Party Software means computer programs or modules (including their
+    documentation) that bear the logo, copyright and/or trademark of a third party
+    (including open source software that are contained in files marked as "open source"
+    or the like) or are otherwise subject to the written license terms of a third party.
+    The term "Software" does not include Third Party Software.  Third Party Software is
+    licensed to you subject to the applicable license terms accompanying it, included
+    in/with it, referenced in it, or otherwise entered into by you with respect to it.
+    Third Party Software license terms include those found in the FOSS licensing zip file
+    accompanying the Software.  Teradata provides source code to certain Third Party
+    Software for certain periods of time in compliance with certain applicable licenses.
+    To request such source code, visit http://developer.teradata.com/download/license/oss-request.
+
+(c) You will not sell, copy, rent, loan, modify, transfer, disclose, embed, sublicense,
+    create derivative works of or distribute the Software, in whole or in part, without
+    Teradata's prior written consent.  You are granted no rights to obtain or use the
+    Software's source code.  You will not reverse-assemble, reverse compile or reverse-
+    engineer the Software, except as expressly permitted by applicable law without the
+    possibility of contractual waiver.  Notwithstanding anything to the contrary, you do
+    not have any license, right, or authority to subject the Software, in whole or in part
+    or as part of a larger work, to any terms of any other agreement, including GNU Public
+    Licenses.
+
+(d) No license rights to the Software will be implied.  The Software, which includes all
+    copies thereof (whether in whole or in part), is and remains the exclusive property of
+    Teradata.  You will ensure that all copies of the Software contain Teradata's copyright
+    notices, as well as all other proprietary legends.  Teradata reserves the right to
+    inspect your use of the Software for purposes of verifying your compliance with the
+    terms and conditions of this Agreement.
+
+3.  Responsibilities.  You are responsible for the installation of the Software, as
+    well as for providing data security and backup operations.  This Agreement does not
+    require Teradata to provide you with any Enhancements, consulting services, technical
+    assistance, installation, training, support, or maintenance of any kind (collectively
+    and individually, "Services").  To the extent that Teradata elects to provide you
+    with any Services, such Services are provided to you at Teradata's sole discretion
+    and may be modified or discontinued at any time for any reason.
+
+4.  DISCLAIMER OF WARRANTY.  TERADATA: (a) PROVIDES SERVICES (IF ANY), (b) LICENSES THE
+    SOFTWARE, AND (c) PROVIDES THIRD PARTY SOFTWARE TO YOU ON AN "AS-IS" BASIS WITHOUT
+    WARRANTIES OF ANY KIND (ORAL OR WRITTEN, EXPRESS OR IMPLIED, OR STATUTORY). WITHOUT
+    LIMITATION TO THE FOREGOING, THERE ARE NO IMPLIED WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT.  TERADATA DOES NOT WARRANT
+    THAT THE SOFTWARE, THIRD PARTY SOFTWARE, OR SERVICES WILL MEET YOUR REQUIREMENTS
+    OR CONFORM TO ANY SPECIFICATIONS, OR THAT THE OPERATION OF THE SOFTWARE OR THIRD
+    PARTY SOFTWARE WILL BE UNINTERRUPTED OR ERROR FREE.  YOU BEAR THE ENTIRE RISK AS
+    TO SATISFACTORY QUALITY, PERFORMANCE, ACCURACY, AND RESULTS OBTAINED FROM THE
+    SOFTWARE, THIRD PARTY SOFTWARE, AND SERVICES.
+
+    SOME JURISDICTIONS RESTRICT DISCLAIMERS OF WARRANTY, SO THE ABOVE DISCLAIMERS MAY
+    NOT FULLY APPLY TO YOU.
+
+5.  LIMITATIONS ON LIABILITY: UNDER NO CIRCUMSTANCES WILL TERADATA'S AND ITS LICENSORS'
+    TOTAL CUMULATIVE LIABILITY FOR CLAIMS RELATING TO THIS AGREEMENT, THE SERVICES,
+    THE SOFTWARE, AND/OR THIRD PARTY SOFTWARE (WHETHER BASED IN CONTRACT, STATUTE,
+    TORT (INCLUDING NEGLIGENCE) OR OTHERWISE) EXCEED US$1,000; PROVIDED, HOWEVER,
+    THAT THE FOREGOING WILL NOT APPLY TO CLAIMS FOR (A) PERSONAL INJURY, INCLUDING
+    DEATH, TO THE EXTENT CAUSED BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT; OR
+    (B) PHYSICAL DAMAGE TO TANGIBLE REAL OR PERSONAL PROPERTY TO THE EXTENT CAUSED
+    BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT EQUAL TO THE AMOUNT OF DIRECT
+    DAMAGES UP TO ONE MILLION DOLLARS PER OCCURRENCE.  IN NO EVENT WILL TERADATA
+    OR ITS LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, PUNITIVE, INCIDENTAL OR
+    CONSEQUENTIAL DAMAGES, OR FOR LOSS OF PROFITS, REVENUE, TIME, OPPORTUNITY OR
+    DATA, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+    SOME JURISDICTIONS RESTRICT LIMITATIONS OF LIABILITY, SO THE ABOVE LIMITATIONS MAY
+    NOT FULLY APPLY TO YOU.
+
+6.  Government Restrictions. You agree that you will not, directly or indirectly, export
+    or transmit any Software without obtaining Teradata's prior written authorization,
+    as well as appropriate governmental approvals, including those required by the
+    U.S. Government. Use and or distribution of this software is subject to export laws
+    and regulations of the United States and other jurisdictions. The links below connect
+    you to applicable U.S. government agencies, and their regulations, that have
+    jurisdiction over this transaction.
+    http://www.bis.doc.gov/
+    http://www.treas.gov/offices/enforcement/ofac/
+
+    In downloading this product, you acknowledge that this transaction is subject to
+    applicable export control laws and you certify that your download, use and/or subsequent
+    distribution of this product is not prohibited under applicable laws and regulations.
+
+    The Government's use, duplication, or disclosure of Teradata's commercial computer
+    software and commercial computer software documentation is subject to: (a) the
+    Restricted Rights Notice set forth in 48 C.F.R. � 52.227-14 (Rights In Data - General);
+    (b) Teradata's standard commercial license rights supplemented by 48 C.F.R. � 52.227-19
+    (Commercial Computer Software - Restricted Rights); and/or (c) the limited rights and
+    license set forth 48 CFR � 252.227-7015 (Technical Data�Commercial Items), as applicable.
+
+7.  Termination and Expiration. A party may terminate this Agreement with or without
+    cause, upon providing written notice to the other parties.  When this Agreement
+    terminates or expires, you will immediately cease all use of the Software, permanently
+    remove the Software from all computers, destroy all copies of the Software, and (upon
+    receipt of Teradata's request) provide a signed written certification that the
+    foregoing has occurred. Sections 4, 5, 6, 7, 8, 9, 10, and 11 will survive expiration
+    or termination of this Agreement.
+
+8.  Choice of Law and Dispute Resolution. The parties will attempt in good faith to resolve
+    any controversy or claim by negotiation or mediation.  If they are unable to do so, and
+    regardless of the causes of action alleged and whether they arise under this Agreement
+    or otherwise, the claim will be resolved by arbitration before a sole arbitrator in
+    Dayton, Ohio pursuant to the then-current Commercial Rules of the American Arbitration
+    Association and the federal substantive and procedural law of arbitration.  The
+    arbitrator's award will be final and binding, and may be entered in any court having
+    jurisdiction thereof, but may include only damages consistent with the limitations in
+    this Agreement.  Each party will bear its own attorney's fees and costs related to the
+    arbitration. The obligations to negotiate, mediate and arbitrate shall not apply to
+    claims for misuse or infringement of a party's intellectual property rights.  Any
+    claim or action must be brought within two years after the claimant knows or should
+    have known of the claim.  New York law will govern the interpretation and enforcement
+    of this Agreement, except that the Federal Arbitration Act will govern the
+    interpretation and enforcement of the arbitrability of claims under this Section.
+
+9.  Feedback.  Notwithstanding anything to the contrary: (a) Teradata will have no
+    obligation of any kind with respect to any Software-related comments, suggestions,
+    design changes or improvements that you elect to provide to Teradata in either verbal
+    or written form (collectively, "Software Feedback"), and (b) Teradata and its
+    affiliates are hereby free to use any ideas, concepts, know-how or techniques, in
+    whole or in part, contained in Software Feedback: (i) for any purpose whatsoever,
+    including developing, manufacturing, and/or marketing products and/or services
+    incorporating Software Feedback in whole or in part, and (ii) without any
+    restrictions or limitations, including requiring the payment of any license fees,
+    royalties, or other consideration.
+
+10. Confidentiality.  You will not disclose the results of any testing or evaluations,
+    including any benchmarks, insofar as it relates to the Software without Teradata's
+    prior written consent.
+
+11. Entire Agreement. This Agreement constitutes the entire understanding of the parties
+    with respect to the Software and Services, and supersede all other prior agreements
+    and understandings whether oral or written.  No oral representation or change to this
+    Agreement will be binding upon either party unless agreed to in writing and signed by
+    authorized representatives of all parties. You will not assign this Agreement or your
+    rights, nor will you delegate your obligations under this Agreement.  Failure by
+    either party to enforce any term or condition of this Agreement will not be deemed
+    a waiver of future enforcement of that or any other term or condition. The provisions
+    of this Agreement are severable. "Include", "includes", and "including" shall be
+    interpreted as introducing a list of examples which do not limit the generality of
+    any preceding words or any words in the list of examples.
```

## Comparing `teradatasqlalchemy-20.0.0.0.data/data/teradatasqlalchemy/README.md` & `teradatasqlalchemy-20.0.0.1.data/data/teradatasqlalchemy/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,150 +1,153 @@
-## Teradata SQL Driver Dialect for SQLAlchemy
-
-This package enables [SQLAlchemy](https://pypi.org/project/SQLAlchemy/) to connect to the Teradata Database.
-
-This package requires 64-bit Python 3.4 or later, and runs on Windows, macOS, and Linux. 32-bit Python is not supported.
-
-For community support, please visit the [Teradata Community forums](https://community.teradata.com/).
-
-For Teradata customer support, please visit [Teradata Access](https://access.teradata.com/).
-
-Copyright 2024 Teradata. All Rights Reserved.
-
-### Table of Contents
-
-* [Release Notes](#release-notes)
-* [Installation](#Installation)
-* [License](#License)
-* [Documentation](#Documentation)
-* [Using the Teradata SQL Driver Dialect for SQLAlchemy](#Using)
-* [Connection Parameters](#ConnectionParameters)
-
-## Release Notes:
-
-### teradatasqlalchemy 20.0.0.0
-- Added a utility to enable querybands for function calls in python packages. Any python client package can import queryband package and can configure its name and version to track the vantage usage with help of logs available with DBQlogTbl.
-
-### teradatasqlalchemy 17.20.0.0
-- Added support for SQLAlchemy >= 2.0
-- `div` operator is no more supported.
-  
-### teradatasqlalchemy 17.0.0.5
-- Added xviews support for below APIs.:
-  - `get_columns`
-  - `get_foreign_keys`
-  - `get_pk_constraint`
-  - `get_schema_names`
-  - `get_transaction_mode`
-  - `get_unique_constraints`
-  
-### teradatasqlalchemy 17.0.0.4
-- Added ART Table support for `get_columns` API.
-
-### teradatasqlalchemy 17.0.0.3
-- Resolved a minor bug.
-  
-### teradatasqlalchemy 17.0.0.2
-- Added support for below Teradata's datatypes:
-  - JSON
-  - XML
-  - TDUDT  
-  - GEOMETRY
-  - MBR
-  - MBB
-- Optimised below APIs
-  - has_table
-  - has_view
-  - get_table_names
-  - get_view_names
-- Minor bug fixes.
-
-
-<a name="Installation"></a>
-
-### Installation
-
-Use pip to install the Teradata SQL Driver Dialect for SQLAlchemy.
-
-Platform       | Command
--------------- | ---
-macOS or Linux | `pip install teradatasqlalchemy`
-Windows        | `py -3 -m pip install teradatasqlalchemy`
-
-When upgrading to a new version of the Teradata SQL Driver Dialect for SQLAlchemy, you may need to use pip install's `--no-cache-dir` option to force the download of the new version.
-
-Platform       | Command
--------------- | ---
-macOS or Linux | `pip install --no-cache-dir -U teradatasqlalchemy`
-Windows        | `py -3 -m pip install --no-cache-dir -U teradatasqlalchemy`
-
-<a name="License"></a>
-
-### License
-
-Use of the Teradata SQL Driver Dialect for SQLAlchemy is governed by the *License Agreement for the Teradata SQL Driver Dialect for SQLAlchemy*.
-
-When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `LICENSE` file is placed in the `teradatasqlalchemy` directory under your Python installation directory.
-
-<a name="Documentation"></a>
-
-### Documentation
-
-When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `README.md` file is placed in the `teradatasqlalchemy` directory under your Python installation directory. This permits you to view the documentation offline, when you are not connected to the Internet.
-
-The `README.md` file is a plain text file containing the documentation for the Teradata SQL Driver Dialect for SQLAlchemy. While the file can be viewed with any text file viewer or editor, your viewing experience will be best with an editor that understands Markdown format.
-
-<a name="Using"></a>
-
-### Using the Teradata SQL Driver Dialect for SQLAlchemy
-
-Your Python script must import the `sqlalchemy` package in order to use the Teradata SQL Driver Dialect for SQLAlchemy.
-
-    import sqlalchemy
-
-After importing the `sqlalchemy` package, your Python script calls the `sqlalchemy.create_engine` function to open a connection to the Teradata Database.
-
-Specify the Teradata Database hostname as the *host* component of the URL. Note that COP Discovery is not implemented yet.
-
-The URL's *host* component may optionally be followed by a slash and question mark `/?` and the URL's *query* component consisting of connection parameters specified as *key*`=`*value* pairs separated by ampersand `&` characters.
-
-The username and password may be specified as a *host* prefix, or as connection URL parameters.
-
-Username and password specified as a *host* prefix:
-
-    eng = sqlalchemy.create_engine('teradatasql://guest:please@whomooz')
-
-Username and password specified as connection URL parameters:
-
-    eng = sqlalchemy.create_engine('teradatasql://whomooz/?user=guest&password=please')
-
-Username and password specified as connection URL parameters take precedence over a *host* prefix, if both are specified.
-
-<a name="ConnectionParameters"></a>
-
-### Connection Parameters
-
-The following table lists the connection parameters currently offered by the Teradata SQL Driver Dialect for SQLAlchemy.
-
-Our goal is consistency for the connection parameters offered by the Teradata SQL Driver Dialect for SQLAlchemy and the Teradata JDBC Driver, with respect to connection parameter names and functionality. For comparison, Teradata JDBC Driver connection parameters are [documented here](http://developer.teradata.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BGBHDDGB).
-
-Parameter          | Default   | Type    | Description
------------------- | --------- | ------- | ---
-`account`          |           | string  | Specifies the Teradata Database account. Equivalent to the Teradata JDBC Driver `ACCOUNT` connection parameter.
-`column_name`      | `false`   | boolean | Controls the behavior of cursor `.description` sequence `name` items. Equivalent to the Teradata JDBC Driver `COLUMN_NAME` connection parameter. False specifies that a cursor `.description` sequence `name` item provides the AS-clause name if available, or the column name if available, or the column title. True specifies that a cursor `.description` sequence `name` item provides the column name if available, but has no effect when StatementInfo parcel support is unavailable.
-`cop`              | `true`    | boolean | Specifies whether COP Discovery is performed. Equivalent to the Teradata JDBC Driver `COP` connection parameter.
-`coplast`          | `false`   | boolean | Specifies how COP Discovery determines the last COP hostname. Equivalent to the Teradata JDBC Driver `COPLAST` connection parameter. When `coplast` is `false` or omitted, or COP Discovery is turned off, then no DNS lookup occurs for the coplast hostname. When `coplast` is `true`, and COP Discovery is turned on, then a DNS lookup occurs for a coplast hostname.
-`database`         |           | string  | Specifies the initial database to use after logon, instead of the user's default database. Equivalent to the Teradata JDBC Driver `DATABASE` connection parameter.
-`dbs_port`         | `1025`    | integer | Specifies Teradata Database port number. Equivalent to the Teradata JDBC Driver `DBS_PORT` connection parameter.
-`encryptdata`      | `false`   | boolean | Controls encryption of data exchanged between the Teradata Database and the Teradata SQL Driver for Python. Equivalent to the Teradata JDBC Driver `ENCRYPTDATA` connection parameter.
-`fake_result_sets` | `false`   | boolean | Controls whether a fake result set containing statement metadata precedes each real result set.
-`lob_support`      | `true`    | boolean | Controls LOB support. Equivalent to the Teradata JDBC Driver `LOB_SUPPORT` connection parameter.
-`log`              | `0`       | integer | Controls debug logging. Somewhat equivalent to the Teradata JDBC Driver `LOG` connection parameter. This parameter's behavior is subject to change in the future. This parameter's value is currently defined as an integer in which the 1-bit governs function and method tracing, the 2-bit governs debug logging, the 4-bit governs transmit and receive message hex dumps, and the 8-bit governs timing. Compose the value by adding together 1, 2, 4, and/or 8.
-`logdata`          |           | string  | Specifies extra data for the chosen logon authentication method. Equivalent to the Teradata JDBC Driver `LOGDATA` connection parameter.
-`logmech`          | `TD2`     | string  | Specifies the logon authentication method. Equivalent to the Teradata JDBC Driver `LOGMECH` connection parameter. Possible values are `TD2` (the default), `JWT`, `LDAP`, `KRB5` for Kerberos, or `TDNEGO`.
-`max_message_body` | `2097000` | integer | Not fully implemented yet and intended for future usage. Equivalent to the Teradata JDBC Driver `MAX_MESSAGE_BODY` connection parameter.
-`partition`        | `DBC/SQL` | string  | Specifies the Teradata Database Partition. Equivalent to the Teradata JDBC Driver `PARTITION` connection parameter.
-`password`         |           | string  | Specifies the Teradata Database password. Equivalent to the Teradata JDBC Driver `PASSWORD` connection parameter.
-`sip_support`      | `true`    | boolean | Controls whether StatementInfo parcel is used. Equivalent to the Teradata JDBC Driver `SIP_SUPPORT` connection parameter.
-`teradata_values`  | `true`    | boolean | Controls whether `str` or a more specific Python data type is used for certain result set column value types.
-`tmode`            | `DEFAULT` | string  | Specifies the transaction mode. Equivalent to the Teradata JDBC Driver `TMODE` connection parameter. Possible values are `DEFAULT` (the default), `ANSI`, or `TERA`.
-`user`             |           | string  | Specifies the Teradata Database username. Equivalent to the Teradata JDBC Driver `USER` connection parameter.
+## Teradata SQL Driver Dialect for SQLAlchemy
+
+This package enables [SQLAlchemy](https://pypi.org/project/SQLAlchemy/) to connect to the Teradata Database.
+
+This package requires 64-bit Python 3.4 or later, and runs on Windows, macOS, and Linux. 32-bit Python is not supported.
+
+For community support, please visit the [Teradata Community forums](https://community.teradata.com/).
+
+For Teradata customer support, please visit [Teradata Access](https://access.teradata.com/).
+
+Copyright 2024 Teradata. All Rights Reserved.
+
+### Table of Contents
+
+* [Release Notes](#release-notes)
+* [Installation](#Installation)
+* [License](#License)
+* [Documentation](#Documentation)
+* [Using the Teradata SQL Driver Dialect for SQLAlchemy](#Using)
+* [Connection Parameters](#ConnectionParameters)
+
+## Release Notes:
+
+### teradatasqlalchemy 20.0.0.1
+- Minor bug fix for inline help.
+
+### teradatasqlalchemy 20.0.0.0
+- Added a utility to enable querybands for function calls in python packages. Any python client package can import queryband package and can configure its name and version to track the vantage usage with help of logs available with DBQlogTbl.
+
+### teradatasqlalchemy 17.20.0.0
+- Added support for SQLAlchemy >= 2.0
+- `div` operator is no more supported.
+  
+### teradatasqlalchemy 17.0.0.5
+- Added xviews support for below APIs.:
+  - `get_columns`
+  - `get_foreign_keys`
+  - `get_pk_constraint`
+  - `get_schema_names`
+  - `get_transaction_mode`
+  - `get_unique_constraints`
+  
+### teradatasqlalchemy 17.0.0.4
+- Added ART Table support for `get_columns` API.
+
+### teradatasqlalchemy 17.0.0.3
+- Resolved a minor bug.
+  
+### teradatasqlalchemy 17.0.0.2
+- Added support for below Teradata's datatypes:
+  - JSON
+  - XML
+  - TDUDT  
+  - GEOMETRY
+  - MBR
+  - MBB
+- Optimised below APIs
+  - has_table
+  - has_view
+  - get_table_names
+  - get_view_names
+- Minor bug fixes.
+
+
+<a name="Installation"></a>
+
+### Installation
+
+Use pip to install the Teradata SQL Driver Dialect for SQLAlchemy.
+
+Platform       | Command
+-------------- | ---
+macOS or Linux | `pip install teradatasqlalchemy`
+Windows        | `py -3 -m pip install teradatasqlalchemy`
+
+When upgrading to a new version of the Teradata SQL Driver Dialect for SQLAlchemy, you may need to use pip install's `--no-cache-dir` option to force the download of the new version.
+
+Platform       | Command
+-------------- | ---
+macOS or Linux | `pip install --no-cache-dir -U teradatasqlalchemy`
+Windows        | `py -3 -m pip install --no-cache-dir -U teradatasqlalchemy`
+
+<a name="License"></a>
+
+### License
+
+Use of the Teradata SQL Driver Dialect for SQLAlchemy is governed by the *License Agreement for the Teradata SQL Driver Dialect for SQLAlchemy*.
+
+When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `LICENSE` file is placed in the `teradatasqlalchemy` directory under your Python installation directory.
+
+<a name="Documentation"></a>
+
+### Documentation
+
+When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `README.md` file is placed in the `teradatasqlalchemy` directory under your Python installation directory. This permits you to view the documentation offline, when you are not connected to the Internet.
+
+The `README.md` file is a plain text file containing the documentation for the Teradata SQL Driver Dialect for SQLAlchemy. While the file can be viewed with any text file viewer or editor, your viewing experience will be best with an editor that understands Markdown format.
+
+<a name="Using"></a>
+
+### Using the Teradata SQL Driver Dialect for SQLAlchemy
+
+Your Python script must import the `sqlalchemy` package in order to use the Teradata SQL Driver Dialect for SQLAlchemy.
+
+    import sqlalchemy
+
+After importing the `sqlalchemy` package, your Python script calls the `sqlalchemy.create_engine` function to open a connection to the Teradata Database.
+
+Specify the Teradata Database hostname as the *host* component of the URL. Note that COP Discovery is not implemented yet.
+
+The URL's *host* component may optionally be followed by a slash and question mark `/?` and the URL's *query* component consisting of connection parameters specified as *key*`=`*value* pairs separated by ampersand `&` characters.
+
+The username and password may be specified as a *host* prefix, or as connection URL parameters.
+
+Username and password specified as a *host* prefix:
+
+    eng = sqlalchemy.create_engine('teradatasql://guest:please@whomooz')
+
+Username and password specified as connection URL parameters:
+
+    eng = sqlalchemy.create_engine('teradatasql://whomooz/?user=guest&password=please')
+
+Username and password specified as connection URL parameters take precedence over a *host* prefix, if both are specified.
+
+<a name="ConnectionParameters"></a>
+
+### Connection Parameters
+
+The following table lists the connection parameters currently offered by the Teradata SQL Driver Dialect for SQLAlchemy.
+
+Our goal is consistency for the connection parameters offered by the Teradata SQL Driver Dialect for SQLAlchemy and the Teradata JDBC Driver, with respect to connection parameter names and functionality. For comparison, Teradata JDBC Driver connection parameters are [documented here](http://developer.teradata.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BGBHDDGB).
+
+Parameter          | Default   | Type    | Description
+------------------ | --------- | ------- | ---
+`account`          |           | string  | Specifies the Teradata Database account. Equivalent to the Teradata JDBC Driver `ACCOUNT` connection parameter.
+`column_name`      | `false`   | boolean | Controls the behavior of cursor `.description` sequence `name` items. Equivalent to the Teradata JDBC Driver `COLUMN_NAME` connection parameter. False specifies that a cursor `.description` sequence `name` item provides the AS-clause name if available, or the column name if available, or the column title. True specifies that a cursor `.description` sequence `name` item provides the column name if available, but has no effect when StatementInfo parcel support is unavailable.
+`cop`              | `true`    | boolean | Specifies whether COP Discovery is performed. Equivalent to the Teradata JDBC Driver `COP` connection parameter.
+`coplast`          | `false`   | boolean | Specifies how COP Discovery determines the last COP hostname. Equivalent to the Teradata JDBC Driver `COPLAST` connection parameter. When `coplast` is `false` or omitted, or COP Discovery is turned off, then no DNS lookup occurs for the coplast hostname. When `coplast` is `true`, and COP Discovery is turned on, then a DNS lookup occurs for a coplast hostname.
+`database`         |           | string  | Specifies the initial database to use after logon, instead of the user's default database. Equivalent to the Teradata JDBC Driver `DATABASE` connection parameter.
+`dbs_port`         | `1025`    | integer | Specifies Teradata Database port number. Equivalent to the Teradata JDBC Driver `DBS_PORT` connection parameter.
+`encryptdata`      | `false`   | boolean | Controls encryption of data exchanged between the Teradata Database and the Teradata SQL Driver for Python. Equivalent to the Teradata JDBC Driver `ENCRYPTDATA` connection parameter.
+`fake_result_sets` | `false`   | boolean | Controls whether a fake result set containing statement metadata precedes each real result set.
+`lob_support`      | `true`    | boolean | Controls LOB support. Equivalent to the Teradata JDBC Driver `LOB_SUPPORT` connection parameter.
+`log`              | `0`       | integer | Controls debug logging. Somewhat equivalent to the Teradata JDBC Driver `LOG` connection parameter. This parameter's behavior is subject to change in the future. This parameter's value is currently defined as an integer in which the 1-bit governs function and method tracing, the 2-bit governs debug logging, the 4-bit governs transmit and receive message hex dumps, and the 8-bit governs timing. Compose the value by adding together 1, 2, 4, and/or 8.
+`logdata`          |           | string  | Specifies extra data for the chosen logon authentication method. Equivalent to the Teradata JDBC Driver `LOGDATA` connection parameter.
+`logmech`          | `TD2`     | string  | Specifies the logon authentication method. Equivalent to the Teradata JDBC Driver `LOGMECH` connection parameter. Possible values are `TD2` (the default), `JWT`, `LDAP`, `KRB5` for Kerberos, or `TDNEGO`.
+`max_message_body` | `2097000` | integer | Not fully implemented yet and intended for future usage. Equivalent to the Teradata JDBC Driver `MAX_MESSAGE_BODY` connection parameter.
+`partition`        | `DBC/SQL` | string  | Specifies the Teradata Database Partition. Equivalent to the Teradata JDBC Driver `PARTITION` connection parameter.
+`password`         |           | string  | Specifies the Teradata Database password. Equivalent to the Teradata JDBC Driver `PASSWORD` connection parameter.
+`sip_support`      | `true`    | boolean | Controls whether StatementInfo parcel is used. Equivalent to the Teradata JDBC Driver `SIP_SUPPORT` connection parameter.
+`teradata_values`  | `true`    | boolean | Controls whether `str` or a more specific Python data type is used for certain result set column value types.
+`tmode`            | `DEFAULT` | string  | Specifies the transaction mode. Equivalent to the Teradata JDBC Driver `TMODE` connection parameter. Possible values are `DEFAULT` (the default), `ANSI`, or `TERA`.
+`user`             |           | string  | Specifies the Teradata Database username. Equivalent to the Teradata JDBC Driver `USER` connection parameter.
```

## Comparing `teradatasqlalchemy-20.0.0.0.dist-info/LICENSE` & `teradatasqlalchemy-20.0.0.1.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-Teradata SQL Driver Dialect for SQLAlchemy
-Copyright 2023 Teradata.  All rights reserved.
-
-LICENSE AGREEMENT
-
-PRODUCT: Teradata SQL Driver Dialect for SQLAlchemy
-
-IMPORTANT - READ THIS AGREEMENT CAREFULLY BEFORE INSTALLING OR USING THE SOFTWARE.
-TERADATA WILL LICENSE THE SOFTWARE TO YOU ONLY IF YOU ACCEPT THE TERMS AND CONDITIONS OF
-THIS AGREEMENT AND MEET THE CONDITIONS FOR USING THE SOFTWARE DESCRIBED BELOW.  BY
-DOWNLOADING, INSTALLING OR USING THE SOFTWARE, YOU (1) AGREE TO THE TERMS AND CONDITIONS
-OF THIS AGREEMENT, AND (2) REPRESENT AND WARRANT THAT YOU POSSESS THE AUTHORITY TO ENTER
-INTO THIS AGREEMENT ON BEHALF OF YOU, YOUR EMPLOYER (WHEN ACTING ON BEHALF OF YOUR EMPLOYER),
-AND/OR A TERADATA-AUTHORIZED LICENSEE (WHEN YOU AND YOUR EMPLOYER ARE ACTING ON BEHALF OF A
-TERADATA-AUTHORIZED LICENSEE).  IF YOU DO NOT ACCEPT THE TERMS AND CONDITIONS OF THIS
-AGREEMENT, DO NOT DOWNLOAD, INSTALL OR USE THE SOFTWARE.
-
-IMPORTANT - BY DOWNLOADING THE SOFTWARE:
-* YOU ACKNOWLEDGE THAT THE SOFTWARE YOU ARE DOWNLOADING FROM TERADATA IS SUBJECT
-  TO THE RESTRICTIONS AND CONTROLS IMPOSED BY UNITED STATES EXPORT REGULATIONS.
-* YOU CERTIFY THAT:
-** YOU DO NOT INTEND TO USE THE SOFTWARE FOR ANY PURPOSE PROHIBITED BY UNITED STATES EXPORT
-   REGULATIONS, INCLUDING, WITHOUT LIMITATION, TERRORISM, CYBER-ATTACKS, CYBER-CRIMES,
-   MONEY-LAUNDERING, INDUSTRIAL ESPIONAGE, OR NUCLEAR, CHEMICAL OR BIOLOGICAL WEAPONS
-   PROLIFERATION.
-** YOU ARE NOT LISTED AS A DENIED PARTY ON ANY LIST GOVERNING UNITED STATES EXPORTS.
-** YOU ARE NOT A NATIONAL OF ANY COUNTRY THAT IS NOT APPROVED FOR EXPORT OF THE SOFTWARE.
-   AS OF 2017, THESE COUNTRIES ARE CUBA, IRAN, NORTH KOREA, AND SYRIA.
-
-This License Agreement ("Agreement") is a legal contract between you (as defined below) and
-Teradata (as defined below) regarding the Software (as defined below).  The terms "you",
-"your" and "yours" collectively and individually refer to you as an individual and to any
-company for which you are acting.  The term "Teradata" refers to either Teradata U.S., Inc.
-for Software deliveries in the US or Teradata Ireland Ltd. for Software deliveries outside
-the United States.  "Software" refers to the software product identified above, which
-consists of computer software code in object code form only, as well as associated
-documentation that Teradata may elect in its sole discretion to provide you.  "Software"
-also includes any and all error corrections, bug fixes, updates, upgrades, or new versions
-or releases of the Software (collectively and individually, "Enhancements") that Teradata
-may elect in its sole discretion to provide you.
-
-1.  Term.  This Agreement commences on the earliest date of the first download, first
-    copying, first installation, or first use of the Software (the "Effective Date").
-    Unless terminated earlier as provided herein, this agreement, including your license
-    to the Software, will expire or terminate on the same date that your Teradata-authorized
-    license to use the Teradata Analytics Platform product expires or terminates (whichever
-    occurs first).
-
-2.  License.
-
-(a) Subject to your compliance with all of the terms and conditions of this Agreement
-    and only during the term of this Agreement, Teradata grants you a nonexclusive,
-    nontransferable, paid up license to install and use the Software on your computer
-    solely for purposes of facilitating your Teradata-authorized license to use the
-    Teradata Analytics Platform.  You may make reasonable archival backup copies of the
-    Software, but may only use an archival copy in lieu of your primary copy and subject
-    to the same restrictions as your primary copy.
-
-(b) The term Third Party Software means computer programs or modules (including their
-    documentation) that bear the logo, copyright and/or trademark of a third party
-    (including open source software that are contained in files marked as "open source"
-    or the like) or are otherwise subject to the written license terms of a third party.
-    The term "Software" does not include Third Party Software.  Third Party Software is
-    licensed to you subject to the applicable license terms accompanying it, included
-    in/with it, referenced in it, or otherwise entered into by you with respect to it.
-    Third Party Software license terms include those found in the FOSS licensing zip file
-    accompanying the Software.  Teradata provides source code to certain Third Party
-    Software for certain periods of time in compliance with certain applicable licenses.
-    To request such source code, visit http://developer.teradata.com/download/license/oss-request.
-
-(c) You will not sell, copy, rent, loan, modify, transfer, disclose, embed, sublicense,
-    create derivative works of or distribute the Software, in whole or in part, without
-    Teradata's prior written consent.  You are granted no rights to obtain or use the
-    Software's source code.  You will not reverse-assemble, reverse compile or reverse-
-    engineer the Software, except as expressly permitted by applicable law without the
-    possibility of contractual waiver.  Notwithstanding anything to the contrary, you do
-    not have any license, right, or authority to subject the Software, in whole or in part
-    or as part of a larger work, to any terms of any other agreement, including GNU Public
-    Licenses.
-
-(d) No license rights to the Software will be implied.  The Software, which includes all
-    copies thereof (whether in whole or in part), is and remains the exclusive property of
-    Teradata.  You will ensure that all copies of the Software contain Teradata's copyright
-    notices, as well as all other proprietary legends.  Teradata reserves the right to
-    inspect your use of the Software for purposes of verifying your compliance with the
-    terms and conditions of this Agreement.
-
-3.  Responsibilities.  You are responsible for the installation of the Software, as
-    well as for providing data security and backup operations.  This Agreement does not
-    require Teradata to provide you with any Enhancements, consulting services, technical
-    assistance, installation, training, support, or maintenance of any kind (collectively
-    and individually, "Services").  To the extent that Teradata elects to provide you
-    with any Services, such Services are provided to you at Teradata's sole discretion
-    and may be modified or discontinued at any time for any reason.
-
-4.  DISCLAIMER OF WARRANTY.  TERADATA: (a) PROVIDES SERVICES (IF ANY), (b) LICENSES THE
-    SOFTWARE, AND (c) PROVIDES THIRD PARTY SOFTWARE TO YOU ON AN "AS-IS" BASIS WITHOUT
-    WARRANTIES OF ANY KIND (ORAL OR WRITTEN, EXPRESS OR IMPLIED, OR STATUTORY). WITHOUT
-    LIMITATION TO THE FOREGOING, THERE ARE NO IMPLIED WARRANTIES OF MERCHANTABILITY,
-    FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT.  TERADATA DOES NOT WARRANT
-    THAT THE SOFTWARE, THIRD PARTY SOFTWARE, OR SERVICES WILL MEET YOUR REQUIREMENTS
-    OR CONFORM TO ANY SPECIFICATIONS, OR THAT THE OPERATION OF THE SOFTWARE OR THIRD
-    PARTY SOFTWARE WILL BE UNINTERRUPTED OR ERROR FREE.  YOU BEAR THE ENTIRE RISK AS
-    TO SATISFACTORY QUALITY, PERFORMANCE, ACCURACY, AND RESULTS OBTAINED FROM THE
-    SOFTWARE, THIRD PARTY SOFTWARE, AND SERVICES.
-
-    SOME JURISDICTIONS RESTRICT DISCLAIMERS OF WARRANTY, SO THE ABOVE DISCLAIMERS MAY
-    NOT FULLY APPLY TO YOU.
-
-5.  LIMITATIONS ON LIABILITY: UNDER NO CIRCUMSTANCES WILL TERADATA'S AND ITS LICENSORS'
-    TOTAL CUMULATIVE LIABILITY FOR CLAIMS RELATING TO THIS AGREEMENT, THE SERVICES,
-    THE SOFTWARE, AND/OR THIRD PARTY SOFTWARE (WHETHER BASED IN CONTRACT, STATUTE,
-    TORT (INCLUDING NEGLIGENCE) OR OTHERWISE) EXCEED US$1,000; PROVIDED, HOWEVER,
-    THAT THE FOREGOING WILL NOT APPLY TO CLAIMS FOR (A) PERSONAL INJURY, INCLUDING
-    DEATH, TO THE EXTENT CAUSED BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT; OR
-    (B) PHYSICAL DAMAGE TO TANGIBLE REAL OR PERSONAL PROPERTY TO THE EXTENT CAUSED
-    BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT EQUAL TO THE AMOUNT OF DIRECT
-    DAMAGES UP TO ONE MILLION DOLLARS PER OCCURRENCE.  IN NO EVENT WILL TERADATA
-    OR ITS LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, PUNITIVE, INCIDENTAL OR
-    CONSEQUENTIAL DAMAGES, OR FOR LOSS OF PROFITS, REVENUE, TIME, OPPORTUNITY OR
-    DATA, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-    SOME JURISDICTIONS RESTRICT LIMITATIONS OF LIABILITY, SO THE ABOVE LIMITATIONS MAY
-    NOT FULLY APPLY TO YOU.
-
-6.  Government Restrictions. You agree that you will not, directly or indirectly, export
-    or transmit any Software without obtaining Teradata's prior written authorization,
-    as well as appropriate governmental approvals, including those required by the
-    U.S. Government. Use and or distribution of this software is subject to export laws
-    and regulations of the United States and other jurisdictions. The links below connect
-    you to applicable U.S. government agencies, and their regulations, that have
-    jurisdiction over this transaction.
-    http://www.bis.doc.gov/
-    http://www.treas.gov/offices/enforcement/ofac/
-
-    In downloading this product, you acknowledge that this transaction is subject to
-    applicable export control laws and you certify that your download, use and/or subsequent
-    distribution of this product is not prohibited under applicable laws and regulations.
-
-    The Government's use, duplication, or disclosure of Teradata's commercial computer
-    software and commercial computer software documentation is subject to: (a) the
-    Restricted Rights Notice set forth in 48 C.F.R. � 52.227-14 (Rights In Data - General);
-    (b) Teradata's standard commercial license rights supplemented by 48 C.F.R. � 52.227-19
-    (Commercial Computer Software - Restricted Rights); and/or (c) the limited rights and
-    license set forth 48 CFR � 252.227-7015 (Technical Data�Commercial Items), as applicable.
-
-7.  Termination and Expiration. A party may terminate this Agreement with or without
-    cause, upon providing written notice to the other parties.  When this Agreement
-    terminates or expires, you will immediately cease all use of the Software, permanently
-    remove the Software from all computers, destroy all copies of the Software, and (upon
-    receipt of Teradata's request) provide a signed written certification that the
-    foregoing has occurred. Sections 4, 5, 6, 7, 8, 9, 10, and 11 will survive expiration
-    or termination of this Agreement.
-
-8.  Choice of Law and Dispute Resolution. The parties will attempt in good faith to resolve
-    any controversy or claim by negotiation or mediation.  If they are unable to do so, and
-    regardless of the causes of action alleged and whether they arise under this Agreement
-    or otherwise, the claim will be resolved by arbitration before a sole arbitrator in
-    Dayton, Ohio pursuant to the then-current Commercial Rules of the American Arbitration
-    Association and the federal substantive and procedural law of arbitration.  The
-    arbitrator's award will be final and binding, and may be entered in any court having
-    jurisdiction thereof, but may include only damages consistent with the limitations in
-    this Agreement.  Each party will bear its own attorney's fees and costs related to the
-    arbitration. The obligations to negotiate, mediate and arbitrate shall not apply to
-    claims for misuse or infringement of a party's intellectual property rights.  Any
-    claim or action must be brought within two years after the claimant knows or should
-    have known of the claim.  New York law will govern the interpretation and enforcement
-    of this Agreement, except that the Federal Arbitration Act will govern the
-    interpretation and enforcement of the arbitrability of claims under this Section.
-
-9.  Feedback.  Notwithstanding anything to the contrary: (a) Teradata will have no
-    obligation of any kind with respect to any Software-related comments, suggestions,
-    design changes or improvements that you elect to provide to Teradata in either verbal
-    or written form (collectively, "Software Feedback"), and (b) Teradata and its
-    affiliates are hereby free to use any ideas, concepts, know-how or techniques, in
-    whole or in part, contained in Software Feedback: (i) for any purpose whatsoever,
-    including developing, manufacturing, and/or marketing products and/or services
-    incorporating Software Feedback in whole or in part, and (ii) without any
-    restrictions or limitations, including requiring the payment of any license fees,
-    royalties, or other consideration.
-
-10. Confidentiality.  You will not disclose the results of any testing or evaluations,
-    including any benchmarks, insofar as it relates to the Software without Teradata's
-    prior written consent.
-
-11. Entire Agreement. This Agreement constitutes the entire understanding of the parties
-    with respect to the Software and Services, and supersede all other prior agreements
-    and understandings whether oral or written.  No oral representation or change to this
-    Agreement will be binding upon either party unless agreed to in writing and signed by
-    authorized representatives of all parties. You will not assign this Agreement or your
-    rights, nor will you delegate your obligations under this Agreement.  Failure by
-    either party to enforce any term or condition of this Agreement will not be deemed
-    a waiver of future enforcement of that or any other term or condition. The provisions
-    of this Agreement are severable. "Include", "includes", and "including" shall be
-    interpreted as introducing a list of examples which do not limit the generality of
-    any preceding words or any words in the list of examples.
+Teradata SQL Driver Dialect for SQLAlchemy
+Copyright 2023 Teradata.  All rights reserved.
+
+LICENSE AGREEMENT
+
+PRODUCT: Teradata SQL Driver Dialect for SQLAlchemy
+
+IMPORTANT - READ THIS AGREEMENT CAREFULLY BEFORE INSTALLING OR USING THE SOFTWARE.
+TERADATA WILL LICENSE THE SOFTWARE TO YOU ONLY IF YOU ACCEPT THE TERMS AND CONDITIONS OF
+THIS AGREEMENT AND MEET THE CONDITIONS FOR USING THE SOFTWARE DESCRIBED BELOW.  BY
+DOWNLOADING, INSTALLING OR USING THE SOFTWARE, YOU (1) AGREE TO THE TERMS AND CONDITIONS
+OF THIS AGREEMENT, AND (2) REPRESENT AND WARRANT THAT YOU POSSESS THE AUTHORITY TO ENTER
+INTO THIS AGREEMENT ON BEHALF OF YOU, YOUR EMPLOYER (WHEN ACTING ON BEHALF OF YOUR EMPLOYER),
+AND/OR A TERADATA-AUTHORIZED LICENSEE (WHEN YOU AND YOUR EMPLOYER ARE ACTING ON BEHALF OF A
+TERADATA-AUTHORIZED LICENSEE).  IF YOU DO NOT ACCEPT THE TERMS AND CONDITIONS OF THIS
+AGREEMENT, DO NOT DOWNLOAD, INSTALL OR USE THE SOFTWARE.
+
+IMPORTANT - BY DOWNLOADING THE SOFTWARE:
+* YOU ACKNOWLEDGE THAT THE SOFTWARE YOU ARE DOWNLOADING FROM TERADATA IS SUBJECT
+  TO THE RESTRICTIONS AND CONTROLS IMPOSED BY UNITED STATES EXPORT REGULATIONS.
+* YOU CERTIFY THAT:
+** YOU DO NOT INTEND TO USE THE SOFTWARE FOR ANY PURPOSE PROHIBITED BY UNITED STATES EXPORT
+   REGULATIONS, INCLUDING, WITHOUT LIMITATION, TERRORISM, CYBER-ATTACKS, CYBER-CRIMES,
+   MONEY-LAUNDERING, INDUSTRIAL ESPIONAGE, OR NUCLEAR, CHEMICAL OR BIOLOGICAL WEAPONS
+   PROLIFERATION.
+** YOU ARE NOT LISTED AS A DENIED PARTY ON ANY LIST GOVERNING UNITED STATES EXPORTS.
+** YOU ARE NOT A NATIONAL OF ANY COUNTRY THAT IS NOT APPROVED FOR EXPORT OF THE SOFTWARE.
+   AS OF 2017, THESE COUNTRIES ARE CUBA, IRAN, NORTH KOREA, AND SYRIA.
+
+This License Agreement ("Agreement") is a legal contract between you (as defined below) and
+Teradata (as defined below) regarding the Software (as defined below).  The terms "you",
+"your" and "yours" collectively and individually refer to you as an individual and to any
+company for which you are acting.  The term "Teradata" refers to either Teradata U.S., Inc.
+for Software deliveries in the US or Teradata Ireland Ltd. for Software deliveries outside
+the United States.  "Software" refers to the software product identified above, which
+consists of computer software code in object code form only, as well as associated
+documentation that Teradata may elect in its sole discretion to provide you.  "Software"
+also includes any and all error corrections, bug fixes, updates, upgrades, or new versions
+or releases of the Software (collectively and individually, "Enhancements") that Teradata
+may elect in its sole discretion to provide you.
+
+1.  Term.  This Agreement commences on the earliest date of the first download, first
+    copying, first installation, or first use of the Software (the "Effective Date").
+    Unless terminated earlier as provided herein, this agreement, including your license
+    to the Software, will expire or terminate on the same date that your Teradata-authorized
+    license to use the Teradata Analytics Platform product expires or terminates (whichever
+    occurs first).
+
+2.  License.
+
+(a) Subject to your compliance with all of the terms and conditions of this Agreement
+    and only during the term of this Agreement, Teradata grants you a nonexclusive,
+    nontransferable, paid up license to install and use the Software on your computer
+    solely for purposes of facilitating your Teradata-authorized license to use the
+    Teradata Analytics Platform.  You may make reasonable archival backup copies of the
+    Software, but may only use an archival copy in lieu of your primary copy and subject
+    to the same restrictions as your primary copy.
+
+(b) The term Third Party Software means computer programs or modules (including their
+    documentation) that bear the logo, copyright and/or trademark of a third party
+    (including open source software that are contained in files marked as "open source"
+    or the like) or are otherwise subject to the written license terms of a third party.
+    The term "Software" does not include Third Party Software.  Third Party Software is
+    licensed to you subject to the applicable license terms accompanying it, included
+    in/with it, referenced in it, or otherwise entered into by you with respect to it.
+    Third Party Software license terms include those found in the FOSS licensing zip file
+    accompanying the Software.  Teradata provides source code to certain Third Party
+    Software for certain periods of time in compliance with certain applicable licenses.
+    To request such source code, visit http://developer.teradata.com/download/license/oss-request.
+
+(c) You will not sell, copy, rent, loan, modify, transfer, disclose, embed, sublicense,
+    create derivative works of or distribute the Software, in whole or in part, without
+    Teradata's prior written consent.  You are granted no rights to obtain or use the
+    Software's source code.  You will not reverse-assemble, reverse compile or reverse-
+    engineer the Software, except as expressly permitted by applicable law without the
+    possibility of contractual waiver.  Notwithstanding anything to the contrary, you do
+    not have any license, right, or authority to subject the Software, in whole or in part
+    or as part of a larger work, to any terms of any other agreement, including GNU Public
+    Licenses.
+
+(d) No license rights to the Software will be implied.  The Software, which includes all
+    copies thereof (whether in whole or in part), is and remains the exclusive property of
+    Teradata.  You will ensure that all copies of the Software contain Teradata's copyright
+    notices, as well as all other proprietary legends.  Teradata reserves the right to
+    inspect your use of the Software for purposes of verifying your compliance with the
+    terms and conditions of this Agreement.
+
+3.  Responsibilities.  You are responsible for the installation of the Software, as
+    well as for providing data security and backup operations.  This Agreement does not
+    require Teradata to provide you with any Enhancements, consulting services, technical
+    assistance, installation, training, support, or maintenance of any kind (collectively
+    and individually, "Services").  To the extent that Teradata elects to provide you
+    with any Services, such Services are provided to you at Teradata's sole discretion
+    and may be modified or discontinued at any time for any reason.
+
+4.  DISCLAIMER OF WARRANTY.  TERADATA: (a) PROVIDES SERVICES (IF ANY), (b) LICENSES THE
+    SOFTWARE, AND (c) PROVIDES THIRD PARTY SOFTWARE TO YOU ON AN "AS-IS" BASIS WITHOUT
+    WARRANTIES OF ANY KIND (ORAL OR WRITTEN, EXPRESS OR IMPLIED, OR STATUTORY). WITHOUT
+    LIMITATION TO THE FOREGOING, THERE ARE NO IMPLIED WARRANTIES OF MERCHANTABILITY,
+    FITNESS FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT.  TERADATA DOES NOT WARRANT
+    THAT THE SOFTWARE, THIRD PARTY SOFTWARE, OR SERVICES WILL MEET YOUR REQUIREMENTS
+    OR CONFORM TO ANY SPECIFICATIONS, OR THAT THE OPERATION OF THE SOFTWARE OR THIRD
+    PARTY SOFTWARE WILL BE UNINTERRUPTED OR ERROR FREE.  YOU BEAR THE ENTIRE RISK AS
+    TO SATISFACTORY QUALITY, PERFORMANCE, ACCURACY, AND RESULTS OBTAINED FROM THE
+    SOFTWARE, THIRD PARTY SOFTWARE, AND SERVICES.
+
+    SOME JURISDICTIONS RESTRICT DISCLAIMERS OF WARRANTY, SO THE ABOVE DISCLAIMERS MAY
+    NOT FULLY APPLY TO YOU.
+
+5.  LIMITATIONS ON LIABILITY: UNDER NO CIRCUMSTANCES WILL TERADATA'S AND ITS LICENSORS'
+    TOTAL CUMULATIVE LIABILITY FOR CLAIMS RELATING TO THIS AGREEMENT, THE SERVICES,
+    THE SOFTWARE, AND/OR THIRD PARTY SOFTWARE (WHETHER BASED IN CONTRACT, STATUTE,
+    TORT (INCLUDING NEGLIGENCE) OR OTHERWISE) EXCEED US$1,000; PROVIDED, HOWEVER,
+    THAT THE FOREGOING WILL NOT APPLY TO CLAIMS FOR (A) PERSONAL INJURY, INCLUDING
+    DEATH, TO THE EXTENT CAUSED BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT; OR
+    (B) PHYSICAL DAMAGE TO TANGIBLE REAL OR PERSONAL PROPERTY TO THE EXTENT CAUSED
+    BY TERADATA'S NEGLIGENCE OR WILLFUL MISCONDUCT EQUAL TO THE AMOUNT OF DIRECT
+    DAMAGES UP TO ONE MILLION DOLLARS PER OCCURRENCE.  IN NO EVENT WILL TERADATA
+    OR ITS LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, PUNITIVE, INCIDENTAL OR
+    CONSEQUENTIAL DAMAGES, OR FOR LOSS OF PROFITS, REVENUE, TIME, OPPORTUNITY OR
+    DATA, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+    SOME JURISDICTIONS RESTRICT LIMITATIONS OF LIABILITY, SO THE ABOVE LIMITATIONS MAY
+    NOT FULLY APPLY TO YOU.
+
+6.  Government Restrictions. You agree that you will not, directly or indirectly, export
+    or transmit any Software without obtaining Teradata's prior written authorization,
+    as well as appropriate governmental approvals, including those required by the
+    U.S. Government. Use and or distribution of this software is subject to export laws
+    and regulations of the United States and other jurisdictions. The links below connect
+    you to applicable U.S. government agencies, and their regulations, that have
+    jurisdiction over this transaction.
+    http://www.bis.doc.gov/
+    http://www.treas.gov/offices/enforcement/ofac/
+
+    In downloading this product, you acknowledge that this transaction is subject to
+    applicable export control laws and you certify that your download, use and/or subsequent
+    distribution of this product is not prohibited under applicable laws and regulations.
+
+    The Government's use, duplication, or disclosure of Teradata's commercial computer
+    software and commercial computer software documentation is subject to: (a) the
+    Restricted Rights Notice set forth in 48 C.F.R. � 52.227-14 (Rights In Data - General);
+    (b) Teradata's standard commercial license rights supplemented by 48 C.F.R. � 52.227-19
+    (Commercial Computer Software - Restricted Rights); and/or (c) the limited rights and
+    license set forth 48 CFR � 252.227-7015 (Technical Data�Commercial Items), as applicable.
+
+7.  Termination and Expiration. A party may terminate this Agreement with or without
+    cause, upon providing written notice to the other parties.  When this Agreement
+    terminates or expires, you will immediately cease all use of the Software, permanently
+    remove the Software from all computers, destroy all copies of the Software, and (upon
+    receipt of Teradata's request) provide a signed written certification that the
+    foregoing has occurred. Sections 4, 5, 6, 7, 8, 9, 10, and 11 will survive expiration
+    or termination of this Agreement.
+
+8.  Choice of Law and Dispute Resolution. The parties will attempt in good faith to resolve
+    any controversy or claim by negotiation or mediation.  If they are unable to do so, and
+    regardless of the causes of action alleged and whether they arise under this Agreement
+    or otherwise, the claim will be resolved by arbitration before a sole arbitrator in
+    Dayton, Ohio pursuant to the then-current Commercial Rules of the American Arbitration
+    Association and the federal substantive and procedural law of arbitration.  The
+    arbitrator's award will be final and binding, and may be entered in any court having
+    jurisdiction thereof, but may include only damages consistent with the limitations in
+    this Agreement.  Each party will bear its own attorney's fees and costs related to the
+    arbitration. The obligations to negotiate, mediate and arbitrate shall not apply to
+    claims for misuse or infringement of a party's intellectual property rights.  Any
+    claim or action must be brought within two years after the claimant knows or should
+    have known of the claim.  New York law will govern the interpretation and enforcement
+    of this Agreement, except that the Federal Arbitration Act will govern the
+    interpretation and enforcement of the arbitrability of claims under this Section.
+
+9.  Feedback.  Notwithstanding anything to the contrary: (a) Teradata will have no
+    obligation of any kind with respect to any Software-related comments, suggestions,
+    design changes or improvements that you elect to provide to Teradata in either verbal
+    or written form (collectively, "Software Feedback"), and (b) Teradata and its
+    affiliates are hereby free to use any ideas, concepts, know-how or techniques, in
+    whole or in part, contained in Software Feedback: (i) for any purpose whatsoever,
+    including developing, manufacturing, and/or marketing products and/or services
+    incorporating Software Feedback in whole or in part, and (ii) without any
+    restrictions or limitations, including requiring the payment of any license fees,
+    royalties, or other consideration.
+
+10. Confidentiality.  You will not disclose the results of any testing or evaluations,
+    including any benchmarks, insofar as it relates to the Software without Teradata's
+    prior written consent.
+
+11. Entire Agreement. This Agreement constitutes the entire understanding of the parties
+    with respect to the Software and Services, and supersede all other prior agreements
+    and understandings whether oral or written.  No oral representation or change to this
+    Agreement will be binding upon either party unless agreed to in writing and signed by
+    authorized representatives of all parties. You will not assign this Agreement or your
+    rights, nor will you delegate your obligations under this Agreement.  Failure by
+    either party to enforce any term or condition of this Agreement will not be deemed
+    a waiver of future enforcement of that or any other term or condition. The provisions
+    of this Agreement are severable. "Include", "includes", and "including" shall be
+    interpreted as introducing a list of examples which do not limit the generality of
+    any preceding words or any words in the list of examples.
```

## Comparing `teradatasqlalchemy-20.0.0.0.dist-info/METADATA` & `teradatasqlalchemy-20.0.0.1.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,178 +1,182 @@
-Metadata-Version: 2.1
-Name: teradatasqlalchemy
-Version: 20.0.0.0
-Summary: Teradata SQL Driver Dialect for SQLAlchemy
-Home-page: http://www.teradata.com/
-Author: Teradata Corporation
-Author-email: teradatasql@teradata.com
-License: Teradata License Agreement
-Keywords: Teradata
-Platform: Windows
-Platform: MacOS X
-Platform: Linux
-Classifier: Programming Language :: Python :: 3 :: Only
-Classifier: Programming Language :: Python :: 3.4
-Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS :: MacOS X
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Topic :: Database :: Front-Ends
-Classifier: License :: Other/Proprietary License
-Requires-Python: >=3.4
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: sqlalchemy (>=1.4.0)
-Requires-Dist: teradatasql (>=17.0.0.2)
-
-## Teradata SQL Driver Dialect for SQLAlchemy
-
-This package enables [SQLAlchemy](https://pypi.org/project/SQLAlchemy/) to connect to the Teradata Database.
-
-This package requires 64-bit Python 3.4 or later, and runs on Windows, macOS, and Linux. 32-bit Python is not supported.
-
-For community support, please visit the [Teradata Community forums](https://community.teradata.com/).
-
-For Teradata customer support, please visit [Teradata Access](https://access.teradata.com/).
-
-Copyright 2024 Teradata. All Rights Reserved.
-
-### Table of Contents
-
-* [Release Notes](#release-notes)
-* [Installation](#Installation)
-* [License](#License)
-* [Documentation](#Documentation)
-* [Using the Teradata SQL Driver Dialect for SQLAlchemy](#Using)
-* [Connection Parameters](#ConnectionParameters)
-
-## Release Notes:
-
-### teradatasqlalchemy 20.0.0.0
-- Added a utility to enable querybands for function calls in python packages. Any python client package can import queryband package and can configure its name and version to track the vantage usage with help of logs available with DBQlogTbl.
-
-### teradatasqlalchemy 17.20.0.0
-- Added support for SQLAlchemy >= 2.0
-- `div` operator is no more supported.
-  
-### teradatasqlalchemy 17.0.0.5
-- Added xviews support for below APIs.:
-  - `get_columns`
-  - `get_foreign_keys`
-  - `get_pk_constraint`
-  - `get_schema_names`
-  - `get_transaction_mode`
-  - `get_unique_constraints`
-  
-### teradatasqlalchemy 17.0.0.4
-- Added ART Table support for `get_columns` API.
-
-### teradatasqlalchemy 17.0.0.3
-- Resolved a minor bug.
-  
-### teradatasqlalchemy 17.0.0.2
-- Added support for below Teradata's datatypes:
-  - JSON
-  - XML
-  - TDUDT  
-  - GEOMETRY
-  - MBR
-  - MBB
-- Optimised below APIs
-  - has_table
-  - has_view
-  - get_table_names
-  - get_view_names
-- Minor bug fixes.
-
-
-<a name="Installation"></a>
-
-### Installation
-
-Use pip to install the Teradata SQL Driver Dialect for SQLAlchemy.
-
-Platform       | Command
--------------- | ---
-macOS or Linux | `pip install teradatasqlalchemy`
-Windows        | `py -3 -m pip install teradatasqlalchemy`
-
-When upgrading to a new version of the Teradata SQL Driver Dialect for SQLAlchemy, you may need to use pip install's `--no-cache-dir` option to force the download of the new version.
-
-Platform       | Command
--------------- | ---
-macOS or Linux | `pip install --no-cache-dir -U teradatasqlalchemy`
-Windows        | `py -3 -m pip install --no-cache-dir -U teradatasqlalchemy`
-
-<a name="License"></a>
-
-### License
-
-Use of the Teradata SQL Driver Dialect for SQLAlchemy is governed by the *License Agreement for the Teradata SQL Driver Dialect for SQLAlchemy*.
-
-When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `LICENSE` file is placed in the `teradatasqlalchemy` directory under your Python installation directory.
-
-<a name="Documentation"></a>
-
-### Documentation
-
-When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `README.md` file is placed in the `teradatasqlalchemy` directory under your Python installation directory. This permits you to view the documentation offline, when you are not connected to the Internet.
-
-The `README.md` file is a plain text file containing the documentation for the Teradata SQL Driver Dialect for SQLAlchemy. While the file can be viewed with any text file viewer or editor, your viewing experience will be best with an editor that understands Markdown format.
-
-<a name="Using"></a>
-
-### Using the Teradata SQL Driver Dialect for SQLAlchemy
-
-Your Python script must import the `sqlalchemy` package in order to use the Teradata SQL Driver Dialect for SQLAlchemy.
-
-    import sqlalchemy
-
-After importing the `sqlalchemy` package, your Python script calls the `sqlalchemy.create_engine` function to open a connection to the Teradata Database.
-
-Specify the Teradata Database hostname as the *host* component of the URL. Note that COP Discovery is not implemented yet.
-
-The URL's *host* component may optionally be followed by a slash and question mark `/?` and the URL's *query* component consisting of connection parameters specified as *key*`=`*value* pairs separated by ampersand `&` characters.
-
-The username and password may be specified as a *host* prefix, or as connection URL parameters.
-
-Username and password specified as a *host* prefix:
-
-    eng = sqlalchemy.create_engine('teradatasql://guest:please@whomooz')
-
-Username and password specified as connection URL parameters:
-
-    eng = sqlalchemy.create_engine('teradatasql://whomooz/?user=guest&password=please')
-
-Username and password specified as connection URL parameters take precedence over a *host* prefix, if both are specified.
-
-<a name="ConnectionParameters"></a>
-
-### Connection Parameters
-
-The following table lists the connection parameters currently offered by the Teradata SQL Driver Dialect for SQLAlchemy.
-
-Our goal is consistency for the connection parameters offered by the Teradata SQL Driver Dialect for SQLAlchemy and the Teradata JDBC Driver, with respect to connection parameter names and functionality. For comparison, Teradata JDBC Driver connection parameters are [documented here](http://developer.teradata.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BGBHDDGB).
-
-Parameter          | Default   | Type    | Description
------------------- | --------- | ------- | ---
-`account`          |           | string  | Specifies the Teradata Database account. Equivalent to the Teradata JDBC Driver `ACCOUNT` connection parameter.
-`column_name`      | `false`   | boolean | Controls the behavior of cursor `.description` sequence `name` items. Equivalent to the Teradata JDBC Driver `COLUMN_NAME` connection parameter. False specifies that a cursor `.description` sequence `name` item provides the AS-clause name if available, or the column name if available, or the column title. True specifies that a cursor `.description` sequence `name` item provides the column name if available, but has no effect when StatementInfo parcel support is unavailable.
-`cop`              | `true`    | boolean | Specifies whether COP Discovery is performed. Equivalent to the Teradata JDBC Driver `COP` connection parameter.
-`coplast`          | `false`   | boolean | Specifies how COP Discovery determines the last COP hostname. Equivalent to the Teradata JDBC Driver `COPLAST` connection parameter. When `coplast` is `false` or omitted, or COP Discovery is turned off, then no DNS lookup occurs for the coplast hostname. When `coplast` is `true`, and COP Discovery is turned on, then a DNS lookup occurs for a coplast hostname.
-`database`         |           | string  | Specifies the initial database to use after logon, instead of the user's default database. Equivalent to the Teradata JDBC Driver `DATABASE` connection parameter.
-`dbs_port`         | `1025`    | integer | Specifies Teradata Database port number. Equivalent to the Teradata JDBC Driver `DBS_PORT` connection parameter.
-`encryptdata`      | `false`   | boolean | Controls encryption of data exchanged between the Teradata Database and the Teradata SQL Driver for Python. Equivalent to the Teradata JDBC Driver `ENCRYPTDATA` connection parameter.
-`fake_result_sets` | `false`   | boolean | Controls whether a fake result set containing statement metadata precedes each real result set.
-`lob_support`      | `true`    | boolean | Controls LOB support. Equivalent to the Teradata JDBC Driver `LOB_SUPPORT` connection parameter.
-`log`              | `0`       | integer | Controls debug logging. Somewhat equivalent to the Teradata JDBC Driver `LOG` connection parameter. This parameter's behavior is subject to change in the future. This parameter's value is currently defined as an integer in which the 1-bit governs function and method tracing, the 2-bit governs debug logging, the 4-bit governs transmit and receive message hex dumps, and the 8-bit governs timing. Compose the value by adding together 1, 2, 4, and/or 8.
-`logdata`          |           | string  | Specifies extra data for the chosen logon authentication method. Equivalent to the Teradata JDBC Driver `LOGDATA` connection parameter.
-`logmech`          | `TD2`     | string  | Specifies the logon authentication method. Equivalent to the Teradata JDBC Driver `LOGMECH` connection parameter. Possible values are `TD2` (the default), `JWT`, `LDAP`, `KRB5` for Kerberos, or `TDNEGO`.
-`max_message_body` | `2097000` | integer | Not fully implemented yet and intended for future usage. Equivalent to the Teradata JDBC Driver `MAX_MESSAGE_BODY` connection parameter.
-`partition`        | `DBC/SQL` | string  | Specifies the Teradata Database Partition. Equivalent to the Teradata JDBC Driver `PARTITION` connection parameter.
-`password`         |           | string  | Specifies the Teradata Database password. Equivalent to the Teradata JDBC Driver `PASSWORD` connection parameter.
-`sip_support`      | `true`    | boolean | Controls whether StatementInfo parcel is used. Equivalent to the Teradata JDBC Driver `SIP_SUPPORT` connection parameter.
-`teradata_values`  | `true`    | boolean | Controls whether `str` or a more specific Python data type is used for certain result set column value types.
-`tmode`            | `DEFAULT` | string  | Specifies the transaction mode. Equivalent to the Teradata JDBC Driver `TMODE` connection parameter. Possible values are `DEFAULT` (the default), `ANSI`, or `TERA`.
-`user`             |           | string  | Specifies the Teradata Database username. Equivalent to the Teradata JDBC Driver `USER` connection parameter.
+Metadata-Version: 2.1
+Name: teradatasqlalchemy
+Version: 20.0.0.1
+Summary: Teradata SQL Driver Dialect for SQLAlchemy
+Home-page: http://www.teradata.com/
+Author: Teradata Corporation
+Author-email: teradatasql@teradata.com
+License: Teradata License Agreement
+Keywords: Teradata
+Platform: Windows
+Platform: MacOS X
+Platform: Linux
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Programming Language :: Python :: 3.4
+Classifier: Programming Language :: Python :: 3.5
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS :: MacOS X
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Topic :: Database :: Front-Ends
+Classifier: License :: Other/Proprietary License
+Requires-Python: >=3.4
+Description-Content-Type: text/markdown
+Requires-Dist: sqlalchemy (>=1.4.0)
+Requires-Dist: teradatasql (>=17.0.0.2)
+
+## Teradata SQL Driver Dialect for SQLAlchemy
+
+This package enables [SQLAlchemy](https://pypi.org/project/SQLAlchemy/) to connect to the Teradata Database.
+
+This package requires 64-bit Python 3.4 or later, and runs on Windows, macOS, and Linux. 32-bit Python is not supported.
+
+For community support, please visit the [Teradata Community forums](https://community.teradata.com/).
+
+For Teradata customer support, please visit [Teradata Access](https://access.teradata.com/).
+
+Copyright 2024 Teradata. All Rights Reserved.
+
+### Table of Contents
+
+* [Release Notes](#release-notes)
+* [Installation](#Installation)
+* [License](#License)
+* [Documentation](#Documentation)
+* [Using the Teradata SQL Driver Dialect for SQLAlchemy](#Using)
+* [Connection Parameters](#ConnectionParameters)
+
+## Release Notes:
+
+### teradatasqlalchemy 20.0.0.1
+- Minor bug fix for inline help.
+
+### teradatasqlalchemy 20.0.0.0
+- Added a utility to enable querybands for function calls in python packages. Any python client package can import queryband package and can configure its name and version to track the vantage usage with help of logs available with DBQlogTbl.
+
+### teradatasqlalchemy 17.20.0.0
+- Added support for SQLAlchemy >= 2.0
+- `div` operator is no more supported.
+
+### teradatasqlalchemy 17.0.0.5
+- Added xviews support for below APIs.:
+  - `get_columns`
+  - `get_foreign_keys`
+  - `get_pk_constraint`
+  - `get_schema_names`
+  - `get_transaction_mode`
+  - `get_unique_constraints`
+
+### teradatasqlalchemy 17.0.0.4
+- Added ART Table support for `get_columns` API.
+
+### teradatasqlalchemy 17.0.0.3
+- Resolved a minor bug.
+
+### teradatasqlalchemy 17.0.0.2
+- Added support for below Teradata's datatypes:
+  - JSON
+  - XML
+  - TDUDT  
+  - GEOMETRY
+  - MBR
+  - MBB
+- Optimised below APIs
+  - has_table
+  - has_view
+  - get_table_names
+  - get_view_names
+- Minor bug fixes.
+
+
+<a name="Installation"></a>
+
+### Installation
+
+Use pip to install the Teradata SQL Driver Dialect for SQLAlchemy.
+
+Platform       | Command
+-------------- | ---
+macOS or Linux | `pip install teradatasqlalchemy`
+Windows        | `py -3 -m pip install teradatasqlalchemy`
+
+When upgrading to a new version of the Teradata SQL Driver Dialect for SQLAlchemy, you may need to use pip install's `--no-cache-dir` option to force the download of the new version.
+
+Platform       | Command
+-------------- | ---
+macOS or Linux | `pip install --no-cache-dir -U teradatasqlalchemy`
+Windows        | `py -3 -m pip install --no-cache-dir -U teradatasqlalchemy`
+
+<a name="License"></a>
+
+### License
+
+Use of the Teradata SQL Driver Dialect for SQLAlchemy is governed by the *License Agreement for the Teradata SQL Driver Dialect for SQLAlchemy*.
+
+When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `LICENSE` file is placed in the `teradatasqlalchemy` directory under your Python installation directory.
+
+<a name="Documentation"></a>
+
+### Documentation
+
+When the Teradata SQL Driver Dialect for SQLAlchemy is installed, the `README.md` file is placed in the `teradatasqlalchemy` directory under your Python installation directory. This permits you to view the documentation offline, when you are not connected to the Internet.
+
+The `README.md` file is a plain text file containing the documentation for the Teradata SQL Driver Dialect for SQLAlchemy. While the file can be viewed with any text file viewer or editor, your viewing experience will be best with an editor that understands Markdown format.
+
+<a name="Using"></a>
+
+### Using the Teradata SQL Driver Dialect for SQLAlchemy
+
+Your Python script must import the `sqlalchemy` package in order to use the Teradata SQL Driver Dialect for SQLAlchemy.
+
+    import sqlalchemy
+
+After importing the `sqlalchemy` package, your Python script calls the `sqlalchemy.create_engine` function to open a connection to the Teradata Database.
+
+Specify the Teradata Database hostname as the *host* component of the URL. Note that COP Discovery is not implemented yet.
+
+The URL's *host* component may optionally be followed by a slash and question mark `/?` and the URL's *query* component consisting of connection parameters specified as *key*`=`*value* pairs separated by ampersand `&` characters.
+
+The username and password may be specified as a *host* prefix, or as connection URL parameters.
+
+Username and password specified as a *host* prefix:
+
+    eng = sqlalchemy.create_engine('teradatasql://guest:please@whomooz')
+
+Username and password specified as connection URL parameters:
+
+    eng = sqlalchemy.create_engine('teradatasql://whomooz/?user=guest&password=please')
+
+Username and password specified as connection URL parameters take precedence over a *host* prefix, if both are specified.
+
+<a name="ConnectionParameters"></a>
+
+### Connection Parameters
+
+The following table lists the connection parameters currently offered by the Teradata SQL Driver Dialect for SQLAlchemy.
+
+Our goal is consistency for the connection parameters offered by the Teradata SQL Driver Dialect for SQLAlchemy and the Teradata JDBC Driver, with respect to connection parameter names and functionality. For comparison, Teradata JDBC Driver connection parameters are [documented here](http://developer.teradata.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BGBHDDGB).
+
+Parameter          | Default   | Type    | Description
+------------------ | --------- | ------- | ---
+`account`          |           | string  | Specifies the Teradata Database account. Equivalent to the Teradata JDBC Driver `ACCOUNT` connection parameter.
+`column_name`      | `false`   | boolean | Controls the behavior of cursor `.description` sequence `name` items. Equivalent to the Teradata JDBC Driver `COLUMN_NAME` connection parameter. False specifies that a cursor `.description` sequence `name` item provides the AS-clause name if available, or the column name if available, or the column title. True specifies that a cursor `.description` sequence `name` item provides the column name if available, but has no effect when StatementInfo parcel support is unavailable.
+`cop`              | `true`    | boolean | Specifies whether COP Discovery is performed. Equivalent to the Teradata JDBC Driver `COP` connection parameter.
+`coplast`          | `false`   | boolean | Specifies how COP Discovery determines the last COP hostname. Equivalent to the Teradata JDBC Driver `COPLAST` connection parameter. When `coplast` is `false` or omitted, or COP Discovery is turned off, then no DNS lookup occurs for the coplast hostname. When `coplast` is `true`, and COP Discovery is turned on, then a DNS lookup occurs for a coplast hostname.
+`database`         |           | string  | Specifies the initial database to use after logon, instead of the user's default database. Equivalent to the Teradata JDBC Driver `DATABASE` connection parameter.
+`dbs_port`         | `1025`    | integer | Specifies Teradata Database port number. Equivalent to the Teradata JDBC Driver `DBS_PORT` connection parameter.
+`encryptdata`      | `false`   | boolean | Controls encryption of data exchanged between the Teradata Database and the Teradata SQL Driver for Python. Equivalent to the Teradata JDBC Driver `ENCRYPTDATA` connection parameter.
+`fake_result_sets` | `false`   | boolean | Controls whether a fake result set containing statement metadata precedes each real result set.
+`lob_support`      | `true`    | boolean | Controls LOB support. Equivalent to the Teradata JDBC Driver `LOB_SUPPORT` connection parameter.
+`log`              | `0`       | integer | Controls debug logging. Somewhat equivalent to the Teradata JDBC Driver `LOG` connection parameter. This parameter's behavior is subject to change in the future. This parameter's value is currently defined as an integer in which the 1-bit governs function and method tracing, the 2-bit governs debug logging, the 4-bit governs transmit and receive message hex dumps, and the 8-bit governs timing. Compose the value by adding together 1, 2, 4, and/or 8.
+`logdata`          |           | string  | Specifies extra data for the chosen logon authentication method. Equivalent to the Teradata JDBC Driver `LOGDATA` connection parameter.
+`logmech`          | `TD2`     | string  | Specifies the logon authentication method. Equivalent to the Teradata JDBC Driver `LOGMECH` connection parameter. Possible values are `TD2` (the default), `JWT`, `LDAP`, `KRB5` for Kerberos, or `TDNEGO`.
+`max_message_body` | `2097000` | integer | Not fully implemented yet and intended for future usage. Equivalent to the Teradata JDBC Driver `MAX_MESSAGE_BODY` connection parameter.
+`partition`        | `DBC/SQL` | string  | Specifies the Teradata Database Partition. Equivalent to the Teradata JDBC Driver `PARTITION` connection parameter.
+`password`         |           | string  | Specifies the Teradata Database password. Equivalent to the Teradata JDBC Driver `PASSWORD` connection parameter.
+`sip_support`      | `true`    | boolean | Controls whether StatementInfo parcel is used. Equivalent to the Teradata JDBC Driver `SIP_SUPPORT` connection parameter.
+`teradata_values`  | `true`    | boolean | Controls whether `str` or a more specific Python data type is used for certain result set column value types.
+`tmode`            | `DEFAULT` | string  | Specifies the transaction mode. Equivalent to the Teradata JDBC Driver `TMODE` connection parameter. Possible values are `DEFAULT` (the default), `ANSI`, or `TERA`.
+`user`             |           | string  | Specifies the Teradata Database username. Equivalent to the Teradata JDBC Driver `USER` connection parameter.
+
+
```


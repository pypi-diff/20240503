# Comparing `tmp/macls-0.4.3-py3-none-any.whl.zip` & `tmp/macls-0.4.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,33 @@
-Zip file size: 53056 bytes, number of entries: 31
--rw-rw-rw-  2.0 fat      196 b- defN 24-Apr-27 06:46 macls/__init__.py
--rw-rw-rw-  2.0 fat     9557 b- defN 23-Oct-24 13:59 macls/predict.py
--rw-rw-rw-  2.0 fat    28617 b- defN 24-Apr-21 10:35 macls/trainer.py
+Zip file size: 53989 bytes, number of entries: 31
+-rw-rw-rw-  2.0 fat      196 b- defN 24-May-03 03:45 macls/__init__.py
+-rw-rw-rw-  2.0 fat     9374 b- defN 24-May-02 04:59 macls/predict.py
+-rw-rw-rw-  2.0 fat    30195 b- defN 24-May-02 05:50 macls/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 13:17 macls/data_utils/__init__.py
 -rw-rw-rw-  2.0 fat    22218 b- defN 24-Apr-27 05:31 macls/data_utils/audio.py
--rw-rw-rw-  2.0 fat      965 b- defN 23-Feb-15 12:59 macls/data_utils/collate_fn.py
--rw-rw-rw-  2.0 fat     3587 b- defN 24-Jan-23 10:26 macls/data_utils/featurizer.py
--rw-rw-rw-  2.0 fat     5630 b- defN 23-Sep-13 12:03 macls/data_utils/reader.py
+-rw-rw-rw-  2.0 fat      930 b- defN 24-May-01 14:06 macls/data_utils/collate_fn.py
+-rw-rw-rw-  2.0 fat     3696 b- defN 24-May-02 04:47 macls/data_utils/featurizer.py
+-rw-rw-rw-  2.0 fat     7214 b- defN 24-May-02 05:50 macls/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     1572 b- defN 23-Aug-07 14:52 macls/data_utils/spec_aug.py
 -rw-rw-rw-  2.0 fat     5712 b- defN 24-Jan-28 03:59 macls/data_utils/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-16 05:03 macls/metric/__init__.py
 -rw-rw-rw-  2.0 fat      329 b- defN 23-Aug-07 15:15 macls/metric/metrics.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 13:17 macls/models/__init__.py
--rw-rw-rw-  2.0 fat    13608 b- defN 23-Aug-30 14:35 macls/models/campplus.py
+-rw-rw-rw-  2.0 fat    13608 b- defN 24-May-01 12:18 macls/models/campplus.py
 -rw-rw-rw-  2.0 fat     5375 b- defN 23-Oct-24 13:59 macls/models/ecapa_tdnn.py
 -rw-rw-rw-  2.0 fat    10504 b- defN 23-Aug-10 10:25 macls/models/eres2net.py
 -rw-rw-rw-  2.0 fat     9707 b- defN 23-Aug-30 14:43 macls/models/panns.py
 -rw-rw-rw-  2.0 fat     3789 b- defN 23-Aug-10 10:25 macls/models/pooling.py
 -rw-rw-rw-  2.0 fat     6776 b- defN 23-Aug-09 09:51 macls/models/res2net.py
 -rw-rw-rw-  2.0 fat     5609 b- defN 23-Aug-09 09:51 macls/models/resnet_se.py
 -rw-rw-rw-  2.0 fat     3050 b- defN 23-Mar-21 11:25 macls/models/tdnn.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 13:17 macls/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2839 b- defN 23-Jan-30 13:17 macls/utils/logger.py
 -rw-rw-rw-  2.0 fat     1067 b- defN 23-Mar-23 11:45 macls/utils/record.py
 -rw-rw-rw-  2.0 fat     1496 b- defN 23-Aug-06 15:19 macls/utils/scheduler.py
 -rw-rw-rw-  2.0 fat     4300 b- defN 24-Apr-27 04:48 macls/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    21616 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2455 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/RECORD
-31 files, 182230 bytes uncompressed, 49166 bytes compressed:  73.0%
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-May-03 03:46 macls-0.4.4.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    22574 b- defN 24-May-03 03:46 macls-0.4.4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-03 03:46 macls-0.4.4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 24-May-03 03:46 macls-0.4.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2455 b- defN 24-May-03 03:46 macls-0.4.4.dist-info/RECORD
+31 files, 186241 bytes uncompressed, 50099 bytes compressed:  73.1%
```

## zipnote {}

```diff
@@ -72,23 +72,23 @@
 
 Filename: macls/utils/scheduler.py
 Comment: 
 
 Filename: macls/utils/utils.py
 Comment: 
 
-Filename: macls-0.4.3.dist-info/LICENSE
+Filename: macls-0.4.4.dist-info/LICENSE
 Comment: 
 
-Filename: macls-0.4.3.dist-info/METADATA
+Filename: macls-0.4.4.dist-info/METADATA
 Comment: 
 
-Filename: macls-0.4.3.dist-info/WHEEL
+Filename: macls-0.4.4.dist-info/WHEEL
 Comment: 
 
-Filename: macls-0.4.3.dist-info/top_level.txt
+Filename: macls-0.4.4.dist-info/top_level.txt
 Comment: 
 
-Filename: macls-0.4.3.dist-info/RECORD
+Filename: macls-0.4.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## macls/__init__.py

```diff
@@ -1,4 +1,4 @@
-__version__ = "0.4.3"
+__version__ = "0.4.4"
 # 项目支持的模型
 SUPPORT_MODEL = ['EcapaTdnn', 'PANNS_CNN6', 'PANNS_CNN10', 'PANNS_CNN14', 'Res2Net', 'ResNetSE', 'TDNN', 'ERes2Net',
                  'CAMPPlus']
```

## macls/predict.py

```diff
@@ -44,16 +44,15 @@
             with open(configs, 'r', encoding='utf-8') as f:
                 configs = yaml.load(f.read(), Loader=yaml.FullLoader)
             print_arguments(configs=configs)
         self.configs = dict_to_object(configs)
         assert self.configs.use_model in SUPPORT_MODEL, f'没有该模型：{self.configs.use_model}'
         # 获取特征器
         self._audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
-                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
-        self._audio_featurizer.to(self.device)
+                                                 method_args=self.configs.preprocess_conf.get('method_args', {}))
         # 获取分类标签
         with open(self.configs.dataset_conf.label_list_path, 'r', encoding='utf-8') as f:
             lines = f.readlines()
         self.class_labels = [l.replace('\n', '') for l in lines]
         # 自动获取列表数量
         if self.configs.model_conf.num_class is None:
             self.configs.model_conf.num_class = len(self.class_labels)
@@ -126,17 +125,16 @@
 
         :param audio_data: 需要识别的数据，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整并带格式的字节文件
         :param sample_rate: 如果传入的事numpy数据，需要指定采样率
         :return: 结果标签和对应的得分
         """
         # 加载音频文件，并进行预处理
         input_data = self._load_audio(audio_data=audio_data, sample_rate=sample_rate)
-        input_data = torch.tensor(input_data.samples, dtype=torch.float32, device=self.device).unsqueeze(0)
-        input_len_ratio = torch.tensor([1], dtype=torch.float32, device=self.device)
-        audio_feature, _ = self._audio_featurizer(input_data, input_len_ratio)
+        input_data = torch.tensor(input_data.samples, dtype=torch.float32).unsqueeze(0)
+        audio_feature = self._audio_featurizer(input_data).to(self.device)
         # 执行预测
         output = self.predictor(audio_feature)
         result = torch.nn.functional.softmax(output, dim=-1)[0]
         result = result.data.cpu().numpy()
         # 最大概率的label
         lab = np.argsort(result)[-1]
         score = result[lab]
@@ -155,25 +153,25 @@
             input_data = self._load_audio(audio_data=audio_data, sample_rate=sample_rate)
             audios_data1.append(input_data.samples)
         # 找出音频长度最长的
         batch = sorted(audios_data1, key=lambda a: a.shape[0], reverse=True)
         max_audio_length = batch[0].shape[0]
         batch_size = len(batch)
         # 以最大的长度创建0张量
-        inputs = np.zeros((batch_size, max_audio_length), dtype='float32')
+        inputs = np.zeros((batch_size, max_audio_length), dtype=np.float32)
         input_lens_ratio = []
         for x in range(batch_size):
             tensor = audios_data1[x]
             seq_length = tensor.shape[0]
             # 将数据插入都0张量中，实现了padding
             inputs[x, :seq_length] = tensor[:]
             input_lens_ratio.append(seq_length / max_audio_length)
-        input_lens_ratio = torch.tensor(input_lens_ratio, dtype=torch.float32, device=self.device)
-        inputs = torch.tensor(inputs, dtype=torch.float32, device=self.device)
-        audio_feature, _ = self._audio_featurizer(inputs, input_lens_ratio)
+        inputs = torch.tensor(inputs, dtype=torch.float32)
+        input_lens_ratio = torch.tensor(input_lens_ratio, dtype=torch.float32)
+        audio_feature = self._audio_featurizer(inputs, input_lens_ratio).to(self.device)
         # 执行预测
         output = self.predictor(audio_feature)
         results = torch.nn.functional.softmax(output, dim=-1)
         results = results.data.cpu().numpy()
         labels, scores = [], []
         for result in results:
             lab = np.argsort(result)[-1]
```

## macls/trainer.py

```diff
@@ -1,31 +1,30 @@
-import io
 import json
 import os
 import platform
 import shutil
 import time
 from datetime import timedelta
 
 import numpy as np
 import torch
 import torch.distributed as dist
 import yaml
 from sklearn.metrics import confusion_matrix
-from torch.utils.data.distributed import DistributedSampler
 from torch.optim.lr_scheduler import CosineAnnealingLR
 from torch.utils.data import DataLoader
+from torch.utils.data.distributed import DistributedSampler
 from torchinfo import summary
 from tqdm import tqdm
 from visualdl import LogWriter
 
 from macls import SUPPORT_MODEL, __version__
 from macls.data_utils.collate_fn import collate_fn
 from macls.data_utils.featurizer import AudioFeaturizer
-from macls.data_utils.reader import CustomDataset
+from macls.data_utils.reader import MAClsDataset
 from macls.data_utils.spec_aug import SpecAug
 from macls.metric.metrics import accuracy
 from macls.models.campplus import CAMPPlus
 from macls.models.ecapa_tdnn import EcapaTdnn
 from macls.models.eres2net import ERes2Net
 from macls.models.panns import PANNS_CNN6, PANNS_CNN10, PANNS_CNN14
 from macls.models.res2net import Res2Net
@@ -56,73 +55,103 @@
         if isinstance(configs, str):
             with open(configs, 'r', encoding='utf-8') as f:
                 configs = yaml.load(f.read(), Loader=yaml.FullLoader)
             print_arguments(configs=configs)
         self.configs = dict_to_object(configs)
         assert self.configs.use_model in SUPPORT_MODEL, f'没有该模型：{self.configs.use_model}'
         self.model = None
+        self.audio_featurizer = None
+        self.train_dataset = None
+        self.train_loader = None
+        self.test_dataset = None
         self.test_loader = None
         self.amp_scaler = None
         # 获取分类标签
         with open(self.configs.dataset_conf.label_list_path, 'r', encoding='utf-8') as f:
             lines = f.readlines()
         self.class_labels = [l.replace('\n', '') for l in lines]
         if platform.system().lower() == 'windows':
             self.configs.dataset_conf.dataLoader.num_workers = 0
             logger.warning('Windows系统不支持多线程读取数据，已自动关闭！')
-        # 获取特征器
-        self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
-                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
-        self.audio_featurizer.to(self.device)
         # 特征增强
         self.spec_aug = SpecAug(**self.configs.dataset_conf.get('spec_aug_args', {}))
         self.spec_aug.to(self.device)
         self.max_step, self.train_step = None, None
         self.train_loss, self.train_acc = None, None
         self.train_eta_sec = None
         self.eval_loss, self.eval_acc = None, None
         self.test_log_step, self.train_log_step = 0, 0
         self.stop_train, self.stop_eval = False, False
 
     def __setup_dataloader(self, is_train=False):
+        # 获取特征器
+        self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
+                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
         if is_train:
-            self.train_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.train_list,
-                                               do_vad=self.configs.dataset_conf.do_vad,
-                                               max_duration=self.configs.dataset_conf.max_duration,
-                                               min_duration=self.configs.dataset_conf.min_duration,
-                                               aug_conf=self.configs.dataset_conf.aug_conf,
-                                               sample_rate=self.configs.dataset_conf.sample_rate,
-                                               use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
-                                               target_dB=self.configs.dataset_conf.target_dB,
-                                               mode='train')
+            self.train_dataset = MAClsDataset(data_list_path=self.configs.dataset_conf.train_list,
+                                              audio_featurizer=self.audio_featurizer,
+                                              do_vad=self.configs.dataset_conf.do_vad,
+                                              max_duration=self.configs.dataset_conf.max_duration,
+                                              min_duration=self.configs.dataset_conf.min_duration,
+                                              aug_conf=self.configs.dataset_conf.aug_conf,
+                                              sample_rate=self.configs.dataset_conf.sample_rate,
+                                              use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                              target_dB=self.configs.dataset_conf.target_dB,
+                                              mode='train')
             # 设置支持多卡训练
             train_sampler = None
             if torch.cuda.device_count() > 1:
                 # 设置支持多卡训练
                 train_sampler = DistributedSampler(dataset=self.train_dataset)
             self.train_loader = DataLoader(dataset=self.train_dataset,
                                            collate_fn=collate_fn,
                                            shuffle=(train_sampler is None),
                                            sampler=train_sampler,
                                            **self.configs.dataset_conf.dataLoader)
         # 获取测试数据
-        self.test_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.test_list,
-                                          do_vad=self.configs.dataset_conf.do_vad,
-                                          max_duration=self.configs.dataset_conf.eval_conf.max_duration,
-                                          min_duration=self.configs.dataset_conf.min_duration,
-                                          sample_rate=self.configs.dataset_conf.sample_rate,
-                                          use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
-                                          target_dB=self.configs.dataset_conf.target_dB,
-                                          mode='eval')
+        self.test_dataset = MAClsDataset(data_list_path=self.configs.dataset_conf.test_list,
+                                         audio_featurizer=self.audio_featurizer,
+                                         do_vad=self.configs.dataset_conf.do_vad,
+                                         max_duration=self.configs.dataset_conf.eval_conf.max_duration,
+                                         min_duration=self.configs.dataset_conf.min_duration,
+                                         sample_rate=self.configs.dataset_conf.sample_rate,
+                                         use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                         target_dB=self.configs.dataset_conf.target_dB,
+                                         mode='eval')
         self.test_loader = DataLoader(dataset=self.test_dataset,
                                       collate_fn=collate_fn,
                                       shuffle=True,
                                       batch_size=self.configs.dataset_conf.eval_conf.batch_size,
                                       num_workers=self.configs.dataset_conf.dataLoader.num_workers)
 
+    # 提取特征保存文件
+    def extract_features(self, save_dir='dataset/features'):
+        self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
+                                                method_args=self.configs.preprocess_conf.get('method_args', {}))
+        for i, data_list in enumerate([self.configs.dataset_conf.train_list, self.configs.dataset_conf.test_list]):
+            # 获取测试数据
+            test_dataset = MAClsDataset(data_list_path=data_list,
+                                        audio_featurizer=self.audio_featurizer,
+                                        do_vad=self.configs.dataset_conf.do_vad,
+                                        sample_rate=self.configs.dataset_conf.sample_rate,
+                                        use_dB_normalization=self.configs.dataset_conf.use_dB_normalization,
+                                        target_dB=self.configs.dataset_conf.target_dB,
+                                        mode='extract_feature')
+            save_data_list = data_list.replace('.txt', '_features.txt')
+            with open(save_data_list, 'w', encoding='utf-8') as f:
+                for i in tqdm(range(len(test_dataset))):
+                    feature, label = test_dataset[i]
+                    feature = feature.numpy()
+                    label = int(label)
+                    save_path = os.path.join(save_dir, str(label), f'{int(time.time() * 1000)}.npy').replace('\\', '/')
+                    os.makedirs(os.path.dirname(save_path), exist_ok=True)
+                    np.save(save_path, feature)
+                    f.write(f'{save_path}\t{label}\n')
+            logger.info(f'{data_list}列表中的数据已提取特征完成，新列表为：{save_data_list}')
+
     def __setup_model(self, input_size, is_train=False):
         # 自动获取列表数量
         if self.configs.model_conf.num_class is None:
             self.configs.model_conf.num_class = len(self.class_labels)
         # 获取模型
         if self.configs.use_model == 'EcapaTdnn':
             self.model = EcapaTdnn(input_size=input_size, **self.configs.model_conf)
@@ -141,16 +170,15 @@
         elif self.configs.use_model == 'ERes2Net':
             self.model = ERes2Net(input_size=input_size, **self.configs.model_conf)
         elif self.configs.use_model == 'CAMPPlus':
             self.model = CAMPPlus(input_size=input_size, **self.configs.model_conf)
         else:
             raise Exception(f'{self.configs.use_model} 模型不存在！')
         self.model.to(self.device)
-        self.audio_featurizer.to(self.device)
-        summary(self.model, input_size=(1, 98, self.audio_featurizer.feature_dim))
+        summary(self.model, input_size=(1, 98, input_size))
         # 使用Pytorch2.0的编译器
         if self.configs.train_conf.use_compile and torch.__version__ >= "2" and platform.system().lower() == 'windows':
             self.model = torch.compile(self.model, mode="reduce-overhead")
         # print(self.model)
         # 获取损失函数
         weight = torch.tensor(self.configs.train_conf.loss_weight, dtype=torch.float, device=self.device)\
             if self.configs.train_conf.loss_weight is not None else None
@@ -285,25 +313,22 @@
             if os.path.exists(old_model_path):
                 shutil.rmtree(old_model_path)
         logger.info('已保存模型：{}'.format(model_path))
 
     def __train_epoch(self, epoch_id, local_rank, writer, nranks=0):
         train_times, accuracies, loss_sum = [], [], []
         start = time.time()
-        for batch_id, (audio, label, input_lens_ratio) in enumerate(self.train_loader):
+        for batch_id, (features, label, input_len) in enumerate(self.train_loader):
             if self.stop_train: break
             if nranks > 1:
-                audio = audio.to(local_rank)
-                input_lens_ratio = input_lens_ratio.to(local_rank)
+                features = features.to(local_rank)
                 label = label.to(local_rank).long()
             else:
-                audio = audio.to(self.device)
-                input_lens_ratio = input_lens_ratio.to(self.device)
+                features = features.to(self.device)
                 label = label.to(self.device).long()
-            features, _ = self.audio_featurizer(audio, input_lens_ratio)
             # 特征增强
             if self.configs.dataset_conf.use_spec_aug:
                 features = self.spec_aug(features)
             # 执行模型计算，是否开启自动混合精度
             with torch.cuda.amp.autocast(enabled=self.configs.train_conf.enable_amp):
                 output = self.model(features)
             # 计算损失值
@@ -382,15 +407,14 @@
         self.__setup_dataloader(is_train=True)
         # 获取模型
         self.__setup_model(input_size=self.audio_featurizer.feature_dim, is_train=True)
 
         # 支持多卡训练
         if nranks > 1 and self.use_gpu:
             self.model.to(local_rank)
-            self.audio_featurizer.to(local_rank)
             self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=[local_rank])
         logger.info('训练数据：{}'.format(len(self.train_dataset)))
 
         self.__load_pretrained(pretrained_model=pretrained_model)
         # 加载恢复模型
         last_epoch, best_acc = self.__load_checkpoint(save_model_path=save_model_path, resume_model=resume_model)
 
@@ -452,20 +476,18 @@
         if isinstance(self.model, torch.nn.parallel.DistributedDataParallel):
             eval_model = self.model.module
         else:
             eval_model = self.model
 
         accuracies, losses, preds, labels = [], [], [], []
         with torch.no_grad():
-            for batch_id, (audio, label, input_lens_ratio) in enumerate(tqdm(self.test_loader)):
+            for batch_id, (features, label, input_lens) in enumerate(tqdm(self.test_loader)):
                 if self.stop_eval: break
-                audio = audio.to(self.device)
-                input_lens_ratio = input_lens_ratio.to(self.device)
+                features = features.to(self.device)
                 label = label.to(self.device).long()
-                features, _ = self.audio_featurizer(audio, input_lens_ratio)
                 output = eval_model(features)
                 los = self.loss(output, label)
                 # 计算准确率
                 acc = accuracy(output, label)
                 accuracies.append(acc)
                 # 模型预测标签
                 label = label.data.cpu().numpy()
```

## macls/data_utils/collate_fn.py

```diff
@@ -1,25 +1,23 @@
-import numpy as np
 import torch
 
 
 # 对一个batch的数据处理
 def collate_fn(batch):
     # 找出音频长度最长的
-    batch = sorted(batch, key=lambda sample: sample[0].shape[0], reverse=True)
-    max_audio_length = batch[0][0].shape[0]
-    batch_size = len(batch)
+    batch_sorted = sorted(batch, key=lambda sample: sample[0].size(0), reverse=True)
+    freq_size = batch_sorted[0][0].size(1)
+    max_freq_length = batch_sorted[0][0].size(0)
+    batch_size = len(batch_sorted)
     # 以最大的长度创建0张量
-    inputs = np.zeros((batch_size, max_audio_length), dtype='float32')
-    input_lens_ratio = []
-    labels = []
+    features = torch.zeros((batch_size, max_freq_length, freq_size), dtype=torch.float32)
+    input_lens, labels = [], []
     for x in range(batch_size):
-        sample = batch[x]
-        tensor = sample[0]
-        labels.append(sample[1])
-        seq_length = tensor.shape[0]
+        tensor, label = batch[x]
+        seq_length = tensor.size(0)
         # 将数据插入都0张量中，实现了padding
-        inputs[x, :seq_length] = tensor[:]
-        input_lens_ratio.append(seq_length/max_audio_length)
-    input_lens_ratio = np.array(input_lens_ratio, dtype='float32')
-    labels = np.array(labels, dtype='int64')
-    return torch.tensor(inputs), torch.tensor(labels), torch.tensor(input_lens_ratio)
+        features[x, :seq_length, :] = tensor[:, :]
+        labels.append(label)
+        input_lens.append(seq_length)
+    labels = torch.tensor(labels, dtype=torch.int64)
+    input_lens = torch.tensor(input_lens, dtype=torch.int64)
+    return features, labels, input_lens
```

## macls/data_utils/featurizer.py

```diff
@@ -1,10 +1,10 @@
 import torch
-from torch import nn
 import torchaudio.compliance.kaldi as Kaldi
+from torch import nn
 from torchaudio.transforms import MelSpectrogram, Spectrogram, MFCC
 
 
 class AudioFeaturizer(nn.Module):
     """音频特征器
 
     :param feature_method: 所使用的预处理方法
@@ -24,40 +24,42 @@
         elif feature_method == 'MFCC':
             self.feat_fun = MFCC(**method_args)
         elif feature_method == 'Fbank':
             self.feat_fun = KaldiFbank(**method_args)
         else:
             raise Exception(f'预处理方法 {self._feature_method} 不存在!')
 
-    def forward(self, waveforms, input_lens_ratio):
+    def forward(self, waveforms, input_lens_ratio=None):
         """从AudioSegment中提取音频特征
 
         :param waveforms: Audio segment to extract features from.
         :type waveforms: AudioSegment
         :param input_lens_ratio: input length ratio
         :type input_lens_ratio: tensor
         :return: Spectrogram audio feature in 2darray.
         :rtype: ndarray
         """
+        if len(waveforms.shape) == 1:
+            waveforms = waveforms.unsqueeze(0)
         feature = self.feat_fun(waveforms)
         feature = feature.transpose(2, 1)
         # 归一化
         feature = feature - feature.mean(1, keepdim=True)
-        # 对掩码比例进行扩展
-        input_lens = (input_lens_ratio * feature.shape[1])
-        mask_lens = torch.round(input_lens).long()
-        mask_lens = mask_lens.unsqueeze(1)
-        input_lens = input_lens.int()
-        # 生成掩码张量
-        idxs = torch.arange(feature.shape[1], device=feature.device).repeat(feature.shape[0], 1)
-        mask = idxs < mask_lens
-        mask = mask.unsqueeze(-1)
-        # 对特征进行掩码操作
-        feature_masked = torch.where(mask, feature, torch.zeros_like(feature))
-        return feature_masked, input_lens
+        if input_lens_ratio is not None:
+            # 对掩码比例进行扩展
+            input_lens = (input_lens_ratio * feature.shape[1])
+            mask_lens = torch.round(input_lens).long()
+            mask_lens = mask_lens.unsqueeze(1)
+            # 生成掩码张量
+            idxs = torch.arange(feature.shape[1], device=feature.device).repeat(feature.shape[0], 1)
+            mask = idxs < mask_lens
+            mask = mask.unsqueeze(-1)
+            # 对特征进行掩码操作
+            feature = torch.where(mask, feature, torch.zeros_like(feature))
+        return feature
 
     @property
     def feature_dim(self):
         """返回特征大小
 
         :return: 特征大小
         :rtype: int
```

## macls/data_utils/reader.py

```diff
@@ -1,88 +1,117 @@
 import os
 import random
 
 import numpy as np
+import torch
 from torch.utils.data import Dataset
 
 from macls.data_utils.audio import AudioSegment
+from macls.data_utils.featurizer import AudioFeaturizer
 from macls.utils.logger import setup_logger
 
 logger = setup_logger(__name__)
 
 
-class CustomDataset(Dataset):
+class MAClsDataset(Dataset):
     def __init__(self,
                  data_list_path,
+                 audio_featurizer: AudioFeaturizer,
                  do_vad=True,
                  max_duration=3,
                  min_duration=0.5,
                  mode='train',
                  sample_rate=16000,
                  aug_conf={},
                  num_speakers=1000,
                  use_dB_normalization=True,
                  target_dB=-20):
         """音频数据加载器
 
         Args:
             data_list_path: 包含音频路径和标签的数据列表文件的路径
+            audio_featurizer: 声纹特征提取器
             do_vad: 是否对音频进行语音活动检测（VAD）来裁剪静音部分
             max_duration: 最长的音频长度，大于这个长度会裁剪掉
             min_duration: 过滤最短的音频长度
             aug_conf: 用于指定音频增强的配置
             mode: 数据集模式。在训练模式下，数据集可能会进行一些数据增强的预处理
             sample_rate: 采样率
             num_speakers: 总说话人数量
             use_dB_normalization: 是否对音频进行音量归一化
             target_dB: 音量归一化的大小
         """
-        super(CustomDataset, self).__init__()
+        super(MAClsDataset, self).__init__()
+        assert mode in ['train', 'eval', 'create_data', 'extract_feature']
         self.do_vad = do_vad
         self.max_duration = max_duration
         self.min_duration = min_duration
         self.mode = mode
         self._target_sample_rate = sample_rate
         self._use_dB_normalization = use_dB_normalization
         self._target_dB = target_dB
         self.aug_conf = aug_conf
         self.num_speakers = num_speakers
         self.noises_path = None
+        # 获取特征器
+        self.audio_featurizer = audio_featurizer
+        # 获取特征裁剪的大小
+        self.max_feature_len = self.get_crop_feature_len()
         # 获取数据列表
         with open(data_list_path, 'r', encoding='utf-8') as f:
             self.lines = f.readlines()
 
     def __getitem__(self, idx):
-        # 分割音频路径和标签
-        audio_path, label = self.lines[idx].strip().split('\t')
-        # 读取音频
-        audio_segment = AudioSegment.from_file(audio_path)
-        # 裁剪静音
-        if self.do_vad:
-            audio_segment.vad()
-        # 数据太短不利于训练
-        if self.mode == 'train':
-            if audio_segment.duration < self.min_duration:
-                return self.__getitem__(idx + 1 if idx < len(self.lines) - 1 else 0)
-        # 重采样
-        if audio_segment.sample_rate != self._target_sample_rate:
-            audio_segment.resample(self._target_sample_rate)
-        # 音频增强
-        if self.mode == 'train':
-            audio_segment = self.augment_audio(audio_segment, **self.aug_conf)
-        # decibel normalization
-        if self._use_dB_normalization:
-            audio_segment.normalize(target_db=self._target_dB)
-        # 裁剪需要的数据
-        audio_segment.crop(duration=self.max_duration, mode=self.mode)
-        return np.array(audio_segment.samples, dtype=np.float32), np.array(int(label), dtype=np.int64)
+        # 分割数据文件路径和标签
+        data_path, label = self.lines[idx].replace('\n', '').split('\t')
+        # 如果后缀名为.npy的文件，那么直接读取
+        if data_path.endswith('.npy'):
+            feature = np.load(data_path)
+            if feature.shape[0] > self.max_feature_len:
+                crop_start = random.randint(0, feature.shape[0] - self.max_feature_len) if self.mode == 'eval' else 0
+                feature = feature[crop_start:crop_start + self.max_feature_len, :]
+            feature = torch.tensor(feature, dtype=torch.float32)
+        else:
+            audio_path, label = self.lines[idx].strip().split('\t')
+            # 读取音频
+            audio_segment = AudioSegment.from_file(audio_path)
+            # 裁剪静音
+            if self.do_vad:
+                audio_segment.vad()
+            # 数据太短不利于训练
+            if self.mode == 'train':
+                if audio_segment.duration < self.min_duration:
+                    return self.__getitem__(idx + 1 if idx < len(self.lines) - 1 else 0)
+            # 重采样
+            if audio_segment.sample_rate != self._target_sample_rate:
+                audio_segment.resample(self._target_sample_rate)
+            # 音频增强
+            if self.mode == 'train':
+                audio_segment = self.augment_audio(audio_segment, **self.aug_conf)
+            # decibel normalization
+            if self._use_dB_normalization:
+                audio_segment.normalize(target_db=self._target_dB)
+            # 裁剪需要的数据
+            if self.mode != 'extract_feature' and audio_segment.duration > self.max_duration:
+                audio_segment.crop(duration=self.max_duration, mode=self.mode)
+            samples = torch.tensor(audio_segment.samples, dtype=torch.float32)
+            feature = self.audio_featurizer(samples)
+            feature = feature.squeeze(0)
+        label = torch.tensor(int(label), dtype=torch.int64)
+        return feature, label
 
     def __len__(self):
         return len(self.lines)
 
+    def get_crop_feature_len(self):
+        samples = torch.randn((1, self.max_duration * self._target_sample_rate))
+        feature = self.audio_featurizer(samples).squeeze(0)
+        freq_len = feature.size(0)
+        return freq_len
+
     # 音频增强
     def augment_audio(self,
                       audio_segment,
                       speed_perturb=False,
                       volume_perturb=False,
                       volume_aug_prob=0.2,
                       noise_dir=None,
```

## Comparing `macls-0.4.3.dist-info/LICENSE` & `macls-0.4.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `macls-0.4.3.dist-info/METADATA` & `macls-0.4.4.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: macls
-Version: 0.4.3
+Version: 0.4.4
 Summary: Audio Classification toolkit on Pytorch
 Home-page: https://github.com/yeyupiaoling/AudioClassification-Pytorch
 Download-URL: https://github.com/yeyupiaoling/AudioClassification-Pytorch.git
 Author: yeyupiaoling
 License: Apache License 2.0
 Keywords: audio,pytorch
 Classifier: Intended Audience :: Developers
@@ -135,15 +135,15 @@
 ```shell
 dataset/UrbanSound8K/audio/fold2/104817-4-0-2.wav	4
 dataset/UrbanSound8K/audio/fold9/105029-7-2-5.wav	7
 dataset/UrbanSound8K/audio/fold3/107228-5-0-0.wav	5
 dataset/UrbanSound8K/audio/fold4/109711-3-2-4.wav	3
 ```
 
-# 修改预处理方法
+# 修改预处理方法（可选）
 
 配置文件中默认使用的是MelSpectrogram预处理方法，如果要使用其他预处理方法，可以修改配置文件中的安装下面方式修改，具体的值可以根据自己情况修改。如果不清楚如何设置参数，可以直接删除该部分，直接使用默认值。
 
 ```yaml
 preprocess_conf:
   # 音频预处理方法，支持：MelSpectrogram、Spectrogram、MFCC、Fbank
   feature_method: 'MelSpectrogram'
@@ -154,14 +154,27 @@
     hop_length: 320
     win_length: 1024
     f_min: 50.0
     f_max: 14000.0
     n_mels: 64
 ```
 
+# 提取特征（可选）
+
+在训练过程中，首先是要读取音频数据，然后提取特征，最后再进行训练。其中读取音频数据、提取特征也是比较消耗时间的，所以我们可以选择提前提取好取特征，训练模型的是就可以直接加载提取好的特征，这样训练速度会更快。这个提取特征是可选择，如果没有提取好的特征，训练模型的时候就会从读取音频数据，然后提取特征开始。提取特征步骤如下：
+
+1. 执行`extract_features.py`，提取特征，特征会保存在`dataset/features`目录下，并生成新的数据列表`train_list_features.txt`和`test_list_features.txt`。
+
+```shell
+python extract_features.py --configs=configs/cam++.yml --save_dir=dataset/features
+```
+
+2. 修改配置文件，将`dataset_conf.train_list`和`dataset_conf.test_list`修改为`train_list_features.txt`和`test_list_features.txt`。
+
+
 ## 训练
 
 接着就可以开始训练模型了，创建 `train.py`。配置文件里面的参数一般不需要修改，但是这几个是需要根据自己实际的数据集进行调整的，首先最重要的就是分类大小`dataset_conf.num_class`，这个每个数据集的分类大小可能不一样，根据自己的实际情况设定。然后是`dataset_conf.batch_size`，如果是显存不够的话，可以减小这个参数。
 
 ```shell
 # 单卡训练
 CUDA_VISIBLE_DEVICES=0 python train.py
```

## Comparing `macls-0.4.3.dist-info/RECORD` & `macls-0.4.4.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-macls/__init__.py,sha256=iUVruvBi_BFZRV6LsDpD2SIswAjsmjVViarSBK9R6C0,196
-macls/predict.py,sha256=tFBFj0HhxWY0_ep8rpH5Srzxb7miA8iDS2rhKrSHb2c,9557
-macls/trainer.py,sha256=aEep-Qgz218_-CTY-IJiCTvEw__aZ23LXlTH77ddw08,28617
+macls/__init__.py,sha256=hJm_b0gqR9Bo1fgpUjzLAoiKn6FBZwVQ10pBedhQ31E,196
+macls/predict.py,sha256=sMjXJAGuQsi1I90AzGVqatAh-qaZ5mT_RewfvoDOu0E,9374
+macls/trainer.py,sha256=Dz2NB4ynpVA8SJz_QXFHVLxehRddfBGjSOnHB_eC50g,30195
 macls/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/data_utils/audio.py,sha256=iyLME4AECoYfQYAXsSWHqrzlJjuAZV-HiwxzEbDDrxg,22218
-macls/data_utils/collate_fn.py,sha256=GxqMcZ5Q2ScQxj6crIUWeVygzWxIk9KAbC6GWW6awrQ,965
-macls/data_utils/featurizer.py,sha256=-MAiHLattWuHK7fa4Wr1-7Jqw_AqKuY7hr-73nEqPBk,3587
-macls/data_utils/reader.py,sha256=CbCmKAdXIkumhTwrM46fWm7FyHmYTGFXlD6ZPVcx1mo,5630
+macls/data_utils/collate_fn.py,sha256=khDNN1B0xZpoXz1dLHJE4JBbXnOLPqcQvVlruVnRBv0,930
+macls/data_utils/featurizer.py,sha256=dgiKRMuvAApNZBt36mdd3RDZJp3z0JTMwYBiHd6nRUg,3696
+macls/data_utils/reader.py,sha256=EbJZksJQJtlPHWUOmRTnKpUhgDcEe3T--rhFTgbb9bg,7214
 macls/data_utils/spec_aug.py,sha256=Sr9-Ss6I7LA3TGMqzML_X-nb_nnMfUxt5KbsA6HJpzM,1572
 macls/data_utils/utils.py,sha256=do9N5iTXwBwBFk2EMVTPuaIWsI1DazxKH-1Nmrlsdmc,5712
 macls/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/metric/metrics.py,sha256=Asj6VaDQXxmcB_HRIJt7v4zdRBgLTyn3Q2mDTFPcgEM,329
 macls/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/models/campplus.py,sha256=vAuddLBPDgUEB85BjVITaqNYO09igDbh_Bxok4N0lH4,13608
 macls/models/ecapa_tdnn.py,sha256=bjiVtLSA01I97CYcAkrvCdx0ekGZMNexOXe5VeXqHb0,5375
@@ -20,12 +20,12 @@
 macls/models/resnet_se.py,sha256=U64nKP2WCuzW6YVROhgrMsy4G7R961Uv-zWT5e-A-9s,5609
 macls/models/tdnn.py,sha256=foMiLbpQ4loRuo-2G31J6yCRq8ZSu48BSpx7OUy4M50,3050
 macls/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/utils/logger.py,sha256=-ssorx8FlHA_wrd2Eq6f4HkOqaOG2YseBGvYAo8NXN8,2839
 macls/utils/record.py,sha256=2i4kz5kPDa9KkbAK_Q34sVIXOkD9TroPROIe5QdzqWg,1067
 macls/utils/scheduler.py,sha256=RXgRf_sntkOjruj30pp6uPE7dqFR06jDa4tRogpl5Us,1496
 macls/utils/utils.py,sha256=pIlc9i-Bk4dLkaYkJdyivfu9UOouec0iQOiZb0xhfLA,4300
-macls-0.4.3.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-macls-0.4.3.dist-info/METADATA,sha256=CWd20a9i0NNvLhyT5YskRgj__6P9DYs95jM6LSzzlqw,21616
-macls-0.4.3.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-macls-0.4.3.dist-info/top_level.txt,sha256=fmi1Rmptc1CNX_eoWedGPlbhlTSDi3liMSfVFvHYE1Q,6
-macls-0.4.3.dist-info/RECORD,,
+macls-0.4.4.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+macls-0.4.4.dist-info/METADATA,sha256=kX1dVBT7lPt2p9KhCW78gO5x0SC3zVD9usfx7I4k3P0,22574
+macls-0.4.4.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+macls-0.4.4.dist-info/top_level.txt,sha256=fmi1Rmptc1CNX_eoWedGPlbhlTSDi3liMSfVFvHYE1Q,6
+macls-0.4.4.dist-info/RECORD,,
```

